[introduction] statistical learning plays key role many areas science finance industry examples learning problems predict whether patient hospitalized due heart attack second heart attack prediction based demo graphic diet clinical measurements patient predict price stock months basis company performance measures economic data identify numbers handwritten zip code digitized image estimate amount glucose blood diabetic person infrared absorption spectrum person blood identify risk factors prostate cancer based clinical demographic variables science learning plays key role fields statistics data mining artificial intelligence intersecting areas engineering disciplines book learning data typical scenario outcome measurement usually quantitative stock price categorical heart attack heart attack wish predict based set features diet clinical measurements training set data observe outcome feature introduction table average percentage words characters email message equal indicated word character chosen words characters showing largest difference spam email george free hpl edu remove spam email measurements set objects people using data build prediction model learner enable predict outcome new unseen objects good learner one accurately predicts outcome examples describe called supervised learning prob lem called supervised presence outcome vari able guide learning process unsupervised learning problem observe features measurements outcome task rather describe data organized clustered devote book supervised learning unsupervised problem less developed literature focus chapter examples real learning problems discussed book 
[introduction, example, email, spam] data example consists information email mes sages study try predict whether email junk email spam objective design automatic spam detector could filter spam clogging users mailboxes email messages true outcome email type email spam available along relative frequencies commonly occurring words punctuation marks email message supervised learning problem outcome class variable email spam also called classification problem table lists words characters showing largest average difference spam email learning method decide features use example might use rule george amp spam else email another form rule might george spam else email introduction lpsa ooo ooo ooo ooo lcavol ooo lweight age ooo lbph ooo oooooo ooo ooo svi ooo lcp ooo oooooo ooo ooo gleason ooo pgg figure scatterplot matrix prostate cancer data first row shows response predictors turn two predictors svi gleason categorical problem errors equal want avoid filtering good email letting spam get desirable less serious consequences discuss number different methods tackling learning problem book 
[introduction, example, prostate, cancer] data example displayed figure come study stamey examined correlation level error data first edition book subject value lweight translates prostate correct value grateful prof stephen link alerting error introduction figure examples handwritten digits postal envelopes prostate specific antigen psa number clinical measures men receive radical prostatectomy goal predict log psa lpsa number measure ments including log cancer volume lcavol log prostate weight lweight age log benign prostatic hyperplasia amount lbph seminal vesicle vasion svi log capsular penetration lcp gleason score gleason percent gleason scores pgg figure scatterplot matrix variables correlations lpsa evident good pre dictive model difficult construct eye supervised learning problem known regression problem outcome measurement quantitative 
[introduction, example, handwritten, digit, recognition] data example come handwritten zip codes envelopes postal mail image segment five digit zip code isolating single digit images eight bit grayscale maps pixel ranging intensity sample images shown figure images normalized approximately size orientation task predict matrix pixel intensities identity image quickly accurately accurate enough resulting algorithm would used part automatic sorting procedure envelopes classification problem error rate needs kept low avoid misdirection introduction mail order achieve low error rate objects assigned know category sorted instead hand 
[introduction, example, dna, expression, microarrays] dna stands deoxyribonucleic acid basic material makes human chromosomes dna microarrays measure expression gene cell measuring amount mrna messenger ribonucleic acid present gene microarrays considered breakthrough technology biology facilitating quantitative study thousands genes simultaneously single sample cells dna microarray works nucleotide sequences thousand genes printed glass slide target sample reference sample labeled red green dyes hybridized dna slide fluoroscopy log red green intensities rna hybridizing site measured result thousand numbers typically ranging say measuring expression level gene target relative reference sample positive values indicate higher expression target versus reference vice versa negative values gene expression dataset collects together expression values series dna microarray experiments column representing experiment therefore several thousand rows representing individ ual genes tens columns representing samples particular ample figure genes rows samples columns although clarity random sample rows shown fig ure displays data set heat map ranging green negative red positive samples cancer tumors different patients challenge understand genes samples ganized typical questions include following samples similar terms expres sion profiles across genes genes similar terms expression profiles across samples certain genes show high low expression certain cancer samples could view task regression problem two categorical predictor variables genes samples response variable level expression however probably useful view unsupervised learning problem example question think samples points dimensional space want cluster together way dna microarray data expression matrix genes rows samples columns human tumor data random sample rows shown display heat map ranging bright green negative expressed bright red positive expressed missing values gray rows columns displayed randomly chosen order introduction 
[introduction, read, book] book designed researchers students broad variety fields statistics artificial intelligence engineering finance others expect reader least one elementary course statistics covering basic topics including linear regression attempted write comprehensive catalog learning methods rather describe important techniques equally notable describe underlying concepts considerations researcher judge learning method tried write book intuitive fashion emphasizing concepts rather math ematical details statisticians exposition naturally reflect backgrounds areas expertise however past eight years attending conferences neural networks data mining machine learning thinking heavily influenced exciting fields influence evident current research book 
[introduction, book, organized] view one must understand simple methods trying grasp complex ones hence giving overview supervis ing learning problem chapter discuss linear methods regression classification chapters chapter describe splines wavelets regularization penalization methods single predictor chapter covers kernel methods local regression sets methods important building blocks high dimensional learn ing techniques model assessment selection topic chapter covering concepts bias variance overfitting methods cross validation choosing models chapter discusses model inference averaging including overview maximum likelihood bayesian ference bootstrap algorithm gibbs sampling bagging related procedure called boosting focus chapter chapters describe series structured methods pervised learning chapters covering regression chap ters focusing classification chapter describes methods unsupervised learning two recently proposed techniques random forests ensemble learning discussed chapters describe undirected graphical models chapter finally study high dimensional problems chapter end chapter discuss computational considerations portant data mining applications including computations scale number observations predictors chapter ends bibliographic notes giving background references material introduction recommend chapters first read sequence chapter also considered mandatory covers central concepts pertain learning methods mind rest book read sequentially sampled depending reader interest symbol indicates technically difficult section one skipped without interrupting flow discussion 
[introduction, book, website] website book located http www stat stanford edu elemstatlearn contains number resources including many datasets used book 
[introduction, note, instructors] successively used first edition book basis two quarter course additional materials second edition could even used three quarter sequence exercises provided end chapter important students access good software tools topics used plus programming languages courses 
[introduction] statistical learning plays key role many areas science finance industry examples learning problems predict whether patient hospitalized due heart attack second heart attack prediction based demo graphic diet clinical measurements patient predict price stock months basis company performance measures economic data identify numbers handwritten zip code digitized image estimate amount glucose blood diabetic person infrared absorption spectrum person blood identify risk factors prostate cancer based clinical demographic variables science learning plays key role fields statistics data mining artificial intelligence intersecting areas engineering disciplines book learning data typical scenario outcome measurement usually quantitative stock price categorical heart attack heart attack wish predict based set features diet clinical measurements training set data observe outcome feature introduction table average percentage words characters email message equal indicated word character chosen words characters showing largest difference spam email george free hpl edu remove spam email measurements set objects people using data build prediction model learner enable predict outcome new unseen objects good learner one accurately predicts outcome examples describe called supervised learning prob lem called supervised presence outcome vari able guide learning process unsupervised learning problem observe features measurements outcome task rather describe data organized clustered devote book supervised learning unsupervised problem less developed literature focus chapter examples real learning problems discussed book 
[introduction, example, email, spam] data example consists information email mes sages study try predict whether email junk email spam objective design automatic spam detector could filter spam clogging users mailboxes email messages true outcome email type email spam available along relative frequencies commonly occurring words punctuation marks email message supervised learning problem outcome class variable email spam also called classification problem table lists words characters showing largest average difference spam email learning method decide features use example might use rule george amp spam else email another form rule might george spam else email introduction lpsa ooo ooo ooo ooo lcavol ooo lweight age ooo lbph ooo oooooo ooo ooo svi ooo lcp ooo oooooo ooo ooo gleason ooo pgg figure scatterplot matrix prostate cancer data first row shows response predictors turn two predictors svi gleason categorical problem errors equal want avoid filtering good email letting spam get desirable less serious consequences discuss number different methods tackling learning problem book 
[introduction, example, prostate, cancer] data example displayed figure come study stamey examined correlation level error data first edition book subject value lweight translates prostate correct value grateful prof stephen link alerting error introduction figure examples handwritten digits postal envelopes prostate specific antigen psa number clinical measures men receive radical prostatectomy goal predict log psa lpsa number measure ments including log cancer volume lcavol log prostate weight lweight age log benign prostatic hyperplasia amount lbph seminal vesicle vasion svi log capsular penetration lcp gleason score gleason percent gleason scores pgg figure scatterplot matrix variables correlations lpsa evident good pre dictive model difficult construct eye supervised learning problem known regression problem outcome measurement quantitative 
[introduction, example, handwritten, digit, recognition] data example come handwritten zip codes envelopes postal mail image segment five digit zip code isolating single digit images eight bit grayscale maps pixel ranging intensity sample images shown figure images normalized approximately size orientation task predict matrix pixel intensities identity image quickly accurately accurate enough resulting algorithm would used part automatic sorting procedure envelopes classification problem error rate needs kept low avoid misdirection introduction mail order achieve low error rate objects assigned know category sorted instead hand 
[introduction, example, dna, expression, microarrays] dna stands deoxyribonucleic acid basic material makes human chromosomes dna microarrays measure expression gene cell measuring amount mrna messenger ribonucleic acid present gene microarrays considered breakthrough technology biology facilitating quantitative study thousands genes simultaneously single sample cells dna microarray works nucleotide sequences thousand genes printed glass slide target sample reference sample labeled red green dyes hybridized dna slide fluoroscopy log red green intensities rna hybridizing site measured result thousand numbers typically ranging say measuring expression level gene target relative reference sample positive values indicate higher expression target versus reference vice versa negative values gene expression dataset collects together expression values series dna microarray experiments column representing experiment therefore several thousand rows representing individ ual genes tens columns representing samples particular ample figure genes rows samples columns although clarity random sample rows shown fig ure displays data set heat map ranging green negative red positive samples cancer tumors different patients challenge understand genes samples ganized typical questions include following samples similar terms expres sion profiles across genes genes similar terms expression profiles across samples certain genes show high low expression certain cancer samples could view task regression problem two categorical predictor variables genes samples response variable level expression however probably useful view unsupervised learning problem example question think samples points dimensional space want cluster together way dna microarray data expression matrix genes rows samples columns human tumor data random sample rows shown display heat map ranging bright green negative expressed bright red positive expressed missing values gray rows columns displayed randomly chosen order introduction 
[introduction, read, book] book designed researchers students broad variety fields statistics artificial intelligence engineering finance others expect reader least one elementary course statistics covering basic topics including linear regression attempted write comprehensive catalog learning methods rather describe important techniques equally notable describe underlying concepts considerations researcher judge learning method tried write book intuitive fashion emphasizing concepts rather math ematical details statisticians exposition naturally reflect backgrounds areas expertise however past eight years attending conferences neural networks data mining machine learning thinking heavily influenced exciting fields influence evident current research book 
[introduction, book, organized] view one must understand simple methods trying grasp complex ones hence giving overview supervis ing learning problem chapter discuss linear methods regression classification chapters chapter describe splines wavelets regularization penalization methods single predictor chapter covers kernel methods local regression sets methods important building blocks high dimensional learn ing techniques model assessment selection topic chapter covering concepts bias variance overfitting methods cross validation choosing models chapter discusses model inference averaging including overview maximum likelihood bayesian ference bootstrap algorithm gibbs sampling bagging related procedure called boosting focus chapter chapters describe series structured methods pervised learning chapters covering regression chap ters focusing classification chapter describes methods unsupervised learning two recently proposed techniques random forests ensemble learning discussed chapters describe undirected graphical models chapter finally study high dimensional problems chapter end chapter discuss computational considerations portant data mining applications including computations scale number observations predictors chapter ends bibliographic notes giving background references material introduction recommend chapters first read sequence chapter also considered mandatory covers central concepts pertain learning methods mind rest book read sequentially sampled depending reader interest symbol indicates technically difficult section one skipped without interrupting flow discussion 
[introduction, book, website] website book located http www stat stanford edu elemstatlearn contains number resources including many datasets used book 
[introduction, note, instructors] successively used first edition book basis two quarter course additional materials second edition could even used three quarter sequence exercises provided end chapter important students access good software tools topics used plus programming languages courses 
[overview, supervised, learning, introduction] first three examples described chapter several components common set variables might denoted inputs measured preset influence one outputs example goal use inputs predict values outputs exercise called supervised learning used modern language machine learning statistical literature inputs often called predictors term use interchangeably inputs classically independent variables pattern recognition literature term features preferred use well outputs called responses classically dependent variables 
[overview, supervised, learning, variable, types, terminology] outputs vary nature among examples glucose prediction example output quantitative measurement measure ments bigger others measurements close value close nature famous iris discrimination example due fisher output qualitative species iris assumes values finite set virginica setosa versicolor handwritten digit example output one different digit classes overview supervised learning explicit ordering classes fact often descrip tive labels rather numbers used denote classes qualitative variables also referred categorical discrete variables well factors types outputs makes sense think using inputs predict output given specific atmospheric measurements today yesterday want predict ozone level tomorrow given grayscale values pixels digitized image handwritten digit want predict class label distinction output type led naming convention prediction tasks regression predict quantitative outputs clas sification predict qualitative outputs see two tasks lot common particular viewed task function approximation inputs also vary measurement type qual itative quantitative input variables also led distinctions types methods used prediction methods defined naturally quantitative inputs naturally qualitative third variable type ordered categorical small medium large ordering values metric notion appropriate difference medium small need large medium discussed chapter qualitative variables typically represented numerically codes easiest case two classes categories suc cess failure survived died often represented single binary digit bit else reasons become apparent numeric codes sometimes referred targets two categories several alternatives available useful commonly used coding via dummy variables level qualitative variable represented vector binary variables bits one time although compact coding schemes possible dummy variables symmetric levels factor typically denote input variable symbol vector components accessed subscripts quantitative outputs denoted qualitative outputs group use uppercase letters referring generic aspects variable observed values written lowercase hence ith observed value written scalar vector matrices represented bold uppercase letters example set input vectors would represented matrix general vectors bold except components convention distinguishes vector inputs least squares nearest neighbors ith observation vector consisting observations variable since vectors assumed column vectors ith row vector transpose moment loosely state learning task follows given value input vector make good prediction output denoted pronounced hat takes values likewise categorical outputs take values set associated two class one approach denote binary coded target treat quantitative output predictions typically lie assign class label according whether approach generalizes level qualitative outputs well need data construct prediction rules often lot thus suppose available set measurements known training data construct prediction rule 
[overview, supervised, learning, squares, nearest, neighbors] section develop two simple powerful prediction methods linear model fit least squares nearest neighbor prediction rule linear model makes huge assumptions structure yields stable possibly inaccurate predictions method nearest neighbors makes mild structural assumptions predictions often accurate unstable 
[overview, supervised, learning, squares, nearest, neighbors, linear, models, least, squares] linear model mainstay statistics past years remains one important tools given vector inputs predict output via model term intercept also known bias machine learning often convenient include constant variable include vector coefficients write linear model vector form inner product overview supervised learning denotes vector matrix transpose column vector modeling single output scalar general vector case would matrix coefficients dimensional input output space represents hyperplane constant included hyperplane includes origin subspace affine set cutting axis point assume intercept included viewed function dimensional input space linear gradient vector input space points steepest uphill direction fit linear model set training data many different methods far popular method least squares approach pick coefficients minimize residual sum squares rss rss quadratic function parameters hence minimum always exists may unique solution easiest characterize matrix notation write rss matrix row input vector vector outputs training set differentiating get normal equations nonsingular unique solution given fitted value ith input arbi trary input prediction entire fitted surface characterized parameters intuitively seems need large data set fit model let look example linear model classification context figure shows scatterplot training data pair inputs data simulated present simulation model important output class variable values blue orange represented scatterplot points two classes linear regression model fit data response coded blue orange fitted values converted fitted class variable according rule orange blue least squares nearest neighbors linear regression response figure classification example two dimensions classes coded binary variable blue orange fit linear regression line decision boundary defined orange shaded region denotes part input space classified orange blue region classified blue set points classified orange corresponds indicated figure two predicted classes separated decision boundary linear case see data several misclassifications sides decision boundary perhaps linear model rigid errors unavoidable remember errors training data said constructed data came consider two possible scenarios scenario training data class generated bivariate gaussian distributions uncorrelated components different means scenario training data class came mixture low variance gaussian distributions individual means distributed gaussian mixture gaussians best described terms generative model one first generates discrete variable determines overview supervised learning component gaussians use generates observation chosen density case one gaussian per class see chapter linear decision boundary best one estimate almost optimal region overlap inevitable future data predicted plagued overlap well case mixtures tightly clustered gaussians story dif ferent linear decision boundary unlikely optimal fact optimal decision boundary nonlinear disjoint much difficult obtain look another classification regression procedure sense opposite end spectrum linear model far better suited second scenario 
[overview, supervised, learning, squares, nearest, neighbors, nearest-neighbor, methods] nearest neighbor methods use observations training set clos est input space form specifically nearest neighbor fit defined follows neighborhood defined closest points training sample closeness implies metric moment assume euclidean distance words find observations closest input space average responses figure use training data figure use nearest neighbor averaging binary coded response method fitting thus proportion orange neighborhood assigning class orange amounts majority vote neighborhood colored regions indicate points input space classified blue orange rule case found evaluating procedure fine grid input space see decision boundaries separate blue orange regions far irregular respond local clusters one class dominates figure shows results nearest neighbor classification assigned value closest point training data case regions classification computed relatively easily correspond voronoi tessellation training data point associated tile bounding region closest input point points tile decision boundary even irregular method nearest neighbor averaging defined exactly way regression quantitative output although would unlikely choice least squares nearest neighbors nearest neighbor classifier figure classification example two dimensions fig ure classes coded binary variable blue orange fit nearest neighbor averaging predicted class hence chosen majority vote amongst nearest neighbors figure see far fewer training observations misclassified figure give much comfort though since figure none training data misclassified little thought suggests nearest neighbor fits error training data approximately increasing function always independent test set would give satisfactory means comparing different methods appears nearest neighbor fits single parameter num ber neighbors compared parameters least squares fits though case see effective number parameters nearest neighbors generally bigger decreases increasing get idea note neighborhoods nonoverlapping would neighborhoods would fit one parameter mean neighborhood also clear cannot use sum squared errors training set criterion picking since would always pick would seem nearest neighbor methods would appropriate mixture scenario described gaussian data decision boundaries nearest neighbors would unnecessarily noisy overview supervised learning nearest neighbor classifier figure classification example two dimensions fig ure classes coded binary variable blue orange predicted nearest neighbor classification 
[overview, supervised, learning, squares, nearest, neighbors, least, squares, nearest, neighbors] linear decision boundary least squares smooth parently stable fit appear rely heavily assumption linear decision boundary appropriate language develop shortly low variance potentially high bias hand nearest neighbor procedures appear rely stringent assumptions underlying data adapt situation however particular subregion decision bound ary depends handful input points particular positions thus wiggly unstable high variance low bias method situations works best particular linear regression appropriate scenario nearest neighbors suitable scenario time come expose oracle data fact simulated model somewhere tween two closer scenario first generated means bivariate gaussian distribution labeled class blue similarly drawn labeled class orange class generated observations follows observation picked random probability least squares nearest neighbors degrees freedom test error train test bayes number nearest neighbors linear figure misclassification curves simulation example used fig ures single training sample size used test sample size orange curves test blue training ror nearest neighbor classification results linear regression bigger orange blue squares three degrees freedom purple line optimal bayes error rate generated thus leading mixture gaussian clus ters class figure shows results classifying new observations generated model compare results least squares nearest neighbors range values large subset popular techniques use today variants two simple procedures fact nearest neighbor simplest captures large percentage market low dimensional problems following list describes ways simple procedures enhanced kernel methods use weights decrease smoothly zero dis tance target point rather effective weights used nearest neighbors high dimensional spaces distance kernels modified phasize variable others overview supervised learning local regression fits linear models locally weighted least squares rather fitting constants locally linear models fit basis expansion original inputs allow arbitrarily complex models projection pursuit neural network models consist sums non linearly transformed linear models 
[overview, supervised, learning, statistical, decision, theory] section develop small amount theory provides frame work developing models discussed informally far first consider case quantitative output place world random variables probability spaces let denote real valued random input vector real valued random put variable joint distribution seek function predicting given values input theory requires loss function penalizing errors prediction far common convenient squared error loss leads criterion choosing epe expected squared prediction error conditioning write epe epe see suffices minimize epe pointwise argmin solution conditional expectation also known regression function thus best prediction point conditional mean best measured average squared error nearest neighbor methods attempt directly implement recipe using training data point might ask average conditioning amounts factoring joint density splitting bivariate integral accordingly statistical decision theory input since typically one observation point settle ave ave denotes average neighborhood containing points closest two approximations happening expectation approximated averaging sample data conditioning point relaxed conditioning region close target point large training sample size points neighborhood likely close gets large average get stable fact mild regularity conditions joint probability distri bution one show light look since seems universal approximator often large sam ples linear structured model appropriate usually get stable estimate nearest neighbors although knowledge learned data well problems though sometimes disastrous section see dimension gets large metric size nearest neighbor hood settling nearest neighborhood surrogate conditioning fail miserably convergence still holds rate convergence decreases dimension increases linear regression fit framework simplest explana tion one assumes regression function approximately linear arguments model based approach specify model regression func tion plugging linear model epe differentiating solve theoretically note conditioned rather used knowledge functional relationship pool values least squares solution amounts replacing expectation averages training data nearest neighbors least squares end approximating conditional expectations averages differ dramatically terms model assumptions least squares assumes well approximated globally linear function overview supervised learning nearest neighbors assumes well approximated locally constant function although latter seems palatable already seen may pay price flexibility many modern techniques described book model based although far flexible rigid linear model example additive models assume retains additivity linear model coordinate function arbitrary turns optimal estimate additive model uses techniques nearest neighbors approximate univariate con ditional expectations simultaneously coordinate functions thus problems estimating conditional expectation high dimen sions swept away case imposing often unrealistic model assumptions case additivity happy criterion happens replace loss function solution case conditional median median different measure location estimates robust conditional mean criteria discontinuities derivatives hindered widespread use resistant loss functions mentioned later chapters squared error analytically convenient popular output categorical variable paradigm works except need different loss function penalizing prediction errors estimate assume values set possible classes loss function represented matrix card zero diagonal nonnegative elsewhere price paid classifying observation belonging class often use zero one loss function misclassifications charged single unit expected prediction error epe expectation taken respect joint distribution condition write epe epe statistical decision theory bayes optimal classifier figure optimal bayes decision boundary simulation example figures since generating density known class boundary calculated exactly exercise suffices minimize epe pointwise argmin loss function simplifies argmin simply max reasonable solution known bayes classifier says classify probable class using conditional discrete dis tribution figure shows bayes optimal decision boundary simulation example error rate bayes classifier called bayes rate overview supervised learning see nearest neighbor classifier directly approximates solution majority vote nearest neighborhood amounts actly except conditional probability point relaxed con ditional probability within neighborhood point probabilities estimated training sample proportions suppose two class problem taken dummy variable proach coded via binary followed squared error loss estima tion corresponded likewise class problem shows dummy variable regression procedure followed classification largest fitted value another way representing bayes classifier although theory exact practice problems occur depending regression model used example linear regression used need positive might suspicious using estimate probability discuss variety approaches modeling chapter 
[overview, supervised, learning, local, methods, high, dimensions] examined two learning techniques prediction far stable biased linear model less stable apparently less biased class nearest neighbor estimates would seem reasonably large set training data could always approximate theoretically optimal conditional expectation nearest neighbor averaging since able find fairly large neighborhood observations close average approach intuition breaks high dimensions phenomenon commonly referred curse dimensionality bellman many manifestations problem examine consider nearest neighbor procedure inputs uniformly distributed dimensional unit hypercube figure suppose send hypercubical neighborhood target point capture fraction observations since corresponds fraction unit volume expected edge length ten dimensions entire range input capture data form local average must cover range input variable neighborhoods longer local reducing dramatically help much either since fewer observations average higher variance fit another consequence sparse sampling high dimensions sample points close edge sample consider data points uniformly distributed dimensional unit ball centered origin suppose consider nearest neighbor estimate origin median local methods high dimensions unit cube fraction volume distance neighborhood figure curse dimensionality well illustrated subcubical neighborhood uniform data unit cube figure right shows side length subcube needed capture fraction volume data different dimensions ten dimensions need cover range coordinate capture data distance origin closest data point given expression exercise complicated expression exists mean distance closest point halfway boundary hence data points closer boundary sample space data point reason presents problem prediction much difficult near edges training sample one must extrapolate neighboring sample points rather interpolate another manifestation curse sampling density pro portional dimension input space sample size thus represents dense sample single input problem sample size required sam pling density inputs thus high dimensions feasible training samples sparsely populate input space let construct another uniform example suppose train ing examples generated uniformly assume true relationship without measurement error use nearest neighbor rule predict test point denote training set overview supervised learning compute expected prediction error procedure averaging samples size since problem deterministic mean squared error mse estimating mse var bias figure illustrates setup broken mse two components become familiar proceed variance squared bias decomposition always possible often useful known bias variance decomposition unless nearest neighbor smaller example average estimate biased downward variance due sampling variance nearest neighbor low dimensions nearest neighbor close bias variance small dimension increases nearest neighbor tends stray target point bias variance incurred samples nearest neighbor distance greater origin thus increases estimate tends often hence mse levels bias variance starts dropping artifact example although highly contrived example similar phenomena occur generally complexity functions many variables grow exponentially dimension wish able estimate functions accuracy function low dimensions need size training set grow exponentially well example function complex interaction variables involved dependence bias term distance depends truth need always dominate nearest neighbor example function always involves dimensions figure variance dominate instead suppose hand know relationship linear fit model least squares train ing data arbitrary test point written ith element since model least squares estimates local methods high dimensions one dimension one two dimensions dimension average distance nearest neighbor distance dimension dimension mse mse dimension mse variance bias figure simulation example demonstrating curse dimensional ity effect mse bias variance input features uniformly distributed top left panel shows target func tion noise demonstrates error nearest neighbor makes estimating training point indicated blue tick mark top right panel illustrates radius nearest neighborhood increases dimension lower left panel shows average radius nearest neighborhoods lower right panel shows mse squared bias variance curves function dimension overview supervised learning one dimension dimension mse mse dimension mse variance bias figure simulation example setup figure function constant one dimension variance dominates unbiased find epe var var var bias incurred additional variance prediction error since target deterministic bias variance depends large selected random assuming ncov epe cov trace cov cov see expected epe increases linearly function slope large small growth vari ance negligible deterministic case imposing heavy restrictions class models fitted avoided curse dimensionality technical details derived exercise figure compares nearest neighbor least squares two situa tions form uniform sample size orange curve local methods high dimensions dimension epe ratio expected prediction error ols linear cubic figure curves show expected prediction error nearest neighbor relative least squares model orange curve blue curve linear first coordinate blue curve cubic figure shown relative epe nearest neighbor least squares appears start around linear case least squares unbiased case discussed epe slightly epe nearest neighbor always since variance case least ratio increases dimension nearest neighbor strays target point cubic case least squares biased moderates ratio clearly could manufacture examples bias least squares would dominate variance nearest neighbor would come winner relying rigid assumptions linear model bias negligible variance error nearest neighbor substantially larger however assumptions wrong bets nearest neighbor may dominate see whole spec trum models rigid linear models extremely flexible nearest neighbor models assumptions biases proposed specifically avoid exponential growth complexity functions high dimensions drawing heavily assumptions delve deeply let elaborate bit concept statistical models see fit prediction framework overview supervised learning 
[overview, supervised, learning, statistical, models, supervised, learning, function, approximation] goal find useful approximation function underlies predictive relationship inputs outputs theoretical setting section saw squared error loss lead regression function quantitative response class nearest neighbor methods viewed direct estimates conditional expectation seen fail least two ways dimension input space high nearest neighbors need close target point result large errors special structure known exist used reduce bias variance estimates anticipate using classes models many cases specif ically designed overcome dimensionality problems dis cuss framework incorporating prediction problem 
[overview, supervised, learning, statistical, models, supervised, learning, function, approximation, statistical, model, joint, distribution, prx] suppose fact data arose statistical model random error independent note model fact conditional distribution depends conditional mean additive error model useful approximation truth systems input output pairs deterministic relationship generally unmeasured variables also contribute including measurement error additive model assumes capture departures deterministic lationship via error problems deterministic relationship hold many classification problems studied machine learning form response surface thought colored map defined training data consist colored examples map goal able color point function deterministic randomness enters location training points moment pursue problems see handled techniques appropriate error based models assumption errors independent identically distributed strictly necessary seems back mind statistical models supervised learning function approximation average squared errors uniformly epe criterion model becomes natural use least squares data criterion model estimation simple modifications made avoid independence assumption example var mean variance depend general conditional distribution depend complicated ways additive error model precludes far concentrated quantitative response additive error models typically used qualitative outputs case tar get function conditional density modeled directly example two class data often reasonable assume data arise independent binary trials probability one particular outcome thus coded version variance depends well var 
[overview, supervised, learning, statistical, models, supervised, learning, function, approximation, supervised, learning] launch statistically oriented jargon present function fitting paradigm machine learning point view suppose simplicity errors additive model reasonable assumption supervised learning attempts learn example teacher one observes system study inputs outputs assembles training set observations observed input values system also fed artificial system known learning algorithm usually com puter program also produces outputs response puts learning algorithm property modify put output relationship response differences original generated outputs process known learning exam ple upon completion learning process hope artificial real outputs close enough useful sets inputs likely encountered practice 
[overview, supervised, learning, statistical, models, supervised, learning, function, approximation, function, approximation] learning paradigm previous section motivation research supervised learning problem fields machine learning analogies human reasoning neural networks biological analogies brain approach taken applied mathe matics statistics perspective function approxima tion estimation data pairs viewed points dimensional euclidean space function domain equal dimensional input subspace related data via model overview supervised learning convenience chapter assume domain dimensional euclidean space although general inputs mixed type goal obtain useful approximation region given representations although somewhat less glamorous learning paradigm treating supervised learning problem function approximation encourages geometrical concepts euclidean spaces mathematical concepts probabilistic inference applied problem approach taken book many approximations encounter associated set parameters modified suit data hand example linear model another class useful approxi mators expressed linear basis expansions suitable set functions transformations input vector traditional examples polynomial trigonometric expan sions example might cos also encounter nonlinear expansions sigmoid transformation common neural network models exp use least squares estimate parameters linear model minimizing residual sum squares rss function seems reasonable criterion additive error model terms function approximation imagine parameterized function surface space observe noisy alizations easy visualize vertical coordinate output figure noise output coordinate find set parameters fitted surface gets close observed points possible close measured sum squared vertical errors rss linear model get simple closed form solution mini mization problem also true basis function methods basis functions hidden parameters otherwise solution requires either iterative methods numerical optimization least squares generally convenient crite rion used cases would make much sense general statistical models supervised learning function approximation figure least squares fitting function two inputs parameters chosen minimize sum squared vertical errors principle estimation maximum likelihood estimation suppose random sample density indexed parameters log probability observed sample log principle maximum likelihood assumes reasonable values probability observed sample largest least squares additive error model equivalent maximum likelihood using conditional likelihood although additional assumption normality seems restrictive results log likelihood data log log term involving last rss scalar negative multiplier interesting example multinomial likelihood regres sion function qualitative output suppose model conditional probabil ity class given indexed parameter vector overview supervised learning log likelihood also referred cross entropy log maximized delivers values best conform data likelihood sense 
[overview, supervised, learning, structured, regression, models] seen although nearest neighbor local methods focus directly estimating function point face problems high dimensions may also inappropriate even low dimensions cases structured approaches make efficient use data section introduces classes structured approaches proceed though discuss need classes 
[overview, supervised, learning, structured, regression, models, difficulty, problem] consider rss criterion arbitrary function rss minimizing leads infinitely many solutions function passing training points solution particular solution chosen might poor predictor test points different training points multiple observation pairs value risk limited case solutions pass average values see exercise situation similar one already visited section indeed finite sample version page sample size sufficiently large repeats guaranteed densely arranged would seem solutions might tend limiting conditional expectation order obtain useful results finite must restrict eligible solutions smaller set functions decide nature restrictions based considerations outside data restrictions sometimes encoded via parametric representation may built learning method either implicitly explicitly restricted classes solutions major topic book one thing clear though restrictions imposed lead unique solution really remove ambiguity classes restricted estimators caused multiplicity solutions infinitely many possible restrictions leading unique solution ambiguity simply transferred choice constraint general constraints imposed learning methods described complexity restrictions one kind another usually means kind regular behavior small neighborhoods input space input points sufficiently close metric exhibits special structure nearly constant linear low order polynomial behavior estimator obtained averaging polynomial fitting neighborhood strength constraint dictated neighborhood size larger size neighborhood stronger constraint sensitive solution particular choice constraint example local constant fits infinitesimally small neighborhoods constraint local linear fits large neighborhoods almost globally linear model restrictive nature constraint depends metric used methods kernel local regression tree based methods directly specify metric size neighborhood nearest neighbor methods discussed far based assumption locally function constant close target input function change much close outputs averaged produce methods splines neural networks basis function methods implicitly define neighborhoods local behavior section discuss concept equivalent kernel see figure page describes local dependence method linear outputs equivalent kernels many cases look like explicitly defined weighting kernels discussed peaked target point falling away smoothly away one fact clear method attempts pro duce locally varying functions small isotropic neighborhoods run problems high dimensions curse dimensionality conversely methods overcome dimensionality problems associated often implicit adaptive metric measuring neighbor hoods basically allow neighborhood simultane ously small directions 
[overview, supervised, learning, classes, restricted, estimators] variety nonparametric regression techniques learning methods fall number different classes depending nature restrictions imposed classes distinct indeed methods fall several classes give brief summary since detailed descriptions overview supervised learning given later chapters classes associated one parameters sometimes appropriately called smoothing parameters control effective size local neighborhood describe three broad classes 
[overview, supervised, learning, classes, restricted, estimators, roughness, penalty, bayesian, methods] class functions controlled explicitly penalizing rss roughness penalty prss rss user selected functional large functions vary rapidly small regions input space example popular cubic smoothing spline one dimensional inputs solution penalized least squares criterion prss roughness penalty controls large values second derivative amount penalty dictated penalty imposed interpolating function functions linear permitted penalty functionals constructed functions dimension special versions created impose special structure ample additive penalties used conjunction additive functions create additive models smooth coordinate functions similarly projection pursuit regression mod els adaptively chosen directions functions associated roughness penalty penalty function regularization methods express prior belief type functions seek exhibit certain type smooth behavior indeed usually cast bayesian framework penalty corre sponds log prior prss log posterior distribution minimizing prss amounts finding posterior mode discuss roughness penalty approaches chapter bayesian paradigm chapter 
[overview, supervised, learning, classes, restricted, estimators, kernel, methods, local, regression] methods thought explicitly providing estimates gression function conditional expectation specifying nature local neighborhood class regular functions fitted locally local neighborhood specified kernel function assigns classes restricted estimators weights points region around see figure page example gaussian kernel weight function based gaussian density function exp assigns weights points die exponentially squared euclidean distance parameter corresponds variance gaussian density controls width neighborhood simplest form kernel estimate nadaraya watson weighted average general define local regression estimate minimizes rss parameterized function low order polynomial examples constant function results nadaraya watson estimate gives popular local linear regression model nearest neighbor methods thought kernel methods data dependent metric indeed metric nearest neighbors training observation ranked kth distance indicator set methods course need modified high dimensions avoid curse dimensionality various adaptations discussed chapter 
[overview, supervised, learning, classes, restricted, estimators, basis, functions, dictionary, methods] class methods includes familiar linear polynomial expan sions importantly wide variety flexible models model linear expansion basis functions overview supervised learning function input term linear refers action parameters class covers wide variety methods cases sequence basis functions prescribed basis polynomials total degree one dimensional polynomial splines degree represented appropriate sequence spline basis functions determined turn knots produce functions piecewise polynomials degree knots joined continuity degree knots example consider linear splines piecewise linear functions one intuitively satisfying basis consists functions mth knot denotes positive part tensor products spline bases used inputs dimensions larger one see section cart mars models chapter parameter total degree polynomial number knots case splines radial basis functions symmetric dimensional kernels located particular centroids example gaussian kernel popular radial basis functions centroids scales determined spline basis functions knots general would like data dictate well including parameters changes regression problem straightforward linear problem combi natorially hard nonlinear problem practice shortcuts greedy algorithms two stage processes used section describes approaches single layer feed forward neural network model linear output weights thought adaptive basis function method model form known activation function projection pursuit model directions bias terms determined estimation meat computation details give chapter adaptively chosen basis function methods also known dictio nary methods one available possibly infinite set dictionary candidate basis functions choose models built employing kind search mechanism model selection bias variance tradeoff 
[overview, supervised, learning, model, selection, biasvariance, tradeoff] models described many others discussed later chapters smoothing complexity parameter determined multiplier penalty term width kernel number basis functions case smoothing spline parameter indexes models ranging straight line fit interpolating model similarly local degree polynomial model ranges degree global polynomial window size infinitely large interpolating fit window size shrinks zero means cannot use residual sum squares training data determine parameters well since would always pick gave interpolating fits hence zero residuals model unlikely predict future data well nearest neighbor regression fit usefully illustrates com peting forces effect predictive ability approximations sup pose data arise model var simplicity assume values sample fixed advance nonrandom expected prediction error also known test generalization error decomposed epe bias var subscripts parentheses indicate sequence nearest neighbors three terms expression first term reducible error variance new test target beyond control even know true second third terms control make mean squared error estimating broken bias component variance component bias term squared difference true mean expected value estimate expectation averages randomness training data term likely increase true function reasonably smooth small closest neighbors values close average overview supervised learning high bias low variance low bias high variance model complexity training sample test sample low high figure test training error function model complexity close grows neighbors away anything happen variance term simply variance average creases inverse varies bias variance tradeoff generally model complexity procedure increased variance tends increase squared bias tends decreases opposite behavior occurs model complexity decreased nearest neighbors model complexity controlled typically would like choose model complexity trade bias variance way minimize test error obvious estimate test error training error unfortunately training error good estimate test error properly account model complexity figure shows typical behavior test training error model complexity varied training error tends decrease whenever increase model complexity whenever fit data harder however much fitting model adapts closely training data generalize well large test error case predictions large variance reflected last term expression contrast model complex enough underfit may large bias resulting poor generalization chapter discuss methods estimating test error prediction method hence estimating optimal amount model complexity given prediction method training set exercises 
[overview, supervised, learning, bibliographic, notes] good general books learning problem duda bishop bishop ripley cherkassky mulier vapnik parts chapter based friedman 
[overview, supervised, learning, exercises] suppose classes associated target vector zeros except one kth position show classifying largest element amounts choosing closest target min elements sum one show compute bayes decision boundary simula tion example figure derive equation edge effect problem discussed page peculiar uniform sampling bounded domains consider inputs drawn spherical multinormal distribution squared distance sample point origin distribution mean consider prediction point drawn distribution let associated unit vector let projection training points direction show distributed expected squared distance origin target point expected squared distance origin hence randomly drawn test point standard deviations origin training points average one standard deviation along direction prediction points see lying edge training set derive equation last line makes use conditioning argument derive equation making use cyclic property trace operator trace trace linearity allows interchange order trace expectation consider regression problem inputs outputs parameterized model fit least squares show observations tied identical values fit obtained reduced weighted least squares problem overview supervised learning suppose sample pairs drawn distribution characterized follows design density regression function mean zero variance construct estimator linear weights depend depend entire training sequence denoted show linear regression nearest neighbor regression mem bers class estimators describe explicitly weights cases decompose conditional mean squared error conditional squared bias conditional variance component like represents entire training sequence decompose unconditional mean squared error squared bias variance component establish relationship squared biases variances two cases compare classification performance linear regression nearest neighbor classification zipcode data particular consider show training test error choice zipcode data available book website www stat stanford edu elemstatlearn consider linear regression model parameters fit least squares set training data drawn random population let least squares estimate suppose test data drawn random pop ulation training data prove exercises expectations random expression exercise brought attention ryan tibshirani homework assignment given andrew overview supervised learning 
[linear, methods, regression, introduction] linear regression model assumes regression function linear inputs linear models largely developed precomputer age statistics even today computer era still good reasons study use simple often provide adequate interpretable description inputs affect output prediction purposes sometimes outperform fancier nonlinear models especially situations small numbers training cases low signal noise ratio sparse data finally linear methods applied transformations inputs considerably expands scope generalizations sometimes called basis function methods discussed chapter chapter describe linear methods regression next chapter discuss linear methods classification topics considerable detail firm belief understanding linear methods essential understanding nonlinear ones fact many nonlinear techniques direct generalizations linear methods discussed linear methods regression 
[linear, methods, regression, linear, regression, models, least, squares] introduced chapter input vector want predict real valued output linear regression model form linear model either assumes regression function linear linear model reasonable approximation unknown parameters coefficients variables come different sources quantitative inputs transformations quantitative inputs log square root square basis expansions leading polynomial representation numeric dummy coding levels qualitative inputs example five level factor input might create together group repre sents effect set level dependent constants since one one others zero interactions variables example matter source model linear parameters typically set training data estimate parameters vector feature measurements ith case popular estimation method least squares pick coefficients minimize residual sum squares rss statistical point view criterion reasonable training observations represent independent random draws popu lation even drawn randomly criterion still valid conditionally independent given inputs figure illustrates geometry least squares fitting dimensional linear regression models least squares 
[linear, methods, regression] figure linear least squares fitting seek linear function minimizes sum squared residuals space occupied pairs note makes assumptions validity model simply finds best linear fit data least squares fitting intuitively satisfying matter data arise criterion measures average lack fit minimize denote matrix row input vector first position similarly let vector outputs training set write residual sum squares rss quadratic function parameters differentiating respect obtain rss rss assuming moment full column rank hence positive definite set first derivative zero obtain unique solution linear methods regression figure dimensional geometry least squares regression two predictors outcome vector orthogonally projected onto hyperplane spanned input vectors projection represents vector least squares predictions predicted values input vector given fitted values training inputs matrix appearing equation sometimes called hat matrix puts hat figure shows different geometrical representation least squares estimate time denote column vectors much follows first column treated like vectors span subspace also referred column space minimize rss xk choosing residual vector orthogonal subspace orthogonality expressed resulting estimate hence orthogonal pro jection onto subspace hat matrix computes orthogonal projection hence also known projection matrix might happen columns linearly independent full rank would occur example two inputs perfectly correlated singular least squares coefficients uniquely defined however fitted values still projection onto column space one way express projection terms column vectors non full rank case occurs often one qualitative inputs coded redundant fashion usually natural way resolve non unique representation recoding dropping redundant columns regression software packages detect redundancies automatically implement linear regression models least squares strategy removing rank deficiencies also occur signal image analysis number inputs exceed number training cases case features typically reduced filtering else fitting controlled regularization section chapter made minimal assumptions true distribu tion data order pin sampling properties assume observations uncorrelated constant vari ance fixed non random variance covariance matrix least squares parameter estimates easily derived given var typically one estimates variance rather denominator makes unbiased estimate draw inferences parameters model additional sumptions needed assume correct model mean conditional expectation linear also assume deviations around expectation additive gaussian hence error gaussian random variable expectation zero variance written easy show multivariate normal distribution mean vector variance covariance matrix shown also chi squared distribution degrees freedom addition statistically independent use distributional properties form tests hypothesis confidence intervals parameters linear methods regression tail probabilities normal figure tail probabilities three distributions standard normal shown appropriate quantiles testing significance levels difference standard normal becomes negligible bigger test hypothesis particular coefficient form standardized coefficient score jth diagonal element null hypothesis distributed distribution degrees freedom hence large absolute value lead rejection null hypothesis replaced known value would standard normal distribution difference tail quantiles distribution standard normal become negligible sample size increases typically use normal quantiles see figure often need test significance groups coefficients simul taneously example test categorical variable levels excluded model need test whether coefficients dummy variables used represent levels set zero use statistic rss rss rss rss residual sum squares least squares fit big ger model parameters rss nested smaller model parameters parameters constrained linear regression models least squares zero statistic measures change residual sum squares per additional parameter bigger model normalized esti mate gaussian assumptions null hypothesis smaller model correct statistic dis tribution shown exercise equivalent statistic dropping single coefficient model large quantiles approach similarly isolate obtain confidence interval percentile normal distribution etc hence standard practice reporting amounts proximate confidence interval even gaussian error assumption hold interval approximately correct coverage approaching sample size similar fashion obtain approximate confidence set entire parameter vector namely percentile chi squared distribution degrees freedom example confidence set generates corresponding confidence set true function namely exercise see also fig ure section examples confidence bands functions 
[linear, methods, regression, example, prostate, cancer] data example come study stamey examined correlation level prostate specific antigen number clinical measures men receive radical prostatectomy variables log cancer volume lcavol log prostate weight lweight age log amount benign prostatic hyperplasia lbph seminal vesicle invasion svi log capsular penetration lcp gleason score gleason percent gleason scores pgg correlation matrix predictors given table shows many strong correlations figure page chapter scatterplot matrix showing every pairwise plot variables see svi binary variable gleason ordered categorical variable see linear methods regression table correlations predictors prostate cancer data lcavol lweight age lbph svi lcp gleason lweight age lbph svi lcp gleason pgg table linear model fit prostate cancer data score coefficient divided standard error roughly score larger two absolute value significantly nonzero level term coefficient std error score intercept lcavol lweight age lbph svi lcp gleason pgg example lcavol lcp show strong relationship response lpsa need fit effects jointly untangle relationships predictors response fit linear model log prostate specific antigen lpsa first standardizing predictors unit variance randomly split dataset training set size test set size plied least squares estimation training set producing estimates standard errors scores shown table scores defined measure effect dropping variable model score greater absolute value approximately significant level example nine parameters tail quantiles distribution predictor lcavol shows strongest effect lweight svi also strong notice lcp significant lcavol model used model without lcavol lcp strongly significant also test exclusion number terms using statistic example consider dropping non significant terms table namely age linear regression models least squares lcp gleason pgg get value hence significant mean prediction error test data contrast predic tion using mean training value lpsa test error called base error rate hence linear model reduces base error rate return example later compare various selection shrinkage methods 
[linear, methods, regression, gaussmarkov, theorem] one famous results statistics asserts least squares estimates parameters smallest variance among linear unbiased estimates make precise also make clear restriction unbiased estimates necessarily wise one observation lead consider biased estimates ridge regression later chapter focus estimation linear combination parameters example predictions form least squares estimate considering fixed linear function response vector assume linear model correct unbiased since gauss markov theorem states linear estima tor unbiased var var proof exercise uses triangle inequality simplicity stated result terms estimation single parameter definitions one state terms entire parameter vector exercise consider mean squared error estimator estimating mse var linear methods regression first term variance second term squared bias gauss markov theorem implies least squares estimator smallest mean squared error linear estimators bias however may well exist biased estimator smaller mean squared error estimator would trade little bias larger reduction variance biased estimates commonly used method shrinks sets zero least squares coefficients may result biased estimate discuss many examples including variable subset selection ridge regression later chapter pragmatic point view models distortions truth hence biased picking right model amounts creating right balance bias variance issues detail chapter mean squared error intimately related prediction accuracy dis cussed chapter consider prediction new response input expected prediction error estimate mse therefore expected prediction error mean squared error differ constant representing variance new observation 
[linear, methods, regression, multiple, regression, simple, univariate, regression] linear model inputs called multiple linear regression model least squares estimates model best understood terms estimates univariate linear model indicate section suppose first univariate model intercept least squares estimate residuals convenient vector notation let define linear regression models least squares inner product write see simple univariate regression provides building block multiple linear regression suppose next inputs columns data matrix orthogonal easy check multiple least squares esti mates equal univariate estimates words inputs orthogonal effect parameter estimates model orthogonal inputs occur often balanced designed experiments orthogonality enforced almost never observational data hence orthogonalize order carry idea suppose next intercept single input least squares coefficient form vector ones view estimate result two applications simple regression steps regress produce residual regress residual give coefficient procedure regress means simple univariate regression intercept producing coefficient residual vector a say adjusted orthogonalized respect step orthogonalizes respect step simple univariate regression using orthogonal predictors figure shows process two general inputs orthogonalization change subspace spanned simply produces orthogonal basis representing recipe generalizes case inputs shown algorithm note inputs step orthogonal hence simple regression coefficients computed fact also multiple regres sion coefficients inner product notation suggestive generalizations linear regression different metric spaces well probability spaces linear methods regression figure least squares regression orthogonalization inputs vector regressed vector leaving residual vector regres sion gives multiple regression coefficient adding together projections gives least squares fit algorithm regression successive orthogonalization initialize regress produce coefficients residual vector regress residual give estimate result algorithm arranging residual step see linear combination since orthogonal form basis column space hence least squares projection onto subspace since alone involves coefficient see coefficient indeed multiple regression coefficient key result exposes effect correlated inputs multiple regression note also rearranging one could last position similar results holds hence stated generally shown jth multiple regression coefficient univariate regression coefficient residual regressing linear regression models least squares multiple regression coefficient represents additional contribution adjusted highly correlated residual vector close zero coefficient unstable true variables correlated set situations might scores table small one set deleted yet cannot delete also obtain alternate formula variance estimates var words precision estimate depends length residual vector represents much unexplained algorithm known gram schmidt procedure multiple regression also useful numerical strategy computing esti mates obtain also entire multiple least squares fit shown exercise represent step algorithm matrix form columns order upper triangular trix entries introducing diagonal matrix jth diagonal entry get called decomposition orthogonal matrix upper triangular matrix decomposition represents convenient orthogonal basis column space easy see example least squares solution given equation easy solve upper triangular exercise linear methods regression 
[linear, methods, regression, multiple, outputs] suppose multiple outputs wish predict inputs assume linear model output training cases write model matrix notation response matrix entry input matrix matrix parameters matrix errors straightforward generalization univariate loss function rss least squares estimates exactly form hence coefficients kth outcome least squares timates regression multiple outputs affect one another least squares estimates errors correlated might seem appropriate modify favor multivariate version specifically suppose cov multivariate weighted criterion rss arises naturally multivariate gaussian theory vector function vector responses observa tion however shown solution given separate regressions ignore correlations exercise vary among observations longer case solution longer decouples section pursue multiple outcome problem consider situations pay combine regressions subset selection 
[linear, methods, regression, subset, selection] two reasons often satisfied least squares estimates first prediction accuracy least squares estimates often low bias large variance prediction accuracy sometimes improved shrinking setting coefficients zero sacrifice little bit bias reduce variance predicted values hence may improve overall prediction accuracy second reason interpretation large number predic tors often would like determine smaller subset exhibit strongest effects order get big picture willing sacrifice small details section describe number approaches variable subset selec tion linear regression later sections discuss shrinkage hybrid approaches controlling variance well dimension reduction strategies fall general heading model selection model selection restricted linear models chapter covers topic detail subset selection retain subset variables elim inate rest model least squares regression used estimate coefficients inputs retained number dif ferent strategies choosing subset 
[linear, methods, regression, subset, selection, best-subset, selection] best subset regression finds subset size gives smallest residual sum squares efficient algorithm leaps bounds procedure furnival wilson makes feasible large figure shows subset models prostate cancer example lower boundary represents models eligible selection best subsets approach note best subset size example need include variable best subset size example subsets nested best subset curve red lower boundary figure necessarily decreasing cannot used select subset size question choose involves tradeoff bias variance along subjective desire parsimony number criteria one may use typically choose smallest model minimizes estimate expected prediction error many approaches discuss chapter similar use training data produce sequence models varying complexity indexed single parameter next section use linear methods regression subset size residual sum squares figure possible subset models prostate cancer example subset size shown residual sum squares model size cross validation estimate prediction error select aic criterion popular alternative defer detailed discussion approaches chapter 
[linear, methods, regression, subset, selection, forward-, backward-stepwise, selection] rather search possible subsets becomes infeasible much larger seek good path forward stepwise selection starts intercept sequentially adds model predictor improves fit many candidate predictors might seem like lot computation however clever dating algorithms exploit decomposition current fit rapidly establish next candidate exercise like best subset gression forward stepwise produces sequence models indexed subset size must determined forward stepwise selection greedy algorithm producing nested quence models sense might seem sub optimal compared best subset selection however several reasons might preferred subset selection computational large cannot compute best subset quence always compute forward stepwise sequence even statistical price paid variance selecting best subset size forward stepwise constrained search lower variance perhaps bias best subset forward stepwise backward stepwise forward stagewise subset size figure comparison four subset selection techniques simulated lin ear regression problem observations standard gaussian variables pairwise correlations equal variables coefficients drawn random distribution rest zero noise resulting signal noise ratio results averaged simulations shown mean squared error estimated coefficient step true backward stepwise selection starts full model sequentially deletes predictor least impact fit candidate dropping variable smallest score exercise backward selection used forward stepwise always used figure shows results small simulation study compare best subset regression simpler alternatives forward backward selection performance similar often case included figure forward stagewise regression next section takes longer reach minimum error linear methods regression prostate cancer example best subset forward backward lection gave exactly sequence terms software packages implement hybrid stepwise selection strategies consider forward backward moves step select best two example package step function uses aic criterion weighing choices takes proper account number parameters fit step add drop performed minimizes aic score traditional packages base selection statistics adding significant terms dropping non significant terms fashion since take proper account multiple testing issues also tempting model search print summary chosen model table however standard errors valid since account search process bootstrap section useful settings finally note often variables come groups dummy variables code multi level categorical predictor smart stepwise pro cedures step add drop whole groups time taking proper account degrees freedom 
[linear, methods, regression, subset, selection, forward-stagewise, regression] forward stagewise regression even constrained forward stepwise regression starts like forward stepwise regression tercept equal centered predictors coefficients initially step algorithm identifies variable correlated current residual computes simple linear regression coefficient residual chosen variable adds current efficient variable continued till none variables correlation residuals least squares fit unlike forward stepwise regression none variables justed term added model consequence forward stagewise take many steps reach least squares fit historically dismissed inefficient turns slow fitting pay dividends high dimensional problems see section forward stagewise variant slowed even quite competitive especially high dimensional problems forward stagewise regression included figure example takes steps get correlations subset size plotted error last step nonzero coefficients although catches best fit takes longer shrinkage methods 
[linear, methods, regression, subset, selection, prostate, cancer, data, example, continued] table shows coefficients number different selection shrinkage methods best subset selection using subsets search ridge regression lasso principal components regression partial least squares method complexity parameter chosen minimize estimate prediction error based tenfold cross validation full details given section briefly cross validation works divid ing training data randomly ten equal parts learning method fit range values complexity parameter nine tenths data prediction error computed remaining one tenth done turn one tenth data ten prediction error estimates averaged obtain estimated prediction error curve function complexity parameter note already divided data training set size test set size cross validation applied training set since selecting shrinkage parameter part training process test set judge performance selected model estimated prediction error curves shown figure many curves flat large ranges near minimum included estimated standard error bands estimated error rate based ten error estimates computed cross validation used one standard error rule pick parsimonious model within one standard error minimum section page rule acknowledges fact tradeoff curve estimated error hence takes conservative approach best subset selection chose use two predictors lcvol lweight last two lines table give average prediction error estimated standard error test set 
[linear, methods, regression, shrinkage, methods] retaining subset predictors discarding rest subset selec tion produces model interpretable possibly lower predic tion error full model however discrete process variables either retained discarded often exhibits high variance reduce prediction error full model shrinkage methods continuous suffer much high variability 
[linear, methods, regression, shrinkage, methods, ridge, regression] ridge regression shrinks regression coefficients imposing penalty size ridge coefficients minimize penalized residual sum linear methods regression subset size error subsets degrees freedom error ridge regression shrinkage factor error lasso number directions error principal components regression number directions error partial least squares figure estimated prediction error curves standard errors various selection shrinkage methods curve plotted function corresponding complexity parameter method horizontal axis chosen model complexity increases move left right estimates prediction error standard errors obtained tenfold cross validation full details given section least complex model within one standard error best chosen indicated purple vertical broken lines shrinkage methods table estimated coefficients test error results different subset shrinkage methods applied prostate data blank entries correspond variables omitted term best subset ridge lasso pcr pls intercept lcavol lweight age lbph svi lcp gleason pgg test error std error squares ridge argmin complexity parameter controls amount shrink age larger value greater amount shrinkage coefficients shrunk toward zero idea penaliz ing sum squares parameters also used neural networks known weight decay chapter equivalent way write ridge problem ridge argmin subject makes explicit size constraint parameters one one correspondence parameters many correlated variables linear regression model coefficients become poorly determined exhibit high variance wildly large positive coefficient one variable canceled similarly large negative coefficient correlated cousin imposing size constraint coefficients problem alleviated ridge solutions equivariant scaling inputs one normally standardizes inputs solving addition linear methods regression notice intercept left penalty term penal ization intercept would make procedure depend origin chosen adding constant targets would simply result shift predictions amount shown exercise solution separated two parts reparametrization using centered inputs gets replaced estimate remaining efficients get estimated ridge regression without intercept using centered henceforth assume centering done input matrix rather columns writing criterion matrix form rss ridge regression solutions easily seen ridge identity matrix notice choice quadratic penalty ridge regression solution linear function solution adds positive constant diagonal inversion makes problem nonsingular even full rank main motivation ridge regression first introduced statistics hoerl kennard traditional descriptions ridge regression start definition choose motivate via provide insight works figure shows ridge coefficient estimates prostate cer example plotted functions effective degrees freedom implied penalty defined page case thonormal inputs ridge estimates scaled version least squares estimates ridge ridge regression also derived mean mode poste rior distribution suitably chosen prior distribution detail sup pose parameters distributed independently one another negative log posterior density assumed known equal expression curly braces exercise thus ridge estimate mode posterior distribution since distribution gaussian also posterior mean singular value decomposition svd centered input matrix gives additional insight nature ridge regression composition extremely useful analysis many statistical methods svd matrix form udv shrinkage methods coefficients lcavol lweight age lbph svi lcp gleason pgg figure profiles ridge coefficients prostate cancer example tuning parameter varied coefficients plotted versus effective degrees freedom vertical line drawn value chosen cross validation linear methods regression orthogonal matrices columns spanning column space columns spanning row space diagonal matrix diagonal entries called singular values one values singular using singular value decomposition write least squares fitted vector simplification note coordinates respect orthonormal basis note also similarity generally different orthogonal bases column space exercise ridge solutions ridge columns note since like linear regression ridge regression computes coordinates respect orthonormal basis shrinks coordinates factors means greater amount shrinkage applied coordinates basis vectors smaller small value mean svd centered matrix another way expressing principal components variables sample covariance matrix given eigen decomposition factor eigenvectors columns also called principal compo nents karhunen loeve directions first principal component direction property largest sample vari ance amongst normalized linear combinations columns sample variance easily seen var var fact derived variable called first principal component hence normalized first principal shrinkage methods largest principal component smallest principal component figure principal components input data points largest prin cipal component direction maximizes variance projected data smallest principal component minimizes variance ridge regression projects onto components shrinks coefficients low variance components high variance components component subsequent principal components maximum variance subject orthogonal earlier ones conversely last principal component minimum variance hence small singular val ues correspond directions column space small variance ridge regression shrinks directions figure illustrates principal components data points two dimensions consider fitting linear surface domain axis sticking page configuration data allow determine gradient accurately long direction short ridge regression protects potentially high variance gradients estimated short directions implicit assumption response tend vary directions high variance inputs often reasonable assumption since predictors often chosen study vary response variable need hold general linear methods regression figure plotted estimated prediction error versus quantity monotone decreasing function effective degrees freedom ridge regression fit usually linear regression fit variables degrees freedom fit number free parameters idea although coefficients ridge fit non zero fit restricted fashion controlled note regularization course always additional one degree freedom intercept removed apriori definition motivated detail section sections figure minimum occurs table shows ridge regression reduces test error full least squares estimates small amount 
[linear, methods, regression, shrinkage, methods, lasso] lasso shrinkage method like ridge subtle important dif ferences lasso estimate defined lasso argmin subject ridge regression parametrize constant stan dardizing predictors solution thereafter fit model without intercept exercise signal processing litera ture lasso also known basis pursuit chen also write lasso problem equivalent lagrangian form lasso argmin notice similarity ridge regression problem ridge penalty replaced lasso penalty latter constraint makes solutions nonlinear closed form expression ridge regression computing lasso solution shrinkage methods quadratic programming problem although see section efficient algorithms available computing entire path solutions varied computational cost ridge regression nature constraint making sufficiently small cause coefficients exactly zero thus lasso kind continuous subset selection chosen larger least squares estimates lasso estimates hand say least squares coefficients shrunk average however nature shrinkage obvious investigate section like subset size variable subset selection penalty parameter ridge regression adaptively chosen minimize estimate expected prediction error figure ease interpretation plotted lasso pre diction error estimates versus standardized parameter value chosen fold cross validation caused four coefficients set zero fifth column table resulting model second lowest test error slightly lower full least squares model standard errors test error estimates last line table fairly large figure shows lasso coefficients standardized tuning rameter varied least squares estimates decrease decrease always strictly monotonic although example vertical line drawn value chosen cross validation 
[linear, methods, regression, shrinkage, methods, discussion, subset, selection, ridge, regression, lasso] section discuss compare three approaches discussed far restricting linear regression model subset selection ridge regression lasso case orthonormal input matrix three procedures explicit solutions method applies simple transformation least squares estimate detailed table ridge regression proportional shrinkage lasso translates coefficient constant factor truncating zero called soft thresholding used context wavelet based smoothing sec tion best subset selection drops variables coefficients smaller largest form hard thresholding back nonorthogonal case pictures help understand lationship figure depicts lasso left ridge regression right two parameters residual sum squares ellip tical contours centered full least squares estimate constraint linear methods regression shrinkage factor coefficients lcavol lweight age lbph svi lcp gleason pgg figure profiles lasso coefficients tuning parameter varied coefficients plotted versus vertical line drawn value chosen cross validation compare figure page lasso profiles hit zero ridge profiles piece wise linear computed points displayed see section details shrinkage methods table estimators case orthonormal columns constants chosen corresponding techniques sign denotes sign argument denotes positive part table estimators shown broken red lines line gray shows unrestricted estimate reference estimator formula best subset size ridge lasso sign best subset ridge lasso figure estimation picture lasso left ridge regression right shown contours error constraint functions solid blue areas constraint regions respectively red ellipses contours least squares error function linear methods regression region ridge regression disk lasso diamond methods find first point elliptical contours hit constraint region unlike disk diamond corners solution occurs corner one parameter equal zero diamond becomes rhomboid many corners flat edges faces many opportunities estimated parameters zero generalize ridge regression lasso view bayes estimates consider criterion argmin contours constant value shown fig ure case two inputs thinking log prior density also equi contours prior distribution parameters value corre sponds variable subset selection penalty simply counts number nonzero parameters corresponds lasso ridge regression notice prior uniform direction concentrates mass coordinate directions prior correspond ing case independent double exponential laplace distribution input density exp case lasso smallest constraint region convex non convex constraint regions make optimization problem difficult view lasso ridge regression best subset selection bayes estimates different priors note however derived posterior modes maximizers posterior common use mean posterior bayes estimate ridge regression also posterior mean lasso best subset selection looking criterion might try using values besides although one might consider estimating data experience worth effort extra variance incurred values suggest compromise lasso ridge regression although case differentiable share ability lasso figure contours constant value given values shrinkage methods elastic net figure contours constant value left plot elastic net penalty right plot although visually similar elastic net sharp non differentiable corners penalty setting coefficients exactly zero partly reason well computational tractability zou hastie introduced elastic net penalty different compromise ridge lasso figure compares penalty elastic net penalty hard detect difference eye elastic net selects variables like lasso shrinks together coefficients correlated predictors like ridge also considerable computational advantages penal ties discuss elastic net section 
[linear, methods, regression, shrinkage, methods, least, angle, regression] least angle regression lar relative newcomer efron viewed kind democratic version forward stepwise regression section see lar intimately connected lasso fact provides extremely efficient algorithm computing entire lasso path figure forward stepwise regression builds model sequentially adding one vari able time step identifies best variable include active set updates least squares fit include active variables least angle regression uses similar strategy enters much predictor deserves first step identifies variable correlated response rather fit variable completely lar moves coefficient variable continuously toward least squares value causing correlation evolving residual decrease absolute value soon another variable catches terms correlation residual process paused second variable joins active set coefficients moved together way keeps correlations tied decreasing process continued linear methods regression variables model ends full least squares fit algorithm provides details termination condition step requires explanation lar algorithm reaches zero residual solution steps centered data algorithm least angle regression standardize predictors mean zero unit norm start residual find predictor correlated move towards least squares coefficient competitor much correlation current residual move direction defined joint least squares coefficient current residual com petitor much correlation current residual continue way predictors entered min steps arrive full least squares solution suppose active set variables beginning kth step let coefficient vector variables step nonzero values one entered zero current residual direction step coefficient profile evolves exercise verifies directions chosen fashion claimed keep correlations tied decreasing fit vector beginning step evolves new fit direction name least angle arises geometrical interpretation process makes smallest equal angle predictors exercise figure shows absolute correlations decreasing joining ranks step lar algorithm using simulated data construction coefficients lar change piecewise linear fash ion figure left panel shows lar coefficient profile evolving function arc length note need take small arc length differentiable curve given piecewise linear lar coefficient profile amounts summing norms changes coefficients step step shrinkage methods arc length lat figure progression absolute correlations step lar procedure using simulated data set six predictors labels top plot indicate variables enter active set step step length measured units arc length least angle regression lasso arc length arc length ffi ffi figure left panel shows lar coefficient profiles simulated data function arc length right panel shows lasso profile identical dark blue coefficient crosses zero arc length linear methods regression steps recheck correlations step using knowledge covari ance predictors piecewise linearity algorithm work exact step length beginning step exercise right panel figure shows lasso coefficient profiles data almost identical left panel differ first time blue coefficient passes back zero prostate data lar coefficient profile turns identical lasso profile figure never crosses zero observations lead simple modification lar algorithm gives entire lasso path also piecewise linear algorithm least angle regression lasso modification non zero coefficient hits zero drop variable active set variables recompute current joint least squares direction lar lasso algorithm extremely efficient requiring order computation single least squares fit using predictors least angle regression always takes steps get full least squares estimates lasso path steps although two often quite similar algorithm lasso modification efficient way computing solution lasso problem especially osborne also discovered piecewise linear path computing lasso called homotopy algorithm give heuristic argument procedures similar although lar algorithm stated terms correlations input features standardized equivalent easier work inner products suppose active set variables stage algorithm tied absolute inner product current residuals express indicates sign inner product common value also consider lasso criterion write vector form let active set variables solution given value variables differentiable stationarity conditions give sign comparing see identical sign matches sign inner product lar shrinkage methods algorithm lasso start differ active coefficient passes zero condition violated variable kicked active set exercise shows equations imply piecewise linear coefficient profile decreases stationarity conditions non active variables require agrees lar algorithm figure compares lar lasso forward stepwise stagewise regression setup figure page except rather problem difficult see aggressive forward stepwise starts overfit quite early well true variables enter model ultimately performs worse slower forward stagewise regression behavior lar lasso similar forward stagewise regression incremental forward stagewise similar lar lasso described sec tion degrees freedom formula lar lasso suppose fit linear model via least angle regression procedure stopping number steps equivalently using lasso bound produces constrained version full least squares fit many parameters degrees freedom used consider first linear regression using subset features subset prespecified advance without reference training data degrees freedom used fitted model defined indeed classical statistics number linearly independent parameters meant degrees freedom alternatively suppose carry best subset selection determine optimal set predictors resulting model parameters sense used degrees freedom need general definition effective degrees freedom adaptively fitted model define degrees freedom fitted vector cov cov refers sampling covariance predicted value corresponding outcome value makes intuitive sense harder fit data larger covariance hence expression useful notion degrees freedom one applied model prediction includes models linear methods regression forward stepwise lar lasso forward stagewise incremental forward stagewise fraction arc length figure comparison lar lasso forward stepwise forward stagewise incremental forward stagewise regression setup figure except rather slower regression ultimately outperforms forward stepwise lar lasso show similar behavior since procedures take different numbers steps across simulation replicates methods plot mse function fraction total arc length toward least squares fit adaptively fitted training data definition motivated discussed sections linear regression fixed predictors easy show likewise ridge regression definition leads closed form expression page cases simple evaluate fit linear think definition context best subset selection size seems clear larger verified estimating cov directly simulation however closed form method estimating best subset selection lar lasso something magical happens techniques adaptive smoother way best subset selection hence estimation degrees freedom tractable specifically shown kth step lar procedure effective degrees freedom fit vector exactly lasso modified lar procedure methods using derived input directions often takes steps since predictors drop hence definition little different lasso stage approximately equals number predictors model approximation works reasonably well anywhere lasso path works best last model sequence contains predictors detailed study degrees freedom lasso may found zou 
[linear, methods, regression, methods, using, derived, input, directions] many situations large number inputs often correlated methods section produce small number linear combinations original inputs used place inputs regression methods differ linear combinations constructed 
[linear, methods, regression, methods, using, derived, input, directions, principal, components, regression] approach linear combinations used principal com ponents defined section principal component regression forms derived input columns regresses since orthogonal regression sum univariate regressions pcr since linear combinations original express solution terms coefficients exercise pcr ridge regression principal components depend scaling inputs typically first standardize note would get back usual least squares estimates since columns span column space get reduced regres sion see principal components regression similar ridge regression operate via principal components input trix ridge regression shrinks coefficients principal components figure shrinking depending size corresponding eigenvalue principal components regression discards smallest eigenvalue components figure illustrates linear methods regression index shrinkage factor ridge pcr figure ridge regression shrinks regression coefficients prin cipal components using shrinkage factors principal component regression truncates shown shrinkage truncation patterns corresponding figure function principal component index figure see cross validation suggests seven terms sulting model lowest test error table 
[linear, methods, regression, methods, using, derived, input, directions, partial, least, squares] technique also constructs set linear combinations inputs regression unlike principal components regression uses dition construction like principal component regression partial least squares pls scale invariant assume standardized mean variance pls begins com puting construct derived input first partial least squares direction hence construction inputs weighted strength univariate effect outcome regressed giving coefficient orthogonalize respect continue process directions obtained manner partial least squares produces sequence derived orthogonal inputs directions principal component regres sion construct directions would get back solution equivalent usual least squares estimates using rections produces reduced regression procedure described fully algorithm since standardized first directions univariate regression coefficients irrelevant constant case subsequent directions methods using derived input directions algorithm partial least squares standardize mean zero variance one set orthogonalize respect output sequence fitted vectors since linear original pls linear coeffi cients recovered sequence pls transformations prostate cancer example cross validation chose pls direc tions figure produced model given rightmost column table optimization problem partial least squares solving since uses response construct directions solution path nonlinear function shown exercise partial least squares seeks directions high variance high correlation response contrast principal components regression keys high variance stone brooks frank friedman particular mth principal component direction solves max var subject sample covariance matrix conditions ensures uncorrelated previous linear com binations mth pls direction solves max corr var subject analysis reveals variance aspect tends dominate partial least squares behaves much like ridge regression principal components regression discuss next section input matrix orthogonal partial least squares finds least squares estimates steps subsequent steps effect linear methods regression since zero exercise also shown sequence pls coefficients represents conjugate gradient sequence computing least squares solutions exercise 
[linear, methods, regression, discussion, comparison, selection, shrinkage, methods] simple settings understand better rela tionship different methods described consider exam ple two correlated inputs correlation assume true regression coefficients figure shows coefficient profiles different methods tuning rameters varied top panel bottom panel tuning parameters ridge lasso vary continuous range best subset pls pcr take two discrete steps least squares solution top panel starting origin ridge regression shrinks coefficients together finally converges least squares pls pcr show similar behavior ridge although discrete extreme best subset overshoots solution backtracks behavior lasso intermediate methods correlation negative lower panel pls pcr roughly track ridge path methods similar one another interesting compare shrinkage behavior different methods recall ridge regression shrinks directions shrinks low variance directions principal components regression leaves high variance directions alone discards rest interestingly shown partial least squares also tends shrink low variance directions actually inflate higher variance directions make pls little unstable cause slightly higher prediction error compared ridge regression full study given frank friedman authors conclude minimizing predic tion error ridge regression generally preferable variable subset selec tion principal components regression partial least squares however improvement latter two methods slight summarize pls pcr ridge regression tend behave similarly ridge regression may preferred shrinks smoothly rather discrete steps lasso falls somewhere ridge regression best subset regression enjoys properties discussion comparison selection shrinkage methods least squares ridge lasso best subset pls pcr least squares ridge best subset pls pcr lasso figure coefficient profiles different methods simple problem two inputs correlation true regression coefficients linear methods regression 
[linear, methods, regression, multiple, outcome, shrinkage, selection] noted section least squares estimates multiple output linear model simply individual least squares estimates outputs apply selection shrinkage methods multiple output case one could apply univariate technique individually outcome multaneously outcomes ridge regression example could apply formula columns outcome matrix using possibly different parameters apply columns using value former strategy would allow different amounts regularization applied different outcomes require estimation separate regularization parameters latter would permit outputs used estimating sole regularization rameter sophisticated shrinkage selection strategies exploit correlations different responses helpful multiple output case suppose example among outputs share structural part models clear case pool observations estimate common combining responses heart canonical correlation analysis cca data reduction technique developed multiple output case similar pca cca finds sequence uncorrelated linear combina tions corresponding sequence uncorrelated linear combinations responses correlations corr successively maximized note min directions found leading canonical response variates linear com binations derived responses best predicted contrast trailing canonical variates poorly predicted didates dropped cca solution computed using general ized svd sample cross covariance matrix assuming centered exercise reduced rank regression izenman van der merwe zidek formalizes approach terms regression model explicitly pools information given error covariance cov solve following multiple outcome shrinkage selection restricted multivariate regression problem argmin rank replaced estimate one show exercise solution given cca sub matrix consisting first columns matrix left canonical vectors generalized inverse writing solution see reduced rank regression performs linear regression pooled response matrix maps coefficients hence fits well back original response space reduced rank fits given hyp usual linear regression projection operator rank cca response projection operator although better estimate would one show solution remains exercise reduced rank regression borrows strength among responses truncat ing cca breiman friedman explored success shrinkage canonical variates smooth version reduced rank regression proposal form compare buu diagonal shrinkage matrix stands curds whey name gave procedure based optimal prediction population setting show diagonal entries mth canonical correlation coefficient note ratio number input variables sample size gets small shrink age factors approach breiman friedman proposed modified versions based training data cross validation general form fitted response form hys linear methods regression uu response shrinkage operator breiman friedman also suggested shrinking space space leads hybrid shrinkage models form ridge ridge regression shrinkage operator page paper discussions thereof contain many details 
[linear, methods, regression, lasso, related, path, algorithms] since publication lar algorithm efron lot activity developing algorithms fitting regularization paths variety different problems addition regularization taken life leading development field compressed sensing signal processing literature donoho candes section discuss related proposals path algorithms starting precursor lar algorithm 
[linear, methods, regression, lasso, related, path, algorithms, incremental, forward, stagewise, regression] present another lar like algorithm time focused forward stagewise regression interestingly efforts understand flexible nonlinear regression procedure boosting led new algorithm linear models lar reading first edition book forward stagewise algorithm incremental forward stagewise regression start residual equal predictors standardized mean zero unit norm find predictor correlated update sign small step size set repeat steps many times residuals uncorrelated predictors algorithm chapter colleague brad efron realized first edition algorithm chapter lasso related path algorithms lcavol lweight age lbph svi lcp gleason pgg lcavol lweight age lbph svi lcp gleason pgg iteration effi effi arc length coefficients figure coefficient profiles prostate data left panel shows incremental forward stagewise regression step size right panel shows infinitesimal version obtained letting profile fit modification lar algorithm example profiles monotone hence identical lasso lar linear models one could explicitly construct piecewise linear lasso paths figure led propose lar procedure section well incremental version forward stagewise regression presented consider linear regression version forward stagewise boosting algorithm proposed section page generates coefficient profile repeatedly updating small amount coefficient variable correlated current residuals algorithm gives details figure left panel shows progress algorithm prostate data step size least squares coefficient residual jth predictor exactly usual forward stagewise procedure outlined section mainly interested small values letting gives right panel figure case identical lasso path figure call limiting procedure infinitesimal forward stagewise regression procedure plays important role non linear adaptive methods like boosting chapters version incremental forward stagewise regression amenable theoretical analysis uhlmann hothorn refer procedure lboost connections boosting linear methods regression efron originally thought lar algorithm implemen tation allowing tied predictor chance update coeffi cients balanced way remaining tied correlation however realized lar least squares fit amongst tied predictors result coefficients moving opposite direction correla tion cannot happen algorithm following modification lar algorithm implements algorithm least angle regression modification find new direction solving constrained least squares prob lem min subject sign modification amounts non negative least squares fit keeping signs coefficients correlations one show achieves optimal balancing infinitesimal update turns variables tied maximal correlation hastie like lasso entire path computed efficiently via lar algorithm consequence results lar profiles monotone non increasing non decreasing figure three methods lar lasso give identical profiles profiles monotone cross zero axis lar lasso identical since different lasso natural ask optimizes criterion answer complex lasso coefficient profile solution differential equation lasso makes timal progress terms reducing residual sum squares per unit increase norm coefficient vector optimal per unit increase arc length traveled along coefficient path hence efficient path discouraged changing directions often constrained lasso fact viewed mono tone version lasso see figure page dramatic exam ple may useful situations coefficient profiles much smoother hence less variance lasso details given section hastie fig ure includes performance similar lasso lasso related path algorithms 
[linear, methods, regression, lasso, related, path, algorithms, piecewise-linear, path, algorithms] least angle regression procedure exploits piecewise linear nature lasso solution paths led similar path algorithms regularized problems suppose solve argmin loss function penalty function convex following sufficient conditions solution path piecewise linear rosset zhu quadratic piecewise quadratic function piecewise linear also implies principle solution path efficiently computed examples include squared absolute error loss huberized losses penalties another example hinge loss function used support vector machine loss piecewise linear penalty quadratic interestingly leads piecewise linear path algorithm dual space details given sec tion 
[linear, methods, regression, lasso, related, path, algorithms, dantzig, selector] candes tao proposed following criterion min subject call solution dantzig selector written equiva lently min subject denotes norm maximum absolute value components vector form resembles lasso replacing squared error loss maximum absolute value gradient note gets large procedures yield least squares solution yield least squares solution minimum norm however smaller values procedure produces different path solutions lasso candes tao show solution linear pro gramming problem hence name dantzig selector honor late linear methods regression george dantzig inventor simplex method linear program ming also prove number interesting mathematical properties method related ability recover underlying sparse coeffi cient vector properties also hold lasso shown later bickel unfortunately operating properties method somewhat unsatisfactory method seems similar spirit lasso especially look lasso stationary conditions like lar gorithm lasso maintains inner product correlation current residual variables active set moves efficients optimally decrease residual sum squares process common correlation decreased monotonically exercise times correlation larger non active variables dantzig selector instead tries minimize maximum inner product current residual predictors hence achieve smaller maximum lasso process curious phenomenon occur size active set variables tied maximum correlation however need coincide active set hence include variable model smaller correlation current residual excluded variables efron seems unreasonable may responsible times inferior prediction accuracy efron also show yield extremely erratic coefficient paths regularization parameter varied 
[linear, methods, regression, lasso, related, path, algorithms, grouped, lasso] problems predictors belong pre defined groups example genes belong biological pathway collections indicator dummy variables representing levels categorical predictor situation may desirable shrink select members group together grouped lasso one way achieve suppose predictors divided groups number group ease notation use matrix represent predictors corresponding th group corresponding coefficient vector grouped lasso minimizes convex criterion min terms accounts varying group sizes euclidean norm squared since euclidean norm vector zero components zero procedure encourages sparsity group individual levels values entire group predictors may drop model procedure lasso related path algorithms proposed bakin lin zhang studied generalized yuan lin generalizations include general norms well allowing overlapping groups predictors zhao also connections methods fitting sparse additive models lin zhang ravikumar 
[linear, methods, regression, lasso, related, path, algorithms, properties, lasso] number authors studied ability lasso related pro cedures recover correct model grow examples work include knight greenshtein ritov tropp donoho meinshausen meinshausen uhlmann tropp zhao wainwright bunea example donoho focuses case considers lasso solution bound gets large limit gives solution minimum norm among models zero training error shows certain assumptions model matrix true model sparse solution identifies correct predictors high probability many results area assume condition model matrix form indexes subset features non zero coefficients true underlying model columns corresponding features similarly features true coefficients equal zero corresponding columns says least squares coef ficients columns large good variables highly correlated nuisance variables regarding coefficients lasso shrinkage causes esti mates non zero coefficients biased towards zero general consistent one approach reducing bias run lasso identify set non zero coefficients fit restricted linear model selected set features always feasible selected set large alternatively one use lasso select set non zero predictors apply lasso using selected predictors first step known relaxed lasso meinshausen idea use cross validation estimate initial penalty parameter lasso second penalty parameter applied selected set predictors since statistical consistency means sample size grows estimates converge true values linear methods regression variables second step less competition noise vari ables cross validation tend pick smaller value hence coefficients shrunken less initial estimate alternatively one modify lasso penalty function larger efficients shrunken less severely smoothly clipped absolute deviation scad penalty fan replaces sign second term square braces reduces amount shrinkage lasso larger values ultimately shrinkage figure shows scad penalty along lasso scad figure lasso two alternative non convex penalties designed penalize large coefficients less scad use last panel however criterion non convex drawback since makes computation much difficult adaptive lasso zou uses weighted penalty form ordinary least squares estimate practical approxi mation penalties discussed section adaptive lasso yields consistent estimates parameters retaining attractive convexity property lasso 
[linear, methods, regression, lasso, related, path, algorithms, pathwise, coordinate, optimization] alternate approach lars algorithm computing lasso solution simple coordinate descent idea proposed daubechies later studied generalized friedman lange others idea fix penalty parameter lagrangian form optimize successively parameter holding parameters fixed current values suppose predictors standardized mean zero unit norm denote current estimate penalty parameter computational considerations rearrange isolate suppressed intercept introduced factor con venience viewed univariate lasso problem response variable partial residual explicit solution resulting update sign soft thresholding operator table page first argument simple least squares coefficient partial residual standardized variable repeated iteration cycling variable turn convergence yields lasso estimate also use simple algorithm efficiently compute lasso solutions grid values start smallest value max max decrease little cycle variables convergence decreased process repeated using previous solution warm start new value faster lars algorithm especially large problems key speed fact quantities updated quickly varies often update leave hand delivers solutions grid values rather entire solution path kind algorithm applied elastic net grouped lasso many models penalty sum functions individual parameters friedman also applied substantial modifications fused lasso section details friedman 
[linear, methods, regression, computational, considerations] least squares fitting usually done via cholesky decomposition matrix decomposition observations features cholesky decomposition requires operations decomposition requires operations depending relative size cholesky sometimes faster hand less numerically stable lawson hansen computation lasso via lar algorithm order computation least squares fit linear methods regression 
[linear, methods, regression, bibliographic, notes] linear regression discussed many statistics books example seber weisberg mardia ridge regression introduced hoerl kennard lasso proposed tibshirani around time lasso type penalties pro posed basis pursuit method signal processing chen least angle regression procedure proposed efron related earlier homotopy procedure osborne osborne algorithm also exploits piecewise linearity used lar lasso algorithm lacks transparency criterion forward stagewise criterion discussed hastie park hastie develop path algorithm similar least angle regression generalized regression models partial least squares introduced wold comparisons shrinkage methods may found copas frank friedman 
[linear, methods, regression, exercises] show statistic dropping single coefficient model equal square corresponding score given data two variables consider fitting cubic polynomial regression model addition plotting fitted curve would like confidence band curve consider following two approaches point form confidence interval linear func tion form confidence set turn generates confidence intervals approaches differ band likely wider conduct small simulation experiment compare two methods gauss markov theorem prove gauss markov theorem least squares estimate parameter variance bigger linear unbiased estimate section matrix inequality holds positive semidefinite show variance covariance matrix least squares estimate variance covariance matrix linear unbiased estimate exercises show vector least squares coefficients obtained single pass gram schmidt procedure algorithm rep resent solution terms decomposition consider ridge regression problem show prob lem equivalent problem argmin give correspondence original char acterize solution modified criterion show similar result holds lasso show ridge regression estimate mean mode posterior distribution gaussian prior gaussian sampling model find relationship regularization parameter ridge formula variances assume parameters distributed independently one another assuming known show minus log posterior density proportional consider decomposition uncentered matrix whose first column ones svd centered matrix show span subspace sub matrix first column removed circumstances sign flips forward stepwise regression suppose decomposi tion matrix multiple regression problem response additional predictors matrix denote current residual wish establish one additional variables reduce residual sum squares included describe efficient procedure backward stepwise regression suppose multiple gression fit along standard errors scores table wish establish variable dropped increase residual sum squares least would show solution multivariate linear regression prob lem given happens covariance matrices different observation linear methods regression show ridge regression estimates obtained ordinary least squares regression augmented data set augment centered matrix additional rows augment zeros introducing artificial data response value zero fitting procedure forced shrink coefficients toward zero related idea hints due abu mostafa model constraints implemented adding artificial data examples satisfy derive expression show pcr show orthogonal case pls stops steps subsequent step algorithm zero verify expression hence show partial least squares directions compromise ordinary regression coef ficient principal component directions derive entries table explicit forms estimators orthogonal case repeat analysis table spam data discussed chapter read conjugate gradient algorithms murray example establish connection algorithms partial least squares show ridge increases tuning parameter property hold lasso partial least squares estimates latter consider tuning parameter successive steps algorithm consider canonical correlation problem show leading pair canonical variates solve problem max generalized svd problem show solution given leading left right singular vectors show entire sequence min also given show solution reduced rank regression problem estimated given hint transform exercises solved terms canonical vectors show generalized inverse show solution exercise change estimated natural quantity consider regression problem variables response hav ing mean zero standard deviation one suppose also variable identical absolute correlation response let least squares coefficient let vector moves fraction toward least squares fit let rss residual sum squares full least squares fit show hence correlations residuals remain equal magnitude progress toward show correlations equal rss hence decrease monotonically zero use results show lar algorithm section keeps correlations tied monotonically decreasing claimed lar directions using notation around equation page show lar direction makes equal angle predictors lar look ahead efron sec starting ginning kth step lar algorithm derive expressions identify next variable enter active set step value occurs using notation around equation page forward stepwise regression enters variable step reduces residual sum squares lar adjusts variables absolute correlation current residuals show two entry criteria necessarily hint let jth linear methods regression variable linearly adjusted variables currently model show first criterion amounts identifying cor largest magnitude lasso lar consider lasso problem lagrange multiplier form minimize fixed setting expression becomes show lagrange dual function karush kuhn tucker optimality conditions along non negativity constraints parameters lagrange multipliers show kkt conditions imply one following three scenarios hence show active predictor must assuming predictors standardized relate correlation jth predictor current residuals suppose set active predictors unchanged show vector thus lasso solution path linear ranges efron rosset zhu exercises suppose given fitted lasso coefficient variable suppose augment set variables identical copy characterize effect exact collinearity describing set solutions using value suppose run ridge regression parameter single variable get coefficient include exact copy refit ridge regression show coefficients identical derive value show general copies variable included ridge regression coefficients consider elastic net optimization problem min show one turn lasso problem using augmented version linear methods regression 
[linear, methods, classification, introduction] chapter revisit classification problem focus linear methods classification since predictor takes values dis crete set always divide input space collection regions labeled according classification saw chapter bound aries regions rough smooth depending prediction function important class procedures decision boundaries linear mean linear methods classification several different ways linear decision boundaries found chapter fit linear regression models class indicator variables classify largest fit suppose classes convenience labeled fitted linear model kth indicator response variable decision boundary class set points set affine set hyperplane since true pair classes input space divided regions constant classification piecewise hyperplanar decision boundaries regression approach member class methods model discriminant functions class classify class largest value discriminant function methods strictly speaking hyperplane passes origin affine set need sometimes ignore distinction refer general hyperplanes linear methods classification model posterior probabilities also class clearly either linear decision boundaries linear actually require monotone transformation linear decision boundaries linear example two classes popular model posterior proba bilities exp exp exp monotone transformation logit transformation log fact see log decision boundary set points log odds zero hyperplane defined discuss two popular different methods result linear log odds logits linear discriminant analysis linear logistic regression although differ derivation essential difference way linear function fit training data direct approach explicitly model boundaries classes linear two class problem dimensional input space amounts modeling decision boundary hyperplane words normal vector cut point look two methods explicitly look separating hyperplanes first well known perceptron model rosenblatt algorithm finds separating hyperplane training data one exists second method due vapnik finds optimally separating hyperplane one exists else finds hyperplane minimizes measure overlap training data treat separable case defer treatment nonseparable case chapter entire chapter devoted linear decision boundaries considerable scope generalization example expand vari able set including squares cross products thereby adding additional variables linear functions augmented space map quadratic functions original space hence linear decision boundaries quadratic decision boundaries figure illustrates idea data left plot uses linear decision boundaries two dimensional space shown right plot uses linear decision boundaries augmented five dimensional space described approach used basis transfor linear regression indicator matrix figure left plot shows data three classes linear decision boundaries found linear discriminant analysis right plot shows quadratic decision boundaries obtained finding linear boundaries five dimensional space linear inequalities space quadratic inequalities original space mation explored later chapters 
[linear, methods, classification, linear, regression, indicator, matrix] response categories coded via indicator variable thus classes indicators else collected together vector training instances form indicator response matrix matrix row single fit linear regression model columns simultaneously fit given chapter details linear regression note coeffi cient vector response column hence coefficient matrix model matrix columns corresponding inputs leading column intercept new observation input classified follows compute fitted output vector identify largest component classify accordingly argmax linear methods classification rationale approach one rather formal justification view regression estimate conditional expectation random variable conditional expectation seems sensible goal real issue good approximation conditional expectation rather rigid linear regression model alternatively reasonable estimates posterior probabilities importantly matter quite straightforward verify long intercept model column however negative greater typically consequence rigid nature linear regression especially make predictions outside hull training data violations guarantee approach work fact many problems gives similar results standard linear meth ods classification allow linear regression onto basis expansions inputs approach lead consistent estimates probabilities size training set grows bigger adaptively include basis elements linear regression onto basis func tions approaches conditional expectation discuss approaches chapter simplistic viewpoint construct targets class kth column identity matrix prediction problem try reproduce appropriate target observation coding response vector ith row observation value might fit linear model least squares min criterion sum squared euclidean distances fitted vectors targets new observation classified computing fitted vector classifying closest target argmin exactly previous approach sum squared norm criterion exactly criterion multi ple response linear regression viewed slightly differently since squared norm sum squares components decouple rearranged separate linear model element note possible nothing model binds different responses together linear regression indicator matrix linear regression linear discriminant analysis figure data come three classes easily separated linear decision boundaries right plot shows boundaries found linear discriminant analysis left plot shows boundaries found linear regres sion indicator response variables middle class completely masked never dominates closest target classification rule easily seen exactly maximum fitted component criterion require fitted values sum serious problem regression approach number classes especially prevalent large rigid nature regression model classes masked others figure illustrates extreme situation three classes perfectly separated linear decision boundaries yet linear regression misses middle class completely figure projected data onto line joining three centroids information orthogonal direction case included coded three response variables three regression lines left panel included see line corresponding middle class horizontal fitted values never dominant thus observations class classified either class class right panel uses quadratic regression rather linear regression simple example quadratic rather linear fit middle class least would solve problem however seen four rather three classes lined like quadratic would come fast enough cubic would needed well loose general rule classes lined polynomial terms degree might needed resolve note also polynomials along derived direction linear methods classification degree error degree error figure effects masking linear regression three class problem rug plot base indicates positions class membership observation three curves panel fitted regressions three class indicator variables example blue class blue blue observations green orange fits linear quadratic polynomials plot training error rate bayes error rate problem lda error rate passing centroids arbitrary orientation dimensional input space one would need general polynomial terms cross products total degree terms resolve worst case scenarios example extreme large small maskings naturally occur realistic illustration figure projection training data vowel recognition problem onto informative two dimensional subspace classes dimensions difficult classification problem best methods achieve around errors test data main point summarized table linear regression error rate close relative linear discriminant analysis error rate seems masking hurt case methods chapter based linear functions well use way avoids masking problem 
[linear, methods, classification, linear, discriminant, analysis] decision theory classification section tells need know class posteriors optimal classification suppose class conditional density class let prior probability class simple application bayes linear discriminant analysis coordinate training data coordinate training data figure two dimensional plot vowel training data eleven classes best view terms lda model section heavy circles projected mean vectors class class overlap considerable table training test error rates using variety linear techniques vowel data eleven classes ten dimensions three account variance via principal components analysis see linear regression hurt masking increasing test training error technique error rates training test linear regression linear discriminant analysis quadratic discriminant analysis logistic regression linear methods classification theorem gives see terms ability classify almost equiv alent quantity many techniques based models class densities linear quadratic discriminant analysis use gaussian densities flexible mixtures gaussians allow nonlinear decision bound aries section general nonparametric density estimates class density allow flexibility section naive bayes models variant previous case assume class densities products marginal densities assume inputs conditionally independent class section suppose model class density multivariate gaussian linear discriminant analysis lda arises special case assume classes common covariance matrix comparing two classes sufficient look log ratio see log log log log equation linear equal covariance matrices cause normal ization factors cancel well quadratic part exponents linear log odds function implies decision boundary classes set linear dimensions hyperplane course true pair classes decision boundaries linear divide regions classified class class etc regions sep arated hyperplanes figure left panel shows idealized example three classes data arise three gaus sian distributions common covariance matrix included linear discriminant analysis figure left panel shows three gaussian distributions covariance different means included contours constant density enclosing probability case bayes decision boundaries pair classes shown broken straight lines bayes decision boundaries separating three classes thicker solid lines subset former right see sample drawn gaussian distribution fitted lda decision boundaries figure contours corresponding highest probability density well class centroids notice decision boundaries perpendicular bisectors line segments joining centroids would case covariance spherical class priors equal see linear discriminant functions log equivalent description decision rule argmax practice know parameters gaussian distributions need estimate using training data number class observations figure right panel shows estimated decision boundaries based sample size three gaussian distributions figure page another example classes gaussian two classes simple correspondence linear dis criminant analysis classification linear least squares lda rule classifies class log log linear methods classification class otherwise suppose code targets two classes respectively easy show coefficient vector least squares proportional lda direction given exercise fact correspondence occurs distinct coding targets see exercise however unless intercepts different hence resulting decision rules different since derivation lda direction via least squares use gaussian assumption features applicability extends beyond realm gaussian data however derivation particular intercept cut point given require gaussian data thus makes sense instead choose cut point empirically minimizes training error given dataset something found work well practice seen mentioned literature two classes lda linear regression class indicator matrix avoids masking problems associated approach hastie correspondence regres sion lda established notion optimal scoring discussed section getting back general discriminant problem assumed equal convenient cancellations occur particular pieces quadratic remain get quadratic discriminant functions qda log log decision boundary pair classes described quadratic equation figure shows example figure page three classes gaussian mixtures section decision bound aries approximated quadratic equations illustrate two popular ways fitting quadratic boundaries right plot uses qda described left plot uses lda enlarged five dimensional quadratic polynomial space differences generally small qda preferred approach lda method convenient substitute estimates qda similar lda except separate covariance matrices must estimated class large mean dramatic increase parameters since decision boundaries functions parameters densities counting number parameters must done care lda seems parameters since need differences figure many similar figures book compute decision bound aries exhaustive contouring method compute decision rule fine lattice points use contouring algorithms compute boundaries linear discriminant analysis figure two methods fitting quadratic boundaries left plot shows quadratic decision boundaries data figure obtained using lda five dimensional space right plot shows quadratic decision boundaries found qda differences small usually case discriminant functions pre chosen class chosen last difference requires parameters likewise qda parameters lda qda perform well amazingly large diverse set classification tasks example statlog project michie lda among top three classifiers datasets qda among top three four datasets one pair top three datasets techniques widely used entire books devoted lda seems whatever exotic tools rage day always available two simple tools question arises lda qda good track record reason likely data approximately gaussian addition lda covariances approximately equal likely reason data support simple decision boundaries linear quadratic estimates provided via gaussian models stable bias variance tradeoff put bias linear decision boundary estimated much lower variance exotic alternatives argument less believable qda since many parameters although perhaps fewer non parametric alternatives although fit covariance matrix compute lda discriminant functions much reduced function required estimate parameters needed compute decision boundaries linear methods classification misclassification rate regularized discriminant analysis vowel data test data train data figure test training errors vowel data using regularized discriminant analysis series values optimum test data occurs around close quadratic discriminant analysis 
[linear, methods, classification, linear, discriminant, analysis, regularized, discriminant, analysis] friedman proposed compromise lda qda allows one shrink separate covariances qda toward common covariance lda methods similar flavor ridge regression regularized covariance matrices form pooled covariance matrix used lda allows continuum models lda qda needs specified practice chosen based performance model validation data cross validation figure shows results rda applied vowel data training test error improve increasing although test error increases sharply large discrepancy training test error partly due fact many repeat measurements small number individuals different training test set similar modifications allow shrunk toward scalar covariance replacing leads general family covariances indexed pair parameters chapter discuss regularized versions lda suitable data arise digitized analog signals images linear discriminant analysis situations features high dimensional correlated lda coefficients regularized smooth sparse original domain signal leads better generalization allows easier interpretation coefficients chapter also deal high dimensional problems example features gene expression measurements microarray studies methods focus case severely regularized versions lda 
[linear, methods, classification, linear, discriminant, analysis, computations, lda] lead next topic briefly digress computations required lda especially qda computations simplified diagonalizing latter suppose compute eigen decomposition orthonormal diagonal matrix positive eigenvalues ingredients log log light computational steps outlined lda classifier implemented following pair steps sphere data respect common covariance estimate udu common covariance esti mate identity classify closest class centroid transformed space modulo effect class prior probabilities 
[linear, methods, classification, linear, discriminant, analysis, reduced-rank, linear, discriminant, analysis] far discussed lda restricted gaussian classifier part popularity due additional restriction allows view informative low dimensional projections data centroids dimensional input space lie affine subspace dimension much larger con siderable drop dimension moreover locating closest centroid ignore distances orthogonal subspace since contribute equally class thus might well project onto centroid spanning subspace make distance comparisons thus fundamental dimension reduction lda namely need consider data subspace dimension linear methods classification instance could allow view data two dimensional plot color coding classes would relinquished information needed lda classification might ask dimensional subspace optimal lda sense fisher defined optimal mean projected centroids spread much possible terms variance amounts finding principal component subspaces centroids principal components described briefly section detail section figure shows optimal two dimensional subspace vowel data eleven classes different vowel sound ten dimensional input space centroids require full space case since shown optimal two dimensional subspace dimensions ordered compute additional dimensions sequence figure shows four additional pairs coordinates also known canonical discriminant variables summary finding sequences optimal subspaces lda involves following steps compute matrix class centroids common covariance matrix within class covariance compute using eigen decomposition compute covariance matrix class covari ance eigen decomposition columns sequence first last define coordinates optimal subspaces combining operations th discriminant variable given fisher arrived decomposition via different route without refer ring gaussian distributions posed problem find linear combination class variance maximized relative within class variance class variance variance class means within class variance pooled variance means figure shows criterion makes sense although direction joining centroids separates means much possible max imizes class variance considerable overlap projected classes due nature covariances taking covariance account well direction minimum overlap found class variance within class variance defined earlier covariance matrix class centroid matrix note total covariance matrix ignoring class information linear discriminant analysis coordinate coordinate figure four projections onto pairs canonical variates notice rank canonical variates increases centroids become less spread lower right panel appear superimposed classes confused linear methods classification figure although line joining centroids defines direction greatest centroid spread projected data overlap covariance left panel discriminant direction minimizes overlap gaussian data right panel fisher problem therefore amounts maximizing rayleigh quotient max equivalently max subject generalized eigenvalue problem given largest eigenvalue hard show exercise optimal identical defined similarly one find next direction orthogonal maximized solution referred discriminant coordinates confused discriminant functions also referred canonical variates since alternative derivation results canonical correlation analysis indicator response matrix predictor matrix line pursued section summarize developments far gaussian classification common covariances leads linear deci sion boundaries classification achieved sphering data respect classifying closest centroid modulo log sphered space since relative distances centroids count one con fine data subspace spanned centroids sphered space subspace decomposed successively optimal subspaces term centroid separation decomposition iden tical decomposition due fisher linear discriminant analysis dimension misclassification rate lda dimension reduction vowel data test data train data figure training test error rates vowel data function dimension discriminant subspace case best error rate dimension figure shows decision boundaries space reduced subspaces motivated data reduction viewing tool also used classification rationale clearly original derivation simply limit distance centroid calculations chosen subspace one show gaussian classification rule additional restriction centroids gaussians lie dimensional subspace fitting model maximum likelihood constructing posterior probabilities using bayes theorem amounts classification rule described exercise gaussian classification dictates log correction factor dis tance calculation reason correction seen figure misclassification rate based area overlap two densities equal implicit figure optimal cut point midway projected means equal moving cut point toward smaller class improve error rate mentioned earlier two classes one derive linear rule using lda method choose cut point minimize misclassification error training data example benefit reduced rank restriction return vowel data classes variables hence possible dimensions classifier compute training test error hierarchical subspaces figure shows results figure shows decision boundaries classifier based two dimensional lda solution close connection fisher reduced rank discriminant analysis regression indicator response matrix turns linear methods classification decision boundaries vowel training data two mensional subspace spanned first two canonical variates note higher dimensional subspace decision boundaries higher dimensional affine planes could represented lines logistic regression lda amounts regression followed eigen decomposition case two classes single discriminant variable identical scalar multiplication either columns connections developed chapter related fact one transforms original predictors lda using identical lda original space exercise 
[linear, methods, classification, logistic, regression] logistic regression model arises desire model posterior probabilities classes via linear functions time ensuring sum one remain model form log log log model specified terms log odds logit transformations reflecting constraint probabilities sum one although model uses last class denominator odds ratios choice denominator arbitrary estimates equivariant choice simple calculation shows exp exp exp clearly sum one emphasize dependence entire rameter set denote probabilities model especially simple since single linear function widely used biostatistical applications binary responses two classes occur quite frequently example patients survive die heart disease condition present absent linear methods classification 
[linear, methods, classification, logistic, regression, fitting, logistic, regression, models] logistic regression models usually fit maximum likelihood using conditional likelihood given since completely specifies conditional distribution multinomial distribution appropriate log likelihood observations log discuss detail two class case since algorithms simplify considerably convenient code two class via response let log likelihood written log log log assume vector inputs includes constant term accommodate intercept maximize log likelihood set derivatives zero score equations equations nonlinear notice since first compo nent first score equation specifies expected number class ones matches observed number hence also class twos solve score equations use newton raphson algo rithm requires second derivative hessian matrix starting old single newton update new old derivatives evaluated old logistic regression convenient write score hessian matrix notation let denote vector values matrix values vector fitted probabilities ith element old diagonal matrix weights ith diagonal element old old newton step thus new old old second third line expressed newton step weighted least squares step response old sometimes known adjusted response equations get solved peatedly since iteration changes hence algorithm referred iteratively reweighted least squares irls since iteration solves weighted least squares problem new arg min seems good starting value iterative procedure although convergence never guaranteed typically algorithm converge since log likelihood concave overshooting occur rare cases log likelihood decreases step size halving guarantee convergence multiclass case newton algorithm also pressed iteratively reweighted least squares algorithm vector responses nondiagonal weight matrix per observation latter precludes simplified algorithms case numer ically convenient work expanded vector directly ercise alternatively coordinate descent methods section used maximize log likelihood efficiently package glmnet friedman fit large logistic regression problems ficiently although designed fit regularized models options allow unregularized fits logistic regression models used mostly data analysis infer ence tool goal understand role input variables linear methods classification table results logistic regression fit south african heart disease data coefficient std error score intercept sbp tobacco ldl famhist obesity alcohol age explaining outcome typically many models fit search parsimonious model involving subset variables possibly interactions terms following example illustrates issues involved 
[linear, methods, classification, logistic, regression, example, south, african, heart, disease] present analysis binary data illustrate traditional statistical use logistic regression model data figure subset coronary risk factor study coris baseline survey carried three rural areas western cape south africa rousseauw aim study establish intensity ischemic heart disease risk factors high incidence region data represent white males response variable presence absence myocardial infarction time survey overall prevalence region cases data set sample controls data described detail hastie tibshirani fit logistic regression model maximum likelihood giving results shown table summary includes scores coefficients model coefficients divided standard errors nonsignificant score suggests coefficient dropped model correspond formally test null hypothesis coefficient question zero others also known wald test score greater approximately absolute value significant level surprises table coefficients must terpreted caution systolic blood pressure sbp significant obesity sign negative confusion result corre lation set predictors sbp obesity significant positive sign however presence many logistic regression sbp ooo ooo ooo ooo ooo ooo ooo tobacco oooo ooo ooo ooo ldl ooo ooo ooo ooo ooo famhist ooo ooo ooo ooo ooo ooo ooo ooo ooo ooo obesity ooo ooo ooo ooo ooo ooo ooo ooo alcohol oooo ooo age figure scatterplot matrix south african heart disease data plot shows pair risk factors cases controls color coded red case variable family history heart disease famhist binary yes linear methods classification table results stepwise logistic regression fit south african heart disease data coefficient std error score intercept tobacco ldl famhist age correlated variables longer needed even get negative sign stage analyst might model selection find subset variables sufficient explaining joint effect prevalence chd one way proceed drop least significant efficient refit model done repeatedly terms dropped model gave model shown table better time consuming strategy refit models one variable removed perform analysis deviance decide variable exclude residual deviance fitted model minus twice log likelihood deviance two models difference individual residual deviances analogy sums squares strategy gave final model one interpret coefficient std error tobacco example tobacco measured total lifetime usage kilo grams median controls cases thus increase lifetime tobacco usage accounts increase odds coronary heart disease exp incorporat ing standard error get approximate confidence interval exp return data chapter see variables nonlinear effects modeled appropriately excluded model 
[linear, methods, classification, logistic, regression, quadratic, approximations, inference] maximum likelihood parameter estimates satisfy self consistency relationship coefficients weighted least squares fit responses logistic regression weights depending apart providing convenient algorithm connection least squares offer weighted residual sum squares familiar pearson chi square statistic quadratic approximation deviance asymptotic likelihood theory says model correct consistent converges true central limit theorem shows distribution con verges asymptotics rived directly weighted least squares fit mimicking normal theory inference model building costly logistic regression models model fitted requires iteration popular shortcuts rao score test tests inclusion term wald test used test exclusion term neither require iterative fitting based maximum likelihood fit current model turns amount adding dropping term weighted least squares fit using weights computations done efficiently without recomputing entire weighted least squares fit software implementations take advantage connections example generalized linear modeling software includes gistic regression part binomial family models exploits fully glm generalized linear model objects treated linear model objects tools available linear models applied auto matically 
[linear, methods, classification, logistic, regression, regularized, logistic, regression] penalty used lasso section used variable selection shrinkage linear regression model logistic gression would maximize penalized version max log    lasso typically penalize intercept term stan dardize predictors penalty meaningful criterion linear methods classification concave solution found using nonlinear programming meth ods koh example alternatively using quadratic approximations used newton algorithm section solve repeated application weighted lasso algorithm interestingly score equations see variables non zero coefficients form sign generalizes section active variables tied generalized correlation residuals path algorithms lar lasso difficult coefficient profiles piecewise smooth rather linear nevertheless progress made using quadratic approximations obesity alcohol sbp tobacco ldl famhist age effi figure regularized logistic regression coefficients south african heart disease data plotted function norm variables standardized unit variance profiles computed exactly plotted points figure shows regularization path south african heart disease data section produced using package glmpath park hastie uses predictor corrector methods convex optimization identify exact values active set non zero coefficients changes vertical lines figure profiles look almost linear examples curvature visible coordinate descent methods section efficient comput ing coefficient profiles grid values package glmnet logistic regression friedman fit coefficient paths large logistic gression problems efficiently large algorithms exploit sparsity predictor matrix allows even larger problems see section details discussion regularized multi nomial models 
[linear, methods, classification, logistic, regression, logistic, regression, lda] section find log posterior odds class linear functions log log linearity consequence gaussian assumption class densities well assumption common covariance matrix linear logistic model construction linear logits log seems models although exactly form difference lies way linear coefficients estimated logistic regression model general makes less assumptions write joint density denotes marginal density inputs lda logistic regression second term right logit linear form arbitrarily chosen last class reference logistic regression model leaves marginal density arbi trary density function fits parameters max imizing conditional likelihood multinomial likelihood proba bilities although totally ignored think marginal density estimated fully nonparametric unrestricted fashion using empirical distribution function places mass observation lda fit parameters maximizing full log likelihood based joint density linear methods classification gaussian density function standard normal theory leads easily estimates given section since linear parameters logistic form functions gaussian param eters get maximum likelihood estimates plugging corre sponding estimates however unlike conditional case marginal density play role mixture density also involves parameters role additional component restriction play relying additional model assumptions information parameters hence estimate efficiently lower variance fact true gaussian worst case ignoring marginal part likelihood constitutes loss efficiency asymptotically error rate efron paraphrasing data conditional likelihood well example observations far decision boundary weighted logistic regression play role estimating common covariance matrix good news also means lda robust gross outliers mixture formulation clear even observations without class labels information parameters often expensive generate class labels unclassified observations come cheaply relying strong model assumptions use types information marginal likelihood thought regularizer requiring sense class densities visible marginal view example data two class logistic regression model per fectly separated hyperplane maximum likelihood estimates parameters undefined infinite see exercise lda coeffi cients data well defined since marginal likelihood permit degeneracies practice assumptions never correct often components qualitative variables generally felt logistic regression safer robust bet lda model relying fewer assumptions experience models give similar results even lda used inappropriately qualitative predictors separating hyperplanes figure toy example two classes separable hyperplane orange line least squares solution misclassifies one training points also shown two blue separating hyperplanes found perceptron learning algorithm different random starts 
[linear, methods, classification, separating, hyperplanes] seen linear discriminant analysis logistic regression estimate linear decision boundaries similar slightly different ways rest chapter describe separating hyperplane classifiers procedures construct linear decision boundaries explicitly try separate data different classes well possible provide basis support vector classifiers discussed chapter math ematical level section somewhat higher previous sections figure shows data points two classes data separated linear boundary included figure blue lines two infinitely many possible separating hyperplanes orange line least squares solution problem obtained regressing response intercept line given least squares solution perfect job separating points makes one error boundary found lda light equivalence linear regression two class case sec tion exercise classifiers compute linear combination input features return sign called perceptrons engineering liter linear methods classification figure linear algebra hyperplane affine set ature late rosenblatt perceptrons set foundations neural network models continue let digress slightly review vector algebra figure depicts hyperplane affine set defined equation since line list properties two points lying hence vector normal surface point signed distance point given kk hence proportional signed distance hyperplane defined 
[linear, methods, classification, separating, hyperplanes, rosenblatts, perceptron, learning, algorithm] perceptron learning algorithm tries find separating hyperplane minimizing distance misclassified points decision boundary separating hyperplanes response misclassified opposite misclassified response goal minimize indexes set misclassified points quantity non negative proportional distance misclassified points decision boundary defined gradient assuming fixed given algorithm fact uses stochastic gradient descent minimize piecewise linear criterion means rather computing sum gradient contributions observation followed step negative gradient direction step taken observation visited hence misclassified observations visited sequence parameters updated via learning rate case taken without loss generality classes linearly separable shown algorithm converges separating hyperplane finite number steps exercise figure shows two solutions toy problem started different random guess number problems algorithm summarized ripley data separable many solutions one found depends starting values finite number steps large smaller gap longer time find data separable algorithm converge cycles develop cycles long therefore hard detect second problem often eliminated seeking hyperplane original space much enlarged space obtained creating linear methods classification many basis function transformations original variables anal ogous driving residuals polynomial regression problem zero making degree sufficiently large perfect separation cannot always achieved example observations two different classes share input may desirable either since resulting model likely overfit generalize well return point end next section rather elegant solution first problem add additional con straints separating hyperplane 
[linear, methods, classification, separating, hyperplanes, optimal, separating, hyperplanes] optimal separating hyperplane separates two classes maximizes distance closest point either class vapnik provide unique solution separating hyperplane problem maximizing margin two classes training data leads better classification performance test data need generalize criterion consider optimization problem max subject set conditions ensure points least signed distance decision boundary defined seek largest associated parameters get rid constraint replacing conditions redefines equivalently since satisfying inequalities positively scaled multiple satisfies arbitrarily set thus equivalent min subject light constraints define empty slab margin around linear decision boundary thickness hence choose maximize thickness convex optimization problem quadratic separating hyperplanes criterion linear inequality constraints lagrange primal func tion minimized setting derivatives zero obtain substituting obtain called wolfe dual subject solution obtained maximizing positive orthant sim pler convex optimization problem standard software used addition solution must satisfy karush kuhn tucker conditions include see words boundary slab boundary slab see solution vector defined terms linear combination support points points defined boundary slab via figure shows optimal separating hyperplane toy example three support points likewise obtained solving support points optimal separating hyperplane produces function classifying new observations sign although none training observations fall margin con struction necessarily case test observations linear methods classification figure data figure shaded region delineates maximum margin separating two classes three support points indicated lie boundary margin optimal separating hyperplane blue line bisects slab included figure boundary found using logistic regression red line close optimal separating hyperplane see section intuition large margin training data lead good separation test data description solution terms support points seems sug gest optimal hyperplane focuses points count robust model misspecification lda solution hand depends data even points far away cision boundary note however identification support points required use data course classes really gaussian lda optimal separating hyperplanes pay price focusing noisier data boundaries classes included figure logistic regression solution prob lem fit maximum likelihood solutions similar case separating hyperplane exists logistic regression always find since log likelihood driven case exercise logistic regression solution shares qualitative features separating hyperplane solution coefficient vector defined weighted least squares fit zero mean linearized response input features weights larger points near decision boundary away data separable feasible solution problem alternative formulation needed one large space using basis transformations lead artificial exercises separation fitting chapter discuss attractive alternative known support vector machine allows overlap minimizes measure extent overlap 
[linear, methods, classification, bibliographic, notes] good general texts classification include duda hand mclachlan ripley mardia concise discussion linear discriminant analysis michie compare large number popular classifiers benchmark datasets lin ear separating hyperplanes discussed vapnik account perceptron learning algorithm follows ripley 
[linear, methods, classification, exercises] show solve generalized eigenvalue problem max subject transforming standard eigenvalue problem suppose features two class response class sizes target coded show lda rule classifies class log log class otherwise consider minimization least squares criterion show solution satisfies simplification hence show direction thus therefore least squares regression coefficient identical lda coefficient scalar multiple linear methods classification show result holds distinct coding two classes find solution hence predicted values consider following rule classify class class otherwise show lda rule unless classes equal numbers observations fisher ripley suppose transform original predictors via linear regression detail let indicator response matrix similarly input get trans formed vector show lda using identical lda original space consider multilogit model classes let vector consisting coefficients define suitably enlarged version input vector accommodate vectorized efficient matrix derive newton raphson algorithm maximizing multinomial log likelihood describe would implement algorithm consider two class logistic regression problem char acterize maximum likelihood estimates slope intercept rameter sample two classes separated point generalize result see figure two classes suppose points general position class labels prove perceptron learning algorithm converges separating hyperplane finite number steps denote hyperplane compact notation let show separability implies existence sep sep given current old perceptron algorithm identifies point misclassified produces update new old show new sep old sep hence algorithm converges separating hyperplane start sep steps ripley consider criterion exercises generalization sum observations consider minimizing subject describe criterion words solve optimal separating hyperplane problem consider multivariate gaussian model additional restriction rank max derive constrained mles show bayes clas sification rule equivalent classifying reduced subspace computed lda hastie tibshirani write computer program perform quadratic discriminant analysis fitting separate gaussian model per class try vowel data compute misclassification error test data data found book website www stat stanford edu elemstatlearn linear methods classification 
[basis, expansions, regularization, introduction] already made use models linear input features regression classification linear regression linear discriminant analysis logistic regression separating hyperplanes rely linear model extremely unlikely true function actually linear regression problems typically nonlinear nonadditive representing linear model usually con venient sometimes necessary approximation convenient linear model easy interpret first order taylor approxima tion sometimes necessary small large linear model might able fit data without overfit ting likewise classification linear bayes optimal decision boundary implies monotone transformation linear inevitably approximation chapter next discuss popular methods moving beyond linearity core idea chapter augment replace vector inputs additional variables transformations use linear models new space derived input features denote mth transformation model basis expansions regularization linear basis expansion beauty approach basis functions determined models linear new variables fitting proceeds simple widely used examples following recovers original linear model allows augment inputs polynomial terms achieve higher order taylor expansions note however number variables grows exponentially gree polynomial full quadratic model variables requires square cross product terms generally degree polynomial log permits nonlinear transformations single inputs generally one use similar functions involv ing several inputs indicator region breaking range nonoverlapping regions results model piecewise constant contribution sometimes problem hand call particular basis functions logarithms power functions often however use basis expansions device achieve flexible representations polynomials example latter although limited global nature tweaking coefficients achieve functional form one region cause function flap madly remote regions chapter consider useful families piecewise polynomials splines allow local polynomial representations also discuss wavelet bases especially useful modeling signals images methods produce dictionary consisting typically large number basis functions far afford fit data along dictionary require method controlling complexity model using basis functions dictionary three common approaches restriction methods decide hand limit class functions additivity example assume model form piecewise polynomials splines size model limited number basis functions used component function selection methods adaptively scan dictionary include basis functions contribute significantly fit model variable selection techniques discussed chap ter useful stagewise greedy approaches cart mars boosting fall category well regularization methods use entire dictionary strict coefficients ridge regression simple example regu larization approach lasso regularization selec tion method discuss sophisticated methods regularization 
[basis, expansions, regularization, piecewise, polynomials, splines] assume section one dimensional piecewise poly nomial function obtained dividing domain contigu ous intervals representing separate polynomial interval figure shows two simple piecewise polynomials first piecewise constant three basis functions since positive disjoint regions least squares estimate model amounts mean mth region top right panel shows piecewise linear fit three additional basis functions needed except special cases would typically prefer third panel also piecewise linear restricted continuous two knots continu ity restrictions lead linear constraints parameters example implies case since two restrictions expect get back two parameters leaving four free parameters direct way proceed case use basis incorpo rates constraints denotes positive part function shown lower right panel figure often prefer smoother functions achieved increasing order local polynomial figure shows series piecewise cubic polynomials fit data basis expansions regularization piecewise constant piecewise linear continuous piecewise linear piecewise linear basis function figure top left panel shows piecewise constant function fit artificial data broken vertical lines indicate positions two knots blue curve represents true function data generated gaussian noise remaining two panels show piecewise lin ear functions fit data top right unrestricted lower left restricted continuous knots lower right panel shows piecewise linear basis function continuous black points indicate sample evaluations piecewise polynomials splines discontinuous continuous continuous first derivative continuous second derivative piecewise cubic polynomials figure series piecewise cubic polynomials increasing orders continuity increasing orders continuity knots function lower right panel continuous continuous first second derivatives knots known cubic spline enforcing one order continuity would lead global cubic polynomial hard show exercise following basis represents cubic spline knots six basis functions corresponding six dimensional linear space functions quick check confirms parameter count regions parameters per region knots constraints per knot basis expansions regularization generally order spline knots piecewise polynomial order continuous derivatives order cubic spline fact piecewise constant function figure order spline continuous piece wise linear function order spline likewise general form truncated power basis set would claimed cubic splines lowest order spline knot discontinuity visible human eye seldom good reason beyond cubic splines unless one interested smooth derivatives practice widely used orders fixed knot splines also known regression splines one needs select order spline number knots placement one simple approach parameterize family splines number basis functions degrees freedom observations termine positions knots example expression generates basis matrix cubic spline functions evaluated observations interior knots appropriate per centiles one explicit however degree knots generates basis linear splines three interior knots returns matrix since space spline functions particular order knot sequence vector space many equivalent bases representing ordinary polynomials truncated power basis conceptually simple attractive numerically powers large numbers lead severe rounding problems spline basis described appendix chapter allows efficient computations even number knots large 
[basis, expansions, regularization, piecewise, polynomials, splines, natural, cubic, splines] know behavior polynomials fit data tends erratic near boundaries extrapolation dangerous problems exacerbated splines polynomials fit beyond boundary knots behave even wildly corresponding global polynomials region conveniently summarized terms point wise variance spline functions fit least squares see example next section details variance calculations figure compares cubic spline four knots eight dimensional function omits default constant term basis since terms like typically included terms model piecewise polynomials splines pointwise variances global linear global cubic polynomial cubic spline knots natural cubic spline knots figure pointwise variance curves four different models con sisting points drawn random assumed error model constant variance linear cubic polynomial fits two four degrees freedom respectively cubic spline natural cubic spline six degrees freedom cubic spline two knots natural spline boundary knots four interior knots uniformly spaced pointwise variances variety different models explosion variance near boundaries clear inevitably worst cubic splines natural cubic spline adds additional constraints namely func tion linear beyond boundary knots frees four degrees freedom two constraints boundary regions spent profitably sprinkling knots interior region tradeoff illustrated terms variance figure price paid bias near boundaries assuming function lin ear near boundaries less information anyway often considered reasonable natural cubic spline knots represented basis functions one start basis cubic splines derive reduced sis imposing boundary constraints example starting truncated power series basis described section arrive exer cise basis expansions regularization basis functions seen zero second third derivative 
[basis, expansions, regularization, piecewise, polynomials, splines, example, south, african, heart, disease, continued] section fit linear logistic regression models south african heart disease data explore nonlinearities functions using natural splines functional form model logit chd vectors coefficients multiplying associated vector natural spline basis functions use four natural spline bases term model example representing sbp basis consisting four basis func tions actually implies three rather two interior knots chosen uniform quantiles sbp plus two boundary knots extremes data since exclude constant term since famhist two level factor coded simple binary dummy variable associated single coefficient fit model compactly combine vectors basis functions constant term one big vector model simply total number parameters sum parameters component term basis function evaluated samples resulting basis matrix point model like linear logistic model algorithms described section apply carried backward stepwise deletion process dropping terms model preserving group structure term rather dropping one coefficient time aic statistic section used drop terms terms remaining final model would cause aic increase deleted model see table figure shows plot final model selected stepwise regression functions displayed variable covariance matrix cov estimated diagonal weight matrix logistic regression hence var pointwise variance function cov appropriate sub matrix shaded region panel defined aic statistic slightly generous likelihood ratio test deviance test sbp obesity included model piecewise polynomials splines absent present sbp tobacco ldl obesity age famhist figure fitted natural spline functions terms final model selected stepwise procedure included pointwise standard error bands rug plot base figure indicates location sample values variable jittered break ties basis expansions regularization table final logistic regression model stepwise deletion natural splines terms column labeled lrt likelihood ratio test statistic term deleted model change deviance full model labeled none terms deviance aic lrt value none sbp tobacco ldl famhist obesity age linear model figure explains since contributions inherently nonlinear effects first may come surprise explanation lies nature retrospective data measurements made sometime patients suffered heart attack many cases already benefited healthier diet lifestyle hence apparent increase risk low values obesity sbp table shows summary selected model 
[basis, expansions, regularization, piecewise, polynomials, splines, example, phoneme, recognition] example use splines reduce flexibility rather increase application comes general heading functional modeling top panel figure displayed sample log periodograms two phonemes measured frequencies goal use data classify spoken phoneme two phonemes chosen difficult separate input feature vector length think vector evaluations function grid frequencies reality continuous analog signal function frequency sampled version gray lines lower panel figure show coefficients linear logistic regression model fit maximum likelihood training sample drawn total coefficients also plotted function frequency fact think model terms continuous counterpart log piecewise polynomials splines frequency log periodogram phoneme examples frequency logistic regression coefficients phoneme classification raw restricted logistic regression figure top panel displays log periodogram function fre quency examples phonemes sampled total log periodogram measured uniformly spaced frequencies lower panel shows coefficients function fre quency logistic regression fit data maximum likelihood using log periodogram values inputs coefficients restricted smooth red curve unrestricted jagged gray curve basis expansions regularization approximate coefficients compute contrast functional appreciable values regions frequency log periodograms differ two classes gray curves rough since input signals fairly strong positive autocorrelation results negative autocorrelation efficients addition sample size effectively provides four obser vations per coefficient applications permit natural regularization force coefficients vary smoothly function frequency red curve lower panel figure shows smooth coefficient curve fit data see lower frequencies offer discriminatory power smoothing allow easier interpretation contrast also produces accurate classifier raw regularized training error test error smooth red curve obtained simple use natural cubic splines represent coefficient function expansion splines practice means basis matrix natural cubic splines defined set frequencies used basis functions knots uniformly placed integers representing frequencies since simply replace input features filtered versions fit linear logistic regression red curve thus 
[basis, expansions, regularization, filtering, feature, extraction] previous example constructed basis matrix transformed features new features filtered versions features used inputs learning procedure previous example linear logistic regression preprocessing high dimensional features general pow erful method improving performance learning algorithm preprocessing need linear general smoothing splines nonlinear function form derived features used inputs linear nonlinear learning procedure example signal image recognition popular approach first transform raw features via wavelet transform section use features inputs neural network chapter wavelets effective capturing discrete jumps edges neural network powerful tool constructing nonlinear functions features predicting target variable using domain knowledge construct appropriate features one often improve upon learning method raw features disposal 
[basis, expansions, regularization, smoothing, splines] discuss spline basis method avoids knot selection prob lem completely using maximal set knots complexity fit controlled regularization consider following problem among functions two continuous derivatives find one minimizes penalized residual sum squares rss fixed smoothing parameter first term measures closeness data second term penalizes curvature function establishes tradeoff two two special cases function interpolates data simple least squares line fit since second derivative tolerated vary rough smooth hope indexes interesting class functions criterion defined infinite dimensional function space fact sobolev space functions second term defined remarkably shown explicit finite dimensional unique minimizer natural cubic spline knots unique values exercise face value seems family still parametrized since many knots implies degrees freedom however penalty term translates penalty spline coefficients shrunk way toward linear fit since solution natural spline write basis expansions regularization age relative change spinal bmd male female figure response relative change bone mineral density mea sured spine adolescents function age separate smoothing spline fit males females choice corresponds degrees freedom dimensional set basis functions repre senting family natural splines section exercise criterion thus reduces rss solution easily seen generalized ridge regression fitted smoothing spline given efficient computational techniques smoothing splines discussed appendix chapter figure shows smoothing spline fit data bone mineral density bmd adolescents response relative change spinal bmd two consecutive visits typically one year apart data color coded gender two separate curves fit simple smoothing splines summary reinforces evidence data growth spurt females precedes males two years cases smoothing parameter approximately choice discussed next section 
[basis, expansions, regularization, smoothing, splines, degrees, freedom, smoother, matrices] yet indicated chosen smoothing spline later chapter describe automatic methods using techniques cross validation section discuss intuitive ways prespecifying amount smoothing smoothing spline prechosen example linear smoother linear operator estimated parameters linear combination denote vector fitted values training predictors fit linear finite linear operator known smoother matrix one consequence linearity recipe producing depend depends linear operators familiar traditional least squares fitting well suppose matrix cubic spline basis functions evaluated training points knot sequence vector fitted spline values given linear operator projection operator also known hat matrix statistics important similarities differences symmetric positive semidefinite matrices idempotent meaning right hand side exceeds left hand side positive semidefinite matrix consequence shrinking nature discuss rank rank expression trace gives dimension projection space also number basis functions hence number rameters involved fit analogy define effective degrees basis expansions regularization freedom smoothing spline trace sum diagonal elements useful definition allows intuitive way parameterize smoothing spline indeed many smoothers well consistent fashion example fig ure specified curves corresponding derived numerically solving trace many arguments supporting definition degrees freedom cover since symmetric positive semidefinite real eigen decomposition proceed convenient rewrite reinsch form depend exercise since solves min known penalty matrix indeed quadratic form representation terms weighted sum squared divided second differences eigen decomposition corresponding eigenvalue figure top shows sults applying cubic smoothing spline air pollution data observations two fits given smoother fit corresponding larger penalty rougher fit smaller penalty lower panels repre sent eigenvalues lower left eigenvectors lower right corresponding smoother matrices highlights eigenrep resentation following eigenvectors affected changes hence whole family smoothing splines particular sequence indexed eigenvectors hence smoothing spline oper ates decomposing complete basis differ entially shrinking contributions using con trasted basis regression method components smoothing splines daggot pressure gradient ozone concentration order eigenvalues figure top smoothing spline fit ozone concentration versus daggot pressure gradient two fits correspond different values smoothing parameter chosen achieve five eleven effective degrees freedom defined trace lower left first eigenvalues two smoothing spline matrices first two exactly lower right third sixth eigenvectors spline smoother matrices case plotted viewed function rug base plots indicate occurrence data points damped functions represent smoothed versions functions using smoother basis expansions regularization either left alone shrunk zero projection matrix eigenvalues equal rest reason smoothing splines referred shrinking smoothers regression splines projection smoothers see figure page sequence ordered decreasing appear increase complexity indeed zero crossing behavior polyno mials increasing degree since see eigenvectors shrunk smoothing spline higher complexity shrunk domain periodic sines cosines different frequencies first two eigenvalues always one correspond two dimensional eigenspace functions linear exercise never shrunk eigenvalues inverse function eigenvalues penalty matrix moderated controls rate decrease zero linear functions penalized one reparametrize smoothing spline using basis vectors demmler reinsch basis case smoothing spline solves min uk columns diagonal matrix elements trace projection smoothers eigenvalues one corresponding dimension pro jection subspace figure depicts smoothing spline matrix rows ordered banded nature representation suggests smoothing spline local fitting method much like locally weighted regression procedures chapter right panel shows detail selected rows call equivalent kernels dimensional identity matrix hat matrix linear regression 
[basis, expansions, regularization, automatic, selection, smoothing, parameters] smoothing parameters regression splines encompass degree splines number placement knots smoothing automatic selection smoothing parameters smoother matrix row row row row row row equivalent kernels figure smoother matrix smoothing spline nearly banded indicating equivalent kernel local support left panel represents elements image right panel shows equivalent kernel weight ing function detail indicated rows basis expansions regularization splines penalty parameter select since knots unique training cubic degree almost always used practice selecting placement number knots regression splines combinatorially complex task unless simplifications enforced mars procedure chapter uses greedy algorithm additional approximations achieve practical compromise discuss 
[basis, expansions, regularization, automatic, selection, smoothing, parameters, fixing, degrees, freedom] since trace monotone smoothing splines vert relationship specify fixing practice achieved simple numerical methods example one use smooth spline specify amount smoothing encour ages traditional mode model selection might try cou ple different values select one based approximate tests residual plots subjective criteria using way pro vides uniform approach compare many different smoothing methods particularly useful generalized additive models chapter several smoothing methods simultaneously used one model 
[basis, expansions, regularization, automatic, selection, smoothing, parameters, biasvariance, tradeoff] figure shows effect choice using smoothing spline simple example sin training sample consists pairs drawn independently model fitted splines three different values shown yellow shaded region figure represents pointwise standard error shaded region since cov cov diagonal contains pointwise variances training bias given bias automatic selection smoothing parameters epe cross validation epe figure top left panel shows epe curves realization nonlinear additive error model remaining panels show data true functions purple fitted curves green yellow shaded standard error bands three different values basis expansions regularization unknown vector evaluations true training expectations variances respect repeated draws samples size model similar fashion var bias computed point exer cise three fits displayed figure give visual demonstration bias variance tradeoff associated selecting smoothing parameter spline fits clearly trims hills fills valleys leads bias dramatic regions high curvature standard error band narrow esti mate badly biased version true function great reliability fitted function close true function although slight amount bias seems evident variance increased appreciably fitted function somewhat wiggly close true function wiggliness also accounts increased width standard error bands curve starting follow individual points closely note figures seeing single realization data hence fitted spline case bias involves expectation leave exercise compute similar figures bias shown well middle curve seems right achieved good compromise bias variance integrated squared prediction error epe combines bias variance single summary epe var bias var mse note averaged training sample giving rise values independently chosen prediction points epe natural quantity interest create tradeoff bias variance blue points top left panel figure suggest spot since know true function access epe need estimate topic discussed detail chapter techniques fold cross validation gcv common use figure include fold leave one cross validation curve nonparametric logistic regression remarkably computed value original fitted values diagonal elements exercise epe curves similar shape entire curve epe curve realizations reversed overall curve approximately unbiased estimate epe curve 
[basis, expansions, regularization, nonparametric, logistic, regression] smoothing spline problem section posed regression setting typically straightforward transfer technology domains consider logistic regression single quantitative input model log implies fitting smooth fashion leads smooth estimate condi tional probability used classification risk scoring construct penalized log likelihood criterion log log log abbreviated first term pression log likelihood based binomial distribution chap ter page arguments similar used section show optimal finite dimensional natural spline knots unique basis expansions regularization values means represent compute first second derivatives  vector elements diagonal matrix weights first derivative nonlinear need use iterative algorithm section using newton raphson linear logistic regression update equation written new old also express update terms fitted values new old referring back see update fits weighted smoothing spline working response exercise form suggestive tempting replace nonparametric weighted regression operator obtain general fami lies nonparametric logistic regression models although one dimensional procedure generalizes naturally higher dimensional extensions heart generalized additive models pursue chapter 
[basis, expansions, regularization, multidimensional, splines] far focused one dimensional spline models proaches multidimensional analogs suppose basis functions representing functions coordinate likewise set functions coordinate dimensional tensor product basis defined used representing two dimensional function multidimensional splines figure tensor product basis splines showing selected pairs two dimensional function tensor product corresponding one dimensional marginals figure illustrates tensor product basis using splines coeffi cients fit least squares generalized dimensions note dimension basis grows exponentially fast yet another manifestation curse dimensionality mars procedure discussed chapter greedy forward algorithm includ ing tensor products deemed necessary least squares figure illustrates difference additive tensor product natural splines simulated classification example chapter logistic regression model logit fit binary sponse estimated decision boundary contour tensor product basis achieve flexibility decision bound ary introduces spurious structure along way basis expansions regularization additive natural cubic splines training error test error bayes error natural cubic splines tensor product training error test error bayes error figure simulation example figure upper panel shows decision boundary additive logistic regression model using natural splines two coordinates total lower panel shows results using tensor product natural spline bases coordinate total broken purple boundary bayes decision boundary problem multidimensional splines one dimensional smoothing splines via regularization generalize high dimensions well suppose pairs seek dimensional regression function idea set problem min appropriate penalty functional stabilizing function example natural generalization one dimensional roughness penalty functions optimizing penalty leads smooth two dimensional surface known thin plate spline shares many properties one dimensional cubic smoothing spline solution approaches interpolating function one smallest penalty solution approaches least squares plane intermediate values solution represented linear expansion basis functions whose coefficients obtained form generalized ridge regression solution form log examples radial basis functions discussed detail next section coefficients found plugging reduces finite dimensional penalized least squares problem penalty finite coefficients satisfy set linear constraints see exercise thin plate splines defined generally arbitrary dimension appropriately general used number hybrid approaches popular practice computational conceptual simplicity unlike one dimensional smoothing splines computational complexity thin plate splines since general sparse structure ploited however univariate smoothing splines get away substantially less knots prescribed solution basis expansions regularization age obesity systolic blood pressure figure thin plate spline fit heart disease data displayed contour plot response systolic blood pressure modeled function age obesity data points indicated well lattice points used knots care taken use knots lattice inside convex hull data red ignore outside green practice usually sufficient work lattice knots covering domain penalty computed reduced expansion using knots reduces computations fig ure shows result fitting thin plate spline heart disease risk factors representing surface contour plot indicated location input features well knots used fit note specified via trace generally one represent expansion arbi trarily large collection basis functions control complexity plying regularizer example could construct basis forming tensor products pairs univariate smoothing spline basis functions using example univariate splines recommended section ingredients leads exponential regularization reproducing kernel hilbert spaces growth basis functions dimension increases typically reduce number functions per coordinate accordingly additive spline models discussed chapter restricted class multidimensional splines represented general formu lation well exists penalty guarantees solution form functions univariate splines case penalty somewhat degenerate natural assume additive simply impose additional penalty component functions naturally extended anova spline decompositions jlt components splines required dimension many choices made maximum order interaction shown order terms include main effects interactions necessarily needed representation use choices regression splines relatively small number basis func tions per coordinate tensor products interactions complete basis smoothing splines include appropri ate regularizers term expansion many cases number potential dimensions features large automatic methods desirable mars mart procedures chapters respectively fall category 
[basis, expansions, regularization, regularization, reproducing, kernel, hilbert, spaces] section cast splines larger context regularization meth ods reproducing kernel hilbert spaces section quite technical skipped disinterested intimidated reader basis expansions regularization general class regularization problems form min loss function penalty functional space functions defined girosi describe quite general penalty functionals form denotes fourier transform positive function falls zero idea increases penalty high frequency components additional assumptions show solutions form span null space penalty functional inverse fourier transform smoothing splines thin plate splines fall framework remarkable feature solution criterion defined infinite dimensional space solution finite dimensional next sections look specific examples 
[basis, expansions, regularization, regularization, reproducing, kernel, hilbert, spaces, spaces, functions, generated, kernels] important subclass problems form generated positive definite kernel corresponding space func tions called reproducing kernel hilbert space rkhs penalty functional defined terms kernel well give brief simplified introduction class models adapted wahba girosi nicely summarized evgeniou let consider space functions generated linear span arbitrary linear combinations form kernel term viewed function first argument indexed second suppose eigen expansion elements expansion terms eigen functions regularization reproducing kernel hilbert spaces constraint def norm induced penalty functional space defined squared norm quantity interpreted generalized ridge penalty functions large eigenvalues expansion get penalized less vice versa rewriting min equivalently min   shown wahba see also exercise solution finite dimensional form basis function function first argument known representer evaluation since easily seen similarly reproducing property hence light reduces finite dimensional crite rion min using vector notation matrix ijth entry simple numerical algorithms used optimize phenomenon whereby infinite dimensional prob lem reduces finite dimensional optimization problem dubbed kernel property literature support vector machines see chapter basis expansions regularization bayesian interpretation class models interpreted realization zero mean stationary gaussian process prior covariance function eigen decomposition produces ries orthogonal eigen functions associated variances typical scenario smooth functions large prior variance rough small prior variances penalty contribution prior joint likelihood penalizes components smaller prior variance compare simplicity dealt case members penalized generally may components wish leave alone linear functions cubic smoothing splines section multidimensional thin plate splines section tensor product splines fall category well cases convenient representation null space consisting example low degree polynomi als get penalized penalty becomes orthogonal projection onto solution form first term repre sents expansion bayesian perspective coefficients components improper priors infinite variance 
[basis, expansions, regularization, regularization, reproducing, kernel, hilbert, spaces, examples, rkhs] machinery driven choice kernel loss function consider first regression using squared error loss case specializes penalized least squares solution characterized two equivalent ways corresponding min  ufedy infinite dimensional generalized ridge regression problem min solution obtained simply regularization reproducing kernel hilbert spaces vector fitted values given estimate also arises kriging estimate gaussian ran dom field spatial statistics cressie compare also smoothing spline fit page penalized polynomial regression kernel vapnik eigen functions span space polynomials total degree example one represent terms orthogonal eigen functions eigenvalues diag orthogonal suppose wish solve penalized polynomial regression problem min substituting get expression form optimize exercise number basis functions large often much larger equation tells use kernel represen tation solution function evaluate kernel times compute solution operations simplicity without implications polynomials inherits scaling factor particular form bearing impact penalty elaborate next section basis expansions regularization radial kernel figure radial kernels mixture data scale parameter kernels centered five points chosen random gaussian radial basis functions preceding example kernel chosen represents expansion polynomials conveniently compute high dimensional inner products example kernel chosen functional form representation gaussian kernel along squared error loss example leads regression model expansion gaussian radial basis functions one centered one training feature vectors coefficients estimated using figure illustrates radial kernels using first coordinate mixture example chapter show five kernel basis functions figure illustrates implicit feature space radial kernel computed kernel matrix eigen decomposition think columns corre sponding eigenvalues empirical estimates eigen expansion although eigenvectors discrete represent functions exercise figure shows largest eigenval ues leading eigenfunctions smooth successively wiggly order increases brings life penalty see coefficients higher order functions get penalized lower order ones right panel figure shows correspond th column estimate evaluated observations alternatively ith row estimated vector basis functions evaluated point although principle infinitely many elements estimate elements regularization reproducing kernel hilbert spaces orthonormal basis feature space figure left panel first normalized eigenvectors kernel matrix first coordinate mixture data viewed estimates eigenfunctions represented functions observed values superimposed color arranged rows starting top left right panel rescaled versions functions left panel kernel computes inner product eigenvalue figure largest eigenvalues beyond effectively zero basis expansions regularization ing feature space representation eigenfunctions note scaling eigenvalues quickly shrinks functions zero leaving effective dimension case corresponding optimization problem stan dard ridge regression although principle implicit feature space infinite dimensional effective dimension dramat ically lower relative amounts shrinkage applied basis function kernel scale parameter plays role well larger implies local functions increases effective dimension feature space see hastie zhu details also known girosi thin plate spline section expansion radial basis functions generated kernel log radial basis functions discussed detail section support vector classifiers support vector machines chapter two class classification problem form parameters chosen minimize min denotes positive part viewed quadratic optimization problem linear constraints requires quadratic programming algorithm solution name support vector arises fact typically many due piecewise zero nature loss function expansion subset see section details 
[basis, expansions, regularization, wavelet, smoothing] seen two different modes operation dictionaries basis functions regression splines select subset bases using either subject matter knowledge else automatically adaptive procedures mars chapter capture smooth non smooth behavior smoothing splines use complete basis shrink coefficients toward smoothness wavelet smoothing time haar wavelets time symmlet wavelets figure selected wavelets different translations dilations haar symmlet families functions scaled suit display wavelets typically use complete orthonormal basis represent func tions shrink select coefficients toward sparse represen tation smooth function represented spline basis functions mostly flat function isolated bumps repre sented bumpy basis functions wavelets bases popular signal processing compression since able represent smooth locally bumpy functions efficient way phenomenon dubbed time frequency localization contrast traditional fourier basis allows frequency localization give details let look haar wavelets left panel figure get intuitive idea wavelet smoothing works vertical axis indicates scale frequency wavelets low scale bottom high scale top scale wavelets packed side side completely fill time axis shown basis expansions regularization selected subset wavelet smoothing fits coefficients basis least squares thresholds discards filters smaller coefficients since many basis functions scale use bases needs discard ones need achieve time frequency localization haar wavelets simple understand smooth enough purposes symmlet wavelets right panel figure orthonormal properties smoother figure displays nmr nuclear magnetic resonance signal appears composed smooth components isolated spikes plus noise wavelet transform using symmlet basis shown lower left panel wavelet coefficients arranged rows lowest scale bottom highest scale top length line segment indicates size coefficient bottom right panel shows wavelet coefficients thresholded threshold procedure given equation soft thresholding rule arises lasso procedure linear regression section notice many smaller coefficients set zero green curve top panel shows back transform thresholded coefficients smoothed version original signal next section give details process including construction wavelets thresholding rule 
[basis, expansions, regularization, wavelet, smoothing, wavelet, bases, wavelet, transform] section give details construction filtering wavelets wavelet bases generated translations dilations single scal ing function also known father red curves figure haar symmlet scaling functions haar basis particu larly easy understand especially anyone experience analysis variance trees since produces piecewise constant representation thus integer generates orthonormal basis functions jumps integers call ref erence space dilations form orthonormal basis space functions piecewise constant intervals length fact generally spanned definition wavelets analysis variance often rep resent pair means grand mean contrast simplification occurs contrast small set zero similar manner might represent function component plus component orthogonal complement written component represents detail might wish set ele ments component zero easy see functions wavelet smoothing nmr signal wavelet transform original signal wavelet transform waveshrunk signal signal signal figure top panel shows nmr signal wavelet shrunk version superimposed green lower left panel represents wavelet trans form original signal using symmlet basis coeffi cient represented height positive negative vertical bar lower right panel represents wavelet coefficients shrunken using waveshrink function plus implements sureshrink method wavelet adaptation donoho johnstone basis expansions regularization haar basis symmlet basis figure haar symmlet father scaling wavelet mother wavelet generated mother wavelet form orthonor mal basis haar family likewise form basis besides representing function level detail level rough components latter broken level detail rough finally get representation form figure page shows particular wavelets notice since spaces orthogonal basis functions orthonormal fact domain discrete time points far basis elements level adding total elements one structured orthonormal basis allows multiresolution analysis illustrate next section helpful understanding construction haar basis often coarse practical purposes fortunately many clever wavelet bases invented figures include daubechies symmlet basis basis smoother elements corresponding haar basis tradeoff wavelet support covering consecutive time intervals rather one haar basis generally symmlet family support consecutive intervals wider support time wavelet die zero wavelet smoothing achieve smoothly note effective support seems much narrower symmlet wavelet vanishing moments one implication order polynomial times points reproduced exactly exercise sense equivalent null space smoothing spline penalty haar wavelets one vanishing moment reproduce constant function symmlet scaling functions one many families wavelet generators operations similar haar basis spanned spanned filter coefficients spanned filter coefficients 
[basis, expansions, regularization, wavelet, smoothing, adaptive, wavelet, filtering] wavelets particularly useful data measured uniform lattice discretized signal image time series focus one dimensional case lattice points convenient suppose response vector orthonormal wavelet basis matrix evaluated uniformly spaced observations called wavelet transform full least squares regression coefficient popular method adaptive wavelet fitting known sure shrinkage stein unbiased risk estimation donoho johnstone start criterion min lasso criterion chapter thonormal leads simple solution sign least squares coefficients translated toward zero truncated zero fitted function vector given inverse wavelet transform basis expansions regularization simple choice log estimate standard deviation noise give motivation choice since orthonormal transformation elements white noise independent gaussian variates mean variance furthermore random variables white noise expected maximum approximately log hence coefficients log likely noise set zero space could basis orthonormal functions polynomials natural splines cosinusoids makes wavelets special particular form basis functions used allows representation localized time frequency let look nmr signal figure wavelet transform computed using symmlet basis notice coefficients descend way stop basis functions ascend level detail coefficients get smaller except locations spiky behavior present wavelet coefficients represent characteristics signal localized time basis functions level translations localized frequency dilation increases detail factor two sense corresponds doubling frequency traditional fourier representation fact mathematical understanding wavelets reveals wavelets particular scale fourier transform restricted limited range octave frequencies shrinking truncation right panel achieved using sure approach described introduction section orthonormal basis matrix columns wavelet basis functions evaluated time points particular case columns corresponding remainder devoted practice depends noise variance estimated data variance coefficients highest level notice similarity sure criterion page smoothing spline criterion page hierarchically structured coarse fine detail although wavelets also localized time within resolution level splines build bias toward smooth functions imposing differential shrinking constants early versions sure shrinkage treated scales equally wavelets function waveshrink many options allow differential shrinkage spline penalty cause pure shrinkage sure penalty shrinkage selection exercises generally smoothing splines achieve compression original signal imposing smoothness wavelets impose sparsity figure com pares wavelet fit using sure shrinkage smoothing spline fit using cross validation two examples different nature nmr data upper panel smoothing spline introduces detail everywhere order capture detail isolated spikes wavelet fit nicely localizes spikes lower panel true function smooth noise relatively high wavelet fit let additional unnecessary wiggles price pays variance additional adaptivity wavelet transform performed matrix multiplication fact using clever pyramidal schemes obtained computations even faster log fast fourier transform fft general construction beyond scope book easy see haar basis exercise likewise inverse wavelet transform also brief glimpse vast growing field large mathematical computational base built wavelets mod ern image compression often performed using two dimensional wavelet representations 
[basis, expansions, regularization, bibliographic, notes] splines splines discussed detail boor green silverman wahba give thorough treatment smoothing splines thin plate splines latter also covers reproducing kernel hilbert spaces see also girosi evgeniou connections many nonparametric regression techniques using rkhs approaches modeling functional data section covered detail ramsay silverman daubechies classic mathematical treatment wavelets useful sources chui wickerhauser donoho johnstone developed sure shrinkage selection technology statistical estimation framework see also vidakovic bruce gao useful applied introduction also describes wavelet software plus 
[basis, expansions, regularization, exercises] show truncated power basis functions represent basis cubic spline two knots indicated basis expansions regularization nmr signal spline wavelet smooth function simulated spline wavelet true figure wavelet smoothing compared smoothing splines two examples panel compares sure shrunk wavelet fit cross validated smoothing spline fit exercises suppose order spline defined pendix page sequence show induction shows example support cubic splines knots show induction splines positive interior support show induction show piecewise polynomial order degree breaks knots show order spline basis function density function convolution uniform random variables write program reproduce figure page consider truncated power series representation cubic splines interior knots let prove natural boundary conditions natural cubic splines sec tion imply following linear constraints coefficients hence derive basis write program classify phoneme data using quadratic dis criminant analysis section since many correlated features filter using smooth basis natural cubic splines sec tion decide beforehand series five different choices number position knots use tenfold cross validation make final selection phoneme data available book website www stat stanford edu elemstatlearn suppose wish fit periodic function known period describe could modify truncated power series basis achieve goal derivation smoothing splines green silverman sup pose natural cubic spline interpolant pairs natural spline basis expansions regularization knot every dimensional space functions determine coefficients interpolates sequence exactly let differentiable function interpolates pairs let use integration parts fact natural cubic spline show hence show equality hold identically zero consider penalized least squares problem min use argue minimizer must cubic spline knots appendix chapter show smoothing spline computations could efficiently carried using dimen sional basis splines describe slightly simpler scheme using dimensional spline basis defined interior knots derive reinsch form smoothing spline derive expression var bias using example create version figure mean several pointwise quantiles shown prove smoothing spline null space spanned functions linear characterize solution following problem min rss observation weights characterize solution smoothing spline problem training data ties exercises fitted smoothing spline sample pairs suppose augment original sample pair refit describe result use derive fold cross validation formula derive constraints thin plate spline expan sion guarantee penalty finite else could one ensure penalty finite exercise derives results quoted section suppose satisfying conditions let show suppose orthogonal show equality iff consider ridge regression problem assume assume kernel computes inner product derive page text would compute matrices given hence show equivalent show h matrix evaluations matrix inner products basis expansions regularization show would modify solution show convert discrete eigen decomposition section estimates eigenfunctions wavelet function symmlet wavelet basis vanishing moments order show implies polynomials order represented exactly defined page show haar wavelet transform signal length computed computations 
[basis, expansions, regularization, appendix, computations, splines] appendix describe spline basis representing polyno mial splines also discuss use computations smoothing splines 
[basis, expansions, regularization, appendix, computations, splines, b-splines] get started need augment knot sequence defined section let two boundary knots typically define domain wish evaluate spline define augmented knot sequence actual values additional knots beyond boundary arbi trary customary make equal respectively denote ith spline basis function order knot sequence defined recursively terms divided appendix computations splines differences follows otherwise also known haar basis functions thus cubic spline basis functions knot sequence recursion contin ued generate spline basis order spline figure shows sequence splines order four knots points since created duplicate knots care taken avoid division zero adopt convention induction note also construction subset required spline basis order knots fully understand properties functions show indeed span space cubic splines knot sequence quires additional mathematical machinery including properties vided differences exercise explores issues scope splines fact bigger advertised knot duplication duplicate interior knot construc tion sequence generate spline sequence resulting basis spans space piecewise polynomials one less continuous derivative duplicated knot general dition repeated boundary knots include interior knot times lowest order derivative discontinuous order thus cubic splines repeats interior knot third derivatives discontinuous repeating jth knot three times leads discontin uous derivative repeating four times leads discontinuous zeroth derivative function discontinuous exactly happens boundary knots repeat knots times spline becomes discontinuous boundary knots undefined beyond boundary local support splines important computational implica tions especially number knots large least squares com putations observations variables basis functions take flops floating point operations appreciable fraction leads algorithms becomes basis expansions regularization splines order splines order splines order splines order figure sequence splines order four ten knots evenly spaced splines local support nonzero interval spanned knots appendix computations splines unacceptable large observations sorted regression matrix consisting spline basis functions evalu ated points many zeros exploited reduce computational complexity back take next section 
[basis, expansions, regularization, appendix, computations, splines, computations, smoothing, splines] although natural splines section provide basis smoothing splines computationally convenient operate larger space unconstrained splines write coefficients cubic spline basis functions solution looks except matrix replaced matrix similarly penalty matrix replaces dimensional although face value seems boundary derivative constraints turns penalty term automatically imposes giving effectively infinite weight non zero derivative beyond boundary practice restricted linear subspace penalty always finite since columns evaluated splines order left right evaluated sorted values cubic splines local support lower banded consequently matrix banded hence cholesky decomposition computed easily one solves back substitution give hence solution operations practice large unnecessary use interior knots reasonable thinning strategy save computations negligible effect fit example smooth spline function plus uses approximately logarithmic strategy knots included even knots used basis expansions regularization 
[kernel, smoothing, methods] chapter describe class regression techniques achieve flexibility estimating regression function domain fitting different simple model separately query point done using observations close target point fit simple model way resulting estimated function smooth localization achieved via weighting function kernel assigns weight based distance kernels typically indexed parameter dictates width neighborhood memory based methods require principle little training work gets done evaluation time parameter needs determined training data model however entire training data set also discuss general classes kernel based techniques tie structured methods chapters useful density estimation classification techniques chapter confused asso ciated recent usage phrase kernel methods chapter kernels mostly used device localization discuss ker nel methods sections chapter contexts kernel computes inner product high dimensional implicit fea ture space used regularized nonlinear modeling make connections methodology chapter end section kernel smoothing methods nearest neighbor kernel epanechnikov kernel figure panel pairs generated random blue curve gaussian errors sin left panel green curve result nearest neighbor running mean smoother red point fitted constant red circles indicate observations contributing fit solid yellow region indicates weights assigned observations right panel green curve kernel weighted average using epanechnikov kernel half window width 
[kernel, smoothing, methods, one-dimensional, kernel, smoothers] chapter motivated nearest neighbor average ave estimate regression function set points nearest squared distance ave denotes average mean idea relax definition conditional expectation illustrated left panel figure compute average neighborhood target point case used nearest neighborhood fit average pairs whose values closest green curve traced apply definition different values green curve bumpy since discontinuous move left right nearest neighborhood remains constant point right becomes closer furthest point neighborhood left time replaces average changes discrete way leading discontinuous discontinuity ugly unnecessary rather give points neighborhood equal weight assign weights die smoothly distance target point right panel shows example using called nadaraya watson kernel weighted one dimensional kernel smoothers average epanechnikov quadratic kernel otherwise fitted function continuous quite smooth right panel figure move target left right points enter neighborhood initially weight zero contribution slowly increases see exercise right panel used metric window size kernel fit change move target point size nearest neighbor smoothing window adapts local density one however also use adaptive neighborhoods kernels need use general notation let width function indexed determines width neighborhood generally constant nearest neighborhoods neigh borhood size replaces kth closest number details one attend practice smoothing parameter determines width local neighborhood determined large implies lower variance averages observations higher bias essentially sume true function constant within window metric window widths constant tend keep bias estimate constant variance inversely proportional local density nearest neighbor window widths exhibit opposite behavior variance stays constant absolute bias varies inversely local density issues arise nearest neighbors ties smoothing techniques one simply reduce data set averaging tied values supplementing new observations unique values additional weight multiples kernel weight kernel smoothing methods epanechnikov tri cube gaussian figure comparison three popular kernels local smoothing calibrated integrate tri cube kernel compact two continuous derivatives boundary support epanechnikov ker nel none gaussian kernel continuously differentiable infinite support leaves general problem deal observation weights operationally simply multiply kernel weights fore computing weighted average nearest neighborhoods natural insist neighborhoods total weight content relative event overflow last observation needed neighborhood weight causes sum weights exceed budget fractional parts used boundary issues arise metric neighborhoods tend contain less points boundaries nearest neighborhoods get wider epanechnikov kernel compact support needed used nearest neighbor window size another popular compact kernel based tri cube function otherwise flatter top like nearest neighbor box differ entiable boundary support gaussian density func tion popular noncompact kernel standard deviation playing role window size figure compares three 
[kernel, smoothing, methods, one-dimensional, kernel, smoothers, local, linear, regression] progressed raw moving average smoothly varying locally weighted average using kernel weighting smooth kernel fit still problems however exhibited figure left panel locally weighted averages badly biased boundaries domain one dimensional kernel smoothers kernel boundary local linear regression boundary figure locally weighted average bias problems near boundaries domain true function approximately linear observations neighborhood higher mean target point despite weighting mean biased upwards fitting locally weighted linear regression right panel bias removed first order asymmetry kernel region fitting straight lines rather constants locally remove bias exactly first order see figure right panel actually bias present interior domain well values equally spaced reasons usually less severe locally weighted linear regression make first order correction locally weighted regression solves separate weighted least squares prob lem target point min estimate notice although fit entire linear model data region use evaluate fit single point define vector valued function let regression matrix ith row diagonal matrix ith diagonal element equation gives explicit expression local linear regression estimate highlights fact estimate linear kernel smoothing methods local linear equivalent kernel boundary local linear equivalent kernel interior figure green points show equivalent kernel local gression weights plotted corresponding display purposes rescaled since fact sum since yellow shaded region rescaled equivalent kernel nadaraya watson local average see local regression automati cally modifies weighting kernel correct biases due asymmetry smoothing window involve weights combine weight ing kernel least squares operations sometimes referred equivalent kernel figure illustrates effect cal linear regression equivalent kernel historically bias nadaraya watson local average kernel methods corrected modifying kernel modifications based theoretical asymptotic mean square error considerations besides tedious implement approximate finite sample sizes local linear gression automatically modifies kernel correct bias exactly first order phenomenon dubbed automatic kernel carpentry consider following expansion using linearity local regression series expansion true function around remainder term involves third higher order derivatives typically small suitable smoothness assumptions one dimensional kernel smoothers local linear interior local quadratic interior figure local linear fits exhibit bias regions curvature true function local quadratic fits tend eliminate bias shown exercise local linear regression hence middle term equals since bias see depends quadratic higher order terms expansion 
[kernel, smoothing, methods, one-dimensional, kernel, smoothers, local, polynomial, regression] stop local linear fits fit local polynomial fits gree min    solution fact expansion tell bias components degree higher exercise figure illustrates local quadratic regression local linear fits tend biased regions curvature true function phenomenon referred trimming hills filling valleys local quadratic regression generally able correct bias course price paid bias reduction increased variance fit right panel figure slightly wiggly especially tails assuming model independent identically distributed mean zero variance var vector equivalent kernel weights shown exercise increases bias variance tradeoff selecting polynomial degree figure illustrates variance curves degree zero one two kernel smoothing methods variance constant linear quadratic figure variances functions local constant linear quadratic regression metric bandwidth tri cube kernel local polynomials summarize collected wisdom issue local linear fits help bias dramatically boundaries modest cost variance local quadratic fits little bound aries bias increase variance lot local quadratic fits tend helpful reducing bias due curvature interior domain asymptotic analysis suggest local polynomials odd degree dominate even degree largely due fact asymptotically mse dominated boundary effects may helpful tinker move local linear fits boundary local quadratic fits interior recommend strategies usually application dictate degree fit example interested extrapolation boundary interest local linear fits probably reliable 
[kernel, smoothing, methods, selecting, width, kernel] kernels parameter controls width epanechnikov tri cube kernel metric width radius support region gaussian kernel standard deviation number nearest neighbors nearest neighborhoods often expressed fraction span total training sample selecting width kernel figure equivalent kernels local linear regression smoother tri cube kernel orange smoothing spline blue matching degrees freedom vertical spikes indicates target points natural bias variance tradeoff change width averaging window explicit local averages window narrow average small number close variance relatively large close individual bias tend small close window wide variance small relative variance effects averaging bias higher using observations guarantee close similar arguments apply local regression estimates say local linear width goes zero estimates approach piecewise linear function interpolates training data width gets infinitely large fit approaches global linear least squares fit data discussion chapter selecting regularization parameter smoothing splines applies repeated local regression smoothers linear estimators smoother matrix built equivalent kernels ijth entry leave one cross validation particularly simple exercise general ized cross validation exercise fold cross validation effective degrees freedom defined trace used calibrate amount smoothing figure compares equivalent kernels smoothing spline local linear regression local regres sion smoother span results trace smoothing spline calibrated equiv alent kernels qualitatively quite similar uniformly spaced irregularly spaced behavior deteriorate kernel smoothing methods 
[kernel, smoothing, methods, local, regression] kernel smoothing local regression generalize naturally two dimensions nadaraya watson kernel smoother fits constant locally weights supplied dimensional kernel local linear gression fit hyperplane locally weighted least squares weights supplied dimensional kernel simple implement generally preferred local constant fit superior performance boundaries let vector polynomial terms maximum degree example get get trivially get solve min produce fit typically kernel radial function radial epanechnikov tri cube kernel euclidean norm since euclidean norm depends units coordinate makes sense standardize predictor example unit standard deviation prior smoothing boundary effects problem one dimensional smoothing much bigger problem two higher dimensions since fraction points boundary larger fact one manifesta tions curse dimensionality fraction points close boundary increases one dimension grows directly modifying kernel accommodate two dimensional boundaries becomes messy especially irregular boundaries local polynomial regression seamlessly performs boundary correction desired order dimensions fig ure illustrates local linear regression measurements astronomical study unusual predictor design star shaped boundary extremely irregular fitted surface must also inter polate regions increasing data sparsity approach boundary local regression becomes less useful dimensions much higher two three discussed detail problems dimensional ity example chapter impossible simultaneously main tain localness low bias sizable sample neighborhood low variance dimension increases without total sample size creasing exponentially visualization also becomes difficult higher dimensions often one primary goals smoothing structured local regression models east west south north velocity east west south north velocity figure left panel shows three dimensional data response velocity measurements galaxy two predictors record positions celestial sphere unusual star shaped design indicates way measurements made results extremely irregular boundary right panel shows results local linear regression smoothing using nearest neighbor window data although scatter cloud wire frame pictures figure look tractive quite difficult interpret results except gross level data analysis perspective conditional plots far useful figure shows analysis environmental data three pre dictors trellis display shows ozone function radiation conditioned two variables temperature wind speed ever conditioning value variable really implies local value local regression panels figure indication range values present panel condi tioning values panel data subsets displayed response versus remaining variable one dimensional local linear regression fit data although quite looking slices fitted three dimensional surface probably useful terms understanding joint behavior data 
[kernel, smoothing, methods, structured, local, regression, models] dimension sample size ratio unfavorable local regression help much unless willing make structural sumptions model much book structured regres sion classification models focus approaches directly related kernel methods kernel smoothing methods temp wind temp wind temp wind temp wind temp wind temp wind temp wind temp wind temp wind temp wind temp wind temp wind temp wind temp wind temp wind temp wind solar radiation langleys cube root ozone cube root ppb figure three dimensional smoothing example response cube root ozone concentration three predictors temperature wind speed radiation trellis display shows ozone function radiation conditioned intervals temperature wind speed indicated darker green orange shaded bars panel contains range condi tioned variables curve panel univariate local linear regression fit data panel structured local regression models 
[kernel, smoothing, methods, structured, local, regression, models, structured, kernels] one line approach modify kernel default spherical ker nel gives equal weight coordinate natural default strategy standardize variable unit standard deviation general approach use positive semidefinite matrix weigh different coordinates entire coordinates directions downgraded omitted imposing appropriate restrictions example diagonal increase decrease influence individual predictors increasing decreasing often predictors many highly correlated arising digitized analog signals images covariance function predictors used tailor metric focuses less say high frequency contrasts exercise proposals made learning parameters multidimensional kernels example projection pursuit regression model discussed chapter flavor low rank versions imply ridge functions general models cumbersome favor instead structured forms regression function discussed next 
[kernel, smoothing, methods, structured, local, regression, models, structured, regression, functions] trying fit regression function every level interaction potentially present natural consider analysis variance anova decompositions form klt introduce structure eliminating higher order terms additive models assume main effect terms second order models terms interactions order two chapter describe iterative backfitting algorithms fitting low order interaction models additive model example kth term assumed known estimate local regression done function turn repeatedly convergence important detail stage one dimensional local regression needed ideas used fit low dimensional anova decompositions important special case structured models class varying coefficient models suppose example divide pre dictors set remainder kernel smoothing methods depth female depth female depth female depth female depth female depth female depth male depth male depth male depth male depth male depth male age diameter aortic diameter age figure panel aorta diameter modeled linear func tion age coefficients model vary gender depth aorta left near top right low clear trend coefficients linear model variables collect vector assume conditionally linear model given linear model coefficients vary natural fit model locally weighted least squares min figure illustrates idea measurements human aorta longstanding claim aorta thickens age model diameter aorta linear function age allow coefficients vary gender depth aorta used local regression model separately males females aorta clearly thicken age higher regions aorta relationship fades distance aorta figure shows intercept slope function depth local likelihood models male age intercept distance aorta age slope female distance aorta figure intercept slope age function distance aorta separately males females yellow bands indicate one stan dard error 
[kernel, smoothing, methods, local, likelihood, models] concept local regression varying coefficient models extremely broad parametric model made local fitting method commodates observation weights examples associated observation parameter linear covariate inference based log likelihood model flexibly using likelihood local inference many likelihood models particular family generalized linear models including logistic log linear models involve covariates linear fashion local likelihood allows relaxation globally linear model one locally linear kernel smoothing methods except different variables associated used defining local likelihood example could linear model fit varying coefficient model maximizing local likelihood autoregressive time series models order form denoting lag set model looks like standard linear model typically fit least squares fitting local least squares kernel allows model vary according short term history series distinguished traditional dynamic linear models vary windowing time illustration local likelihood consider local version multiclass linear logistic regression model chapter data consist features associated categorical response linear model form local log likelihood class model written log exp notice used subscript first line pick appro priate numerator definition model centered local regressions fitted poste rior probabilities simply local likelihood models systolic blood pressure prevalence chd obesity prevalence chd figure plot shows binary response chd coronary heart dis ease function risk factor south african heart disease data plot computed fitted prevalence chd using local linear logistic regression model unexpected increase prevalence chd lower ends ranges retrospective data subjects already undergone treatment reduce blood pressure weight shaded region plot indicates estimated pointwise standard error band model used flexible multiclass classification moderately low dimensions although successes reported high dimensional zip code classification problem generalized additive models chapter using kernel smoothing methods closely related avoid dimensionality problems assuming additive structure regres sion function simple illustration fit two class local linear logistic model heart disease data chapter figure shows univariate local logistic models fit two risk factors separately useful screening device detecting nonlinearities data little visual information offer case unexpected anomaly uncovered data may gone unnoticed traditional methods since chd binary indicator could estimate conditional preva lence simply smoothing binary response directly resorting likelihood formulation amounts fitting locally constant logistic regression model exercise order enjoy bias correction local linear smoothing natural operate unrestricted logit scale typically logistic regression compute parameter estimates well standard errors done locally well kernel smoothing methods systolic blood pressure chd group density estimate figure kernel density estimate systolic blood pressure chd group density estimate point average contribution kernels point scaled kernels factor make graph readable produce shown plot estimated pointwise standard error bands fitted prevalence 
[kernel, smoothing, methods, kernel, density, estimation, classification] kernel density estimation unsupervised learning procedure historically precedes kernel regression also leads naturally simple family procedures nonparametric classification 
[kernel, smoothing, methods, kernel, density, estimation, classification, kernel, density, estimation] suppose random sample drawn probability density wish estimate point simplicity assume arguing natural local estimate form small metric neighborhood around width estimate bumpy smooth parzen estimate preferred kernel density estimation classification systolic blood pressure density estimates chd chd systolic blood pressure posterior estimate figure left panel shows two separate density estimates systolic blood pressure chd versus chd groups using gaussian kernel density estimate right panel shows estimated posterior probabilities chd using counts observations close weights decrease distance case popular choice gaussian kernel figure shows gaussian kernel density fit sample values systolic blood pressure chd group letting denote gaussian density mean zero standard deviation form convolution sample empirical distribution dis tribution puts mass observed jumpy smoothed adding independent gaussian noise observation parzen density estimate equivalent local average improvements proposed along lines local regression log scale densities see loader pursue natural generalization gaussian density estimate amounts using gaussian product kernel kernel smoothing methods figure population class densities may interesting structure left disappears posterior probabilities formed right 
[kernel, smoothing, methods, kernel, density, estimation, classification, kernel, density, classification] one use nonparametric density estimates classification straight forward fashion using bayes theorem suppose class problem fit nonparametric density estimates separately classes also estimates class priors usually sample proportions figure uses method estimate prevalence chd heart risk factor study compared left panel fig ure main difference occurs region high sbp right panel figure region data sparse classes since gaussian kernel density estimates use metric kernels density estimates low poor quality high variance regions local logistic regression method uses tri cube kernel bandwidth effectively widens kernel region makes use local linear assumption smooth estimate logit scale classification ultimate goal learning separate class den sities well may unnecessary fact misleading figure shows example densities multimodal pos terior ratio quite smooth learning separate densities data one might decide settle rougher high variance fit capture features irrelevant purposes estimating posterior probabilities fact classification ultimate goal need estimate posterior well near decision boundary two classes set 
[kernel, smoothing, methods, kernel, density, estimation, classification, naive, bayes, classifier] technique remained popular years despite name also known idiot bayes especially appropriate kernel density estimation classification dimension feature space high making density estimation unattractive naive bayes model assumes given class features independent assumption generally true simplify estimation dramatically individual class conditional marginal densities estimated separately using one dimensional kernel density estimates fact generalization original naive bayes procedures used univariate gaussians represent marginals component discrete appropriate histogram estimate used provides seamless way mixing variable types feature vector despite rather optimistic assumptions naive bayes classifiers often outperform far sophisticated alternatives reasons related figure although individual class density estimates may biased bias might hurt posterior probabilities much especially near decision regions fact problem may able withstand considerable bias savings variance naive assumption earns starting derive logit transform using class base log log log log log form generalized additive model described detail chapter models fit quite different ways though differences explored exercise relationship naive bayes generalized additive models analogous linear discriminant analysis logistic regression section kernel smoothing methods 
[kernel, smoothing, methods, radial, basis, functions, kernels] chapter functions represented expansions basis functions art flexible modeling using basis expansions consists picking appropriate family basis functions con trolling complexity representation selection regularization families basis functions elements defined locally example splines defined locally flexibility desired particular region region needs represented basis functions case splines translates knots tensor products local basis functions deliver basis functions local basis functions local example truncated power bases splines sigmoidal basis functions used neural networks see chapter composed function nev ertheless show local behavior particular signs values coefficients causing cancellations global effects example truncated power basis equivalent spline basis space functions cancellation exact case kernel methods achieve flexibility fitting simple models region local target point localization achieved via weighting kernel individual observations receive weights radial basis functions combine ideas treating kernel func tions basis functions leads model basis element indexed location prototype parameter scale parameter popular choice standard gaussian density function several approaches learning parameters simplicity focus least squares methods regression use gaussian kernel optimize sum squares respect parameters min  ufedy exp model commonly referred rbf network alterna tive sigmoidal neural network discussed chapter playing role weights criterion nonconvex radial basis functions kernels figure gaussian radial basis functions fixed width leave holes top panel renormalized gaussian radial basis functions avoid prob lem produce basis functions similar respects splines multiple local minima algorithms optimization similar used neural networks estimate separately given former estimation latter simple least squares problem often kernel parameters chosen unsupervised way using distribution alone one methods fit gaussian mixture density model training provides centers scales even adhoc approaches use clustering methods locate prototypes treat hyper parameter obvious drawback approaches conditional distribution particular say action concentrated positive side much simpler implement would seem attractive reduce parameter set assume constant value undesirable side effect creating holes regions none kernels appreciable support illustrated figure upper panel renormalized radial basis functions avoid problem lower panel nadaraya watson kernel regression estimator viewed expansion renormalized radial basis functions kernel smoothing methods basis function located every observation coefficients note similarity expansion solution page regularization problem induced kernel radial basis functions form bridge modern kernel methods local fitting technology 
[kernel, smoothing, methods, mixture, models, density, estimation, classification] mixture model useful tool density estimation viewed kind kernel method gaussian mixture model form mixing proportions gaussian density mean covariance matrix general mixture models use component densities place gaussian gaussian mixture model far popular parameters usually fit maximum likelihood using algorithm described chapter special cases arise covariance matrices constrained scalar form radial basis expansion addition fixed max imum likelihood estimate approaches kernel density estimate using bayes theorem separate mixture densities class lead flex ible models taken detail chapter figure shows application mixtures heart disease risk factor study top row histograms age chd chd groups separately combined right using combined data fit two component mixture form scalars constrained equal fitting done via algorithm chapter note procedure use knowledge chd labels resulting estimates component densities shown lower left middle panels lower right panel shows component den sities orange blue along estimated mixture density green mixture models density estimation classification chd age count chd age count combined age count age mixture estimate age mixture estimate age mixture estimate figure application mixtures heart disease risk factor study top row histograms age chd chd groups separately combined bottom row estimated component densities gaussian mix ture model bottom left bottom middle bottom right estimated component densities blue orange along estimated mixture density green orange density large standard deviation approximates uniform density mixture model also provides estimate probability observation belongs component age example suppose threshold value hence define compare classification observation chd mixture model mixture model chd yes although mixture model use chd labels done fair job discovering two chd subpopulations linear logistic regression using chd response achieves error rate fit data using maximum likelihood section kernel smoothing methods 
[kernel, smoothing, methods, computational, considerations] kernel local regression density estimation memory based meth ods model entire training data set fitting done evaluation prediction time many real time applications make class methods infeasible computational cost fit single observation flops except oversimplified cases square kernels comparison expansion basis functions costs one evaluation typically log basis function methods initial cost least smoothing parameter kernel methods typically deter mined line example using cross validation cost flops popular implementations local regression loess function plus locfit procedure loader use triangulation schemes reduce computations compute fit exactly carefully chosen locations use blending techniques interpolate fit elsewhere per evaluation 
[kernel, smoothing, methods, bibliographic, notes] vast literature kernel methods attempt summarize rather point good references extensive bibliographies loader gives excellent coverage cal regression likelihood also describes state art software fitting models fan gijbels cover models theoretical aspect hastie tibshirani discuss local gression context additive modeling silverman gives good overview density estimation scott 
[kernel, smoothing, methods, exercises] show nadaraya watson kernel smooth fixed metric bandwidth gaussian kernel differentiable said epanechnikov kernel said epanechnikov kernel adaptive nearest neighbor bandwidth show local linear regression define show local polynomial regression degree including local constants show local polynomial regression degree implications bias exercises show section increases degree local polynomial suppose predictors arise sampling relatively smooth analog curves uniformly spaced abscissa values denote cov conditional covariance matrix predictors assume change much discuss nature maha lanobis choice metric compare might construct kernel downweights high frequency components distance metric ignores completely show fitting locally constant multinomial logit model form amounts smoothing binary response indicators class separately using nadaraya watson kernel smoother kernel weights suppose software fitting local regression specify exactly monomials included fit could use software fit varying coefficient model variables derive expression leave one cross validated residual sum squares local polynomial regression suppose continuous response predictor model joint density using multivariate gaussian kernel estimator note kernel case would product kernel show conditional mean derived estimate nadaraya watson estimator extend result classification pro viding suitable kernel estimation joint distribution continuous discrete explore differences naive bayes model generalized additive logistic regression model terms model sumptions estimation variables discrete say corresponding gam suppose samples generated model independent identically distributed mean zero variance assumed fixed non random estimate using linear smoother local regression smoothing spline etc smoothing parameter thus vector fitted values given consider sample prediction error kernel smoothing methods predicting new responses input values show aver age squared residual training data asr biased estimate optimistic asr trace unbiased show gaussian mixture model likelihood maximized describe write computer program perform local linear discrimi nant analysis query point training data receive weights weighting kernel ingredients linear deci sion boundaries see section computed weighted averages try program zipcode data show training test rors series five pre chosen values zipcode data available book website www stat stanford edu elemstatlearn 
[model, assessment, selection, introduction] generalization performance learning method relates predic tion capability independent test data assessment performance extremely important practice since guides choice learning method model gives measure quality ultimately chosen model chapter describe illustrate key methods perfor mance assessment show used select models begin chapter discussion interplay bias variance model complexity 
[model, assessment, selection, bias, variance, model, complexity] figure illustrates important issue assessing ability learn ing method generalize consider first case quantitative interval scale response target variable vector inputs prediction model estimated training set loss function measuring errors denoted typical choices squared error absolute error model assessment selection model complexity prediction error high bias low bias high variance low variance figure behavior test sample training sample error model complexity varied light blue curves show training error err light red curves show conditional test error err training sets size model complexity increased solid curves show expected test error err expected training error err test error also referred generalization error prediction error independent test sample err drawn randomly joint distribution population training set fixed test error refers error specific training set related quantity expected pre diction error expected test error err err note expectation averages everything random includ ing randomness training set produced figure shows prediction error light red curves err simulated training sets size lasso section used produce sequence fits solid red curve average hence estimate err estimation err goal although see err amenable statistical analysis methods effectively esti mate expected error seem possible estimate conditional bias variance model complexity error effectively given information training set discussion point given section training error average loss training sample err would like know expected test error estimated model model becomes complex uses training data able adapt complicated underlying structures hence decrease bias increase variance intermediate model complexity gives minimum expected test error unfortunately training error good estimate test error seen figure training error consistently decreases model complexity typically dropping zero increase model complexity enough however model zero training error overfit training data typically generalize poorly story similar qualitative categorical response taking one values set labeled convenience typically model probabilities monotone transformations arg max cases nearest neighbor classification chapters produce directly typical loss functions loss log log log likelihood quantity log likelihood sometimes referred deviance test error err population mis classification error classifier trained err expected misclassification error training error sample analogue example err log sample log likelihood model log likelihood used loss function general response densities poisson gamma exponential log normal others density indexed parameter depends predictor log model assessment selection definition makes log likelihood loss gaussian distribution match squared error loss ease exposition remainder chapter use represent situations since focus mainly quantitative response squared error loss setting situations appropriate translations obvious chapter describe number methods estimating expected test error model typically model tuning parameter parameters write predictions tuning parameter varies complexity model wish find value minimizes error produces minimum average test error curve figure said brevity often suppress dependence important note fact two separate goals might mind model selection estimating performance different models order choose best one model assessment chosen final model estimating predic tion error generalization error new data data rich situation best approach problems randomly divide dataset three parts training set validation set test set training set used fit models validation set used estimate prediction error model selection test set used assessment generalization error final chosen model ideally test set kept vault brought end data analysis suppose instead use test set repeatedly choosing model smallest test set error test set error final chosen model underestimate true test error sometimes substantially difficult give general rule choose number observations three parts depends signal noise ratio data training sample size typical split might training validation testing test train validation test train validation test validation train validation test train methods chapter designed situations insufficient data split three parts difficult give general rule much training data enough among things depends signal noise ratio underlying function complexity models fit data bias variance decomposition methods chapter approximate validation step either alytically aic bic mdl srm efficient sample use cross validation bootstrap besides use model selection also examine extent method provides reliable estimate test error final chosen model jumping topics first explore detail nature test error bias variance tradeoff 
[model, assessment, selection, biasvariance, decomposition] chapter assume var derive expression expected prediction error regression fit input point using squared error loss err bias var irreducible error bias variance first term variance target around true mean cannot avoided matter well estimate unless second term squared bias amount average estimate differs true mean last term variance expected squared deviation around mean typically complex make model lower squared bias higher variance nearest neighbor regression fit expressions sim ple form err assume simplicity training inputs fixed ran domness arises number neighbors inversely related model complexity small estimate potentially adapt better underlying increase bias squared difference average nearest neighbors typically increase variance decreases linear model fit parameter vector components fit least squares err model assessment selection vector linear weights produce fit hence var variance changes average taken sample values hence err sample error model complexity directly related num ber parameters test error err ridge regression fit identical form except linear weights variance term different bias term also different linear model family ridge regression break bias finely let denote parameters best fitting linear approximation arg min expectation taken respect distribution input variables write average squared bias ave model bias ave estimation bias first term right hand side average squared model bias error best fitting linear approximation true function second term average squared estimation bias error average estimate best fitting linear approximation linear models fit ordinary least squares estimation bias zero restricted fits ridge regression positive trade benefits reduced variance model bias reduced enlarging class linear models richer collection models including interactions transformations variables model figure shows bias variance tradeoff schematically case linear models model space set linear predictions inputs black dot labeled closest fit blue shaded region indicates error see truth training sample also shown variance least squares fit indicated large yellow circle centered black dot labeled closest fit population bias variance decomposition realization closest fit population estimation bias space variance estimation closest fit truth model bias restricted 
[model, assessment, selection, biasvariance, decomposition, shrunken, fit] model space model figure schematic behavior bias variance model space set possible predictions model closest fit labeled black dot model bias truth shown along variance indicated large yellow circle centered black dot labeled closest fit population shrunken regularized fit also shown additional estimation bias smaller prediction error due decreased variance model assessment selection fit model fewer predictors regularize coef ficients shrinking toward zero say would get shrunken fit shown figure fit additional estimation bias due fact closest fit model space hand smaller variance decrease variance exceeds increase squared bias worthwhile 
[model, assessment, selection, biasvariance, decomposition, example, biasvariance, tradeoff] figure shows bias variance tradeoff two simulated examples observations predictors uniformly distributed hypercube situations follows left panels apply nearest neighbors right panels greater otherwise use best subset linear regression size top row regression squared error loss bottom row classi fication loss figures show prediction error red squared bias green variance blue computed large test sample regression problems bias variance add produce predic tion error curves minima nearest neighbors linear model classification loss bottom figures interesting phenomena seen bias variance curves top figures prediction error refers misclassifi cation rate see prediction error longer sum squared bias variance nearest neighbor classifier prediction error decreases stays number neighbors increased despite fact squared bias rising linear model classi fier minimum occurs regression improvement model dramatic see bias variance seem interact determining prediction error happen simple explanation first phe nomenon suppose given input point true probability class expected value estimate squared bias considerable prediction error zero since make correct decision words estimation errors leave right side decision boundary hurt exercise demonstrates phenomenon analytically also shows interaction effect bias variance overall point bias variance tradeoff behaves differently loss squared error loss turn means best choices tuning parameters may differ substantially two bias variance decomposition number neighbors regression subset size linear model regression number neighbors classification subset size linear model classification figure expected prediction error orange squared bias green vari ance blue simulated example top row regression squared error loss bottom row classification loss models nearest neighbors left best subset regression size right variance bias curves regression classification prediction error curve different model assessment selection settings one base choice tuning parameter estimate prediction error described following sections 
[model, assessment, selection, optimism, training, error, rate] discussions error rate estimation confusing make clear quantities fixed random continue need definitions elaborating material sec tion given training set gen eralization error model err note training set fixed expression point new test data point drawn joint distribution data averaging training sets yields expected error err amenable statistical analysis mentioned earlier turns methods effectively estimate expected error rather see section point typically training error err less true error err data used fit method assess error see exercise fitting method typically adapts training data hence apparent training error err overly optimistic estimate generalization error err part discrepancy due evaluation points occur quantity err thought extra sample error since test input vectors need coincide training input vectors nature optimism err easiest understand focus instead sample error err notation indicates observe new response values training points define optimism indeed first edition book section sufficiently clear optimism training error rate difference err training error err err err typically positive since err usually biased downward estimate prediction error finally average optimism expectation optimism training sets predictors training set fixed expectation training set outcome values hence used notation instead usually estimate expected error rather way estimate expected error err rather conditional error err squared error loss functions one show quite generally cov cov indicates covariance thus amount err underesti mates true error depends strongly affects prediction harder fit data greater cov thereby increas ing optimism exercise proves result squared error loss fitted value regression loss classification entropy loss fitted probability class summary important relation err err cov expression simplifies obtained linear fit inputs basis functions example cov additive error model err err expression basis definition effective number parameters discussed section optimism increases linearly model assessment selection number inputs basis functions use decreases training sample size increases versions hold approximately error models binary data entropy loss obvious way estimate prediction error estimate optimism add training error err methods described next section aic bic others work way special class estimates linear parameters contrast cross validation bootstrap methods described later chapter direct estimates extra sample error err gen eral tools used loss function nonlinear adaptive fitting techniques sample error usually direct interest since future values features likely coincide training set values comparison models sample error convenient often leads effective model selection reason relative rather absolute size error matters 
[model, assessment, selection, estimates, in-sample, prediction, error] general form sample estimates err err estimate average optimism using expression applicable parameters fit squared error loss leads version called statistic err estimate noise variance obtained mean squared error low bias model using criterion adjust training error factor proportional number basis functions used akaike information criterion similar generally appli cable estimate err log likelihood loss function used relies relationship similar holds asymptotically log loglik family densities containing true density maximum likelihood estimate loglik maximized log likelihood loglik log estimates sample prediction error example logistic regression model using binomial log likelihood aic loglik gaussian model variance assumed known aic statistic equivalent refer collectively aic use aic model selection simply choose model giving small est aic set models considered nonlinear complex models need replace measure model complexity discuss section given set models indexed tuning parameter denote err training error number parameters model set models define aic err function aic provides estimate test error curve find tuning parameter minimizes final chosen model note basis functions chosen adaptively longer holds example total inputs choose best fitting linear model inputs optimism exceed put another way choosing best fitting model inputs effective number parameters fit figure shows aic action phoneme recognition example section page input vector log periodogram spoken vowel quantized uniformly spaced frequencies lin ear logistic regression model used predict phoneme class coefficient function expansion spline sis functions given basis natural cubic splines used knots chosen uniformly range frequencies using aic select number basis functions approximately minimize err entropy loss simple formula cov holds exactly linear models additive errors squared error loss approximately linear models log likelihoods particular formula hold general loss efron although many authors nevertheless use context right panel figure model assessment selection number basis functions log likelihood log likelihood loss train test aic number basis functions misclassification error loss figure aic used model selection phoneme recogni tion example section logistic regression coefficient function modeled expansion spline basis functions left panel see aic statistic used estimate err using log likeli hood loss included estimate err based independent test sample well except extremely parametrized case parameters observations right panel done loss although aic formula strictly apply reasonable job case 
[model, assessment, selection, effective, number, parameters] concept number parameters generalized especially models regularization used fitting suppose stack outcomes vector similarly predictions linear fitting method one write matrix depending input vectors linear fitting methods include linear regression original fea tures derived basis set smoothing methods use quadratic shrinkage ridge regression cubic smoothing splines effective number parameters defined trace sum diagonal elements also known effective degrees freedom note orthogonal projection matrix onto basis bayesian approach bic set spanned features trace turns trace exactly correct quantity replace number parameters statistic arises additive error model var one show cov trace motivates general definition cov exercises section page gives intuition definition trace context smoothing splines models like neural networks minimize error function weight decay penalty regularization effective number parameters form eigenvalues hessian matrix expression follows make quadratic approximation error function solution bishop 
[model, assessment, selection, bayesian, approach, bic] bayesian information criterion bic like aic applicable settings fitting carried maximization log likelihood generic form bic bic loglik log bic statistic times also known schwarz criterion schwarz gaussian model assuming variance known loglik equals constant err squared error loss hence write bic err log therefore bic proportional aic factor replaced log assuming bic tends penalize complex models heavily giving preference simpler models selection aic typically estimated mean squared error low bias model classification problems use multinomial log likelihood leads similar relationship aic using cross entropy error measure model assessment selection note however misclassification error measure arise bic context since correspond log likelihood data probability model despite similarity aic bic motivated quite different way arises bayesian approach model selection describe suppose set candidate models corresponding model parameters wish choose best model among assuming prior distribution parameters model posterior probability given model represents training data compare two models form posterior odds odds greater one choose model otherwise choose model rightmost quantity called bayes factor contribution data toward posterior odds typically assume prior models uniform constant need way approximating called laplace approximation integral followed simplifications ripley page gives log log log maximum likelihood estimate number free parameters model define loss function log equivalent bic criterion equation therefore choosing model minimum bic equivalent choos ing model largest approximate posterior probability framework gives compute bic criterion set minimum description length models giving bic estimate posterior probability model bic bic thus estimate best model also assess relative merits models considered model selection purposes clear choice aic bic bic asymptotically consistent selection criterion means given family models including true model prob ability bic select correct model approaches one sample size case aic tends choose models complex hand finite samples bic often chooses models simple heavy penalty complexity 
[model, assessment, selection, minimum, description, length] minimum description length mdl approach gives selection cri terion formally identical bic approach motivated optimal coding viewpoint first review theory coding data compression apply model selection think datum message want encode send someone else receiver think model way encoding datum choose parsimonious model shortest code transmission suppose first possible messages might want transmit code uses finite alphabet length example might use binary code length example four possible messages binary coding message code code known instantaneous prefix code code pre fix receiver knows possible codes knows exactly message completely sent restrict discussion instantaneous prefix codes one could use coding could permute codes example use codes decide use depends often sending messages example sending often makes sense use shortest code using kind strategy shorter codes frequent messages average message length shorter model assessment selection general messages sent probabilities famous theorem due shannon says use code lengths log average message length satisfies length log right hand side also called entropy distribution inequality equality probabilities satisfy example respectively coding shown optimal achieves entropy lower bound general lower bound cannot achieved procedures like huffmann coding scheme get close bound note infinite set messages entropy replaced log result glean following transmit random variable probability density func tion require log bits information henceforth change notation log log log convenience introduces unimportant multiplicative constant apply result problem model selection model parameters data consisting inputs outputs let conditional probability outputs model assume receiver knows inputs wish transmit outputs message length required transmit outputs length log log log probability target values given inputs second term average code length transmitting model parameters first term average code length transmitting discrepancy model actual target values example suppose single target parameter input simplicity message length length constant log note smaller shorter average message length since concentrated around mdl principle says choose model mini mizes recognize negative log posterior distribu tion hence minimizing description length equivalent maximizing posterior probability hence bic criterion derived approximation log posterior probability also viewed device approximate model choice minimum description length vapnik chervonenkis dimension figure solid curve function sin green solid blue hollow points illustrate associated indicator function sin shatter separate arbitrarily large number points choosing appropriately high frequency note ignored precision random variable coded finite code length cannot code continuous variable exactly however code within tolerance message length needed log probability interval well proximated zpr small since log zpr log log means ignore constant log use log measure message length preceding view mdl model selection says choose model highest posterior probability however many bayes ians would instead inference sampling posterior distribution 
[model, assessment, selection, vapnikchervonenkis, dimension] difficulty using estimates sample error need specify number parameters complexity used fit although effective number parameters introduced section useful nonlinear models fully general vapnik chervonenkis theory provides general measure complexity gives associated bounds optimism give brief review theory suppose class functions indexed parameter vector assume indicator function takes values linear indi cator function seems reasonable say complexity class number parameters sin real number function sin shown figure wiggly function gets even rougher frequency increases one parameter despite seem reasonable conclude less complexity linear indicator function dimension model assessment selection figure first three panels show class lines plane shatter three points last panel shows class cannot shatter four points line put hollow points one side solid points hence dimension class straight lines plane three note class nonlinear curves could shatter four points hence dimension greater three vapnik chervonenkis dimension way measuring com plexity class functions assessing wiggly members dimension class defined largest number points configuration shattered members set points said shattered class functions matter assign binary label point member class perfectly separate figure shows dimension linear indicator functions plane since four points shattered set lines general linear indicator function dimensions dimension also number free parameters hand shown family sin infinite dimension figure suggests appropriate choice set points shattered class exercise far discussed dimension indicator functions extended real valued functions dimension class real valued functions defined dimension indicator class takes values range one use dimension constructing estimate extra sample prediction error different types results available using concept dimension one prove results optimism training error using class functions example result following fit training points using class functions dimension probability least training vapnik chervonenkis dimension sets err err err binary classification err err regression log log bounds hold simultaneously members taken cherkassky mulier pages recommend value regression suggest classification make recommendation corresponding worst case scenarios also give alternative practical bound regression err err log log free tuning constants bounds suggest optimism increases decreases qualitative agreement aic correction given however results stronger rather giving expected optimism fixed func tion give probabilistic upper bounds functions hence allow searching class vapnik structural risk minimization srm approach fits nested quence models increasing dimensions chooses model smallest value upper bound note upper bounds like ones often loose rule good criteria model selection relative absolute size test error important main drawback approach difficulty calculating dimension class functions often crude upper bound dimension obtainable may adequate example structural risk minimization program successfully carried support vector classifier discussed section 
[model, assessment, selection, vapnikchervonenkis, dimension, example, continued] figure shows results aic bic srm used select model size examples figure examples labeled knn model index refers neighborhood size labeled reg refers subset size using selection method aic estimated best model found true prediction error err test set training set computed prediction error best model assessment selection reg knn reg linear class knn class linear increase best aic reg knn reg linear class knn class linear increase best bic reg knn reg linear class knn class linear increase best srm figure boxplots show distribution relative error err min err max err min err four scenarios figure error using chosen model relative best model training sets size represented boxplot errors computed test sets size cross validation worst possible model choices min err max err boxplots show distribution quantity err min err max err min err represents error using chosen model relative best model linear regression model complexity measured number features mentioned section underestimates since charge search best model size also used dimension linear classifier nearest neighbors used quantity additive error gression model justified exact effective degrees free dom exercise know corresponds dimen sion used constants results srm changed different constants choice gave favorable sults repeated srm selection using alternative practical bound got almost identical results misclassification error used err least restrictive model knn since results zero training error aic criterion seems work well four scenarios despite lack theoretical support loss bic nearly well performance srm mixed 
[model, assessment, selection, cross-validation] probably simplest widely used method estimating predic tion error cross validation method directly estimates expected extra sample error err average generalization error method applied independent test sample joint distribution mentioned earlier might hope cross validation estimates conditional error training set held fixed see section cross validation typically estimates well expected prediction error 
[model, assessment, selection, cross-validation, k-fold, cross-validation] ideally enough data would set aside validation set use assess performance prediction model since data often scarce usually possible finesse problem fold cross validation uses part available data fit model different part test split data roughly equal sized parts example scenario looks like model assessment selection validation train train train train kth part third fit model parts data calculate prediction error fitted model predicting kth part data combine estimates prediction error details let indexing function indicates partition observation allocated randomization denote fitted function computed kth part data removed cross validation estimate prediction error typical choices see case known leave one cross validation case ith observation fit computed using data except ith given set models indexed tuning parameter denote th model fit kth part data removed set models define function provides estimate test error curve find tuning parameter minimizes final chosen model fit data interesting wonder quantity fold cross validation estimates might guess estimates pected error err since training sets fold quite different original training set hand might guess cross validation estimates conditional error err turns cross validation estimates effectively average error err discussed section value choose cross validation estimator approximately unbiased true expected prediction ror high variance training sets similar one another computational burden also considerable requiring applications learning method certain special problems computation done quickly see exercises cross validation size training set err figure hypothetical learning curve classifier given task plot err versus size training set dataset observations fold cross validation would use training sets size would behave much like full set however dataset observations fivefold cross validation would use training sets size would result considerable overestimate prediction error hand say cross validation lower variance bias could problem depending performance learning method varies size training set figure shows hypothetical learning curve classifier given task plot err versus size training set performance classifier improves training set size increases observations increasing number brings small benefit training set observations fivefold cross validation would estimate performance classifier training sets size figure virtually performance training set size thus cross validation would suffer much bias however training set observations fivefold cross validation would estimate performance classifier training sets size figure would underestimate err hence estimate err cross validation would biased upward summarize learning curve considerable slope given training set size five tenfold cross validation overestimate true prediction error whether bias drawback practice depends objective hand leave one cross validation low bias high variance overall five tenfold cross validation recommended good compromise see breiman spector kohavi figure shows prediction error tenfold cross validation curve estimated single training set scenario bottom right panel figure two class classification problem using lin model assessment selection subset size misclassification error figure prediction error orange tenfold cross validation curve blue estimated single training set scenario bottom right panel figure ear model best subsets regression subset size standard error bars shown standard errors individual misclassification error rates ten parts curves minima although curve rather flat beyond often one standard error rule used cross validation choose par simonious model whose error one standard error error best model looks like model predictors would chosen true model uses generalized cross validation provides convenient approximation leave one cross validation linear fitting squared error loss fined section linear fitting method one write many linear fitting methods ith diagonal element see exercise gcv approximation gcv trace cross validation quantity trace effective number parameters defined section gcv computational advantage settings trace computed easily individual elements smoothing problems gcv also alleviate tendency cross validation undersmooth similarity gcv aic seen approximation exercise 
[model, assessment, selection, cross-validation, wrong, right, way, cross-validation] consider classification problem large number predictors may arise example genomic proteomic applications typical strategy analysis might follows screen predictors find subset good predictors show fairly strong univariate correlation class labels using subset predictors build multivariate classifier use cross validation estimate unknown tuning parameters estimate prediction error final model correct application cross validation consider scenario samples two equal sized classes quantitative predictors standard gaussian independent class labels true test error rate classifier carried recipe choosing step predictors highest correlation class labels using nearest neighbor classifier based predictors step simulations setting average error rate far lower true error rate happened problem predictors unfair advantage chosen step basis samples leaving samples variables selected cor rectly mimic application classifier completely independent test set since predictors already seen left samples figure top panel illustrates problem selected pre dictors largest correlation class labels samples chose random set samples would five fold cross validation computed correlations pre selected predictors class labels samples top panel see correlations average rather one might expect correct way carry cross validation example divide samples cross validation folds groups random fold model assessment selection correlations selected predictors outcome frequency wrong way correlations selected predictors outcome frequency right way figure cross validation wrong right way histograms shows correlation class labels randomly chosen samples predic tors chosen using incorrect upper red correct lower green versions cross validation find subset good predictors show fairly strong uni variate correlation class labels using samples except fold using subset predictors build multivariate classi fier using samples except fold use classifier predict class labels samples fold error estimates step accumulated folds produce cross validation estimate prediction error lower panel figure shows correlations class labels predictors chosen step correct procedure samples typical fold see average zero general multistep modeling procedure cross validation must applied entire sequence modeling steps particular samples must left selection filtering steps applied one qualification initial unsupervised screening steps done fore samples left example could select predictors cross validation highest variance across samples starting cross validation since filtering involve class labels give predictors unfair advantage point may seem obvious reader seen blunder committed many times published papers top rank journals large numbers predictors common genomic areas potential consequences error also increased dramatically see ambroise mclachlan detailed discussion issue 
[model, assessment, selection, cross-validation, cross-validation, really, work] examine behavior cross validation high dimensional classification problem consider scenario samples two equal sized classes quantitative predictors indepen dent class labels true error rate classifier consider simple univariate classifier single split minimizes misclassification error stump stumps trees single split used boosting methods chapter simple argument sug gests cross validation work properly setting fitting entire training set find predictor splits data well fold cross validation predictor split ths data well hence cross validation error small much less thus give accurate estimate error investigate whether argument correct figure shows result simulation setting predictors samples two equal sized classes predictors standard gaussian distribution panel top left shows number training errors stumps fit training data marked color six predictors yielding fewest errors top right panel training errors shown stumps fit random ths partition data samples tested remaining four samples colored points indicate predictors marked top left panel see stump blue predictor whose stump best top left panel makes two four test errors better random happened preceding argument ignored fact cross validation model must completely retrained fold argument made scientist proteomics lab meeting led material section model assessment selection predictor error full training set error error predictor blue class label full errors figure simulation study investigate performance cross vali dation high dimensional problem predictors independent class labels top left panel shows number errors made individual stump classifiers full training set observations top right panel shows errors made individual stumps trained random split dataset ths observations tested remaining servations best performers depicted colored dots panel bottom left panel shows effect estimating split point fold colored points correspond four samples ths validation set split point derived full dataset classifies four samples correctly split point estimated ths data com mits two errors four validation samples bottom right see overall result five fold cross validation applied simulated datasets average error rate bootstrap methods process present example means best predictor corresponding split point found ths data effect predictor choice seen top right panel since class labels independent predictors performance stump ths training data contains information performance remain ing effect choice split point shown bottom left panel see data predictor corresponding blue dot top left plot colored points indicate data remaining points belong ths optimal split points predictor based full training set ths data indicated split based full data makes errors ths data cross validation must base split ths data incurs two errors four samples results applying five fold cross validation simulated datasets shown bottom right panel would hope average cross validation error around true expected prediction error classifier hence cross validation behaved hand considerable variability error underscor ing importance reporting estimated standard error estimate see exercise another variation problem 
[model, assessment, selection, bootstrap, methods] bootstrap general tool assessing statistical accuracy first describe bootstrap general show used estimate extra sample prediction error cross validation boot strap seeks estimate conditional error err typically estimates well expected prediction error err suppose model fit set training data denote training set basic idea randomly draw datasets replacement training data sample size original training set done times say producing bootstrap datasets shown figure refit model bootstrap datasets examine behavior fits replications figure quantity computed data ample prediction input point bootstrap sampling estimate aspect distribution example variance var model assessment selection bootstrap bootstrap replications samples sample training figure schematic bootstrap process wish assess sta tistical accuracy quantity computed dataset training sets size drawn replacement original dataset quantity interest computed bootstrap training set values used assess statistical accuracy note var thought monte carlo estimate variance sampling empirical distribution function data apply bootstrap estimate prediction error one proach would fit model question set bootstrap samples keep track well predicts original training set predicted value model fitted bth boot strap dataset estimate err boot however easy see err boot provide good estimate general reason bootstrap datasets acting training samples original training set acting test sample two samples observations common overlap make overfit predictions look unrealistically good reason cross validation explicitly uses non overlapping data training test samples consider example nearest neighbor classifier applied two class classification problem number observations bootstrap methods class predictors class labels fact independent true error rate contributions bootstrap estimate err boot zero unless observation appear bootstrap sample latter case correct expectation observation bootstrap sample hence expectation err boot far correct error rate mimicking cross validation better bootstrap estimate tained observation keep track predictions boot strap samples containing observation leave one bootstrap estimate prediction error defined err set indices bootstrap samples contain observation number samples computing err either choose large enough ensure greater zero leave terms corresponding zero leave one bootstrap solves overfitting problem suffered err boot training set size bias mentioned discussion cross validation average number distinct observations boot strap sample bias roughly behave like twofold cross validation thus learning curve considerable slope sample size leave one bootstrap biased upward estimate true error estimator designed alleviate bias defined err err err derivation estimator complex intuitively pulls leave one bootstrap estimate toward training error rate hence reduces upward bias use constant relates estimator works well light fitting situations break overfit ones example due breiman suppose two equal size classes targets independent class labels apply one nearest neighbor rule err model assessment selection err err however true error rate one improve estimator taking account amount overfitting first define information error rate error rate prediction rule inputs class labels independent estimate obtained evaluating prediction rule possible combinations targets predictors example consider dichotomous classification problem let observed proportion responses equaling let served proportion predictions equaling rule like nearest neighbors value multi category generalization using relative overfitting rate defined err err err quantity ranges overfitting err err overfitting equals information value err finally define estimator err err err weight ranges err ranges err err derivation compli cated roughly speaking produces compromise leave one bootstrap training error rate depends amount overfitting nearest neighbor problem class labels indepen dent inputs err err correct expectation problems less overfitting err lie somewhere err err 
[model, assessment, selection, bootstrap, methods, example, continued] figure shows results tenfold cross validation boot strap estimate four problems figures figure bootstrap methods reg knn reg linear class knn class linear increase best cross validation reg knn reg linear class knn class linear increase best bootstrap figure boxplots show distribution relative error err min err max err min err four scenar ios figure error using chosen model relative best model training sets represented boxplot figure shows boxplots err min err max err min err error using chosen model relative best model different training sets represented boxplot mea sures perform well overall perhaps slightly worse aic figure conclusion particular problems fitting methods minimization either aic cross validation bootstrap yields model fairly close best available note purpose model selec tion measures could biased affect things long bias change relative performance methods example addition constant measures would change resulting chosen model however many adaptive nonlinear techniques like trees estimation effective number parameters difficult makes methods like aic impractical leaves cross validation bootstrap methods choice different question well method estimate test error average aic criterion overestimated prediction error cho model assessment selection sen model respectively four scenarios bic performing similarly contrast cross validation overestimated error bootstrap hence extra work involved computing cross validation bootstrap measure worthwhile accurate estimate test error required fitting methods like trees cross validation boot strap underestimate true error search best tree strongly affected validation set situations separate test set provide unbiased estimate test error 
[model, assessment, selection, conditional, expected, test, error] figures examine question whether cross validation good job estimating err error conditional given training set expression page opposed expected test error training sets generated reg linear setting top right panel figure figure shows conditional error curves err function subset size top left next two panels show fold fold cross validation latter also known leave one loo thick red curve plot expected error err thick black curves expected cross validation curves lower right panel shows well cross validation approximates conditional expected error one might expected fold approximate err well since almost uses full training sample fit new test point fold hand might expected estimate err well since averages somewhat different training sets figure appears fold better job fold estimating err estimates err even better indeed similarity two black curves red curve suggests curves approximately unbiased err fold less variance similar trends reported efron figure shows scatterplots fold fold cross validation error estimates versus true conditional error simulations although scatterplots indicate much correlation lower right panel shows part correlations negative curi ous phenomenon observed negative correlation explains neither form estimates err well broken lines plot drawn err expected error best subset size see forms approximately unbiased expected error variation test error different training sets quite substantial among four experimental conditions reg linear scenario showed highest correlation actual predicted test error conditional expected test error prediction error subset size error fold error subset size error leave one error subset size error approximation error subset size mean absolute deviation err err err figure conditional prediction error err fold cross validation leave one cross validation curves simulations top right panel figure thick red curve expected prediction error err thick black curves expected curves lower right panel shows mean absolute deviation curves conditional error err blue green well expected error err orange model assessment selection subset size prediction error error subset size prediction error error subset size prediction error error subset size correlation leave one fold figure plots estimates error versus true conditional error training sets simulation setup top right panel figure fold leave one depicted different colors first three panels correspond different subset sizes vertical horizontal lines drawn err although appears little cor relation plots see lower right panel part correlation negative exercises phenomenon also occurs bootstrap estimates error would guess estimate conditional prediction error conclude estimation test error particular training set easy general given data training set instead cross validation related methods may provide reasonable estimates expected error err 
[model, assessment, selection, bibliographic, notes] key references cross validation stone stone allen aic proposed akaike bic introduced schwarz madigan raftery give overview bayesian model selection mdl criterion due rissa nen cover thomas contains good description coding theory complexity dimension described vapnik stone showed aic leave one cross validation asymp totically equivalent generalized cross validation described golub wahba discussion topic may found monograph wahba see also hastie tibshirani chapter bootstrap due efron see efron tibshi rani overview efron proposes number bootstrap estimates prediction error including optimism estimates efron compares gcv bootstrap estimates error rates use cross validation bootstrap model selection stud ied breiman spector breiman shao zhang kohavi estimator proposed efron tibshirani cherkassky published study performance srm model selection regression response study section complained unfair srm applied properly response found issue journal hastie 
[model, assessment, selection, exercises] derive estimate sample error loss show err err model assessment selection bayes classifier err irreducible bayes error using approximation var show sign var exp cumulative gaussian distribution function increasing func tion value value think sign kind boundary bias term depends true side boundary lies notice also bias variance combine multiplicative rather additive fashion side bias negative decreasing variance decrease misclassification error hand opposite side bias positive pays increase variance increase improve chance falls correct side friedman let linear smoothing ith diagonal element show arising least squares projections cubic smoothing splines cross validated residual written use result show find general conditions smoother make result hold consider sample prediction error training error err case squared error loss err err exercises add subtract expression expand hence establish average optimism training error cov given linear smoother show cov trace justifies use effective number parameters show additive error model effective degrees freedom nearest neighbors regression fit use approximation expose relationship aic gcv main difference model used estimate noise variance show set functions sin shatter following points line hence dimension class sin infinite prostate data chapter carry best subset linear regression analysis table third column left compute aic bic five tenfold cross validation bootstrap estimates prediction error discuss results referring example section suppose instead predictors binary hence need estimate split points predictors independent class labels large probably find predictor splits entire training data perfectly hence would split validation data one fifth data perfectly well predictor would therefore zero cross validation error mean cross validation provide good estimate test error situation question suggested model assessment selection 
[model, inference, averaging, introduction] book fitting learning models achieved minimizing sum squares regression minimizing cross entropy classification fact minimizations instances maximum likelihood approach fitting chapter provide general exposition maximum likeli hood approach well bayesian method inference boot strap introduced chapter discussed context relation maximum likelihood bayes described finally present related techniques model averaging improvement including com mittee methods bagging stacking bumping 
[model, inference, averaging, bootstrap, maximum, likelihood, methods, smoothing, example] bootstrap method provides direct computational way assessing uncertainty sampling training data illustrate bootstrap simple one dimensional smoothing problem show connection maximum likelihood model inference averaging spline basis figure left panel data smoothing example right panel set seven spline basis functions broken vertical lines indicate placement three knots denote training data one dimensional input outcome either continuous categorical example consider data points shown left panel figure suppose decide fit cubic spline data three knots placed quartiles values seven dimensional lin ear space functions represented example linear expansion spline basis functions see section seven functions shown right panel figure think representing conditional mean let matrix ijth element usual estimate obtained minimizing squared error training set given corresponding fit shown top left panel figure estimated covariance matrix var estimated noise variance letting standard error predic bootstrap maximum likelihood methods figure top left spline smooth data top right spline smooth plus minus standard error bands bottom left ten bootstrap repli cates spline smooth bottom right spline smooth standard error bands computed bootstrap distribution model inference averaging tion top right panel figure plotted since point standard normal distribution represent approximate pointwise confidence bands could apply bootstrap example draw datasets size replacement training data sampling unit pair bootstrap dataset fit cubic spline fits ten samples shown bottom left panel figure using bootstrap samples form pointwise confidence band percentiles find fifth largest smallest values plotted bottom right panel figure bands look similar top right little wider endpoints actually close connection least squares estimates bootstrap maximum likelihood suppose assume model errors gaussian bootstrap method described sample placement training data called nonparametric bootstrap really means method model free since uses raw data specific parametric model generate new datasets consider variation bootstrap called parametric bootstrap simulate new responses adding gaussian noise predicted values process repeated times say resulting boot strap datasets form recompute spline smooth confidence bands method actly equal least squares bands top right panel number bootstrap samples goes infinity function estimated bootstrap sample given distribution notice mean distribution least squares estimate standard deviation approximate formula bootstrap maximum likelihood methods 
[model, inference, averaging, bootstrap, maximum, likelihood, methods, maximum, likelihood, inference] turns parametric bootstrap agrees least squares previous example model additive gaussian errors general parametric bootstrap agrees least squares maximum likelihood review begin specifying probability density probability mass function observations expression represents one unknown parameters gov ern distribution called parametric model example normal distribution mean variance maximum likelihood based likelihood function given probability observed data model likelihood defined positive multiplier taken one think function data fixed denote logarithm log sometimes abbreviate expression called log likelihood value log called log likelihood component method maximum likelihood chooses value maximize likelihood function used assess precision need definitions score function defined model inference averaging assuming likelihood takes maxi mum interior parameter space information matrix evaluated often called observed information fisher information expected information finally let denote true value standard result says sampling distribution maximum likelihood estimator limiting normal distribution independently sampling suggests sampling distribution may approximated represents maximum likelihood estimate observed data corresponding estimates standard errors obtained confidence points constructed either approximation confidence point form respectively percentile standard normal distribution accurate confidence intervals derived likelihood function using chi squared approximation number components resulting confi dence interval set percentile chi squared distribution degrees freedom bayesian methods let return smoothing example see maximum likelihood yields parameters log likelihood log maximum likelihood estimate obtained setting giving usual estimates given information matrix block diagonal block corresponding estimated variance agrees least squares estimate 
[model, inference, averaging, bootstrap, maximum, likelihood, methods, bootstrap, versus, maximum, likelihood] essence bootstrap computer implementation nonparametric parametric maximum likelihood advantage bootstrap maximum likelihood formula allows compute maximum like lihood estimates standard errors quantities settings formulas available example suppose adaptively choose cross validation number position knots define splines rather fix advance denote collection knots positions standard errors confidence bands account adaptive choice way analytically bootstrap compute spline smooth adaptive choice knots bootstrap sample percentiles resulting curves capture variability noise targets well particular example confidence bands shown look much different fixed bands problems adaptation used important effect capture 
[model, inference, averaging, bayesian, methods] bayesian approach inference specify sampling model density probability mass function data given parameters model inference averaging prior distribution parameters reflecting knowledge see data compute posterior distribution represents updated knowledge see data understand posterior distribution one might draw samples summarize computing mean mode bayesian approach differs standard frequentist method inference use prior distribution express uncertainty present seeing data allow uncertainty remaining seeing data expressed form posterior distribution posterior distribution also provides basis predicting values future observation new via predictive distribution new new contrast maximum likelihood approach would use new data density evaluated maximum likelihood estimate predict future data unlike predictive distribution account uncertainty estimating let walk bayesian approach smoothing example start parametric model given equation assume moment known assume observed feature values fixed randomness data comes solely varying around mean second ingredient need prior distribution distributions functions fairly complex entities one approach use gaussian process prior specify prior covariance two function values wahba neal take simpler route considering finite spline basis instead provide prior coefficients implicitly defines prior choose gaussian prior centered zero choices prior correlation matrix variance discussed implicit process prior hence gaussian covariance kernel cov bayesian methods figure smoothing example ten draws gaussian prior distri bution function posterior distribution also gaussian mean covariance cov corresponding posterior values cov choose prior correlation matrix settings prior chosen subject matter knowledge parameters willing say function smooth guaranteed expressing smooth low dimensional basis splines hence take prior correlation matrix identity number basis functions large might suf ficient additional smoothness enforced imposing restrictions exactly case smoothing splines section figure shows ten draws corresponding prior generate posterior values function generate values posterior giving corresponding posterior value ten posterior curves shown figure two different values used prior variance notice similar right panel looks bootstrap distribution bottom left panel model inference averaging figure smoothing example ten draws posterior distribution function two different values prior variance purple curves posterior means figure page similarity accident posterior distribution bootstrap distribution coincide hand posterior curves left panel figure smoother bootstrap curves imposed prior weight smoothness distribution called noninformative prior gaussian models maximum likelihood parametric bootstrap anal yses tend agree bayesian analyses use noninformative prior free parameters tend agree constant prior posterior distribution proportional likelihood correspon dence also extends nonparametric case nonparametric bootstrap approximates noninformative bayes analysis section details however done things proper bayesian point view used noninformative constant prior replaced maximum likelihood estimate posterior standard bayesian analysis would also put prior typically calculate joint posterior integrate rather extract maximum posterior distribution map estimate relationship bootstrap bayesian inference 
[model, inference, averaging, relationship, bootstrap, bayesian, inference] consider first simple example observe single obser vation normal distribution carry bayesian analysis need specify prior convenient common choice would giving posterior distribution larger take concentrated posterior becomes around maximum likelihood estimate limit obtain noninformative constant prior posterior distribution parametric bootstrap distribution gen erate bootstrap values maximum likelihood estimate sampling density three ingredients make correspondence work choice noninformative prior dependence log likelihood data maximum likelihood estimate hence write log likelihood symmetry log likelihood constant properties essentially hold gaussian distribu tion however also hold approximately multinomial distribu tion leading correspondence nonparametric bootstrap bayes inference outline next assume discrete sample space categories let probability sample point falls category observed proportion category let denote estimator take prior distribution sym metric dirichlet distribution parameter model inference averaging prior probability mass function proportional posterior density sample size letting obtain noninformative prior gives bootstrap distribution obtained sampling replacement data expressed sampling category proportions multinomial distribution specifically mult mult denotes multinomial distribution probability mass function distribution similar pos terior distribution support mean nearly covariance matrix hence bootstrap distribution closely approximate posterior distribution sense bootstrap distribution represents approximate nonparametric noninformative posterior distribution parameter bootstrap distribution obtained painlessly without formally specify prior without sample posterior distribution hence might think bootstrap distribution poor man bayes posterior perturbing data bootstrap approxi mates bayesian effect perturbing parameters typically much simpler carry 
[model, inference, averaging, algorithm] algorithm popular tool simplifying difficult maximum likelihood problems first describe context simple mixture model 
[model, inference, averaging, algorithm, two-component, mixture, model] section describe simple mixture model density estimation associated algorithm carrying maximum likelihood estimation natural connection gibbs sampling methods bayesian inference mixture models discussed demonstrated sev eral parts book particular sections left panel figure shows histogram fictitious data points table algorithm density figure mixture example left panel histogram data right panel maximum likelihood fit gaussian densities solid red responsibility dotted green left component density observation function table twenty fictitious data points used two component mixture example figure would like model density data points due apparent modality gaussian distribution would appropriate seems two separate underlying regimes instead model mixture two normal distributions generative representation explicit generate probability depending outcome deliver either let denote normal density parameters density suppose wish fit model data figure maxi mum likelihood parameters log likelihood based training cases log model inference averaging direct maximization quite difficult numerically sum terms inside logarithm however simpler proach consider unobserved latent variables taking values comes model otherwise comes model suppose knew values log likelihood would log log log log maximum likelihood estimates would sample mean variance data similarly would sample mean variance data estimate would proportion since values actually unknown proceed iterative fashion substituting expected value also called responsibility model observation use proce dure called algorithm given algorithm special case gaussian mixtures expectation step soft assignment observation model current estimates parameters used assign responsibilities according relative density training points model maximization step responsibilities used weighted maximum likelihood fits update estimates parameters good way construct initial guesses simply choose two random set equal overall sample variance mixing proportion started value note actual maximizer likelihood occurs put spike infinite height one data point gives infinite likelihood useful solution hence actually looking good local maximum likelihood one complicate matters one local maximum example ran algorithm number different initial guesses parameters chose run gave highest maximized likelihood figure shows progress algorithm maximizing log likelihood table shows maximum likelihood estimate proportion observations class selected iterations procedure algorithm algorithm algorithm two component gaussian mixture take initial guesses parameters see text expectation step compute responsibilities maximization step compute weighted means variances mixing probability iterate steps convergence table selected iterations algorithm mixture example iteration final maximum likelihood estimates right panel figure shows estimated gaussian mixture density procedure solid red curve along responsibilities dotted green curve note mixtures also useful supervised learning section show gaussian mixture model leads version radial basis functions model inference averaging iteration observed data log likelihood figure algorithm observed data log likelihood function iteration number 
[model, inference, averaging, algorithm, algorithm, general] procedure example baum welch algorithm maximizing likelihoods certain classes problems problems ones maximization likelihood difficult made easier enlarging sample latent unobserved data called data augmentation latent data model memberships problems latent data actual data observed missing algorithm gives general formulation algorithm observed data log likelihood depending parameters latent missing data complete data log likelihood based complete density mixture problem given mixture example simply replaced responsibilities maximizers step weighted means variances give explanation algorithm works general since write terms log likelihoods based conditional density taking conditional expectations respect distribution governed parameter gives algorithm algorithm algorithm start initial guesses parameters expectation step jth step compute function dummy argument maximization step determine new estimate maxi mizer iterate steps convergence step algorithm maximizes rather actual objective function succeed maximizing note expectation log likelihood density indexed respect density indexed hence jensen inequality maximized function see exercise maximizes see hence iteration never decreases log likelihood argument also makes clear full maximization step necessary need find value increases function first argument procedures called gem generalized algorithms algorithm also viewed minorization procedure see exercise 
[model, inference, averaging, algorithm, maximizationmaximization, procedure] different view procedure joint maximization algorithm consider function log distribution latent data mixture example comprises set probabilities note evaluated log likelihood model inference averaging model parameters latent data parameters figure maximization maximization view algorithm shown contours augmented observed data log likelihood step equivalent maximizing log likelihood parameters latent data distribution step maximizes parameters log likelihood red curve corresponds observed data log likelihood profile obtained maximizing value observed data function expands domain log likelihood facilitate maximization algorithm viewed joint maximization method fixing one argument maximizing maximizer fixed shown exercise distribution computed step example mixture example step maximize fixed maximizing first term since second term involve finally since observed data log likelihood agree maximization former accomplishes maxi mization latter figure shows schematic view process view algorithm leads alternative maximization proce holds including mcmc sampling posterior algorithm gibbs sampler take initial values repeat generate continue step joint distribution change dures example one need maximize respect latent data parameters could instead maximize one time alternating step 
[model, inference, averaging, mcmc, sampling, posterior] defined bayesian model one would like draw samples resulting posterior distribution order make inferences parameters except simple models often difficult computa tional problem section discuss markov chain monte carlo mcmc approach posterior sampling see gibbs sampling mcmc procedure closely related algorithm main dif ference samples conditional distributions rather maximizing consider first following abstract problem random variables wish draw sample joint distribution suppose difficult easy simulate conditional distributions gibbs sampling procedure alternatively simulates distri butions process stabilizes provides sample desired joint distribution procedure defined algorithm regularity conditions shown procedure even tually stabilizes resulting random variables indeed sample joint distribution occurs despite fact samples clearly independent dif ferent formally gibbs sampling produces markov chain whose stationary distribution true joint distribution hence term markov chain monte carlo surprising true joint dis tribution stationary process successive steps leave marginal distributions unchanged model inference averaging note need know explicit form conditional densities need able sample procedure reaches stationarity marginal density subset variables approximated density estimate applied sample values however explicit form conditional density available better estimate say marginal density obtained exercise averaged last members sequence allow initial burn period stationarity reached getting back bayesian inference goal draw sample joint posterior parameters given data gibbs sampling helpful easy sample conditional distribution parameter given parameters example gaussian mixture problem detailed next close connection gibbs sampling posterior algorithm exponential family models key consider latent data procedure another parameter gibbs sampler make explicit gaussian mixture problem take parameters simplicity fix variances mixing proportion maximum likelihood values unknown parameters means gibbs sampler mixture problem given algorithm see steps steps pro cedure except sample rather maximize step rather compute maximum likelihood responsibilities gibbs sampling procedure simulates latent data distri butions step rather compute maximizers posterior simulate conditional distribution figure shows iterations gibbs sampling mean param eters lower upper shown left panel proportion class observations right horizontal broken lines drawn maximum likelihood estimate values case values seem stabilize quite quickly distributed evenly around maximum likelihood values mixture model simplified order make clear connection gibbs sampling algorithm realisti cally one would put prior distribution variances mixing proportion include separate gibbs sampling steps sam ple posterior distributions conditional parameters one also incorporate proper informative priors mean param mcmc sampling posterior algorithm gibbs sampling mixtures take initial values repeat generate equation set generate continue step joint distribution change gibbs iteration mean parameters gibbs iteration mixing proportion figure mixture example left panel values two mean param eters gibbs sampling horizontal lines drawn maximum likelihood estimates right panel proportion values gibbs sampling iterations horizontal line drawn model inference averaging eters priors must improper lead degenerate posterior mixing weight one component gibbs sampling one number recently developed procedures sampling posterior distributions uses conditional sampling parameter given rest useful structure prob lem makes sampling easy carry methods require structure example metropolis hastings algorithm computational bayesian methods applied sophisticated learning algorithms gaussian process models neural networks details may found references given bibliographic notes end chapter 
[model, inference, averaging, bagging] earlier introduced bootstrap way assessing accuracy parameter estimate prediction show use bootstrap improve estimate prediction section investigated relationship bootstrap bayes approaches found bootstrap mean approximately posterior average bagging exploits connection consider first regression problem suppose fit model training data obtaining predic tion input bootstrap aggregation bagging averages predic tion collection bootstrap samples thereby reducing variance bootstrap sample fit model giving prediction bagging estimate defined bag denote empirical distribution putting equal probability data points fact true bagging estimate defined expression monte carlo estimate true bagging estimate approaching bagged estimate differ original estimate latter nonlinear adaptive function data example bag spline smooth section average curves bottom left panel figure value spline smoother linear data fix inputs hence sample using parametric bootstrap equation bag exercise hence bagging reproduces original smooth bagging top left panel figure approximately true bag using nonparametric bootstrap interesting example regression tree denotes tree prediction input vector regression trees described chap ter bootstrap tree typically involve different features original might different number terminal nodes bagged estimate average prediction trees suppose tree produces classifier class response useful consider underlying indicator vector function value single one zeroes arg max bagged estimate bag vector equal proportion trees predicting class bagged classifier selects class votes trees bag arg max bag often require class probability estimates rather classifications tempting treat voting proportions estimates probabilities simple two class example shows fail regard suppose true probability class bagged classifiers accurately predict incorrect many classifiers however already underlying function estimates class probabilities trees class proportions terminal node alternative bagging strategy average instead rather vote indicator vectors produce improved estimates class probabilities also tends produce bagged classifiers lower variance especially small see figure next example 
[model, inference, averaging, bagging, example, trees, simulated, data] generated sample size two classes features standard gaussian distribution pairwise correlation response generated according bayes error test sample size also generated population fit classification trees training sample bootstrap samples classification trees described chapter pruning used figure shows original tree eleven bootstrap trees notice trees different different splitting features cutpoints test error original tree bagged tree shown figure ample trees high variance due correlation predictors bagging succeeds smoothing variance hence reducing test error bagging dramatically reduce variance unstable procedures like trees leading improved prediction simple argument shows model inference averaging original tree figure bagging trees simulated dataset top left panel shows original tree eleven trees grown bootstrap samples shown tree top split annotated bagging number bootstrap samples test error bagged trees original tree bayes consensus probability figure error curves bagging example figure shown test error original tree bagged trees function number bootstrap samples orange points correspond consensus vote green points average probabilities bagging helps squared error loss short averaging reduces variance leaves bias unchanged assume training observations indepen dently drawn distribution consider ideal aggregate timator fixed bootstrap dataset consists observations sampled note bagging estimate drawing bootstrap samples actual population rather data estimate use practice convenient analysis write extra error right hand side comes variance around mean therefore true population aggregation never creases mean squared error suggests bagging drawing samples training data often decrease mean squared error argument hold classification loss cause nonadditivity bias variance setting bagging model inference averaging good classifier make better bagging bad classifier make worse simple example using randomized rule suppose classifier predicts proba bility predicts probability misclassification error bagged classifier classification understand bagging effect terms consensus independent weak learners dietterich let bayes optimal decision two class example suppose weak learners error rate let consensus vote class since weak learn ers assumed independent bin gets large concept popularized outside statistics wisdom crowds surowiecki collective knowledge diverse independent body people typically exceeds knowledge single individual harnessed voting course main caveat independent bagged trees figure illustrates power consensus vote simulated example voters knowledge chapter see random forests improve bagging reducing correlation sampled trees note bag model simple structure model lost example bagged tree longer tree interpretation model clearly drawback stable procedures like near est neighbors typically affected much bagging unfortunately unstable models helped bagging unstable emphasis interpretability lost bagging process figure shows example bagging help data points shown two features two classes separated gray linear boundary choose classifier single axis oriented split choosing split along either produces largest decrease training misclassification error decision boundary obtained bagging decision rule bootstrap samples shown blue curve left panel poor job capturing true boundary single split rule derived training data splits near middle range hence little contribution away center averaging probabilities rather classifications help bagging estimates expected class probabilities single split rule averaged many replications note expected class probabilities computed bagging cannot realized single replication way woman cannot children sense bagging increases somewhat space models individual base classifier however help many examples greater enlargement model class needed boosting way bagging probability informed person correct expected correct wisdom crowds consensus individual figure simulated academy awards voting members vote cat egories nominations category voters knowledge represented probability selecting correct candidate category means knowledge category experts chosen random results show expected correct based simulations consensus well individuals error bars indicate one standard deviation see example informed category chance selecting correct candidate consensus doubles expected performance individual model inference averaging bagged decision rule boosted decision rule figure data two features two classes separated linear boundary left panel decision boundary estimated bagging decision rule single split axis oriented classifier right panel decision boundary boosting decision rule classifier test error rates respectively boosting described chapter described chapter decision boundary right panel result boosting procedure roughly captures diagonal boundary 
[model, inference, averaging, model, averaging, stacking] section viewed bootstrap values estimator approximate posterior values corresponding parameter kind nonparamet ric bayesian analysis viewed way bagged estimate approximate posterior bayesian mean contrast training sample estimate corresponds mode posterior since posterior mean mode minimizes squared error loss surprising bagging often reduce mean squared error discuss bayesian model averaging generally set candidate models training set models may type different parameter values subsets linear regression different models task neural networks regression trees suppose quantity interest example prediction fixed feature value posterior distribution model averaging stacking posterior mean bayesian prediction weighted average individual predictions weights proportional posterior probability model formulation leads number different model averaging strate gies committee methods take simple unweighted average predic tions model essentially giving equal probability model ambitiously development section shows bic criterion used estimate posterior model probabilities applicable cases different models arise parametric model different parameter values bic gives weight model pending well fits many parameters uses one also carry bayesian recipe full model parameters write principle one specify priors numerically com pute posterior probabilities used model averaging weights however seen real evidence worth effort relative much simpler bic approximation approach model averaging frequentist viewpoint given predictions squared error loss seek weights argmin input value fixed observations dataset target distributed according solution population linear regression full regression smaller error single model combining models never makes things worse population level model inference averaging course population linear regression available natural replace linear regression training set simple examples work well example represent prediction best subset inputs size among total inputs linear regression would put weight largest model problem put models footing taking account complexity number inputs example stacked generalization stacking way let prediction using model applied dataset ith training observation removed stacking estimate weights obtained least squares linear regression detail stacking weights given argmin final prediction using cross validated pre dictions stacking avoids giving unfairly high weight models higher complexity better results obtained restricting weights nonnegative sum seems like reasonable restriction interpret weights posterior model probabilities equation leads tractable quadratic programming problem close connection stacking model selection via leave one cross validation section restrict minimization weight vectors one unit weight rest zero leads model choice smallest leave one cross validation error rather choose single model stacking combines estimated optimal weights often lead better prediction less interpretability choice one models stacking idea actually general described one use learning method linear regression combine models weights could also depend input location way learning methods stacked top one another improve prediction performance 
[model, inference, averaging, stochastic, search, bumping] final method described chapter involve averaging combining models rather technique finding better single model bumping uses bootstrap sampling move randomly model space problems fitting method finds many local minima bump ing help method avoid getting stuck poor solutions stochastic search bumping regular node tree bumped node tree figure data two features two classes blue orange dis playing pure interaction left panel shows partition found three splits standard greedy tree growing algorithm vertical grey line near left edge first split broken lines two subsequent splits gorithm idea make good initial split makes poor choice right panel shows near optimal splits found bumping tree growing algorithm times bagging draw bootstrap samples fit model rather average predictions choose model estimated bootstrap sample best fits training data detail draw boot strap samples fit model giving predictions input point choose model produces smallest prediction error averaged original training set squared error example choose model obtained bootstrap sample arg min corresponding model predictions convention also include original training sample set bootstrap samples method free pick original model lowest training error perturbing data bumping tries move fitting procedure around good areas model space example data points causing procedure find poor solution bootstrap sample omits data points procedure better solution another example consider classification data figure notorious exclusive xor problem two classes blue orange two input features features exhibiting pure inter model inference averaging action splitting data splitting resulting strata vice versa tree based classifier could achieve per fect discrimination however greedy short sighted cart algorithm section tries find best split either feature splits resulting strata balanced nature data initial splits appear useless procedure essentially gener ates random split top level actual split found data shown left panel figure bootstrap sampling data bumping breaks balance classes reasonable number bootstrap samples chance produce least one tree initial split near either using bootstrap samples bumping found near optimal splits shown right panel figure shortcoming greedy tree growing algorithm exacerbated add number noise features independent class label tree growing algorithm cannot distinguish others gets seriously lost since bumping compares different models training data one must ensure models roughly complexity case trees would mean growing trees number terminal nodes bootstrap sample bumping also help problems difficult optimize fitting criterion perhaps lack smoothness trick optimize different convenient criterion bootstrap samples choose model producing best results desired criterion training sample 
[model, inference, averaging, bibliographic, notes] many books classical statistical inference cox hink ley silvey give nontechnical accounts bootstrap due efron described fully efron tibshi rani hall good modern book bayesian inference gelman lucid account application bayesian methods neural networks given neal statistical appli cation gibbs sampling due geman geman gelfand smith related work tanner wong markov chain monte carlo methods including gibbs sampling metropolis hastings algorithm discussed spiegelhalter algorithm due dempster discussants per make clear much related earlier work view joint maximization scheme penalized complete data log likelihood elucidated neal hinton credit csiszar tusn ady hathaway noticed connection earlier bag ging proposed breiman stacking due wolpert exercises breiman contains accessible discussion statisticians leblanc tibshirani describe variations stacking based boot strap model averaging bayesian framework recently advo cated madigan raftery bumping proposed tibshi rani knight 
[model, inference, averaging, exercises] let probability density functions jensen equality states random variable convex function use jensen inequality show log maximized function hence show stated equation consider maximization log likelihood dis tributions use grange multipliers show solution conditional distribution justify estimate using relationship consider bagging method section let estimate spline smoother section consider parametric bootstrap equation applied estimator show bag using parametric bootstrap generate bootstrap samples bagging estimate bag converges original estimate suggest generalizations loss functions figure two classes design appropriate plot compare consider bone mineral density data figure fit cubic smooth spline relative change spinal bmd function age use cross validation estimate optimal amount smoothing construct pointwise confidence bands derlying function compute posterior mean covariance true function via compare posterior bands obtained model inference averaging compute bootstrap replicates fitted curves bottom left panel figure compare results obtained minorization algorithm hunter lange lange function said minorize function domain useful maximizing since easy show non decreasing update argmax analogous definitions majorization minimizing function resulting algorithms known algorithms minorize maximize majorize minimize show algorithm section example gorithm using log minorize observed data log likelihood note first term involves relevant parameter 
[additive, models, trees, related, methods] chapter begin discussion specific methods super vised learning techniques assume different structured form unknown regression function finesse curse dimensionality course pay possible price misspecifying model case tradeoff made take chapters left describe five related techniques generalized additive models trees multivariate adaptive regression splines patient rule induction method hierarchical mixtures experts 
[additive, models, trees, related, methods, generalized, additive, models] regression models play important role many data analyses providing prediction classification rules data analytic tools understand ing importance different inputs although attractively simple traditional linear model often fails situations real life effects often linear earlier chapters described techniques used predefined basis functions achieve nonlinearities section describes automatic flexible statistical methods may used identify characterize nonlinear regression effects methods called generalized additive models regression setting generalized additive model form additive models trees related methods usual represent predictors outcome unspecified smooth nonparametric functions model function using expansion basis functions chapter resulting model could fit simple least squares approach different fit function using scatterplot smoother cubic smoothing spline kernel smoother provide algorithm simultaneously estimating functions section two class classification recall logistic regression model binary data discussed section relate mean binary response predictors via linear regression model logit link function log additive logistic regression model replaces linear term general functional form log unspecified smooth function non parametric form functions makes model flexible additivity retained allows interpret model much way additive logistic regression model example generalized additive model general conditional mean response related additive function predictors via link function examples classical link functions following identity link used linear additive models gaussian response data logit probit probit link function modeling binomial probabilities probit function inverse gaussian cumulative distribution function probit log log linear log additive models poisson count data three arise exponential family sampling models addition include gamma negative binomial distributions families generate well known class generalized linear models extended way generalized additive models functions estimated flexible manner using algorithm whose basic building block scatterplot smoother estimated func tion reveal possible nonlinearities effect generalized additive models functions need nonlinear easily mix linear parametric forms nonlinear terms necessity inputs qualitative variables factors nonlinear terms restricted main effects either nonlinear components two variables separate curves level factor thus following would qualify semiparametric model vector predictors modeled linearly effect kth level qualitative input effect predictor modeled nonparametrically indexes levels qualitative input thus creates interaction term effect nonparametric function two features additive models replace linear models wide variety settings example additive decomposition time series seasonal component trend error term 
[additive, models, trees, related, methods, generalized, additive, models, fitting, additive, models] section describe modular algorithm fitting additive models generalizations building block scatterplot smoother fitting nonlinear effects flexible way concreteness use scatterplot smoother cubic smoothing spline described chapter additive model form error term mean zero given observations criterion like penalized sum squares section specified problem prss tuning parameters shown minimizer additive cubic spline model functions additive models trees related methods algorithm backfitting algorithm additive models initialize cycle functions change less prespecified threshold cubic spline component knots unique values however without restrictions model solution unique constant identifiable since add subtract constants functions adjust accordingly standard convention assume functions average zero data easily seen ave case addition restriction matrix input values ijth entry full column rank strictly convex criterion minimizer unique matrix singular linear part components cannot uniquely determined nonlinear parts buja furthermore simple iterative procedure exists finding solution set ave never changes apply cubic smoothing spline targets function obtain new estimate done predictor turn using current estimates functions computing process continued estimates stabilize procedure given detail algorithm known backfitting resulting fit analogous multiple regression linear models principle second step algorithm needed since smoothing spline fit mean zero response mean zero exer cise practice machine rounding cause slippage justment advised algorithm accommodate fitting methods exactly way specifying appropriate smoothing operators univariate regression smoothers local polynomial gression kernel methods generalized additive models linear regression operators yielding polynomial fits piecewise con stant fits parametric spline fits series fourier fits complicated operators surface smoothers second higher order interactions periodic smoothers seasonal effects consider operation smoother training points represented operator matrix see section degrees freedom jth term approximately computed trace analogy degrees freedom smoothers discussed chapters large class linear smoothers backfitting equivalent gauss seidel algorithm solving certain linear system equations details given exercise logistic regression model generalized additive models appropriate criterion penalized log likelihood maximize backfitting procedure used conjunction likelihood maximizer usual newton raphson routine maximizing log likelihoods gen eralized linear models recast irls iteratively reweighted least squares algorithm involves repeatedly fitting weighted linear regression working response variable covariates regression yields new value parameter estimates turn give new work ing responses weights process iterated see section generalized additive model weighted linear regression simply replaced weighted backfitting algorithm describe algorithm detail logistic regression generally chapter hastie tibshirani 
[additive, models, trees, related, methods, generalized, additive, models, example, additive, logistic, regression] probably widely used model medical research logistic model binary data model outcome coded indicating event like death relapse disease indicating event wish model probability event given values prognostic factors goal usually understand roles prognostic factors rather classify new individuals logistic models also used applica tions one interested estimating class probabilities use risk screening apart medical applications credit risk screening popular application generalized additive logistic model form log functions estimated backfitting algorithm within newton raphson procedure shown algorithm additive models trees related methods algorithm local scoring algorithm additive logistic regres sion model compute starting values log ave sample proportion ones set define exp iterate construct working target variable construct weights fit additive model targets weights ing weighted backfitting algorithm gives new estimates continue step change functions falls pre specified threshold additive model fitting step algorithm requires weighted scatterplot smoother smoothing procedures accept observation weights exercise see chapter hastie tibshirani details additive logistic regression model generalized han dle two classes using multilogit formulation outlined section formulation straightforward extension algorithms fitting models complex see yee wild details vgam software currently available http www stat auckland yee example predicting email spam apply generalized additive model spam data introduced chapter data consists information email messages study screen email spam junk email data publicly available ftp ics uci edu donated george forman hewlett packard laboratories palo alto california response variable binary values email spam predictors described quantitative predictors percentage words email match given word examples include business address internet generalized additive models table test data confusion matrix additive logistic regression model fit spam training data overall test error rate predicted class true class email spam email spam free george idea could customized individual users quantitative predictors percentage characters email match given character characters average length uninterrupted sequences capital letters capave length longest uninterrupted sequence capital letters capmax sum length uninterrupted sequences capital letters captot coded spam email zero test set size randomly chosen leaving observations training set generalized additive model fit using cubic smoothing spline nominal four degrees freedom predictor means predictor smoothing spline parameter chosen trace smoothing spline operator matrix constructed using observed values convenient way specifying amount smoothing complex model spam predictors long tailed distribution fitting gam model log transformed variable actually log plots figure shown function original variables test error rates shown table overall error rate comparison linear logistic regression test error rate table shows predictors highly significant additive model ease interpretation table contribution variable decomposed linear component remaining nonlinear com ponent top block predictors positively correlated spam bottom block negatively correlated linear component weighted least squares linear fit fitted curve predictor nonlinear part residual linear component estimated additive models trees related methods table significant predictors additive model fit spam train ing data coefficients represent linear part along standard errors score nonlinear value test nonlinearity name num coefficient std error score nonlinear value positive effects remove internet free business hpl capmax captot negative effects george edu function summarized coefficient standard error score latter coefficient divided standard error considered significant exceeds appropriate quantile standard normal dis tribution column labeled nonlinear value test nonlinearity estimated function note however effect predictor fully adjusted entire effects predictors linear parts predictors shown table judged signifi cant least one tests linear nonlinear level two sided figure shows estimated functions significant predictors appearing table many nonlinear effects appear account strong discontinuity zero example probability spam drops significantly frequency george increases zero change much suggests one might replace frequency predictors indicator variable zero count resort linear logistic model gave test error rate including linear effects frequencies well dropped test error appears nonlinearities additive model additional predictive power generalized additive models remove internet free business hpl george edu capmax captot figure spam analysis estimated functions significant predictors rug plot along bottom frame indicates observed values cor responding predictor many predictors nonlinearity picks discontinuity zero additive models trees related methods serious classify genuine email message spam since good email would filtered would reach user alter balance class error rates changing losses see section assign loss predicting true class class predicting true class class estimated bayes rule predicts class probability greater example take true class class error rates change ambitiously encourage model fit better data class using weights class observations class observations use estimated bayes rule predict gave error rates true class class respectively discuss issue unequal losses context tree based models fitting additive model one check whether inclusion interactions significantly improve fit done manually inserting products significant inputs automatically via mars procedure section example uses additive model automatic fashion data analysis tool additive models often used interactive fashion adding dropping terms determine effect calibrating amount smoothing terms one move seamlessly linear models partially linear models terms modeled flexibly see hastie tibshirani details 
[additive, models, trees, related, methods, generalized, additive, models, summary] additive models provide useful extension linear models making flexible still retaining much interpretability familiar tools modeling inference linear models also available additive models seen example table backfitting procedure fitting models simple modular allowing one choose fitting method appropriate input variable result become widely used statistical community however additive models limitations large data mining plications backfitting algorithm fits predictors feasi ble desirable large number available bruto procedure hastie tibshirani chapter combines backfitting selec tion inputs designed large data mining problems also recent work using lasso type penalties estimate sparse ditive models example cosso procedure lin zhang spam proposal ravikumar large problems forward stagewise approach boosting chapter effective also allows interactions included model tree based methods 
[additive, models, trees, related, methods, tree-based, methods, background] tree based methods partition feature space set rectangles fit simple model like constant one conceptually simple yet powerful first describe popular method tree based regression classification called cart later contrast major competitor let consider regression problem continuous response puts taking values unit interval top left panel figure shows partition feature space lines parallel coordinate axes partition element model different constant however problem although partitioning line simple description like resulting regions complicated describe simplify matters restrict attention recursive binary partitions like top right panel figure first split space two regions model response mean region choose variable split point achieve best fit one regions split two regions process continued stopping rule applied example top right panel figure first split region split region split finally region split result process partition five regions shown figure corresponding regression model predicts constant region model represented binary tree bottom left panel figure full dataset sits top tree observations satisfying condition junction assigned left branch others right branch terminal nodes leaves tree correspond regions bottom right panel figure perspective plot regression surface model illustration chose node means make plot key advantage recursive binary tree interpretability feature space partition fully described single tree two inputs partitions like top right panel figure difficult draw binary tree representation works way representation also popular among medical scientists perhaps mimics way doctor thinks tree stratifies additive models trees related methods figure partitions cart top right panel shows partition two dimensional feature space recursive binary splitting used cart applied fake data top left panel shows general partition cannot obtained recursive binary splitting bottom left panel shows tree cor responding partition top right panel perspective plot prediction surface appears bottom right panel tree based methods population strata high low outcome basis patient characteristics 
[additive, models, trees, related, methods, tree-based, methods, regression, trees] turn question grow regression tree data consists inputs response observations algorithm needs automatically decide splitting variables split points also topology shape tree suppose first partition regions model response constant region adopt criterion minimization sum squares easy see best average region ave finding best binary partition terms minimum sum squares generally computationally infeasible hence proceed greedy algorithm starting data consider splitting variable split point define pair half planes seek splitting variable split point solve min min min choice inner minimization solved ave ave splitting variable determination split point done quickly hence scanning inputs determination best pair feasible found best split partition data two resulting regions repeat splitting process two regions process repeated resulting regions large grow tree clearly large tree might overfit data small tree might capture important structure additive models trees related methods tree size tuning parameter governing model complexity optimal tree size adaptively chosen data one approach would split tree nodes decrease sum squares due split exceeds threshold strategy short sighted however since seemingly worthless split might lead good split preferred strategy grow large tree stopping splitting process minimum node size say reached large tree pruned using cost complexity pruning describe define subtree tree obtained pruning collapsing number internal non terminal nodes index terminal nodes node representing region let denote number terminal nodes letting define cost complexity criterion idea find subtree minimize tuning parameter governs tradeoff tree size goodness fit data large values result smaller trees conversely smaller values notation suggests solution full tree discuss adaptively choose one show unique smallest subtree minimizes find use weakest link pruning successively collapse internal node produces smallest per node increase continue produce single node root tree gives finite sequence subtrees one show sequence must contain see breiman ripley details estimation achieved five tenfold cross validation choose value minimize cross validated sum squares final tree 
[additive, models, trees, related, methods, tree-based, methods, classification, trees] target classification outcome taking values changes needed tree algorithm pertain criteria splitting nodes pruning tree regression used squared error node tree based methods entropy gini index misclassification error figure node impurity measures two class classification function proportion class cross entropy scaled pass impurity measure defined suitable classification node representing region observations let proportion class observations node classify obser vations node class arg max majority class node different measures node impurity include following misclassification error gini index cross entropy deviance log two classes proportion second class three mea sures max log log respectively shown figure three similar cross entropy gini index differentiable hence amenable numerical optimization comparing see need weight node impurity measures number observations two child nodes created splitting node addition cross entropy gini index sensitive changes node probabilities misclassification rate example two class problem observations class denote suppose one split created nodes additive models trees related methods created nodes splits produce mis classification rate second split produces pure node probably preferable gini index cross entropy lower second split reason either gini index cross entropy used growing tree guide cost complexity pruning three measures used typically misclassification rate gini index interpreted two interesting ways rather classify observations majority class node could classify class probability training error rate rule node gini index similarly code observation class zero otherwise variance node response summing classes gives gini index 
[additive, models, trees, related, methods, tree-based, methods, issues] categorical predictors splitting predictor possible unordered values possible partitions values two groups com putations become prohibitive large however outcome computation simplifies order predictor classes according proportion falling outcome class split predictor ordered predictor one show gives optimal split terms cross entropy gini index among possible splits result also holds quantitative outcome square error loss cat egories ordered increasing mean outcome although intuitive proofs assertions trivial proof binary outcomes given breiman ripley proof quantita tive outcomes found fisher multicategory outcomes simplifications possible although various approximations proposed loh vanichsetakul partitioning algorithm tends favor categorical predictors many levels number partitions grows exponentially choices likely find good one data hand lead severe overfitting large variables avoided loss matrix classification problems consequences misclassifying observations serious classes others example probably worse predict person heart attack actually vice versa account define loss matrix loss incurred classifying class obser vation class typically loss incurred correct classifications tree based methods incorporate losses modeling process could modify gini index would expected loss incurred randomized rule works multi class case two class case effect since coefficient two classes better approach weight observations class used multiclass case function depend observation weighting used deviance well effect observation weighting alter prior probability classes terminal node empirical bayes rule implies classify class arg min missing predictor values suppose data missing predictor values variables might discard observation missing values could lead serious depletion training set alternatively might try fill impute missing values say mean predictor nonmissing observations tree based models two better approaches first applicable categorical predictors simply make new category missing might dis cover observations missing values measurement behave differently nonmissing values second general approach construction surrogate variables considering predictor split use observations predictor missing chosen best primary predictor split point form list surrogate predictors split points first surrogate predictor corresponding split point best mimics split training data achieved primary split second surrogate predictor corresponding split point second best sending observations tree either training phase prediction use surrogate splits order primary splitting predictor missing surrogate splits exploit correlations predictors try alleviate effect missing data higher cor relation missing predictor predictors smaller loss information due missing value general problem missing data discussed section binary splits rather splitting node two groups stage might consider multiway splits two groups sometimes useful good general strategy problem multiway splits fragment data quickly leaving insufficient data next level hence would want use splits needed since multiway splits achieved series binary splits latter preferred additive models trees related methods tree building procedures discussion focuses cart classification regression tree implementation trees popular methodology later versions quinlan early versions program limited categorical predictors used top rule pruning recent developments become quite similar cart significant feature unique scheme deriving rule sets tree grown splitting rules define terminal nodes sometimes simplified one condition dropped without changing subset observations fall node end simplified set rules defining terminal node longer follow tree structure simplicity might make attractive user linear combination splits rather restricting splits form one allow splits along linear combinations form weights split point optimized minimize relevant criterion gini index improve predictive power tree hurt interpretability computationally discreteness split point search precludes use smooth optimization weights better way incorporate linear combination splits hierarchical mixtures experts hme model topic section instability trees one major problem trees high variance often small change data result different series splits making interpre tation somewhat precarious major reason instability hierarchical nature process effect error top split propagated splits one alleviate degree trying use stable split criterion inherent instability removed price paid estimating simple tree based structure data bagging section averages many trees reduce variance lack smoothness another limitation trees lack smoothness prediction sur face seen bottom right panel figure classification loss hurt much since bias estimation class probabilities limited effect however degrade performance regression setting would normally expect underlying function smooth mars procedure described section tree based methods table spam data confusion rates node tree chosen cross validation test data overall error rate predicted true email spam email spam viewed modification cart designed alleviate lack smoothness difficulty capturing additive structure another problem trees difficulty modeling additive struc ture regression suppose example zero mean noise binary tree might make first split near next level would split nodes order capture additive structure might happen sufficient data model given special encouragement find structure ten rather two additive effects would take many fortuitous splits recreate structure data analyst would hard pressed recognize estimated tree blame attributed binary tree structure advantages drawbacks mars method section gives tree structure order capture additive structure 
[additive, models, trees, related, methods, tree-based, methods, spam, example, continued] applied classification tree methodology spam example intro duced earlier used deviance measure grow tree mis classification rate prune figure shows fold cross validation error rate function size pruned tree along stan dard errors mean ten replications test error curve shown orange note cross validation error rates indexed sequence values tree size trees grown different folds value might imply different sizes sizes shown base plot refer sizes pruned original tree error flattens around terminal nodes giving pruned tree figure distinct features chosen tree overlap significant features additive model table overall error rate shown table higher additive model table consider rightmost branches tree branch right spam warning characters sign additive models trees related methods tree size misclassification rate figure results spam example blue curve fold cross val idation estimate misclassification rate function tree size standard error bars minimum occurs tree size terminal nodes using one standard error rule orange curve test error tracks error quite closely cross validation indexed values shown tree sizes shown refer size original tree indexed however addition phrase occurs frequently likely company business classify email cases test set satisfying criteria correctly classified second condition met addition average length repeated capital letters capave larger classify spam test cases seven misclassified medical classification problems terms sensitivity specificity used characterize rule defined follows sensitivity probability predicting disease given true state disease specificity probability predicting non disease given true state non disease tree based methods spam spam spam spam spam spam spam spam spam spam spam spam email email email email email email email email email email email email email email email email email email email email email removelt georgelt hplt capmaxlt receivelt edult ourlt capavelt freelt businesslt georgelt hplt capavelt removegt georgegt hpgt capmaxgt receivegt edugt ourgt capavegt freegt businessgt georgegt hpgt capavegt figure pruned tree spam example split variables shown blue branches classification shown every node numbers terminal nodes indicate misclassification rates test data additive models trees related methods specificity sensitivity tree gam weighted tree figure roc curves classification rules fit spam data curves closer northeast corner represent better classifiers case gam classifier dominates trees weighted tree achieves better sensitivity higher specificity unweighted tree numbers legend repre sent area curve think spam email presence absence disease spectively table sensitivity specificity analysis used equal losses let loss associated predicting class object class varying relative sizes losses increase sensitivity decrease specificity rule vice versa example want avoid marking good email spam thus want specificity high achieve setting say bayes rule terminal node classifies class spam proportion spam class zero otherwise prim bump hunting receiver operating characteristic curve roc commonly used summary assessing tradeoff sensitivity specificity plot sensitivity versus specificity vary parameters classification rule varying loss applying bayes rule node tree selected figure produced roc curve shown figure standard error curve near approximately hence standard error difference see order achieve specificity close sensitivity drop area curve commonly used quantitative summary extending curve linearly direction defined area approximately comparison included roc curve gam model fit data section gives better classification rule loss area rather modifying bayes rule nodes better take full account unequal losses growing tree done section two classes losses may incorporated tree growing process using weight observation class chose fit size tree tree higher sensitivity high values specificity original tree poorly extreme top splits original tree departs application tree grown using clearly better original tree area roc curve used sometimes called statistic interestingly shown area roc curve equivalent mann whitney statistic wilcoxon rank sum test median difference prediction scores two groups hanley mcneil evaluating contribution additional predictor added standard model statistic may informative measure new predictor significant terms change model deviance show small increase statistic example removal highly significant term george model table results decrease statistic less instead useful examine additional predictor changes classification individual sample basis good discussion point appears cook 
[additive, models, trees, related, methods, prim, bump, hunting] tree based methods regression partition feature space box shaped regions try make response averages box differ additive models trees related methods ent possible splitting rules defining boxes related binary tree facilitating interpretation patient rule induction method prim also finds boxes feature space seeks boxes response average high hence looks maxima target function exercise known bump hunting minima rather maxima desired one simply works negative response values prim also differs tree based partitioning methods box definitions described binary tree makes interpretation collection rules difficult however removing binary tree constraint individual rules often simpler main box construction method prim works top starting box containing data box compressed along one face small amount observations falling outside box peeled face chosen compression one resulting largest box mean compression performed process repeated stopping current box contains minimum number data points process illustrated figure data points uni formly distributed unit square color coded plot indicates response taking value red zero blue otherwise panels shows successive boxes found top peeling procedure peeling proportion remaining data points stage figure shows mean response values box box compressed top sequence computed prim reverses process expanding along edge expansion increases box mean called pasting since top procedure greedy step expansion often possible result steps sequence boxes different numbers observation box cross validation combined judgment data analyst used choose optimal box size denote indices observations box found step prim procedure removes observations training set two step process top peeling followed bottom pasting repeated remaining dataset entire process peated several times producing sequence boxes box defined set rules involving subset predictors like summary prim procedure given algorithm prim handle categorical predictor considering partitions predictor cart missing values also handled manner similar cart prim designed regression quantitative response prim bump hunting figure illustration prim algorithm two classes indicated blue class red class points procedure starts rectangle broken black lines surrounding data peels away points along one edge prespecified amount order maximize mean points remaining box starting top left panel sequence peelings shown pure red region isolated bottom right panel iteration number indicated top panel number observations box box mean figure box mean function number observations box additive models trees related methods algorithm patient rule induction method start training data maximal box containing data consider shrinking box compressing one face peel proportion observations either highest values predictor lowest choose peeling produces highest response mean remaining box typically repeat step minimal number observations say remain box expand box along face long resulting box mean increases steps give sequence boxes different numbers obser vations box use cross validation choose member sequence call box remove data box dataset repeat steps obtain second box continue get many boxes desired variable two class outcome handled simply coding simple way deal classes simultaneously one approach run prim separately class versus baseline class advantage prim cart patience nary splits cart fragments data quite quickly assuming splits equal size observations make log splits running data prim peels proportion training points stage perform approximately log log peeling steps running data example log log log taking account must integer number observations stage prim fact peel times case ability prim patient help top greedy algorithm find better solution 
[additive, models, trees, related, methods, prim, bump, hunting, spam, example, continued] applied prim spam data response coded spam email first two boxes found prim summarized mars multivariate adaptive regression splines rule global mean box mean box support training test rule capave captot edu rule remain mean box mean box support training test rule remove george box support proportion observations falling box first box purely spam contains test data second box contains test observations spam together two boxes contain data spam next boxes shown quite small containing data predictors listed order importance interestingly top splitting variables cart tree figure appear prim first box 
[additive, models, trees, related, methods, mars, multivariate, adaptive, regression, splines] mars adaptive procedure regression well suited high dimensional problems large number inputs viewed generalization stepwise linear regression modification cart method improve latter performance regression setting introduce mars first point view later make connection cart mars uses expansions piecewise linear basis functions form means positive part otherwise otherwise additive models trees related methods figure basis functions solid orange broken blue used mars example functions shown fig ure function piecewise linear knot value terminology chapter linear splines call two functions reflected pair discussion idea form reflected pairs input knots observed value input therefore collection basis functions input values distinct basis functions alto gether note although basis function depends single example considered function entire input space model building strategy like forward stepwise linear regression instead using original inputs allowed use functions set products thus model form function product two functions given choice coefficients estimated minimiz ing residual sum squares standard linear regression real art however construction functions start constant function model functions set candidate functions depicted figure stage consider new basis function pair products function model set one reflected pairs add model term form mars multivariate adaptive regression splines constant figure schematic mars forward model building procedure left basis functions currently model initially constant function right candidate basis functions considered building model pairs piecewise linear basis functions figure knots unique observed values predictor stage consider products candidate pair basis function model product decreases residual error added current model illustrate first three steps procedure selected functions shown red additive models trees related methods figure function resulting multiplication two piecewise linear mars basis functions produces largest decrease training error coefficients estimated least squares along coefficients model winning products added model process continued model set contains preset maximum number terms example first stage consider adding model function form since multiplication constant function produces function suppose best choice pair basis functions added set next stage consider including pair products form choices third choice produces functions depicted figure end process large model form model typically overfits data backward deletion procedure applied term whose removal causes smallest increase resid ual squared error deleted model stage producing estimated best model size number terms one could use cross validation estimate optimal value computational mars multivariate adaptive regression splines savings mars procedure instead uses generalized cross validation criterion defined gcv value effective number parameters model accounts number terms models plus number parameters used selecting optimal positions knots mathematical simulation results suggest one pay price three parameters selecting knot piecewise linear regression thus linearly independent basis functions model knots selected forward process formula model restricted additive details penalty used using choose model along backward sequence minimizes gcv piecewise linear basis functions particular model strategy key property functions figure ability operate locally zero part range mul tiplied together figure result nonzero small part feature space component functions nonzero result regression surface built parsimoniously using nonzero components locally needed important since one spend parameters carefully high dimensions run quickly use basis functions polynomials would produce nonzero product everywhere would work well second important advantage piecewise linear basis function concerns computation consider product function reflected pairs input appears require fitting single input linear regression models uses oper ations making total operations however exploit simple form piecewise linear function first fit reflected pair rightmost knot knot moved successively one position time left basis functions differ zero left part domain constant right part hence move update fit operations allows try every knot operations forward modeling strategy mars hierarchical sense multiway products built products involving terms already model example four way product added model one three way components already model philosophy high order interaction likely exist lower order footprints exist well need true reasonable working assumption avoids search exponentially growing space alternatives additive models trees related methods rank model test misclassification error gcv choice figure spam data test error misclassification rate mars pro cedure function rank number independent basis functions model one restriction put formation model terms input appear product prevents formation higher order powers input increase decrease sharply near boundaries feature space powers approximated stable way piecewise linear functions useful option mars procedure set upper limit order interaction example one set limit two allowing pairwise products piecewise linear functions three higher way products aid interpretation final model upper limit one results additive model 
[additive, models, trees, related, methods, mars, multivariate, adaptive, regression, splines, spam, example, continued] applied mars spam data analyzed earlier chapter enhance interpretability restricted mars second degree interactions although target two class variable used squared error loss function nonetheless see section figure shows test error misclassification rate function rank number independent sis functions model error rate levels slightly higher generalized additive model discussed earlier gcv chose model size roughly smallest model giving optimal performance leading interactions found mars volved inputs remove free captot however interactions give improvement performance generalized ditive model mars multivariate adaptive regression splines 
[additive, models, trees, related, methods, mars, multivariate, adaptive, regression, splines, example, simulated, data] examine performance mars three contrasting scenarios observations predictors errors independent standard normal distributions scenario data generation model noise standard deviation chosen signal noise ratio call tensor product scenario product term gives surface looks like figure scenario scenario total predictors inputs independent response scenario structure neural network scenarios ideally suited mars scenario contains high order interactions may difficult mars approximate ran five simulations model recorded results scenario mars typically uncovered correct model almost per fectly scenario found correct structure also found extraneous terms involving predictors let true mean let mse ave test mse ave test represent mean square error constant model fitted mars model estimated averaging test values table shows proportional decrease model error scenario mse mse mse values shown means standard error five simulations performance mars degraded slightly inclusion useless inputs scenario performs substantially worse scenario additive models trees related methods table proportional decrease model error mars applied three different scenarios scenario mean tensor product tensor product neural network 
[additive, models, trees, related, methods, mars, multivariate, adaptive, regression, splines, issues] mars classification mars method algorithm extended handle classification problems several strategies suggested two classes one code output treat problem regression spam example two classes one use indicator response approach described section one codes response classes via indicator variables per forms multi response mars regression latter use common set basis functions response variables classification made class largest predicted response value however tential masking problems approach described section generally superior approach optimal scoring method discussed section stone developed hybrid mars called polymars specif ically designed handle classification problems uses multiple logistic framework described section grows model forward stage wise fashion like mars stage uses quadratic approximation multinomial log likelihood search next basis function pair found enlarged model fit maximum likelihood process repeated relationship mars cart although might seem quite different mars cart strategies actually strong similarities suppose take mars procedure make following changes replace piecewise linear basis functions step functions model term involved multiplication candidate term gets replaced interaction hence available interactions changes mars forward procedure cart tree growing algorithm multiplying step function pair reflected hierarchical mixtures experts step functions equivalent splitting node step second restriction implies node may split leads attractive binary tree representation cart model hand restriction makes difficult cart model additive structures mars forgoes tree structure gains ability capture additive effects mixed inputs mars handle mixed predictors quantitative qualitative natural way much like cart mars considers possible binary partitions categories qualitative predictor two groups partition generates pair piecewise constant basis functions indicator functions two sets categories basis pair treated used forming tensor products basis functions already model 
[additive, models, trees, related, methods, hierarchical, mixtures, experts] hierarchical mixtures experts hme procedure viewed variant tree based methods main difference tree splits hard decisions rather soft probabilistic ones node observation goes left right probabilities depending input val ues computational advantages since resulting parameter optimization problem smooth unlike discrete split point search tree based approach soft splits might also help prediction accuracy provide useful alternative description data differences hmes cart implementa tion trees hme linear logistic regression model fit terminal node instead constant cart splits multiway binary splits probabilistic functions linear combination inputs rather single input standard use cart however relative merits choices clear discussed end section simple two level hme model shown figure thought tree soft splits non terminal node however inven tors methodology use different terminology terminal nodes called experts non terminal nodes called gating networks idea expert provides opinion prediction response combined together gating networks see model formally mixture model two level model figure extend multiple levels hence name hierarchical mixtures experts additive models trees related methods gating gating gating gating gating gating gating gating gating network network network network network network network network network network network network network network network network network network network network expert expert expert expert expert expert expert expert expert expert expert figure two level hierarchical mixture experts hme model consider regression classification problem described earlier chapter data either continuous binary valued response vector valued input ease nota tion assume first element one account intercepts hme defined top gating network output vector unknown parameters represents soft way split figure probability assigning observation feature vector jth branch notice groups take coefficient one elements get logistic curve infinite slope case gating probabilities either corresponding hard split input second level gating networks similar form hierarchical mixtures experts probability assignment th branch given assignment jth branch level expert terminal node model response variable form differs according problem regression gaussian linear regression model used classification linear logistic regression model used denoting collection parameters total probability mixture model mixture probabilities determined gating network models estimate parameters maximize log likelihood data log parameters convenient method algorithm describe mixtures section define latent variables zero except single one interpret branching decisions made top level gating network similarly define latent variables describe gating decisions second level step algorithm computes expectations given current values parameters expectations used observation weights step procedure estimate parameters expert networks parameters internal nodes estimated version multiple logistic regression expectations probability profiles used response vectors logistic regressions hierarchical mixtures experts approach promising competitor cart trees using soft splits rather hard decision rules capture situations transition low high response gradual log likelihood smooth function unknown weights hence amenable numerical optimization model similar cart linear combination splits latter difficult optimize additive models trees related methods hand knowledge methods finding good tree topology hme model cart typically one uses fixed tree depth possibly output cart procedure emphasis research hmes prediction rather interpretation final model close cousin hme latent class model lin typically one layer nodes latent classes interpreted groups subjects show similar response behavior 
[additive, models, trees, related, methods, missing, data] quite common observations missing values one input features usual approach impute fill missing values way however first issue dealing problem determining wheth missing data mechanism distorted observed data roughly speaking data missing random mechanism resulting omission independent unobserved value precise definition given little rubin suppose response vector matrix inputs missing denote obs observed entries let obs obs finally indicator matrix ijth entry missing zero otherwise data said missing random mar distribution depends data obs obs parameters distribution data said missing completely random mcar distribution depend observed missing data mcar stronger assumption mar imputation methods rely mcar validity example patient measurement taken doctor felt sick observation would mar mcar case missing data mechanism causes observed training data give distorted picture true population data imputation dangerous instance often determination whether features mcar must made information data collection process categorical features one way diagnose problem code missing additional class fit model training data see class missing predictive response missing data assuming features missing completely random number ways proceeding discard observations missing values rely learning algorithm deal missing values training phase impute missing values training approach used relative amount missing data small otherwise avoided regarding cart one learning algorithm deals effectively missing values surrogate splits section mars prim use similar approaches generalized additive modeling observations missing given input feature omitted partial residuals smoothed feature backfitting algorithm fitted values set zero since fitted curves mean zero model includes intercept amounts assigning average fitted value missing observations learning methods imputation approach necessary simplest tactic impute missing value mean median nonmissing values feature note procedure generalized additive models analogous features least moderate degree dependence one better estimating predictive model feature given features imputing missing value prediction model choosing learning method imputation features one must remember choice distinct method used predicting thus flexible adaptive method often pre ferred even eventual purpose carrying linear regression addition many missing feature values training set learning method must able deal missing feature values cart therefore ideal choice imputation engine imputation missing values typically treated tually observed ignores uncertainty due imputation introduce additional uncertainty estimates predictions response model one measure additional uncertainty multiple imputations hence creating many different training sets predictive model fit training set variation across training sets assessed cart used imputation engine multiple imputations could done sampling values corresponding terminal nodes additive models trees related methods 
[additive, models, trees, related, methods, computational, considerations] observations predictors additive model fitting requires number applications one dimensional smoother regression method required number cycles backfitting algorithm usually less often less depends amount correlation inputs cubic smoothing splines example log operations needed initial sort operations spline fit hence total operations additive model fit log mpn trees require log operations initial sort predictor typically another log operations split computations splits occurred near edges predictor ranges number could increase mars requires pmn operations add basis function model terms already present pool predictors hence build term model requires computations quite prohibitive reasonable fraction components hme typically inexpensive fit step regressions class logistic regression algorithm however take long time converge sizable hme models considered costly fit 
[additive, models, trees, related, methods, bibliographic, notes] comprehensive source generalized additive models text name hastie tibshirani different applications work medical problems discussed hastie hastie herman software implementation splus described chambers hastie green silverman discuss penalization spline models variety settings efron tibshirani give exposition modern developments statistics including generalized additive models nonmathematical audience classification regression trees date back least far morgan sonquist followed modern approaches breiman quinlan prim method due friedman fisher mars introduced friedman additive precursor friedman silverman hierarchical mixtures experts proposed jordan jacobs see also jacobs exercises 
[additive, models, trees, related, methods, exercises] show smoothing spline fit preserves linear part fit words represents linear regression fits smoothing matrix show true local linear regression section hence argue adjustment step second line algorithm unnecessary let known matrix known vector unknown vector gauss seidel algorithm solving linear system equations works successively solving element jth equation fixing current guesses process repeated convergence golub van loan consider additive model observations terms jth term fit linear smoother consider following system equations                vector evaluations jth function data points vector response values show backfitting blockwise gauss seidel algorithm solving system equations let symmetric smoothing operators matrices eigenvalues consider backfitting algorithm response vector smoothers show starting values algorithm converges give formula final iterates backfitting equations consider backfitting procedure orthog onal projections let overall regression matrix whose columns span col col col col denotes column space matrix show estimating equations                equivalent least squares normal equations vector coefficients additive models trees related methods suppose smoother used estimate terms two term additive model variables identical assume symmetric eigenvalues show backfitting residual converges residual sum squares con verges upward residual sum squares converge upward less structured situations fit compare fit single term fit hint use eigen decomposition help comparison degrees freedom tree given data mean variance fitting operation let define degrees freedom fit cov consider fit estimated regression tree fit set predictors terms number terminal nodes give rough formula degrees freedom fit generate observations predictors inde pendent standard gaussian variates fix values generate response values also standard gaussian indepen dent predictors fit regression trees data fixed size terminal nodes hence estimate degrees freedom fit ten simulations response average results get good estimate degrees freedom compare estimates degrees freedom discuss regression tree fit linear operation could write matrix degrees freedom would suggest way compute approximate matrix regression tree compute compare resulting degrees freedom consider ozone data figure fit additive model cube root ozone concentration function temperature wind speed radiation compare results obtained via trellis display figure fit trees mars prim data compare results found figure 
[boosting, additive, trees, boosting, methods] boosting one powerful learning ideas introduced last twenty years originally designed classification problems seen chapter profitably extended regression well motivation boosting procedure combines outputs many weak classifiers produce powerful committee perspective boosting bears resemblance bagging committee based approaches section however shall see connection best superficial boosting fundamentally differ ent begin describing popular boosting algorithm due freund schapire called adaboost consider two class problem output variable coded given vector predictor variables classifier produces prediction taking one two values error rate training sample err expected error rate future predictions weak classifier one whose error rate slightly better random guessing purpose boosting sequentially apply weak classification algorithm repeatedly modified versions data thereby producing sequence weak classifiers boosting additive trees training sample weighted sample weighted sample weighted sample training sample weighted sample weighted sample weighted sample weighted sample training sample weighted sample training sample weighted sample weighted sample weighted sample weighted sample weighted sample weighted sample training sample weighted sample sign final classifier figure schematic adaboost classifiers trained weighted ver sions dataset combined produce final prediction predictions combined weighted majority vote produce final prediction sign computed boosting algorithm weight contribution respective effect give higher influence accurate classifiers sequence figure shows schematic adaboost procedure data modifications boosting step consist applying weights training observations initially weights set first step simply trains classifier data usual manner successive iteration observation weights individually modi fied classification algorithm reapplied weighted observa tions step observations misclassified classifier induced previous step weights increased whereas weights decreased classified correctly thus iterations proceed observations difficult classify correctly ceive ever increasing influence successive classifier thereby forced boosting methods algorithm adaboost initialize observation weights fit classifier training data using weights compute err compute log err err set exp output sign concentrate training observations missed previous ones sequence algorithm shows details adaboost algorithm current classifier induced weighted observations line resulting weighted error rate computed line line calculates weight given producing final classifier line individual weights observations updated next iteration line observations misclassified weights scaled factor exp increasing relative influence inducing next classifier sequence adaboost algorithm known discrete adaboost fried man base classifier returns discrete class label base classifier instead returns real valued prediction probability mapped interval adaboost modified appropriately see real adaboost friedman power adaboost dramatically increase performance even weak classifier illustrated figure features standard independent gaussian deterministic target fined otherwise median chi squared random variable degrees freedom sum squares standard gaussians training cases approximately cases class test observations weak classifier stump two terminal node classification tree applying classifier alone training data set yields poor test set error rate compared boosting additive trees boosting iterations test error single stump node tree figure simulated data test error rate boosting stumps function number iterations also shown test error rate single stump node classification tree random guessing however boosting iterations proceed error rate steadily decreases reaching iterations thus boosting simple weak classifier reduces prediction error rate almost factor four also outperforms single large classification tree error rate since introduction much written explain success adaboost producing accurate classifiers work centered using classification trees base learner improvements often dramatic fact breiman nips workshop referred adaboost trees best shelf classifier world see also breiman especially case data mining applications discussed fully section later chapter 
[boosting, additive, trees, boosting, methods, outline, chapter] outline developments chapter show adaboost fits additive model base learner optimizing novel exponential loss function loss function boosting fits additive model similar negative binomial log likelihood sections population minimizer exponential loss function shown log odds class probabilities section describe loss functions regression classification robust squared error exponential loss section argued decision trees ideal base learner data mining applications boosting sections develop class gradient boosted models gbms boosting trees loss function section importance slow learning emphasized implemented shrinkage new term enters model section well randomization section tools interpretation fitted model described section 
[boosting, additive, trees, boosting, fits, additive, model] success boosting really mysterious key lies pression boosting way fitting additive expansion set elementary basis functions basis functions individual classifiers generally basis function expansions take form expansion coefficients usually simple functions multivariate argument characterized set parameters discuss basis expansions detail chapter additive expansions like heart many learning techniques covered book single hidden layer neural networks chapter sigmoid function param eterizes linear combination input variables signal processing wavelets section popular choice parameterizing location scale shifts mother wavelet multivariate adaptive regression splines section uses truncated power spline basis functions parameterizes variables values knots boosting additive trees algorithm forward stagewise additive modeling initialize compute arg min set trees parameterizes split variables split points internal nodes predictions terminal nodes typically models fit minimizing loss function averaged training data squared error likelihood based loss function min many loss functions basis functions quires computationally intensive numerical optimization techniques ever simple alternative often found feasible rapidly solve subproblem fitting single basis function min 
[boosting, additive, trees, forward, stagewise, additive, modeling] forward stagewise modeling approximates solution sequen tially adding new basis functions expansion without adjusting parameters coefficients already added outlined algorithm iteration one solves optimal basis function corresponding coefficient add cur rent expansion produces process repeated previously added terms modified squared error loss exponential loss adaboost one simply residual current model ith observation thus squared error loss term best fits current residuals added expansion step idea basis least squares regression boosting discussed section however show near end next section squared error loss generally good choice classification hence need consider loss criteria 
[boosting, additive, trees, exponential, loss, adaboost] show adaboost algorithm equivalent forward stagewise additive modeling algorithm using loss function exp appropriateness criterion addressed next section adaboost basis functions individual classifiers using exponential loss function one must solve arg min exp classifier corresponding coefficient added step expressed arg min exp exp since depends neither regarded weight applied observa tion weight depends individual weight values change iteration solution obtained two steps first value solution arg min boosting additive trees classifier minimizes weighted error rate predicting easily seen expressing criterion turn written plugging solving one obtains log err err err minimized weighted error rate err approximation updated causes weights next iteration using fact becomes quantity defined line adaboost gorithm factor multiplies weights value effect thus equivalent line algorithm one view line adaboost algorithm method approximately solving minimization hence hence conclude adaboost minimizes exponential loss criterion via forward stagewise additive modeling approach figure shows training set misclassification error rate aver age exponential loss simulated data problem figure training set misclassification error decreases zero around erations remains exponential loss keeps decreasing notice also figure test set misclassification error continues improve iteration clearly adaboost optimizing training set misclassification error exponential loss sensitive changes estimated class probabilities exponential loss boosting iterations training error misclassification rate exponential loss figure simulated data boosting stumps misclassification error rate training set average exponential loss exp iterations misclassification error zero exponential loss continues decrease 
[boosting, additive, trees, exponential, loss] adaboost algorithm originally motivated differ ent perspective presented previous section equivalence forward stagewise additive modeling based exponential loss discovered five years inception studying properties exponential loss criterion one gain insight procedure dis cover ways might improved principal attraction exponential loss context additive modeling computational leads simple modular reweighting aboost algorithm however interest inquire statistical properties estimate well estimated first question answered seeking population minimizer easy show friedman arg min log boosting additive trees equivalently thus additive expansion produced adaboost estimating one half log odds justifies using sign classifi cation rule another loss criterion population minimizer nomial negative log likelihood deviance also known cross entropy interpreting logit transform let define binomial log likelihood loss function log log equivalently deviance log since population maximizer log likelihood true probabilities see population minimizers deviance thus using either criterion leads solution population level note proper log likelihood since logarithm probability mass function binary random variable 
[boosting, additive, trees, loss, functions, robustness] section examine different loss functions classification regression closely characterize terms robustness extreme data robust loss functions classification although exponential binomial deviance yield solution applied population joint distribution true finite data sets criteria monotone decreasing functions margin classification response margin plays role analogous residuals regression classification rule sign implies observations positive margin classified correctly whereas negative margin misclassified decision boundary defined loss functions robustness misclassification exponential binomial deviance squared error support vector figure loss functions two class classification response prediction class prediction sign losses misclassification sign exponential exp binomial deviance log exp squared error support vector see section function scaled passes point goal classification algorithm produce positive margins frequently possible loss criterion used classification penalize negative margins heavily positive ones since positive margin observations already correctly classified figure shows exponential binomial deviance criteria function margin also shown misclassification loss gives unit penalty negative mar gin values penalty positive ones exponential deviance loss viewed monotone continuous approximations misclassification loss continuously penalize increasingly negative margin values heavily reward increasingly positive ones difference degree penalty associated nomial deviance increases linearly large increasingly negative margin whereas exponential criterion increases influence observa tions exponentially point training process exponential criterion concen trates much influence observations large negative margins binomial deviance concentrates relatively less influence observa boosting additive trees tions evenly spreading influence among data therefore far robust noisy settings bayes error rate close zero especially situations misspecification class labels training data performance adaboost empirically observed dramatically degrade situations also shown figure squared error loss minimizer cor responding risk population arg min classification rule sign squared error loss good surrogate misclassification error seen figure monotone decreasing function increasing margin mar gin values increases quadratically thereby placing increasing influence error observations correctly classified increas ing certainty thereby reducing relative influence incorrectly classified thus class assignment goal monotone creasing criterion serves better surrogate loss function figure page chapter includes modification quadratic loss berized square hinge loss rosset enjoys favorable properties binomial deviance quadratic loss svm hinge loss population minimizer quadratic zero becomes linear since quadratic functions easier compute exponentials experience suggests useful alternative binomial deviance class classification response takes values unordered set see sections seek classifier taking values sufficient know class conditional proba bilities bayes classifier arg max principal though need learn simply one largest however data mining applications interest often class probabilities rather per forming class assignment section logistic model generalizes naturally classes ensures sum one note different functions one per class redundancy functions since adding arbitrary leaves model unchanged traditionally one set zero example loss functions robustness prefer retain symmetry impose constraint binomial deviance extends naturally class multinomial deviance loss function log log two class case criterion penalizes incorrect predictions linearly degree incorrectness zhu generalize exponential loss class problems see exercise details robust loss functions regression regression setting analogous relationship exponential loss binomial log likelihood relationship squared error loss absolute loss population solutions squared error loss median absolute loss symmetric error distributions however finite samples squared error loss places much emphasis observations large absolute residuals fitting process thus far less robust performance severely degrades long tailed error distributions especially grossly mis measured values outliers robust criteria abso lute loss perform much better situations statistical bustness literature variety regression loss criteria proposed provide strong resistance absolute immunity gross outliers nearly efficient least squares gaussian errors often better either error distributions moderately heavy tails one criterion huber loss criterion used regression huber otherwise figure compares three loss functions considerations suggest robustness concern especially case data mining applications see section squared error loss regression exponential loss classification best criteria statistical perspective however lead elegant modular boosting algorithms context forward stagewise additive modeling squared error loss one simply fits base learner residuals current model step boosting additive trees squared error absolute error huber figure comparison three loss functions regression plotted function margin huber loss function combines good properties squared error loss near zero absolute error loss large exponential loss one performs weighted fit base learner output values weights exp using robust criteria directly place give rise simple feasible boosting algorithms however section show one derive simple elegant boosting algorithms based differentiable loss criterion thereby producing highly robust boosting procedures data mining 
[boosting, additive, trees, off-the-shelf, procedures, data, mining] predictive learning important aspect data mining seen book wide variety methods developed predic tive learning data particular method situations particularly well suited others performs badly compared best done data attempted characterize appropriate situations discussions spective methods however seldom known advance procedure perform best even well given problem table summarizes characteristics number learning methods industrial commercial data mining applications tend especially challenging terms requirements placed learning procedures data sets often large terms number observations number variables measured thus computational con shelf procedures data mining table characteristics different learning methods key good fair poor characteristic neural svm trees mars nets kernels natural handling data mixed type handling missing values robustness outliers input space insensitive monotone transformations inputs computational scalability large ability deal irrel evant inputs ability extract linear combinations features interpretability predictive power siderations play important role also data usually messy inputs tend mixtures quantitative binary categorical vari ables latter often many levels generally many missing values complete observations rare distributions numeric predic tor response variables often long tailed highly skewed case spam data section fitting generalized additive model first log transformed predictors order get reasonable fit addition usually contain substantial fraction gross mis measurements outliers predictor variables generally measured different scales data mining applications usually small fraction large number predictor variables included analysis actually relevant prediction also unlike many applications pat tern recognition seldom reliable domain knowledge help create especially relevant features filter irrelevant ones inclu sion dramatically degrades performance many methods addition data mining applications generally require interpretable mod els enough simply produce predictions also desirable information providing qualitative understanding relationship boosting additive trees joint values input variables resulting predicted sponse value thus black box methods neural networks quite useful purely predictive settings pattern recognition far less useful data mining requirements speed interpretability messy nature data sharply limit usefulness learning procedures shelf methods data mining shelf method one directly applied data without requiring great deal time consuming data preprocessing careful tuning learning procedure well known learning methods decision trees come closest meeting requirements serving shelf procedure data mining relatively fast construct produce interpretable models trees small discussed section naturally incorporate mixtures numeric categorical predictor variables missing values invariant strictly monotone transforma tions individual predictors result scaling general transformations issue immune effects pre dictor outliers perform internal feature selection integral part procedure thereby resistant completely immune inclusion many irrelevant predictor variables properties decision trees largely reason emerged popular learning method data mining trees one aspect prevents ideal tool predictive learning namely inaccuracy seldom provide predictive curacy comparable best achieved data hand seen section boosting decision trees improves accuracy often dramatically time maintains desirable properties data mining advantages trees sacrificed boosting speed interpretability adaboost robustness overlapping class distributions especially mislabeling training data gradient boosted model gbm generalization tree boosting attempts mitigate problems produce accurate effective shelf procedure data mining 
[boosting, additive, trees, example, spam, data] details gradient boosting demonstrate abili ties two class classification problem spam data introduced chapter used example many procedures chapter sections applying gradient boosting data resulted test error rate using test set used section comparison additive logistic regression achieved cart tree fully grown boosting trees pruned cross validation mars standard error estimates around although gradient boosting significantly better using mcnemar test exercise section develop relative importance measure predictor well partial dependence plot describing predictor contribution fitted model illustrate spam data figure displays relative importance spectrum predictor variables clearly predictors important others sep arating spam email frequencies character strings remove estimated four relevant predictor variables end spectrum character strings table virtually relevance quantity modeled log odds spam versus email log spam email see section figure shows partial dependence log odds selected important predictors two positively associated spam remove two negatively associated edu particular dependencies seen essentially monotonic general agreement corresponding functions found additive logistic regression model see figure page running gradient boosted model data terminal node trees produces purely additive main effects model log odds corresponding error rate compared full gradient boosted model terminal node trees although significant slightly higher error rate suggests may interactions among important predictor variables diagnosed two variable partial dependence plots figure shows one several plots displaying strong interaction effects one sees low frequencies log odds spam greatly increased high frequencies log odds spam tend much lower roughly constant function frequency decreases functional relationship strengthens 
[boosting, additive, trees, boosting, trees] regression classification trees discussed detail section partition space joint predictor variable values disjoint regions represented terminal nodes tree constant assigned region predictive rule boosting additive trees remove free capave capmax george captot edu money business receive internet email meeting mail people technology hpl order address make font project data original report conference lab credit parts table direct telnet labs addresses relative importance figure predictor variable importance spectrum spam data variable names written vertical axis boosting trees partial dependence remove partial dependence edu partial dependence partial dependence figure partial dependence log odds spam four important pre dictors red ticks base plots deciles input variable figure partial dependence log odds spam email func tion joint frequencies character boosting additive trees thus tree formally expressed parameters usually treated meta parameter parameters found minimizing empirical risk arg min formidable combinatorial optimization problem usually settle approximate suboptimal solutions useful divide opti mization problem two parts finding given given estimating typically trivial often mean falling region mis classification loss modal class observations falling region finding difficult part approximate solutions found note also finding entails estimating well typical strategy use greedy top recursive partitioning algorithm find addition sometimes necessary approximate smoother convenient criterion optimizing arg min given estimated precisely using original criterion section described strategy classification trees gini index replaced misclassification loss growing tree identifying boosted tree model sum trees induced forward stagewise manner algorithm step forward stagewise procedure one must solve arg min boosting trees region set constants next tree given current model given regions finding optimal constants region typically straightforward arg min finding regions difficult even difficult single tree special cases problem simplifies squared error loss solution harder single tree simply regression tree best predicts current residuals mean residuals corresponding region two class classification exponential loss stagewise approach gives rise adaboost method boosting classification trees algo rithm particular trees restricted scaled classification trees showed section solution tree minimizes weighted error rate weights scaled classification tree mean restriction without restriction still simplifies exponential loss weighted exponential criterion new tree arg min exp straightforward implement greedy recursive partitioning algorithm using weighted exponential loss splitting criterion given one show exercise solution weighted log odds corresponding region log requires specialized tree growing algorithm practice prefer approximation presented uses weighted least squares gression tree using loss criteria absolute error huber loss place squared error loss regression deviance place exponential loss classification serve robustify boosting trees unfortunately unlike nonrobust counterparts robust criteria give rise simple fast boosting algorithms general loss criteria solution given typically straightforward since simple location estimate boosting additive trees absolute loss median residuals respective region criteria fast iterative algorithms exist solving usually faster single step approximations adequate problem tree induction simple fast algorithms exist solving general loss criteria approximations like become essential 
[boosting, additive, trees, numerical, optimization, via, gradient, boosting] fast approximate algorithms solving differentiable loss criterion derived analogy numerical optimization loss using predict training data goal minimize respect con strained sum trees ignoring constraint minimizing viewed numerical optimization arg min parameters values approximating func tion data points numerical optimization procedures solve sum component vectors initial guess successive induced based current parameter vector sum previously induced updates numerical optimization methods differ prescrip tions computing increment vector step 
[boosting, additive, trees, numerical, optimization, via, gradient, boosting, steepest, descent] steepest descent chooses scalar gradient evaluated components gradient numerical optimization via gradient boosting step length solution arg min current solution updated process repeated next iteration steepest descent viewed greedy strategy since local direction rapidly decreasing 
[boosting, additive, trees, numerical, optimization, via, gradient, boosting, gradient, boosting] forward stagewise boosting algorithm also greedy strategy step solution tree one maximally reduces given current model fits thus tree predic tions analogous components negative gradient principal difference tree compo nents independent con strained predictions terminal node decision tree whereas negative gradient unconstrained maximal descent direction solution stagewise approach analogous line search steepest descent difference performs separate line search components correspond separate terminal region minimizing loss training data goal steep est descent would preferred strategy gradient trivial calculate differentiable loss function whereas solving difficult robust criteria discussed section unfor tunately gradient defined training data points whereas ultimate goal generalize new data repre sented training set possible resolution dilemma induce tree mth iteration whose predictions close possible negative gradient using squared error measure closeness leads arg min one fits tree negative gradient values least squares noted section fast algorithms exist least squares decision tree induction although solution regions identical regions solve generally sim ilar enough serve purpose case forward stagewise boosting additive trees table gradients commonly used loss functions setting loss function regression regression sign regression huber sign th quantile classification deviance kth component boosting procedure top decision tree induction approximation procedures constructing tree corre sponding constants region given table summarizes gradients commonly used loss functions squared error loss negative gradient ordinary residual equivalent standard least squares boosting absolute error loss negative gradient sign residual iteration fits tree sign current residuals least squares huber regression negative gradient compromise two see table classification loss function multinomial deviance least squares trees constructed iteration tree fit respective negative gradient vector ikm given although separate trees built iteration related binary classification one tree needed exercise 
[boosting, additive, trees, numerical, optimization, via, gradient, boosting, implementations, gradient, boosting] algorithm presents generic gradient tree boosting algorithm regression specific algorithms obtained inserting different loss cri teria first line algorithm initializes optimal constant model single terminal node tree components negative gradient computed line referred general ized pseudo residuals gradients commonly used loss functions summarized table right sized trees boosting algorithm gradient tree boosting algorithm initialize arg min compute fit regression tree targets giving terminal regions compute arg min update output algorithm classification similar lines repeated times iteration class using result line different coupled tree expansions produce probabilities via classification details given exercise two basic tuning parameters number iterations sizes constituent trees original implementation algorithm called mart multiple additive regression trees referred first edi tion book many figures chapter produced mart gradient boosting described implemented gbm package ridgeway gradient boosted models freely avail able gbm package used section extensively chap ters another implementation boosting mboost hothorn uhlmann commercial implementation gradient boost ing mart called treenet available salford systems inc 
[boosting, additive, trees, right-sized, trees, boosting] historically boosting considered technique combining mod els trees tree building algorithm regarded boosting additive trees primitive produced models combined boosting proce dure scenario optimal size tree estimated separately usual manner built section large oversized tree first induced bottom procedure employed prune estimated optimal number terminal nodes approach sumes implicitly tree last one expansion except perhaps last tree clearly poor assump tion result trees tend much large especially early iterations substantially degrades performance increases computation simplest strategy avoiding problem restrict trees size iteration terminal node regression tree induced thus becomes meta parameter entire boosting procedure adjusted maximize estimated performance data hand one get idea useful values considering properties target function arg min expected value population joint distribution target function one minimum prediction risk future data function trying approximate one relevant property degree coordinate vari ables interact one another captured anova analysis variance expansion jkl jkl first sum functions single predictor variable particular functions jointly best approximate loss criterion used called main effect second sum two variable functions added main effects best fit called second order interactions respective variable pair third sum represents third order interactions many problems encountered practice low order interaction effects tend dominate case models produce strong higher order interaction effects large decision trees suffer accuracy interaction level tree based approximations limited tree size namely interaction effects level greater pos sible since boosted models additive trees limit extends well setting single split decision stump produces boosted models main effects interactions per mitted two variable interaction effects also allowed right sized trees boosting number terms test error stumps node node adaboost figure boosting different sized trees applied example used figure since generative model additive stumps perform best boosting algorithm used binomial deviance loss algorithm shown comparison adaboost algorithm suggests value chosen reflect level dominant interactions course generally unknown situations tend low figure illustrates effect interaction order choice simulation example generative function additive sum quadratic monomials boosting models incurs unnecessary variance hence higher test error figure compares coordinate functions found boosted stumps true functions although many applications insufficient unlikely required experience far indicates works well context boosting results fairly insensitive particular choices range one fine tune value trying several different values choosing one produces low est risk validation sample however seldom provides significant improvement using boosting additive trees coordinate functions additive logistic trees figure coordinate functions estimated boosting stumps sim ulated example used figure true quadratic functions shown comparison 
[boosting, additive, trees, regularization] besides size constituent trees meta parameter gradient boosting number boosting iterations iteration usually reduces training risk large enough risk made arbitrarily small however fitting training data well lead overfitting degrades risk future predictions thus optimal number minimizing future risk application dependent convenient way estimate monitor prediction risk function validation sample value minimizes risk taken estimate analogous early stopping strategy often used neural networks section 
[boosting, additive, trees, regularization, shrinkage] controlling value possible regularization strategy ridge regression neural networks shrinkage techniques employed well see sections simplest implementation shrinkage context boosting scale contribution tree factor added current approximation line algorithm replaced parameter regarded controlling learning rate boosting procedure smaller values shrinkage result larger training risk number iterations thus control prediction risk training data however parameters regularization operate independently smaller values lead larger values training risk tradeoff empirically found friedman smaller values favor better test error require correspondingly larger values fact best strategy appears set small choose early stopping yields dramatic improvements shrinkage regression probability estimation corresponding improvements misclassification risk via less still substantial price paid improvements computa tional smaller values give rise larger values computation proportional latter however seen many iterations generally computationally feasible even large data sets partly due fact small trees induced step pruning figure shows test error curves simulated example figure gradient boosted model mart trained using binomial deviance using either stumps six terminal node trees shrinkage benefits shrinkage evident especially binomial deviance tracked shrinkage test error curve reaches lower value stays many iterations section draws connection forward stagewise shrinkage boosting use penalty regularizing model parame ters lasso argue penalties may superior penalties used methods support vector machine 
[boosting, additive, trees, regularization, subsampling] saw section bootstrap averaging bagging improves performance noisy classifier averaging chapter discusses detail variance reduction mechanism sampling followed averaging exploit device gradient boosting improve performance computational efficiency stochastic gradient boosting friedman iteration sample fraction training observations without replacement grow next tree using subsample rest algorithm identical typical value although large substantially smaller sampling reduce computing time fraction many cases actually produces accurate model figure illustrates effect subsampling using simulated example classification regression example see cases sampling along shrinkage slightly outperformed rest appears subsampling without shrinkage poorly boosting additive trees boosting iterations test set deviance shrinkage shrinkage stumps deviance boosting iterations test set misclassification error shrinkage shrinkage stumps misclassification error boosting iterations test set deviance shrinkage shrinkage node trees deviance boosting iterations test set misclassification error shrinkage shrinkage node trees misclassification error figure test error curves simulated example figure using gradient boosting mart models trained using binomial viance either stumps six terminal node trees without shrinkage left panels report test deviance right panels show misclassification error beneficial effect shrinkage seen cases especially deviance left panels interpretation boosting iterations test set deviance deviance node trees boosting iterations test set absolute error shrinkage shrink sample shrink sample absolute error figure test error curves simulated example showing effect stochasticity curves labeled sample different subsample training data used time tree grown left panel models fit gbm using binomial deviance loss function right hand panel using square error loss downside four parameters set typically early explorations determine suitable values leaving primary parameter 
[boosting, additive, trees, interpretation] single decision trees highly interpretable entire model com pletely represented simple two dimensional graphic binary tree easily visualized linear combinations trees lose important feature must therefore interpreted different way 
[boosting, additive, trees, interpretation, relative, importance, predictor, variables] data mining applications input predictor variables seldom equally relevant often substantial influence sponse vast majority irrelevant could well included often useful learn relative importance contri bution input variable predicting response boosting additive trees single decision tree breiman proposed measure relevance predictor variable sum internal nodes tree node one input variables used partition region associated node two subregions within separate constant fit response values particular variable chosen one gives maximal estimated improvement squared error risk constant fit entire region squared relative importance variable sum squared improvements internal nodes chosen splitting variable importance measure easily generalized additive tree expansions simply averaged trees due stabilizing effect averaging measure turns reliable counterpart single tree also shrinkage section masking important variables others highly correlated much less problem note refer squared relevance actual relevances respective square roots since measures relative customary assign largest value scale others accordingly figure shows relevant importance inputs predicting spam versus email class classification separate models induced consisting sum trees case generalizes relevance separating class observations classes overall relevance obtained averaging classes interpretation figures illustrate use averaged separate relative importances 
[boosting, additive, trees, interpretation, partial, dependence, plots] relevant variables identified next step attempt understand nature dependence approximation joint values graphical renderings function arguments provides comprehensive summary dependence joint values input variables unfortunately visualization limited low dimensional views easily display functions one two arguments either continuous discrete mixed variety different ways book filled displays functions slightly higher dimensions plotted conditioning particular sets values one two arguments producing trellis plots becker two three variables viewing functions corre sponding higher dimensional arguments difficult useful alterna tive sometimes view collection plots one shows partial dependence approximation selected small sub set input variables although collection seldom provide comprehensive depiction approximation often produce helpful clues especially dominated low order interactions consider subvector input predictor variables indexed let complement set general function principle depend input variables one way define average partial dependence marginal average serve useful description effect chosen subset example variables strong interactions partial dependence functions used interpret results black box learning method estimated values occurring training data requires pass data set joint values evaluated computationally intensive lattice boosting additive trees even moderately sized data sets fortunately decision trees rapidly computed tree without reference data exercise important note partial dependence functions defined represent effect accounting erage effects variables effect ignoring effects latter given con ditional expectation best least squares approximation function alone quantities unlikely event independent example effect chosen variable subset happens purely additive produces additive constant effect purely multiplicative produces multiplicative constant factor hand produce either case fact produce strong effects variable subsets dependence viewing plots partial dependence boosted tree approxima tion selected variables subsets help provide qualitative description properties illustrations shown sections owing limitations computer graphics human percep tion size subsets must small course large number subsets chosen among usually much smaller set highly relevant predictors likely informative also subsets whose effect approximately additive multiplicative revealing class classification separate models one class one related respective probabilities log log thus monotone increasing function respective prob ability logarithmic scale partial dependence plots respective relevant predictors help reveal log odds realizing class depend respective input variables illustrations 
[boosting, additive, trees, illustrations] section illustrate gradient boosting number larger datasets using different loss functions appropriate 
[boosting, additive, trees, illustrations, california, housing] data set pace barry available carnegie mellon statlib repository consists aggregated data neighborhoods census block groups california response vari able median house value neighborhood measured units predictor variables demographics median income medinc housing density reflected number houses house average occupancy house aveoccup also included predictors location neighborhood longitude latitude several quantities reflecting properties houses neighborhood erage number rooms averooms bedrooms avebedrms thus total eight predictors numeric fit gradient boosting model using mart procedure terminal nodes learning rate huber loss criterion predicting numeric response randomly divided dataset training set test set figure shows average absolute error aae function number iterations training data test data test error seen decrease monotonically increasing rapidly early stages leveling nearly constant iterations increase thus choice particular value critical long small tends case many applications shrinkage strategy tends eliminate problem overfitting especially larger data sets value aae iterations compared optimal constant predictor median terms familiar quantities squared multiple correlation coefficient model pace barry use sophisticated spatial auto regression procedure prediction neighborhood based median house values nearby neighborhoods using predictors covariates experimenting transformations achieved predicting log using log response corresponding value gradient boosting http lib stat cmu edu boosting additive trees iterations absolute error training test absolute error train error test error figure average absolute error function number iterations california housing data figure displays relative variable importances eight predictor variables surprisingly median income neigh borhood relevant predictor longitude latitude average occupancy roughly half relevance income whereas others somewhat less influential figure shows single variable partial dependence plots relevant nonlocation predictors note plots strictly smooth consequence using tree based models decision trees produce discontinuous piecewise constant models carries sums trees course many pieces unlike meth ods discussed book smoothness constraint imposed result arbitrarily sharp discontinuities modeled fact curves generally exhibit smooth trend estimated best predict response problem often case hash marks base plot delineate deciles data distribution corresponding variables note data density lower near edges especially larger values causes curves somewhat less well determined regions vertical scales plots give visual comparison relative importance different variables partial dependence median house value median income monotonic increasing nearly linear main body data house value generally monotonic decreasing increasing average occupancy except perhaps average occupancy rates less one median house illustrations medinc longitude aveoccup latitude houseage averooms avebedrms population relative importance figure relative importance predictors california housing data value nonmonotonic partial dependence average number rooms minimum approximately three rooms increasing smaller larger values median house value seen weak partial dependence house age inconsistent importance ranking figure suggests weak main effect may masking stronger interac tion effects variables figure shows two variable partial dependence housing value joint values median age average cupancy interaction two variables apparent values average occupancy greater two house value nearly independent median age whereas values less two strong dependence age figure shows two variable partial dependence fitted model joint values longitude latitude displayed shaded contour plot clearly strong dependence median house value neighborhood location california note figure plot house value versus location ignoring effects predictors like partial dependence plots represents effect location accounting effects neighborhood house attributes viewed representing extra premium one pays location premium seen relatively large near pacific coast especially bay area los angeles san diego boosting additive trees medinc partial dependence aveoccup partial dependence houseage partial dependence averooms partial dependence figure partial dependence housing value nonlocation vari ables california housing data red ticks base plot deciles input variables aveoccup houseage figure partial dependence house value median age aver age occupancy appears strong interaction effect two variables illustrations longitude latitude figure partial dependence median house value location cal ifornia one unit prices values plotted relative overall median gions northern central valley southeastern desert regions california location costs considerably less 
[boosting, additive, trees, illustrations, new, zealand, fish] plant animal ecologists use regression models predict species pres ence abundance richness function environmental variables although many years simple linear parametric models popu lar recent literature shows increasing interest sophisticated mod els generalized additive models section gam multivariate adaptive regression splines section mars boosted regression trees leathwick leathwick model boosting additive trees presence abundance black oreo dory marine fish found oceanic waters around new zealand figure shows locations trawls deep water net fishing maximum depth red points indicate trawls black oreo present one hundred species regularly recorded catch size species recorded trawl along species catch number environmental mea surements available trawl include average depth trawl avgdepth temperature salinity water since latter two strongly correlated depth leathwick derived instead tempresid salresid residuals obtained two measures adjusted depth via separate non parametric regres sions sstgrad measure gradient sea surface temperature chla broad indicator ecosytem productivity via satellite image measurements suspartmatter provides measure suspended particulate matter particularly coastal waters also satellite derived goal analysis estimate probability finding black oreo trawl well expected catch size standardized take account effects variation trawl speed distance well mesh size trawl net authors used logistic regression estimating probability catch size might seem natural assume poisson distribution model log mean count often appropriate excessive number zeros although specialized approaches developed zero inflated poisson lambert chose simpler approach non negative catch size second term estimated logistic regression first term estimated using trawls positive catch logistic regression authors used gradient boosted model gbm binomial deviance loss function depth trees shrink age factor positive catch regression modeled log using gbm squared error loss also depth trees logged predictions cases used fold cross validation selecting number terms well shrinkage factor models data maps shown kindly provided john leathwick national institute water atmospheric research new zealand jane elith school botany university melbourne collection research trawl data took place funded new zealand ministry fisheries version package gbm ver illustrations figure map new zealand surrounding exclusive economic zone showing locations trawls small blue dots taken red points indicate trawls species black oreo dory present boosting additive trees number trees mean deviance gbm test gbm gam test specificity sensitivity auc gam gbm figure left panel shows mean deviance function number trees gbm logistic regression model fit presence absence data shown fold cross validation training data bars test deviance test data also shown comparison test deviance using gam model term right panel shows roc curves test data chosen gbm model vertical line left plot gam model figure left panel shows mean binomial deviance quence gbm models fold test data mod est improvement performance gam model fit using smoothing splines degrees freedom per term right panel shows roc curves see section models measures predictive performance point view performance looks simi lar gbm perhaps slight edge summarized auc area curve point equal sensitivity specificity gbm achieves gam figure summarizes contributions variables logistic gbm fit see well defined depth range black oreo caught much frequent capture colder waters give details quantitative catch model important variables much predictors used models available fine geographi cal grid fact derived environmental atlases satellite ages like see leathwick details also means predictions made grid imported gis mapping systems figure shows prediction maps presence catch size standardized common set trawl conditions since predictors vary continuous fashion geographical location predictions illustrations orbvel speed distance disorgmatter codendsize pentade tidalcurr slope chlacase sstgrad salresid suspartmatter avgdepth tempresid relative influence tempresid tempresid avgdepth avgdepth suspartmatter suspartmatter salresid salresid sstgrad sstgrad figure top left panel shows relative influence computed gbm logistic regression model remaining panels show partial pendence plots leading five variables plotted scale comparison ability model interactions automatically select variables well robustness outliers missing data gbm models rapidly gaining popularity data rich enthusiastic community 
[boosting, additive, trees, illustrations, demographics, data] section illustrate gradient boosting multiclass classifica tion problem using mart data come questionnaires filled shopping mall customers san francisco bay area impact resources inc columbus among questions concerning demographics illustration goal predict occupation ing variables predictors hence identify demographic variables discriminate different occupational categories randomly divided data training set test set used node trees learning rate figure shows occupation class values along corresponding error rates overall error rate compared null rate obtained predicting numerous boosting additive trees figure geological prediction maps presence probability left map catch size right map obtained gradient boosted models class prof man professional managerial four best predicted classes seen retired student prof man homemaker figure shows relative predictor variable importances aver aged classes figure displays individual relative importance distributions four best predicted classes one sees relevant predictors generally different respective class exception age among three relevant predicting retired student prof man figure shows partial dependence log odds age three classes abscissa values ordered codes respective equally spaced age intervals one sees accounting contri butions variables odds retired higher older people whereas opposite case student odds professional managerial highest middle aged people results course surprising illustrate inspecting partial dependences separately class lead sensible results 
[boosting, additive, trees, bibliographic, notes] schapire developed first simple boosting procedure pac learning framework valiant kearns vazirani schapire illustrations sales unemployed military clerical labor homemaker prof man retired student error rate overall error rate figure error rate occupation demographics data age income edu hsld stat mar dlinc sex ethnic mar stat typ home lang num hsld children yrs relative importance figure relative importance predictors averaged classes demographics data boosting additive trees age mar dlinc sex ethnic income hsld stat mar stat lang typ home children edu num hsld yrs relative importance class retired hsld stat age income mar stat edu ethnic num hsld typ home sex mar dlinc lang yrs children relative importance class student edu income age mar dlinc ethnic hsld stat typ home sex num hsld lang mar stat yrs children relative importance class prof man sex mar dlinc children ethnic num hsld edu mar stat lang typ home income age hsld stat yrs relative importance class homemaker figure predictor variable importances separately four classes lowest error rate demographics data illustrations age partial dependence retired age partial dependence student age partial dependence prof man figure partial dependence odds three different occupations age demographics data showed weak learner could always improve performance train ing two additional classifiers filtered versions input data stream weak learner algorithm producing two class classifier performance guaranteed high probability significantly better coin flip learning initial classifier first training points learned new sample points half misclas sified learned points disagree boosted classifier majority vote schapire strength weak learnability theorem proves improved performance freund proposed boost majority variation combined many weak learners simultaneously improved performance simple boosting algorithm schapire theory supporting boosting additive trees algorithms requires weak learner produce classifier fixed error rate led adaptive realistic adaboost freund schapire offspring assumption dropped freund schapire schapire singer provide theory support algorithms form upper bounds generalization error theory evolved computational learning community initially based concepts pac learning theo ries attempting explain boosting come game theory freund schapire breiman breiman theory schapire bounds theory associated adaboost algorithms interesting tend loose practical portance practice boosting achieves results far impressive bounds would imply schapire meir atsch give useful overviews recent first edition book friedman friedman form basis expo sition chapter friedman analyze adaboost statistically derive exponential criterion show estimates log odds class probability propose additive tree models right sized trees anova representation section multiclass logit formulation friedman developed gradient boosting shrinkage classification regression friedman explored stochastic variants boosting mason also embraced gradient approach boosting published discussions friedman shows controversy boosting works since publication first edition book debates continued spread statistical community series papers consistency boosting jiang lugosi vayatis zhang bartlett traskin mease wyner series simulation examples challenge interpre tations boosting response friedman puts objections rest recent survey uhlmann hothorn supports approach boosting 
[boosting, additive, trees, exercises] derive expression update parameter adaboost prove result minimizer population version adaboost criterion one half log odds show marginal average recovers additive multiplicative functions conditional expec tation exercises write program implementing adaboost trees redo computations example figure plot train ing error well test error discuss behavior investigate number iterations needed make test error finally start rise change setup example follows define two classes features class standard indepen dent gaussian variates class features also standard independent gaussian conditioned event classes significant overlap feature space repeat adaboost experiments figure discuss results multiclass exponential loss zhu class clas sification problem consider coding otherwise let define exp using lagrange multipliers derive population minimizer subject zero sum constraint relate class probabilities show multiclass boosting using loss function leads reweighting algorithm similar adaboost section mcnemar test agresti report test error rates spam data generalized additive model gam gradient boosting gbm test sample size show standard error estimates since test data used methods error rates correlated cannot perform two sample test compare methods directly test observation leading summary gbm gam correct error correct error boosting additive trees mcnemar test focuses discordant errors conduct test show gam makes significantly errors gradient boosting two sided value derive expression consider class problem targets coded observation class zero otherwise suppose current model see section wish update model observations region predictor space adding constants write multinomial log likelihood problem first second derivatives using diagonal hessian matrix starting show one step approximate newton update exp prefer update sum zero current model using symmetry arguments show appropriate update defined consider class problem targets coded observation class zero otherwise using multinomial deviance loss function symmetric logistic transform use arguments leading gradient boosting algorithm derive algorithm hint see exercise step iii show class classification one tree needs grown gradient boosting iteration show compute partial dependence function efficiently referring let assume bivariate gaussian mean zero vari ance one show even though function exercises algorithm gradient boosting class classification initialize set compute ikm fit regression tree targets ikm giving terminal regions jkm iii compute jkm jkm ikm jkm ikm ikm update jkm jkm output boosting additive trees 
[neural, networks, introduction] chapter describe class learning methods developed separately different fields statistics artificial intelligence based essentially identical models central idea extract linear com binations inputs derived features model target nonlinear function features result powerful learning method widespread applications many fields first discuss projection pursuit model evolved domain semiparamet ric statistics smoothing rest chapter devoted neural network models 
[neural, networks, projection, pursuit, regression] generic supervised learning problem assume input vector components target let unit vectors unknown parameters projection pursuit regression ppr model form additive model derived features rather inputs functions unspecified esti neural networks figure perspective plots two ridge functions left exp right sin mated along directions using flexible smoothing method see function called ridge function varies direction defined vector scalar variable projection onto unit vector seek model fits well hence name projection pursuit figure shows examples ridge functions example left function varies direction example right ppr model general since operation forming nonlinear functions linear combinations generates surprisingly large class models example product written higher order products represented simi larly fact taken arbitrarily large appropriate choice ppr model approximate continuous function arbitrarily well class models called universal approximator however generality comes price interpretation fitted model usually difficult input enters model complex multi faceted way result ppr model useful prediction useful producing understandable model data model known single index model econometrics exception slightly general linear regression model offers similar interpretation fit ppr model given training data seek approximate minimizers error function projection pursuit regression functions direction vectors smoothing problems need either explicitly implicitly impose com plexity constraints avoid overfit solutions consider one term drop subscript given direction vector form derived variables one dimensional smoothing problem apply scatterplot smoother smoothing spline obtain estimate hand given want minimize gauss newton search convenient task quasi newton method part hessian involving second derivative discarded simply derived follows let old current estimate write old old old give old old old old minimize right hand side carry least squares regression target old old old input weights old intercept bias term produces updated coef ficient vector new two steps estimation iterated convergence one term ppr model model built forward stage wise manner adding pair stage number implementation details although smoothing method principle used conve nient method provides derivatives local regression smooth ing splines convenient step previous steps readjusted using backfitting procedure described chapter may lead ultimately fewer terms clear whether improves prediction performance usually readjusted partly avoid excessive compu tation although principle could well number terms usually estimated part forward stage wise strategy model building stops next term appreciably improve fit model cross validation also used determine neural networks many applications density estimation friedman friedman projection pursuit idea used particular see discussion ica section relationship exploratory projection pursuit however projection pursuit gression model widely used field statistics perhaps time introduction computational demands exceeded capabilities readily available computers represent important intellectual advance one blossomed reincarnation field neural networks topic rest chapter 
[neural, networks, neural, networks] term neural network evolved encompass large class models learning methods describe widely used vanilla neu ral net sometimes called single hidden layer back propagation network single layer perceptron great deal hype surrounding neural networks making seem magical mysterious make clear section nonlinear statistical models much like projection pursuit regression model discussed neural network two stage regression classification model typ ically represented network diagram figure network applies regression classification regression typically one output unit top however networks handle multiple quantitative responses seamless fashion deal general case class classification units top kth unit modeling probability class target measurements coded variable kth class derived features created linear combinations inputs target modeled function linear combinations activation function usually chosen sigmoid see figure plot sometimes gaussian radial basis functions chapter used producing known radial basis function network neural network diagrams like figure sometimes drawn additional bias unit feeding every unit hidden output layers neural networks figure schematic single hidden layer feed forward neural network thinking constant additional input feature bias unit captures intercepts model output function allows final transformation vector outputs regression typically choose identity function early work class classification also used identity function later abandoned favor softmax function course exactly transformation used multilogit model section produces positive estimates sum one sec tion discuss problems linear activation functions par ticular potentially severe masking effects units middle network computing derived features called hidden units values directly served general one hidden layer illustrated example end chapter think basis expansion original inputs neural network stan dard linear model linear multilogit model using transformations inputs however important enhancement basis expansion techniques discussed chapter parameters basis functions learned data neural networks figure plot sigmoid function exp red curve commonly used hidden layer neural network included blue curve purple curve scale parameter controls activation rate see large amounts hard activation note shifts activation threshold notice identity function entire model collapses linear model inputs hence neural network thought nonlinear generalization linear model regression classification introducing nonlinear transformation greatly enlarges class linear models figure see rate activation sigmoid depends norm small unit indeed operating linear part activation function notice also neural network model one hidden layer exactly form projection pursuit model described difference ppr model uses nonparametric functions neural network uses far simpler function based three free parameters argument detail viewing neural network model ppr model identify mth unit vector since lower complexity general nonparametric surprising neural network might use functions ppr model typically uses fewer terms example finally note name neural networks derives fact first developed models human brain unit represents neuron connections links figure represent synapses early models neurons fired total signal passed unit exceeded certain threshold model corresponds fitting neural networks use step function later neural network recognized useful tool nonlinear statistical modeling purpose step function smooth enough optimization hence step function replaced smoother threshold function sigmoid figure 
[neural, networks, fitting, neural, networks] neural network model unknown parameters often called weights seek values make model fit training data well denote complete set weights consists weights weights regression use sum squared errors measure fit error function classification use either squared error cross entropy deviance log corresponding classifier argmax softmax activation function cross entropy error function neural network model exactly linear logistic regression model hidden units parameters estimated maximum likelihood typically want global minimizer likely overfit solution instead regularization needed achieved directly penalty term indirectly early stopping details given next section generic approach minimizing gradient descent called back propagation setting compositional form model gradient easily derived using chain rule differen tiation computed forward backward sweep network keeping track quantities local unit neural networks back propagation detail squared error loss let let derivatives given derivatives gradient descent update iter ation form learning rate discussed write quantities errors current model output hidden layer units respectively definitions errors satisfy known back propagation equations using updates implemented two pass algorithm forward pass current weights fixed predicted values computed formula backward pass errors computed back propagated via give errors sets errors used compute gradients updates via issues training neural networks two pass procedure known back propagation also called delta rule widrow hoff computational components cross entropy form sum squares error function derived exercise advantages back propagation simple local nature back propagation algorithm hidden unit passes receives infor mation units share connection hence implemented efficiently parallel architecture computer updates kind batch learning parame ter updates sum training cases learning also carried online processing observation one time updat ing gradient training case cycling training cases many times case sums equations replaced single summand training epoch refers one sweep entire training set online training allows network handle large training sets also update weights new observations come learning rate batch learning usually taken con stant also optimized line search minimizes error function update online learning decrease zero iteration learning form stochastic approxima tion robbins munro results field ensure convergence satisfied example back propagation slow reason usually method choice second order techniques newton method attractive second derivative matrix hessian large better approaches fitting include conjugate gradients variable metric methods avoid explicit computation second derivative matrix still providing faster convergence 
[neural, networks, issues, training, neural, networks] quite art training neural networks model generally overparametrized optimization problem nonconvex unstable unless certain guidelines followed section summarize important issues 
[neural, networks, issues, training, neural, networks, starting, values] note weights near zero operative part sigmoid figure roughly linear hence neural network collapses approximately linear model exercise usually starting values weights chosen random values near zero hence model starts nearly linear becomes nonlinear weights increase individual neural networks units localize directions introduce nonlinearities needed use exact zero weights leads zero derivatives perfect symmetry algorithm never moves starting instead large weights often leads poor solutions 
[neural, networks, issues, training, neural, networks, overfitting] often neural networks many weights overfit data global minimum early developments neural networks either design accident early stopping rule used avoid fitting train model stop well approach global minimum since weights start highly regular ized linear solution effect shrinking final model toward linear model validation dataset useful determining stop since expect validation error start increasing explicit method regularization weight decay anal ogous ridge regression used linear models section add penalty error function tuning parameter larger values tend shrink weights toward zero typically cross validation used estimate effect penalty simply add terms respective gradient expressions forms penalty proposed example known weight elimination penalty effect shrinking smaller weights figure shows result training neural network ten hidden units without weight decay upper panel weight decay lower panel mixture example chapter weight decay clearly improved prediction figure shows heat maps estimated weights training grayscale versions called hinton diagrams see weight decay dampened weights layers resulting weights spread fairly evenly ten hidden units 
[neural, networks, issues, training, neural, networks, scaling, inputs] since scaling inputs determines effective scaling weights bottom layer large effect quality final issues training neural networks neural network units weight decay training error test error bayes error neural network units weight decay training error test error bayes error figure neural network mixture example chapter upper panel uses weight decay overfits training data lower panel uses weight decay achieves close bayes error rate broken purple boundary use softmax activation function cross entropy error neural networks weight decay weight decay figure heat maps estimated weights training neural networks figure display ranges bright green negative bright red positive solution outset best standardize inputs mean zero standard deviation one ensures inputs treated equally regularization process allows one choose meaningful range random starting weights standardized inputs typical take random uniform weights range 
[neural, networks, issues, training, neural, networks, number, hidden, units, layers] generally speaking better many hidden units hidden units model might enough flexibility capture nonlinearities data many hidden units extra weights shrunk toward zero appropriate regularization used typically number hidden units somewhere range number increasing number inputs num ber training cases common put reasonably large number units train regularization researchers use cross validation estimate optimal number seems unneces sary cross validation used estimate regularization parameter choice number hidden layers guided background knowledge experimentation layer extracts features input regres sion classification use multiple hidden layers allows construction hierarchical features different levels resolution example effective use multiple layers given section 
[neural, networks, issues, training, neural, networks, multiple, minima] error function nonconvex possessing many local minima result final solution obtained quite dependent choice start example simulated data ing weights one must least try number random starting configura tions choose solution giving lowest penalized error probably better approach use average predictions collection net works final prediction ripley preferable averaging weights since nonlinearity model implies averaged solution could quite poor another approach via bagging aver ages predictions networks training randomly perturbed versions training data described section 
[neural, networks, example, simulated, data] generated data two additive error models sum sigmoids radial standard gaussian variate first model second sigmoid model radial model exp gaussian errors variance chosen signal noise ratio var var var var models took training sample size test sample size fit neural networks weight decay various num bers hidden units recorded average test error test random starting weights one training set gen erated results typical average training set test errors shown figure note zero hidden unit model refers linear least squares regression neural network perfectly suited sum sigmoids model two unit model perform best achieving error close bayes rate recall bayes rate regression squared error error variance figures report test error relative bayes error notice however hid den units overfitting quickly creeps starting weights model worse linear model zero hidden unit model even two hidden units two ten starting weight configurations pro duced results better linear model confirming importance multiple starting values radial function sense difficult neural net spherically symmetric preferred directions see right neural networks number hidden units test error sum sigmoids number hidden units test error radial figure boxplots test error simulated data example relative bayes error broken horizontal line true function sum two sigmoids left radial function right test error displayed different starting weights single hidden layer neural network number units indicated panel figure poorly case test error staying well bayes error note different vertical scale left panel fact since constant fit sample average achieves relative error snr see neural networks perform increasingly worse mean example used fixed weight decay parameter rep resenting mild amount regularization results left panel figure suggest regularization needed greater num bers hidden units figure repeated experiment sum sigmoids model weight decay left panel stronger weight decay right panel weight decay overfitting becomes even severe larger numbers hidden units weight decay value produces good results numbers hidden units appear overfitting number units increase finally figure shows test error ten hidden unit network varying weight decay parameter wide range value approximately optimal summary two free parameters select weight decay number hidden units learning strategy one could fix either parameter value corresponding least constrained model ensure model rich enough use cross validation choose parameter least constrained values zero weight decay ten hidden units comparing left panel figure figure see test error less sensitive value weight example simulated data number hidden units test error weight decay number hidden units test error weight decay figure boxplots test error simulated data example relative bayes error true function sum two sigmoids test error displayed ten different starting weights single hidden layer neural network number units indicated two panels represent weight decay left strong weight decay right weight decay parameter test error sum sigmoids hidden unit model figure boxplots test error simulated data example true function sum two sigmoids test error displayed ten different starting weights single hidden layer neural network ten hidden units weight decay parameter value indicated neural networks figure examples training cases zip code data image bit grayscale representation handwritten digit decay parameter hence cross validation parameter would preferred 
[neural, networks, example, zip, code, data] example character recognition task classification handwritten numerals problem captured attention machine learning neural network community many years remained benchmark problem field figure shows examples normalized hand written digits automatically scanned envelopes postal service original scanned digits binary different sizes orientations images shown deslanted size normal ized resulting grayscale images cun pixel values used inputs neural network classifier black box neural network ideally suited pattern recogni tion task partly pixel representation images lack certain invariances small rotations image consequently early tempts neural networks yielded misclassification rates around various examples problem section show pioneering efforts handcraft neural network overcome deficiencies cun ultimately led state art neural network performance cun although current digit datasets tens thousands training test examples sample size deliberately modest order figures tables example recreated cun example zip code data shared weights net net net local connectivity net net figure architecture five networks used zip code example phasize effects examples obtained scanning actual hand drawn digits generating additional images random hor izontal shifts details may found cun digits training set test set five different networks fit data net hidden layer equivalent multinomial logistic regression net one hidden layer hidden units fully connected net two hidden layers locally connected net two hidden layers locally connected weight sharing net two hidden layers locally connected two levels weight sharing depicted figure net example inputs one input pixels ten output units digits predicted value represents estimated probability image digit class neural networks training epochs correct test data net net net net net figure test performance curves function number train ing epochs five networks table applied zip code data cun networks sigmoidal output units fit sum squares error function first network hidden layer hence nearly equivalent linear multinomial regression model exer cise net single hidden layer network hidden units kind described training set error networks since cases parameters training observations evolution test error training epochs shown figure linear network net starts overfit fairly quickly test performance others level successively superior values three networks additional features demonstrate power flexibility neural network paradigm introduce constraints network natural problem hand allow complex connectivity fewer parameters net uses local connectivity means hidden unit con nected small patch units layer first hidden layer array unit takes inputs patch input layer units first hidden layer one unit apart recep tive fields overlap one row column hence two pixels apart second hidden layer inputs patch units one unit apart receptive fields two units apart weights connections set zero local connectivity makes unit responsible extracting local features layer example zip code data table test set performance five different neural networks hand written digit classification example cun network architecture links weights correct net single layer network net two layer network net locally connected net constrained network net constrained network reduces considerably total number weights many hidden units net net fewer links hence weights achieves similar performance net net local connectivity shared weights units local feature map perform operation different parts image achieved sharing weights first hidden layer net two arrays unit takes input patch like net however units single feature map share set nine weights bias parameter forces extracted features different parts image computed linear functional consequently networks sometimes known convolutional networks second hidden layer net weight sharing net gradient error function respect shared weight sum gradients respect connection controlled weights question table gives number links number weights optimal test performance networks see net links fewer weights net superior test performance net four feature maps second hidden layer unit connected local patch layer weights shared feature maps see net best errors compared vanilla network net clever design network net motivated fact features handwriting style appear one part digit result many person years experimentation similar networks gave better performance zip code problems learning method time early example also shows neural networks fully automatic tool sometimes advertised statistical models subject matter knowledge used improve performance network later outperformed tangent distance approach simard described section explicitly incorpo rates natural affine invariances point digit recognition datasets become test beds every new learning procedure researchers worked neural networks hard drive error rates writing best error rates large database training test observations derived standard nist databases reported following cun tangent distance nearest neighbor classifier sec tion degree polynomial svm section lenet complex version convolutional net work described boosted lenet boosting described chapter lenet predecessor lenet cun report much larger table performance results evident many groups working hard bring test error rates report standard error error estimates based binomial average implies error rates within one another statistically equivalent realistically standard error even higher since test data implicitly used tuning various procedures 
[neural, networks, discussion] projection pursuit regression neural networks take nonlinear func tions linear combinations derived features inputs powerful general approach regression classification shown compete well best learning methods many problems tools especially effective problems high signal noise ratio settings prediction without interpretation goal less effective problems goal describe physical pro cess generated data roles individual inputs input enters model many places nonlinear fashion authors hinton plot diagram estimated weights hidden unit try understand feature unit extracting limited however lack identifiability parameter vectors often solutions spanning linear space ones found training giving predicted values national institute standards technology maintain large databases cluding handwritten character databases http www nist gov srd bayesian neural nets nips challenge roughly authors suggest carrying principal com ponent analysis weights try find interpretable solution general difficulty interpreting models limited use fields like medicine interpretation model important great deal research training neural net works unlike methods like cart mars neural networks smooth functions real valued parameters facilitates development bayesian inference models next sections discusses success ful bayesian implementation neural networks 
[neural, networks, bayesian, neural, nets, nips, challenge] classification competition held five labeled train ing datasets provided participants organized neural information processing systems nips workshop data sets constituted two class classification problems different sizes variety domains see table feature measurements valida tion dataset also available participants developed applied statistical learning procedures make predictions datasets could submit predictions web site validation set period weeks feedback participants asked submit predictions separate test set received results finally class labels validation set released participants one week train algorithms combined training validation sets submit final pre dictions competition website total groups participated eventually making submissions validation test sets respectively emphasis feature extraction competition arti ficial probes added data noise features dis tributions resembling real features independent class labels percentage probes added dataset relative total set features shown table thus learning algorithm figure way identifying probes downweighting eliminating number metrics used evaluate entries including percentage correct test set area roc curve combined score compared pair classifiers head head results competition interesting detailed guyon notable result entries neal zhang clear overall winners final competition finished first neural networks table nips challenge data sets column labeled number features dorothea dataset features binary val number training validation test cases respectively dataset domain feature percent val type probes arcene mass spectrometry dense dexter text classification sparse dorothea drug discovery sparse gisette digit recognition dense madelon artificial dense three five datasets remaining two datasets winning entries neal zhang used series pre processing feature selection steps followed bayesian neural networks dirichlet diffusion trees combinations methods focus bayesian neural network approach try discern aspects approach important success rerun programs compare results boosted neural networks boosted trees related methods 
[neural, networks, bayesian, neural, nets, nips, challenge, bayes, boosting, bagging] let first review briefly bayesian approach inference appli cation neural networks given training data assume sam pling model parameters neal zhang use two hidden layer neural network output nodes class probabilities binary outcomes given prior distribution posterior distribution parameters test case features new predictive distribution label new new new new new equation since integral intractable sophisticated markov chain monte carlo mcmc methods used sample posterior distribution new new hundred values generated simple average values estimates integral neal zhang use diffuse gaussian priors parame ters particular mcmc approach used called hybrid monte carlo may important success method includes auxiliary momentum vector implements hamiltonian dynamics potential function target density done avoid bayesian neural nets nips challenge random walk behavior successive candidates move across sample space larger steps tend less correlated hence converge target distribution rapidly neal zhang also tried different forms pre processing features univariate screening using tests automatic relevance determination latter method ard weights coefficients jth feature first hidden layer units share common prior variance prior mean zero posterior distributions variance computed features whose posterior variance concentrates small values discarded thus three main features approach could portant success feature selection pre processing neural network model bayesian inference model using mcmc according neal zhang feature screening carried purely computational efficiency mcmc procedure slow large number features need use feature selection avoid overfitting posterior average takes care automatically would like understand reasons success bayesian method view power modern bayesian methods lie use formal inference procedure people would believe priors high dimensional complex neural network model actually correct rather bayesian mcmc approach gives efficient way sampling relevant parts model space averaging predictions high probability models bagging boosting non bayesian procedures simi larity mcmc bayesian model bayesian approach fixes data perturbs parameters according current estimate poste rior distribution bagging perturbs data fashion estimates model give new set model parameters end simple average model predictions different bagged samples computed boosting similar bagging fits model additive models individual base learner learned using non samples write models form new new new neural networks cases large collection model parameters bayesian model average estimates posterior mean sampling posterior distribution bagging well parameters refit bootstrap samples training data boosting weights equal typically chosen nonrandom sequential fashion constantly improve fit 
[neural, networks, bayesian, neural, nets, nips, challenge, performance, comparisons] based similarities decided compare bayesian neural networks boosted trees boosted neural networks random forests bagged neural networks five datasets table bagging boosting neural networks methods previously used work decided try success bayesian neural networks competition good performance bagging boosting trees also felt bagging boosting neural nets could assess choice model well model search strategy details learning methods compared bayesian neural nets results taken neal zhang using bayesian approach fitting neural networks models two hidden layers units ran networks timing purposes boosted trees used gbm package version language tree depth shrinkage factors varied dataset dataset consistently bagged data boosting iteration default shrinkage tree depth boosted neural networks since boosting typically effective weak learners boosted single hidden layer neural network two four units fit nnet package version random forests used package randomforest version default settings parameters bagged neural networks used architecture bayesian neural network two hidden layers units fit using neal language package flexible bayesian modeling release matlab neural net toolbox version bayesian neural nets nips challenge test error arcene dexter dorothea gisette madelon univariate screened features bayesian neural nets boosted trees boosted neural nets random forests bagged neural networks test error arcene dexter dorothea gisette madelon ard reduced features figure performance different learning methods five problems using univariate screening features top panel reduced feature set automatic relevance determination error bars top plot width equal one standard error difference two error rates problems several competitors within error bound analysis carried nicholas johnson full details may found johnson results shown figure table figure table show bayesian boosted bagged neural networks boosted trees random forests using screened reduced features sets error bars top plot indicate one standard error difference two error rates bayesian neural networks emerge winner although datasets differences test error rates statistically significant random forests performs best among competitors using selected feature set boosted neural networks perform best reduced feature set nearly match bayesian neural net superiority boosted neural networks boosted trees suggest neural network model better suited particular prob lems specifically individual features might good predictors also thank isabelle guyon help preparing results section neural networks table performance different methods values average rank test error across five problems low good mean computation time standard error mean minutes screened features ard reduced features method average average average average rank time rank time bayesian neural networks boosted trees boosted neural networks random forests bagged neural networks linear combinations features work better however impressive performance random forests odds explanation came surprise since reduced feature sets come bayesian neural network approach methods use screened features legitimate self contained procedures however suggest better methods internal feature selection might help overall performance boosted neural networks table also shows approximate training time required method non bayesian methods show clear advantage overall superior performance bayesian neural networks may due fact neural network model well suited five problems mcmc approach provides efficient way exploring portant part parameter space averaging resulting models according quality bayesian approach works well smoothly parametrized models like neural nets yet clear works well non smooth models like trees 
[neural, networks, computational, considerations] observations predictors hidden units training epochs neural network fit typically requires operations many packages available fitting neural networks probably many exist mainstream statistical methods available software varies widely quality learning problem neural networks sensitive issues input scaling software carefully chosen tested exercises 
[neural, networks, bibliographic, notes] projection pursuit proposed friedman tukey spe cialized regression friedman stuetzle huber gives scholarly overview roosen hastie present formulation using smoothing splines motivation neural networks dates back mcculloch pitts widrow hoff reprinted derson rosenfeld rosenblatt hebb heavily influenced development learning algorithms resurgence neural networks mid due werbos parker rumelhart proposed back propagation algorithm today many books written topic broad range audiences readers book hertz bishop ripley may informative bayesian learning neural networks described neal zip code example taken cun see also cun cun discuss theoretical topics approximation properties neural networks work barron girosi jones results summarized ripley 
[neural, networks, exercises] establish exact correspondence projection pur suit regression model neural network particular show single layer regression network equivalent ppr model mth unit vector establish similar equivalence classification network consider neural network quantitative outcome using squared error loss identity output function suppose weights input hidden layer nearly zero show resulting model nearly linear inputs derive forward backward propagation equations cross entropy loss function consider neural network class outcome uses cross entropy loss network hidden layer show model equivalent multinomial logistic model described chapter write program fit single hidden layer neural network ten hidden units via back propagation weight decay neural networks apply observations model sigmoid function standard normal independent standard normal generate test sample size plot training test error curves function number training epochs different values weight decay parameter discuss overfitting behavior case vary number hidden units network determine minimum number needed perform well task write program carry projection pursuit regression using cubic smoothing splines fixed degrees freedom fit data previous exercise various values smoothing parameter number model terms find minimum number model terms necessary model perform well compare number hidden units previous exercise fit neural network spam data section compare results additive model given chapter compare classification performance interpretability final model 
[support, vector, machines, flexible, discriminants, introduction] chapter describe generalizations linear decision boundaries classification optimal separating hyperplanes introduced chap ter case two classes linearly separable cover extensions nonseparable case classes overlap tech niques generalized known support vector machine produces nonlinear boundaries constructing linear boundary large transformed version feature space second set methods generalize fisher linear discriminant analysis lda generalizations include flexible discriminant analysis facilitates construction non linear boundaries manner similar support vector machines penalized discriminant analysis problems signal image clas sification large number features highly correlated mixture discriminant analysis irregularly shaped classes 
[support, vector, machines, flexible, discriminants, support, vector, classifier] chapter discussed technique constructing optimal separat ing hyperplane two perfectly separated classes review generalize nonseparable case classes may separable linear boundary flexible discriminants margin kk kk margin kk kk figure support vector classifiers left panel shows separable case decision boundary solid line broken lines bound shaded maximal margin width kk right panel shows nonseparable overlap case points labeled wrong side margin amount points correct side margin maximized subject total budget constant hence total distance points wrong side margin training data consists pairs define hyperplane unit vector kk classification rule induced sign geometry hyperplanes reviewed section show gives signed distance point hyperplane since classes separable find function hence able find hyperplane creates biggest margin training points class see figure optimization problem max kk subject captures concept band figure units away hyperplane either side hence units wide called margin showed problem conveniently rephrased min kk subject support vector classifier dropped norm constraint note kk expression usual way writing support vector criterion separated data convex optimization problem quadratic cri terion linear inequality constraints solution characterized section suppose classes overlap feature space one way deal overlap still maximize allow points wrong side margin define slack variables two natural ways modify constraint constant two choices lead different solutions first choice seems natural since measures overlap actual distance margin second choice measures overlap relative distance changes width margin however first choice results nonconvex optimization problem second convex thus leads standard support vector classifier use idea formulation value constraint proportional amount prediction wrong side margin hence bounding sum bound total proportional amount predictions fall wrong side margin misclassifications occur bounding value say bounds total number training misclassifications section drop norm constraint define kk write equivalent form min kk subject constant usual way support vector classifier defined non separable case however find confusing presence fixed scale constraint prefer start right panel figure illustrates overlapping case nature criterion see points well inside class boundary play big role shaping boundary seems like attractive property one differentiates linear dis criminant analysis section lda decision boundary deter mined covariance class distributions positions class centroids see section logistic regression similar support vector classifier regard flexible discriminants 
[support, vector, machines, flexible, discriminants, support, vector, classifier, computing, support, vector, classifier] problem quadratic linear inequality constraints hence convex optimization problem describe quadratic programming solution using lagrange multipliers computationally convenient express equivalent form min kk subject cost parameter replaces constant separable case corresponds lagrange primal function kk minimize setting respective derivatives zero get well positivity constraints substituting obtain lagrangian wolfe dual objec tive function gives lower bound objective function feasible point maximize subject addition karush kuhn tucker conditions include constraints together equations uniquely char acterize solution primal dual problem support vector classifier see solution form nonzero coefficients observations constraints exactly met due observations called support vectors since represented terms alone among support points lie edge margin hence characterized remainder see margin points used solve typically use average solutions numerical stability maximizing dual simpler convex quadratic programming problem primal solved standard techniques murray example given solutions decision function written sign sign tuning parameter procedure cost parameter 
[support, vector, machines, flexible, discriminants, support, vector, classifier, mixture, example, continued] figure shows support vector boundary mixture example figure page two overlapping classes two different values cost parameter classifiers rather similar performance points wrong side boundary support vectors addition points correct side boundary close margin also support vectors margin larger hence larger values focus attention correctly classified points near decision boundary smaller values involve data away either way misclassified points given weight matter far away example procedure sensitive choices rigidity linear boundary optimal value estimated cross validation dis cussed chapter interestingly leave one cross validation error bounded proportion support points data reason leaving observation support vector change solution hence observations classified correctly original boundary classified correctly cross validation process however bound tends high generally useful choosing respectively examples flexible discriminants training error test error bayes error training error test error bayes error figure linear support vector boundary mixture data exam ple two overlapping classes two different values broken lines indicate margins support points points wrong side margin black solid dots support points falling exactly margin upper panel observations support points lower panel broken purple curve background bayes decision boundary support vector machines kernels 
[support, vector, machines, flexible, discriminants, support, vector, machines, kernels] support vector classifier described far finds linear boundaries input feature space linear methods make pro cedure flexible enlarging feature space using basis expansions polynomials splines chapter generally linear boundaries enlarged space achieve better training class separation trans late nonlinear boundaries original space basis functions selected procedure fit classifier using input features produce nonlinear function classifier sign support vector machine classifier extension idea dimension enlarged space allowed get large infinite cases might seem computations would become pro hibitive would also seem sufficient basis functions data would separable overfitting would occur first show svm technology deals issues see fact svm classifier solving function fitting problem using particular criterion form regularization part much bigger class problems includes smoothing splines chapter reader may wish consult section provides background material overlaps somewhat next two sections 
[support, vector, machines, flexible, discriminants, support, vector, machines, kernels, computing, svm, classification] represent optimization problem solution special way involves input features via inner products directly transformed feature vectors see particular choices inner products computed cheaply lagrange dual function form see solution function written given determined solving flexible discriminants involve inner products fact need specify transformation require knowledge kernel function computes inner products transformed space symmetric positive semi definite function see section three popular choices svm literature dth degree polynomial radial basis exp kx neural network tanh consider example feature space two inputs polynomial kernel degree choose see solution written role parameter clearer enlarged feature space since perfect separation often achievable large value discourage positive lead overfit wiggly boundary original feature space small value encourage small value kk turn causes hence boundary smoother figure show two nonlinear support vector machines applied mixture example chapter regularization parameter chosen cases achieve good test error radial basis kernel produces boundary quite similar bayes optimal boundary example compare figure early literature support vectors claims kernel property support vector machine unique allows one finesse curse dimensionality neither claims true issues next three subsections support vector machines kernels svm degree polynomial feature space training error test error bayes error svm radial kernel feature space training error test error bayes error figure two nonlinear svms mixture data upper plot uses degree polynomial kernel lower radial basis kernel case tuned approximately achieve best test error performance worked well cases radial basis kernel performs best close bayes optimal might expected given data arise mixtures gaussians broken purple curve background bayes decision boundary flexible discriminants hinge loss binomial deviance squared error class huber figure support vector loss function hinge loss compared negative log likelihood loss binomial deviance logistic regression squared ror loss huberized version squared hinge loss shown function rather symmetry case deviance huber asymptotes svm loss rounded interior scaled limiting left tail slope 
[support, vector, machines, flexible, discriminants, support, vector, machines, kernels, svm, penalization, method] consider optimization problem min kk subscript indicates positive part form loss penalty familiar paradigm function estimation easy show exercise solution examination hinge loss function shows reasonable two class classification compared traditional loss functions figure compares log likelihood loss logistic regression well squared error loss variant thereof negative log likelihood binomial deviance similar tails svm loss giving zero penalty points well inside margin support vector machines kernels table population minimizers different loss functions fig ure logistic regression uses binomial log likelihood deviance linear discriminant analysis exercise uses squared error loss svm hinge loss estimates mode posterior class probabilities whereas others estimate linear transformation probabilities loss function minimizing function binomial deviance log log svm hinge loss sign squared error huberised square hinge loss otherwise linear penalty points wrong side far away squared error hand gives quadratic penalty points well inside margin strong influence model well squared hinge loss like quadratic except zero points inside margin still rises quadratically left tail less robust hinge deviance misclassified observations recently rosset zhu proposed huberized version squared hinge loss converts smoothly linear loss characterize loss functions terms timating population level consider minimizing table summarizes results whereas hinge loss estimates classifier others estimate transformation class posterior probabilities huberized square hinge loss shares attractive properties logistic regression smooth loss function estimates probabili ties well svm hinge loss support points formulation casts svm regularized function estimation problem coefficients linear expansion shrunk toward zero excluding constant represents hierar chical basis ordered structure ordered roughness flexible discriminants uniform shrinkage makes sense rougher elements vector smaller norm loss function table except squared error called margin maximizing loss functions rosset means data separable limit defines optimal separating hyperplane 
[support, vector, machines, flexible, discriminants, support, vector, machines, kernels, function, estimation, reproducing, kernels] describe svms terms function estimation reproducing kernel hilbert spaces kernel property abounds material discussed detail section provides another view support vector classifier helps clarify works suppose basis arises possibly finite eigen expansion positive definite kernel write min identical form page section theory reproducing kernel hilbert spaces described guarantees finite dimensional solution form particular see equivalent version optimization crite rion equation section see also wahba min matrix kernel evaluations pairs training features exercise models quite general include example entire fam ily smoothing splines additive interaction spline models discussed logistic regression separable data diverges converges optimal separating direction support vector machines kernels chapters detail wahba hastie tibshirani expressed generally min structured space functions appropriate reg ularizer space example suppose space additive functions solution additive cubic spline kernel representa tion kernel appropriate univariate smoothing spline wahba conversely discussion also shows example kernels described used convex loss function also lead finite dimensional representation form figure uses kernel functions figure except using binomial log likelihood loss function fitted function hence estimate log odds log conversely get estimate class probabilities fitted models quite similar shape performance examples details given section happen svms sizable fraction values zero nonsupport points two examples figure fractions respectively consequence piecewise linear nature first part criterion lower class overlap training data greater fraction reducing generally reduce overlap allowing flexible small number support points means evaluated quickly important lookup time course reducing overlap much lead poor generalization zhu assisted preparation examples flexible discriminants degree polynomial feature space training error test error bayes error radial kernel feature space training error test error bayes error figure logistic regression versions svm models fig ure using identical kernels hence penalties log likelihood loss instead svm loss function two broken contours correspond posterior probabilities class vice versa bro ken purple curve background bayes decision boundary support vector machines kernels table skin orange shown mean standard error mean test error simulations bruto fits additive spline model adap tively mars fits low order interaction model adaptively test error method noise features six noise features classifier svm poly svm poly svm poly bruto mars bayes 
[support, vector, machines, flexible, discriminants, support, vector, machines, kernels, svms, curse, dimensionality] section address question whether svms edge curse dimensionality notice expression allowed fully general inner product space powers products example terms form given equal weight kernel cannot adapt concentrate subspaces number features large class separation occurred linear subspace spanned say kernel would easily find structure would suffer many dimensions search one would build knowledge subspace kernel tell ignore first two inputs knowledge available priori much statistical learning would made much easier major goal adaptive methods discover structure support statements illustrative example generated observations two classes first class four standard normal independent features second class also four standard normal independent features conditioned relatively easy problem second harder problem aug mented features additional six standard gaussian noise fea tures hence second class almost completely surrounds first like skin surrounding orange four dimensional subspace bayes ror rate problem irrespective dimension generated test observations compare different procedures average test errors simulations without noise features shown table line uses support vector classifier original feature space lines refer support vector machine dimension polynomial kernel support vector procedures chose cost parameter minimize test error fair possible flexible discriminants test error test error curves svm radial kernel figure test error curves function cost parameter radial kernel svm classifier mixture data top plot scale parameter radial kernel exp optimal value depends quite strongly scale kernel bayes error rate indicated broken horizontal lines method line fits additive spline model response least squares using bruto algorithm additive models described hastie tibshirani line uses mars multivariate adaptive regression splines allowing interaction orders described chap ter comparable svm poly bruto mars ability ignore redundant variables test error used choose smoothing parameters either lines original feature space hyperplane cannot separate classes support vector classifier line poorly polynomial sup port vector machine makes substantial improvement test error rate adversely affected six noise features also sensitive choice kernel second degree polynomial kernel line best since true decision boundary second degree polynomial however higher degree polynomial kernels lines much worse bruto performs well since boundary additive bruto mars adapt well performance deteriorate much presence noise 
[support, vector, machines, flexible, discriminants, support, vector, machines, kernels, path, algorithm, svm, classifier] regularization parameter svm classifier cost parameter inverse common usage set high leading often somewhat overfit classifiers figure shows test error mixture data function using different radial kernel parameters narrow peaked kernels heaviest regularization small called support vector machines kernels figure simple example illustrates svm path algorithm left panel plot illustrates state model points orange blue width soft margin two blue points misclassified two ange points correctly classified wrong side margin three square shaped points exactly margins right panel plot shows piecewise linear profiles horizontal broken line indicates state model left plot value used figure intermediate value required clearly situations need determine good choice perhaps cross validation describe path algorithm spirit section efficiently fitting entire sequence svm models obtained varying convenient use loss penalty formulation along figure leads solution given value lagrange multipliers case lie figure illustrates setup shown kkt optimal ity conditions imply labeled points fall three distinct groups flexible discriminants observations correctly classified outside margins lagrange multipliers examples orange points blue points observations sitting margins lagrange multipliers examples orange blue observations inside margins examples blue orange idea path algorithm follows initially large margin wide points inside margin decreases decreases margin gets narrower points move inside margins outside margins change continuity points linger margin transition see points make fixed contributions make contribution changes decreases small number points margin since points results small set linear equations prescribe hence changes transitions results piecewise linear paths breaks occur points cross margin figure right panel shows profiles small example left panel although described linear svms exactly idea works nonlinear models replaced details found hastie package svmpath available cran fitting models 
[support, vector, machines, flexible, discriminants, support, vector, machines, kernels, support, vector, machines, regression] section show svms adapted regression quantitative response ways inherit properties svm classifier first discuss linear regression model handle nonlinear generalizations estimate consider min imization kk support vector machines kernels figure left panel shows insensitive error function used support vector regression machine right panel shows error function used huber robust regression blue curve beyond function changes quadratic linear otherwise insensitive error measure ignoring errors size less left panel figure rough analogy support vector classification setup points correct side deci sion boundary far away ignored optimization regression low error points ones small residuals interesting contrast error measures used robust gression statistics popular due huber form shown right panel figure function reduces quadratic linear contributions observations absolute residual greater prechosen constant makes fitting less sensitive liers support vector error measure also linear tails beyond addition flattens contributions cases small residuals minimizers solution function shown form flexible discriminants positive solve quadratic programming problem min subject constraints due nature constraints typically subset solution values nonzero associated data values called support vectors case classification setting solution depends input values inner products thus generalize methods richer spaces defining appropriate inner product example one defined note parameters associated criterion seem play different roles parameter loss function like note depend scale hence scale response hence use instead might consider using preset values value achieves efficiency gaussian quantity traditional regularization parameter estimated example cross validation 
[support, vector, machines, flexible, discriminants, support, vector, machines, kernels, regression, kernels] discussed section kernel property unique sup port vector machines suppose consider approximation regression function terms set basis functions estimate minimize general error measure choice solution form support vector machines kernels notice form radial basis function expansion regularization estimate discussed chapters concreteness let work case let basis matrix imth element suppose large simplicity assume constant absorbed see exercise alternative estimate minimizing penalized least squares criterion kk solution determined appears need evaluate matrix inner products transformed space however premultiply give matrix consists inner products pairs obser vations evaluation inner product kernel easy show directly case predicted values arbitrary satisfy support vector machine need specify evaluate large set functions inner product kernel need evaluated training points points predictions careful choice eigenfunctions particular easy evaluate kernels means example computed cost evaluations rather direct cost note however property depends choice squared norm kk penalty hold example norm may lead superior model flexible discriminants 
[support, vector, machines, flexible, discriminants, support, vector, machines, kernels, discussion] support vector machine extended multiclass problems sentially solving many two class problems classifier built pair classes final classifier one dominates kressel friedman hastie tibshirani alternatively one could use multinomial loss function along suitable kernel section svms applications many supervised unsupervised learning problems time writing empirical evidence suggests performs well many real learning problems finally mention connection support vector machine structural risk minimization suppose training points basis expansion contained sphere radius let sign sign one show class functions kk dimension satisfying separates training data optimally kk probability least training sets vapnik page error test log log support vector classifier one first practical learning pro cedures useful bounds dimension could obtained hence srm program could carried however deriva tion balls put around data points process depends observed values features hence strict sense complexity class fixed priori seeing features regularization parameter controls upper bound dimension classifier following srm paradigm could choose minimizing upper bound test error given however clear advantage use cross validation choice 
[support, vector, machines, flexible, discriminants, generalizing, linear, discriminant, analysis] section discussed linear discriminant analysis lda funda mental tool classification remainder chapter discuss class techniques produce better classifiers lda directly generalizing lda virtues lda follows simple prototype classifier new observation classified class closest centroid slight twist distance measured mahalanobis metric using pooled covariance estimate generalizing linear discriminant analysis lda estimated bayes classifier observations multi variate gaussian class common covariance matrix since assumption unlikely true might seem much virtue decision boundaries created lda linear leading deci sion rules simple describe implement lda provides natural low dimensional views data exam ple figure informative two dimensional view data dimensions ten classes often lda produces best classification results simplicity low variance lda among top three classifiers datasets studied statlog project michie unfortunately simplicity lda causes fail number situa tions well often linear decision boundaries adequately separate classes large possible estimate complex decision boundaries quadratic discriminant analysis qda often useful allows quadratic decision boundaries generally would like able model irregular decision boundaries aforementioned shortcoming lda often paraphrased saying single prototype per class insufficient lda uses single prototype class centroid plus common covariance matrix describe spread data class many situations several prototypes appropriate end spectrum may way many corre lated predictors example case digitized analogue signals images case lda uses many parameters estimated high variance performance suffers cases need restrict regularize lda even remainder chapter describe class techniques attend issues generalizing lda model achieved largely three different ideas first idea recast lda problem linear regression problem many techniques exist generalizing linear regression flexible nonparametric forms regression turn leads flexible forms discriminant analysis call fda cases interest study predated emergence svms flexible discriminants regression procedures seen identify enlarged set predictors via basis expansions fda amounts lda enlarged space paradigm used svms case many predictors pixels digitized image want expand set already large second idea fit lda model penalize coefficients smooth otherwise coherent spatial domain image call procedure penalized discriminant analysis pda fda expanded basis set often large regularization also required svms achieved via suitably regularized regression context fda model third idea model class mixture two gaus sians different centroids every component gaussian within classes sharing covariance matrix allows complex decision boundaries allows subspace reduction lda call extension mixture discriminant analysis mda three generalizations use common framework exploiting connection lda 
[support, vector, machines, flexible, discriminants, flexible, discriminant, analysis] section describe method performing lda using linear gression derived responses turn leads nonparametric flex ible alternatives lda chapter assume observations quantitative response falling one classes measured features suppose function assigns scores classes transformed class labels timally predicted linear regression training sample form solve min restrictions avoid trivial solution mean zero unit vari ance training data produces one dimensional separation classes generally find sets independent scorings class labels corresponding linear maps chosen optimal multiple regression scores maps chosen minimize average squared residual asr flexible discriminant analysis set scores assumed mutually orthogonal normalized respect appropriate inner product prevent trivial zero solutions going road shown sequence discriminant canonical vectors derived section identical sequence constant mardia hastie moreover mahalanobis distance test point kth class centroid given mean kth class depend coordinate weights defined terms mean squared residual th optimally scored fit section saw canonical distances needed classification gaussian setup equal covariances class summarize lda performed sequence linear regressions fol lowed classification closest class centroid space fits analogy applies reduced rank version full rank case real power result generalizations invites replace linear regression fits far flexible nonparametric fits analogy achieve flexible classifier lda mind generalized additive fits spline functions mars models like general form regression problems defined via criterion asr regularizer appropriate forms nonparametric regres sion smoothing splines additive splines lower order anova spline models also included classes functions associated penalties generated kernels section describe computations involved generalization let consider simple example suppose use degree polynomial regression decision boundaries implied quadratic surfaces since fitted functions quadratic flexible discriminants figure data consist points generated solid black ellipse decision boundary found fda using degree two polynomial regression dashed purple circle bayes decision boundary lda squares cancel comparing distances could achieved identical quadratic boundaries conventional way augmenting original predictors squares cross products enlarged space one performs lda linear boundaries enlarged space map quadratic boundaries original space classic example pair multivariate gaussians centered origin one covariance matrix figure illustrates bayes decision boundary sphere kxk log linear boundary enlarged space many nonparametric regression procedures operate generating basis expansion derived variables performing linear regression enlarged space mars procedure chapter exactly form smoothing splines additive spline models generate extremely large basis set basis functions additive splines perform penalized regression fit enlarged space svms well see also kernel based regression example section fda case shown perform penalized linear discriminant analysis enlarged space elaborate section linear boundaries enlarged space map nonlinear boundaries reduced space exactly paradigm used support vector machines section illustrate fda speech recognition example used chapter classes predictors classes correspond flexible discriminant analysis ooo ooo ooo ooo oooo ooo oooo ooo ooo oooo ooo ooo ooo ooo coordinate training data coordinate training data linear discriminant analysis ooo ooo ooo ooo ooo oooo ooo ooo ooo ooo coordinate training data coordinate training data flexible discriminant analysis bruto figure left plot shows first two lda canonical variates vowel training data right plot shows corresponding projection fda bruto used fit model plotted fitted regression functions notice improved separation colors represent eleven different vowel sounds vowel sounds contained different words words preceded symbols represent vowel word vowel word vowel word vowel word heed hod hid hoard head hood hard heard hud eight speakers spoke word six times training set likewise seven speakers test set ten predictors derived digitized speech rather complicated way standard speech recognition world thus training observations test observations figure shows two dimensional projections produced lda fda fda model used adaptive additive spline regression functions model points plotted right plot coordinates routine used plus called bruto hence heading plot table see flexible model ing helped separate classes case table shows training test error rates number classification techniques fda mars refers friedman multivariate adaptive regression splines degree means pairwise products permitted notice fda mars best classification results obtained reduced rank subspace flexible discriminants table vowel recognition data performance results results neural networks best among much larger set taken neural network archive notation fda bruto refers regression method used fda technique error rates training test lda softmax qda cart cart linear combination splits single layer perceptron multi layer perceptron hidden units gaussian node network hidden units nearest neighbor fda bruto softmax fda mars degree best reduced dimension softmax fda mars degree best reduced dimension softmax 
[support, vector, machines, flexible, discriminants, flexible, discriminant, analysis, computing, fda, estimates] computations fda coordinates simplified many portant cases particular nonparametric regression procedure represented linear operator denote operator vector responses vector fits additive splines property smoothing parameters fixed mars basis functions selected subscript denotes entire set smoothing parameters case optimal scoring equivalent canonical correlation problem solution computed single eigen decomposition pursued exercise resulting algorithm presented create indicator response matrix responses otherwise five class problem might look like following flexible discriminant analysis computational steps multivariate nonparametric regression fit multiresponse adaptive nonparametric regression giving fitted values let linear operator fits final chosen model vector fitted regression functions optimal scores compute eigen decomposition eigenvectors normalized diagonal matrix estimated class prior probabilities update model step using optimal scores first functions constant function trivial solution remaining functions discriminant functions constant function along normalization causes remaining functions centered correspond regression method linear regression projection operator fda linear discriminant anal ysis software reference computational considerations section page makes good use modularity fda function method argument allows one supply regression function long follows natural conventions regression functions provide allow polynomial regression adaptive additive models mars efficiently handle multiple responses step single call regression routine eigen decomposition step simulta neously computes optimal scoring functions section discussed pitfalls using linear regression indicator response matrix method classification particular vere masking occur three classes fda uses fits regression step transforms produce useful discriminant functions devoid pitfalls exercise takes another view phenomenon flexible discriminants 
[support, vector, machines, flexible, discriminants, penalized, discriminant, analysis] although fda motivated generalizing optimal scoring also viewed directly form regularized discriminant analysis suppose regression procedure used fda amounts linear regression onto basis expansion quadratic penalty coefficients asr choice depends problem expansion spline basis functions might constrain smooth case additive splines spline basis functions coordinate resulting total basis functions case block diagonal steps fda viewed generalized form lda call penalized discriminant analysis pda enlarge set predictors via basis expansion use penalized lda enlarged space penalized mahalanobis distance given within class covariance matrix derived vari ables decompose classification subspace using penalized metric max bet subject loosely speaking penalized mahalanobis distance tends give less weight rough coordinates weight smooth ones since penalty diagonal applies linear combinations rough smooth classes problems first step involving basis expansion needed already far many correlated predictors leading example objects classified digitized analog signals log periodogram fragment spoken speech sampled set frequencies see figure page grayscale pixel values digitized image handwritten digit penalized discriminant analysis lda coefficient pda coefficient lda coefficient pda coefficient lda coefficient pda coefficient lda coefficient pda coefficient lda coefficient pda coefficient lda coefficient pda coefficient lda coefficient pda coefficient lda coefficient pda coefficient lda coefficient pda coefficient figure images appear pairs represent nine discrim inant coefficient functions digit recognition problem left member pair lda coefficient right member pda coefficient regularized enforce spatial smoothness also intuitively clear cases regularization needed take digitized image example neighboring pixel values tend correlated often almost implies pair corresponding lda coefficients pixels wildly different opposite sign thus cancel applied similar pixel values positively correlated predictors lead noisy negatively correlated coeffi cient estimates noise results unwanted sampling variance reasonable strategy regularize coefficients smooth spatial domain images pda computations proceed fda except appropriate penalized regression method used chosen penalizes roughness viewed image figure page shows examples handwritten digits figure shows dis criminant variates using lda pda produced lda appear salt pepper images produced pda smooth ages first smooth image seen coefficients linear contrast functional separating images dark central vertical strip ones possibly sevens images hollow middle zeros fours figure supports interpretation dif ficulty allows interpretation second coordinate flexible discriminants pda discriminant coordinate pda discriminant coordinate figure first two penalized canonical variates evaluated test data circles indicate class centroids first coordinate contrasts mainly second contrasts mixture discriminant analysis examples discussed detail hastie also show regularization improves classification performance lda independent test data factor around cases tried 
[support, vector, machines, flexible, discriminants, mixture, discriminant, analysis] linear discriminant analysis viewed prototype classifier class represented centroid classify closest using appropriate metric many situations single prototype sufficient represent inhomogeneous classes mixture models appro priate section review gaussian mixture models show generalized via fda pda methods discussed earlier gaussian mixture model kth class density mixing proportions sum one prototypes kth class specification covariance matrix used metric throughout given model class class posterior probabilities given represent class prior probabilities saw calculations special case two components chapter lda estimate parameters maximum likelihood using joint log likelihood based log sum within log makes rather messy optimization problem tackled directly classical natural method computing maximum likelihood estimates mles mixture distributions algorithm dempster known possess good conver gence properties alternates two steps flexible discriminants step given current parameters compute responsibility sub class within class class observations step compute weighted mles parameters component gaussians within classes using weights step step algorithm apportions unit weight observation class various subclasses assigned class close centroid particular subclass far others receive mass close one subclass hand observations halfway two subclasses get approximately equal weight step observation class used times estimate parameters component densities different weight algorithm studied detail chapter algorithm requires initialization impact since mixture likelihoods generally multimodal software referenced computational considerations page allows several strategies describe default user supplies number subclasses per class within class means clustering model multiple random starts fitted data partitions observations disjoint groups initial weight matrix consisting zeros ones created assumption equal component covariance matrix throughout buys additional simplicity incorporate rank restrictions mixture formulation like lda understand review little known fact lda rank lda fit section equivalent maximum likelihood fit gaussian model different mean vectors class confined rank subspace exercise inherit property mixture model maximize log likelihood subject rank constraints centroids rank algorithm available step turns weighted version lda classes furthermore use optimal scoring solve weighted lda problem allows use weighted version fda pda stage one would expect addition increase number classes similar increase number observations kth class factor turns case linear operators used optimal scoring regression enlarged indicator matrix collapses case blurred response matrix intuitively pleasing example suppose classes subclasses per class might mixture discriminant analysis            entries class row correspond remaining steps d update    step mda simple modifications add considerable flexibility mixture model dimension reduction step lda fda pda limited number classes particular classes reduction possible mda substitutes subclasses classes allows look low dimensional views subspace spanned subclass centroids subspace often important one discrimination using fda pda step adapt even par ticular situations example fit mda models digitized analog signals images smoothness constraints built figure compares fda mda mixture example 
[support, vector, machines, flexible, discriminants, mixture, discriminant, analysis, example, waveform, data] illustrate ideas popular simulated example taken breiman pages used hastie tibshirani elsewhere three class problem vari ables considered difficult pattern recognition problem predictors defined class class class uniform standard normal vari ates shifted triangular waveforms max flexible discriminants fda mars degree training error test error bayes error mda subclasses per class training error test error bayes error figure fda mda mixture data upper plot uses fda mars regression procedure lower plot uses mda five mixture centers per class indicated mda solution close bayes optimal might expected given data arise mixtures gaussians broken purple curve background bayes decision boundary mixture discriminant analysis class class class figure examples waveforms generated model gaussian noise added figure shows example waveforms class table shows results mda applied waveform data well several methods chapters train ing sample observations equal priors used roughly observations class used test samples size two mda models described caption figure shows leading canonical variates penalized mda model evaluated test data might guessed classes appear lie edges triangle repre sented three points space thereby forming vertices triangle class represented convex combination pair vertices hence lie edge also clear visually information lies first two dimensions percentage variance explained first two coordinates would lose nothing truncating solution bayes risk problem estimated breiman mda comes close optimal rate surprising since structure mda model similar generating model flexible discriminants table results waveform data values averages ten sim ulations standard error average parentheses five entries line taken hastie first model line mda three subclasses per class next line except discriminant coefficients penalized via roughness penalty effectively third corresponding penalized lda pda model technique error rates training test lda qda cart fda mars degree fda mars degree mda subclasses mda subclasses penalized pda penalized bayes discriminant var discriminant var subclasses penalized discriminant var discriminant var subclasses penalized figure two dimensional views mda model fitted sample waveform model points independent test data projected leading two canonical coordinates left panel third fourth right panel subclass centers indicated exercises 
[support, vector, machines, flexible, discriminants, mixture, discriminant, analysis, computational, considerations] training cases predictors support vectors support vector machine requires mpn operations assuming scale well although computational shortcuts avail able platt since evolving rapidly reader urged search web latest technology lda requires operations pda complexity fda depends regression method used many techniques linear additive models mars general splines kernel based regression methods typically require operations software available fitting fda pda mda models package mda also available plus 
[support, vector, machines, flexible, discriminants, bibliographic, notes] theory behind support vector machines due vapnik scribed vapnik burgeoning literature svms online bibliography created maintained alex smola bernhard sch olkopf found http www kernel machines org treatment based wahba evgeniou tutorial burges burges linear discriminant analysis due fisher rao connection optimal scoring dates back least breiman ihaka simple form fisher strong connections correspondence analysis greenacre description flexible penalized mixture discriminant analysis taken hastie hastie hastie tibshirani three summarized hastie see also ripley 
[support, vector, machines, flexible, discriminants, exercises] show criteria equivalent show solution solution particular kernel consider modification penalize constant formulate problem characterize solution suppose perform reduced subspace linear discriminant anal ysis group problem compute canonical variables flexible discriminants mension given matrix discriminant coefficients dimension show denotes mahalanobis distance respect covari ance show expression left measures difference mahalanobis squared distances distributions projected onto subspace spanned data phoneme subset available book website http www stat stanford edu elemstatlearn consists digitized log periodograms phonemes uttered speakers speaker produced phonemes five classes appropriate plot vector features frequencies produce separate plot phoneme curves frequency class plan use nearest prototype classification scheme classify curves phoneme classes particular use means clustering algorithm class kmeans classify observations class closest cluster center curves high dimensional rather small sample size variables ratio decide restrict prototypes smooth functions frequency particular decide represent prototype matrix natural spline basis functions knots uniformly chosen boundary knots describe proceed analytically particular avoid costly high dimensional fitting procedures hint may help restrict orthogonal implement procedure phoneme data try divide data training set test set making sure speakers split across sets use centers per class use knots taking care start means procedure starting values value compare results suppose regression procedure used fda section linear expansion basis functions let diagonal matrix class proportions exercises show optimal scoring problem written vector notation min ky hk vector real numbers matrix evaluations suppose normalization interpret normalizations terms original scored show normalization partially optimized leads max subject normalization constraints projection operator corresponding basis matrix suppose include constant function show largest eigenvalue let matrix scores columns suppose normalization show solution given complete set eigenvectors first eigenvector trivial takes care centering scores remainder characterize optimal scoring solution derive solution penalized optimal scoring problem show coefficients found optimal scoring proportional discriminant directions found linear discriminant analysis let fitted indicator response matrix linear regression matrix consider reduced features show lda using equivalent lda original space kernels linear discriminant analysis suppose wish carry linear discriminant analysis two classes using vector transformations input variables since high dimensional use regularized within class covariance matrix show model estimated using inner products hence kernel property support vector machines also shared regularized linear discriminant analysis mda procedure models class mixture gaussians hence mixture center belongs one one class general model allows mixture center shared classes take joint density labels features flexible discriminants mixture joint densities furthermore assume model consists regions centered class profile posterior class distribution given denominator marginal distribution show model called mda viewed generalization mda since corresponds mixing proportions kth class derive algorithm mda show initial weight matrix constructed mda volving separate means clustering class algorithm mda identical original mda procedure 
[prototype, methods, nearest-neighbors, introduction] chapter discuss simple essentially model free methods classification pattern recognition highly unstruc tured typically useful understanding nature relationship features class outcome however black box prediction engines effective often among best performers real data problems nearest neighbor technique also used regression touched chapter works reason ably well low dimensional problems however high dimensional features bias variance tradeoff work favorably nearest neighbor regression classification 
[prototype, methods, nearest-neighbors, prototype, methods] throughout chapter training data consists pairs class label taking values pro totype methods represent training data set points feature space prototypes typically examples training sam ple except case nearest neighbor classification discussed later prototype associated class label classification query point made class closest prototype closest usually defined euclidean distance feature space feature prototypes nearest neighbors standardized overall mean variance training sample euclidean distance appropriate quantitative features discuss distance measures qualitative kinds feature values chapter methods effective prototypes well positioned capture distribution class irregular class boundaries represented enough prototypes right places feature space main challenge figure many prototypes use put methods differ according number way prototypes selected 
[prototype, methods, nearest-neighbors, prototype, methods, k-means, clustering] means clustering method finding clusters cluster centers set unlabeled data one chooses desired number cluster centers say means procedure iteratively moves centers minimize total within cluster variance given initial set centers means algorithm alternates two steps center identify subset training points cluster closer center means feature data points cluster computed mean vector becomes new center cluster two steps iterated convergence typically initial centers randomly chosen observations training data details means procedure well generalizations allowing different variable types general distance measures given chapter use means clustering classification labeled data steps apply means clustering training data class sepa rately using prototypes per class assign class label prototypes classify new feature class closest prototype figure upper panel shows simulated example three classes two features used prototypes per class show clas sification regions decision boundary notice number means refers number cluster centers since already reserved denote number classes denote number clusters prototype methods means prototypes per class lvq prototypes per class figure simulated example three classes five prototypes per class data class generated mixture gaussians upper panel prototypes found applying means clustering algo rithm separately class lower panel lvq algorithm starting means solution moves prototypes away decision bound ary broken purple curve background bayes decision boundary prototypes nearest neighbors algorithm learning vector quantization lvq choose initial prototypes class example sampling training points random class sample training point randomly replacement let index closest prototype class move prototype towards training point learning rate different classes move prototype away training point repeat step decreasing learning rate iteration wards zero prototypes near class boundaries leading potential misclassifica tion errors points near boundaries results obvious shortcoming method class classes say positioning prototypes class better approach discussed next uses data position prototypes 
[prototype, methods, nearest-neighbors, prototype, methods, learning, vector, quantization] technique due kohonen prototypes placed strategically respect decision boundaries hoc way lvq online algorithm observations processed one time idea training points attract prototypes correct class repel prototypes iterations settle prototypes close training points class learning rate decreased zero iteration following guidelines stochastic approximation learning rates section figure lower panel shows result lvq using means solution starting values prototypes tended move away decision boundaries away prototypes competing classes procedure described actually called lvq modifications lvq lvq etc proposed sometimes improve per formance drawback learning vector quantization methods fact nearest neighbor classifiers defined algorithms rather optimization fixed criteria makes difficult understand properties 
[prototype, methods, nearest-neighbors, prototype, methods, gaussian, mixtures] gaussian mixture model also thought prototype method similar spirit means lvq discuss gaussian mixtures detail sections cluster described terms gaussian density centroid means covari ance matrix comparison becomes crisper restrict component gaussians scalar covariance matrix exercise two steps alternating algorithm similar two steps means step observation assigned responsibility weight cluster based likelihood correspond ing gaussians observations close center cluster likely get weight cluster weight every clus ter observations half way two clusters divide weight accordingly step observation contributes weighted means covariances every cluster consequence gaussian mixture model often referred soft clustering method means hard similarly gaussian mixture models used represent fea ture density class produces smooth posterior probabilities classifying see page often interpreted soft classification fact classification rule arg max figure compares results means gaussian mixtures simulated mixture problem chapter see although decision boundaries roughly similar mixture model smoother although prototypes approximately positions also see procedures devote blue prototype incorrectly region northwest gaussian mixture classifier ultimately ignore region means cannot lvq gave similar results means example shown 
[prototype, methods, nearest-neighbors, k-nearest-neighbor, classifiers] classifiers memory based require model fit given query point find training points closest distance classify using majority vote among neighbors prototypes nearest neighbors means prototypes per class training error test error bayes error gaussian mixtures subclasses per class training error test error bayes error figure upper panel shows means classifier applied mixture data example decision boundary piecewise linear lower panel shows gaussian mixture model common covariance component gaussians algorithm mixture model started means solution broken purple curve background bayes decision boundary nearest neighbor classifiers ties broken random simplicity assume features real valued use euclidean distance feature space typically first standardize features mean zero variance since possible measured different units chapter discuss distance measures appropriate qualitative ordinal features combine mixed data adaptively chosen distance metrics discussed later chapter despite simplicity nearest neighbors successful large number classification problems including handwritten digits satellite image scenes ekg patterns often successful class many possible prototypes decision boundary irregular figure upper panel shows decision boundary nearest neighbor classifier applied three class simulated data decision boundary fairly smooth compared lower panel nearest neighbor classifier used close relationship nearest neighbor prototype methods nearest neighbor classification training point prototype figure shows training test tenfold cross validation errors function neighborhood size two class mixture problem since tenfold errors averages ten numbers estimate standard error uses training point closest query point bias nearest neighbor estimate often low variance high famous result cover hart shows asymptotically error rate nearest neighbor classifier never twice bayes rate rough idea proof follows using squared error loss assume query point coincides one training points bias zero true asymptotically dimension feature space fixed training data fills space dense fashion error bayes rule variance bernoulli random variate target query point error nearest neighbor rule twice variance bernoulli random variate one contribution training query targets give detail misclassification loss let dominant class true conditional probability class bayes error nearest neighbor error asymptotic nearest neighbor error rate random rule pick classification test point random probabili prototypes nearest neighbors nearest neighbors nearest neighbor figure nearest neighbor classifiers applied simulation data figure broken purple curve background bayes decision boundary nearest neighbor classifiers number neighbors misclassification errors test error fold training error bayes error nearest neighbors training error test error bayes error figure nearest neighbors two class mixture data upper panel shows misclassification errors function neighborhood size stan dard error bars included fold cross validation lower panel shows decision boundary nearest neighbors appears optimal minimizing test error broken purple curve background bayes decision boundary prototypes nearest neighbors ties nearest neighbor error rate twice bayes error rate generally one show exercise many additional results kind derived ripley sum marizes number result provide rough idea best performance possible given problem example nearest neighbor rule error rate asymptotically bayes error rate least kicker asymptotic part assumes bias nearest neighbor rule zero real problems bias substantial adaptive nearest neighbor rules described later chapter attempt alleviate bias simple nearest neighbors bias variance characteristics dictate optimal number near neighbors given problem illustrated next example 
[prototype, methods, nearest-neighbors, k-nearest-neighbor, classifiers, example, comparative, study] tested nearest neighbors means lvq classifiers two sim ulated problems independent features uniformly distributed two class target variable defined follows problem easy  ufedsign    problem difficult hence first problem two classes separated hyperplane second problem two classes form checkerboard pattern hypercube defined first three features bayes error rate zero problems training test observations figure shows mean standard error misclassification error nearest neighbors means lvq ten realizations tuning parameters varied see means lvq give nearly identical results best choices tuning parameters means lvq outperform nearest neighbors first problem perform similarly second problem notice best value tuning parameter clearly situation dependent example nearest neighbors outperforms nearest neighbor factor nearest neighbor classifiers number neighbors misclassification error nearest neighbors easy number prototypes per class misclassification error means amp lvq easy number neighbors misclassification error nearest neighbors difficult number prototypes per class misclassification error means amp lvq difficult figure mean one standard error misclassification error near est neighbors means blue lvq red ten realizations two sim ulated problems easy difficult described text prototypes nearest neighbors spectral band spectral band spectral band spectral band land usage predicted land usage figure first four panels landsat images agricultural area four spectral bands depicted heatmap shading remaining two panels give actual land usage color coded predicted land usage using five nearest neighbor rule described text first problem nearest neighbor best second problem factor results underline importance using objective data based method like cross validation estimate best value tuning parameter see figure chapter 
[prototype, methods, nearest-neighbors, k-nearest-neighbor, classifiers, example, k-nearest-neighbors, image, scene, classification] statlog project michie used part landsat image benchmark classification pixels figure shows four heat map images two visible spectrum two infrared area agricultural land australia pixel class label element set red soil cotton vegetation stubble mixture gray soil damp gray soil damp gray soil determined manually research assistants surveying area lower middle panel shows actual land usage shaded different colors indicate classes objective classify land usage pixel based information four spectral bands five nearest neighbors produced predicted map shown bot tom right panel computed follows pixel extracted neighbor feature map pixel immediate neighbors nearest neighbor classifiers figure pixel neighbor feature map see figure done separately four spectral bands giving input features per pixel five nearest neighbors classi fication carried dimensional feature space resulting test error rate see figure methods used statlog project including lvq cart neural networks linear discriminant analysis many others nearest neighbors performed best task hence likely decision boundaries quite irregular 
[prototype, methods, nearest-neighbors, k-nearest-neighbor, classifiers, invariant, metrics, tangent, distance] problems training features invariant certain natural transformations nearest neighbor classifier exploit invari ances incorporating metric used measure distances objects give example idea used great success resulting classifier outperformed others time development simard problem handwritten digit recognition discussed chapter section inputs grayscale images pixels examples shown figure top figure shown actual orientation middle rotated either direction rotations often occur real handwriting obvious eye still small rotations hence want nearest neighbor classifier consider two close together similar however grayscale pixel values rotated look quite different original image hence two objects far apart euclidean distance wish remove effect rotation measuring distances two digits class consider set pixel values consisting original rotated versions one dimensional curve depicted green curve passing figure figure shows stylized version two images indicated might two different example image drawn curve rotated versions image called prototypes nearest neighbors statlog results method test misclassification error lvq rbf alloc cart neural newid qda smart logistic lda dann figure test error performance number classifiers reported statlog project entry dann variant nearest neighbors using adaptive metric section figure examples grayscale images handwritten digits nearest neighbor classifiers tangent transformations linear equation images pixel space figure top row shows original orientation middle rotated versions green curve middle figure depicts set rotated dimensional space red line tangent line curve original image tangent line equation shown bottom figure invariance manifolds context rather using usual euclidean distance two images use shortest distance two curves words distance two images taken shortest euclidean distance rotated version first image rotated version second image distance called invariant metric principle one could carry nearest neighbor classification using invariant metric however two problems first difficult calculate real images second allows large trans formations lead poor performance example would considered close rotation need restrict attention small rotations use tangent distance solves problems shown figure approximate invariance manifold image tangent original image tangent computed estimating direction vector small rotations image sophisticated spatial smoothing methods exercise large rotations tangent image longer looks like problem large transformations alleviated prototypes nearest neighbors transformations transformations tangent distance euclidean distance distance transformed figure tangent distance computation two images rather using euclidean distance shortest distance two curves use shortest distance two tangent lines idea compute invariant tangent line training image query image classified compute invariant tangent line find closest line among lines training set class digit corresponding closest line predicted class query image figure two tangent lines intersect forced draw two dimensional representation actual dimensional situation probability two lines intersecting effectively zero simpler way achieve invariance would add training set number rotated versions training image use standard nearest neighbor classifier idea called hints abu mostafa works well space invariances small far presented simplified version problem addition rotation six types transformations would like classifier invariant translation two directions scaling two directions sheer character thickness hence curves tangent lines figures actually dimensional manifolds hyperplanes infeasible add transformed versions training image capture possibilities tangent manifolds provide elegant way capturing invariances table shows test misclassification error problem training images test digits postal services database carefully constructed neural network simple nearest neighbor adaptive nearest neighbor methods table test error rates handwritten zip code problem method error rate neural net nearest neighbor euclidean distance nearest neighbor tangent distance tangent distance nearest neighbor rules tangent distance nearest neighbor classifier works remarkably well test error rates near human eye notoriously difficult test set practice turned nearest neighbors slow online classification application see section neural network classifiers subsequently developed mimic 
[prototype, methods, nearest-neighbors, adaptive, nearest-neighbor, methods] nearest neighbor classification carried high dimensional feature space nearest neighbors point far away causing bias degrading performance rule quantify consider data points uniformly distributed unit cube let radius nearest neighborhood centered origin median volume sphere radius dimensions fig ure shows median radius various training sample sizes dimensions see median radius quickly approaches dis tance edge cube done problem consider two class situation figure two features nearest neighborhood query point depicted circular region implicit near neighbor classification assumption class probabilities roughly con stant neighborhood hence simple averages give good estimates however example class probabilities vary horizontal direction knew would stretch neighborhood verti cal direction shown tall rectangular region reduce bias estimate leave variance general calls adapting metric used nearest neighbor classification resulting neighborhoods stretch directions class probabilities change much high dimensional feature space class probabilities might change low dimensional subspace hence considerable advantage adapting metric prototypes nearest neighbors dimension median radius figure median radius nearest neighborhood uniform data observations dimensions nearest neighborhoods figure points uniform cube vertical line sepa rating class red green vertical strip denotes nearest neighbor region using horizontal coordinate find nearest neighbors target point solid dot sphere shows nearest neighbor region using ordinates see case extended class red region dominated wrong class instance adaptive nearest neighbor methods friedman proposed method rectangular neighbor hoods found adaptively successively carving away edges box containing training data describe discriminant adaptive nearest neighbor dann rule hastie tibshirani earlier related proposals appear short fukunaga myles hand query point neighborhood say points formed class distribution among points used decide deform neighborhood adapt metric adapted metric used nearest neighbor rule query point thus query point potentially different metric used figure clear neighborhood stretched direction orthogonal line joining class centroids direction also coincides linear discriminant boundary direction class probabilities change least general direction maximum change orthogonal line joining class cen troids see figure page assuming local discriminant model information contained local within class covari ance matrices needed determine optimal shape neighborhood discriminant adaptive nearest neighbor dann metric query point defined pooled within class covariance matrix class covariance matrix computed using nearest neighbors around computation metric used nearest neighbor rule complicated formula actually quite simple operation first spheres data respect stretches neighborhood zero eigenvalue directions matrix sphered data makes sense since locally observed class means dif fer directions parameter rounds neighborhood infinite strip ellipsoid avoid using points far away query point value seems work well general figure shows resulting neighborhoods problem classes form two con centric circles notice neighborhoods stretch orthogonally decision boundaries classes present neighborhood pure regions one class neighborhoods remain circular prototypes nearest neighbors figure neighborhoods found dann procedure various query points centers crosses two classes data one class surrounding nearest neighbors used estimate local met rics shown resulting metrics used form nearest neighborhoods cases matrix identity matrix 
[prototype, methods, nearest-neighbors, adaptive, nearest-neighbor, methods, example] generate two class data ten dimensions analogous two dimensional example figure ten predictors class dependent standard normal conditioned radius greater less predictors class independent stan dard normal without restriction observations class hence first class almost completely surrounds second class full ten dimensional space example pure noise variables kind nearest neighbor subset selection rule might able weed given point feature space class discrimination occurs along one direction however direction changes move across feature space variables important somewhere space figure shows boxplots test error rates ten realiza tions standard nearest neighbors lvq discriminant adaptive nearest neighbors used prototypes per class lvq make comparable nearest neighbors since adaptive metric significantly reduces error rate compared lvq standard nearest neighbors adaptive nearest neighbor methods lvq dann test error figure ten dimensional simulated example boxplots test error rates ten realizations standard nearest neighbors lvq centers discriminant adaptive nearest neighbors 
[prototype, methods, nearest-neighbors, adaptive, nearest-neighbor, methods, global, dimension, reduction, nearest-neighbors] discriminant adaptive nearest neighbor method carries local mension reduction dimension reduction separately query point many problems also benefit global dimension duction apply nearest neighbor rule optimally chosen subspace original feature space example suppose two classes form two nested spheres four dimensions feature space additional six noise features whose distribution independent class would like discover important four dimensional subspace carry nearest neighbor classification reduced sub space hastie tibshirani discuss variation discriminant adaptive nearest neighbor method purpose training point centroids sum squares matrix computed matrices averaged training points let eigenvectors matrix ordered largest smallest eigenvalue eigenvectors span optimal sub spaces global subspace reduction derivation based fact best rank approximation solves least squares problem min rank trace since contains information local discriminant subspace strength discrimination subspace seen prototypes nearest neighbors way finding best approximating subspace dimension series subspaces weighted least squares exercise four dimensional sphere example mentioned examined hastie tibshirani four eigenvalues turn large eigenvectors nearly spanning interesting subspace remaining six near zero operationally project data leading four dimensional subspace carry nearest neighbor classification satellite image classification example section technique labeled dann figure used nearest neighbors globally reduced subspace also connections technique sliced inverse regression proposal duan authors use similar ideas regression setting global rather local computations assume exploit spherical symmetry feature distribution estimate interesting subspaces 
[prototype, methods, nearest-neighbors, computational, considerations] one drawback nearest neighbor rules general computational load finding neighbors storing entire training set observations predictors nearest neighbor classification requires operations find neighbors per query point fast algorithms finding nearest neighbors friedman friedman reduce load somewhat hastie simard reduce computations tangent distance developing analogs means clustering context invariant metric reducing storage requirements difficult various editing condensing procedures proposed idea isolate subset training set suffices nearest neighbor predictions throw away remaining training data intuitively seems important keep training points near decision boundaries correct side boundaries points far boundaries could discarded multi edit algorithm devijver kittler divides data cyclically training test sets computing nearest neighbor rule training set deleting test points misclassified idea keep homogeneous clusters training observations condensing procedure hart goes trying keep important exterior points clusters starting single ran domly chosen observation training set additional data item processed one time adding training set misclas sified nearest neighbor rule computed current training set procedures surveyed dasarathy ripley also applied learning procedures besides nearest exercises neighbors methods sometimes useful much practical experience found systematic comparison performance literature 
[prototype, methods, nearest-neighbors, bibliographic, notes] nearest neighbor method goes back least fix hodges extensive literature topic reviewed dasarathy chapter ripley contains good summary means cluster ing due lloyd macqueen kohonen intro duced learning vector quantization tangent distance method due simard hastie tibshirani proposed discrim inant adaptive nearest neighbor technique 
[prototype, methods, nearest-neighbors, exercises] consider gaussian mixture model covariance matrices assumed scalar fixed param eter discuss analogy means clustering algorithm algorithm fitting mixture model detail show limit two methods coincide derive formula median radius nearest neighborhood let error rate bayes rule class problem true class probabilities given suming test point training point identical features prove arg max hence argue error rate nearest neighbor rule converges size training set creases value bounded statement theorem cover hart taken chapter ripley short proof also given prototypes nearest neighbors consider image function two dimensional spatial domain paper coordinates represents affine transformation image matrix decompose via way parameters identifying four affine transformations two scale shear rotation clearly identified using chain rule show derivative parameters represented terms two spatial derivatives using two dimensional kernel smoother chapter describe implement procedure images quantized pixels let square positive semi definite trices let write eigen decomposition show best rank approx imation min rank trace given hint write trace trace trace consider problem shape averaging particular matrices points sampled corresponding positions handwritten cursive letters seek affine invariant average also letters following property minimizes min characterize solution solution suffer letters big dominate average alternative approach minimize instead min derive solution problem criteria differ use svd simplify comparison two approaches exercises consider application nearest neighbors easy hard problems left panel figure replicate results left panel figure estimate misclassification errors using fivefold cross validation compare error rate curves consider aic like penalization training set misclassifica tion error specifically add training set misclassification error approximate number parameters ing number nearest neighbors compare plots resulting penalized misclassification error method gives better estimate optimal number nearest neighbors cross validation aic generate data two classes two features features independent gaussian variates standard deviation mean vectors class class feature vector apply random rotation angle chosen uniformly generate observations class form training set class test set apply four different classifiers nearest neighbors nearest neighbors hints ten randomly rotated versions data point added training set applying nearest neighbors invariant metric nearest neighbors using euclidean distance invari ant rotations origin tangent distance nearest neighbors case choose number neighbors tenfold cross validation compare results prototypes nearest neighbors 
[unsupervised, learning, introduction] previous chapters concerned predicting values one outputs response variables given set input predictor variables denote inputs ith training case let response measurement predictions based training sample previously solved cases joint values variables known called supervised learning learn ing teacher metaphor student presents swer training sample supervisor teacher provides either correct answer error associated stu dent answer usually characterized loss function example one supposes random variables represented joint probability density supervised learning formally characterized density estimation problem one concerned determining properties conditional density usually properties interest location parameters minimize expected error argmin unsupervised learning conditioning one joint marginal density values alone pervised learning typically direct concern one interested mainly properties conditional density since ten low dimension usually one location interest problem greatly simplified discussed previous chapters many approaches successfully addressing supervised learning variety contexts chapter address unsupervised learning learning without teacher case one set observations random vector joint density goal directly infer properties probability density without help supervisor teacher providing correct answers degree error observation dimension sometimes much higher supervised learn ing properties interest often complicated simple location estimates factors somewhat mitigated fact represents variables consideration one required infer properties change conditioned changing values another set variables low dimensional problems say variety effective nonparametric methods directly estimating density values representing graphically silverman owing curse dimensionality methods fail high dimensions one must settle estimating rather crude global models gaussian mixtures various simple descriptive statistics characterize generally descriptive statistics attempt characterize values collections values relatively large principal components multidimensional scaling self organizing maps principal curves example attempt identify low dimensional manifolds within space represent high data density provides information associations among variables whether considered functions smaller set latent variables cluster anal ysis attempts find multiple convex regions space contain modes tell whether represented mixture simpler densities representing distinct types classes servations mixture modeling similar goal association rules attempt construct simple descriptions conjunctive rules describe regions high density special case high dimensional binary valued data supervised learning clear measure success lack thereof used judge adequacy particular situations compare effectiveness different methods various situations association rules lack success directly measured expected loss joint dis tribution estimated variety ways including cross validation context unsupervised learning direct measure success difficult ascertain validity inferences drawn output unsupervised learning algorithms one must resort heuristic arguments motivating algorithms often case supervised learning well also judgments quality results uncomfortable situation led heavy proliferation proposed methods since effectiveness matter opinion cannot verified directly chapter present unsupervised learning techniques among commonly used practice additionally others favored authors 
[unsupervised, learning, association, rules] association rule analysis emerged popular tool mining com mercial data bases goal find joint values variables appear frequently data base often applied binary valued data referred market basket analysis context observations sales trans actions occurring checkout counter store variables represent items sold store observation variable assigned one two values jth item pur chased part transaction whereas purchased variables frequently joint values one represent items frequently purchased together information quite useful stocking shelves cross marketing sales promotions catalog design consumer segmentation based buying patterns generally basic goal association rule analysis find collection prototype values feature vector probability density evaluated values rela tively large general framework problem viewed mode finding bump hunting formulated problem impossibly dif ficult natural estimator fraction observations problems involve small number variables assume small number val ues number observations nearly always small reliable estimation order tractable problem goals analysis generality data applied must greatly simplified first simplification modifies goal instead seeking values large one seeks regions space high probability unsupervised learning content relative size support let represent set possible values jth variable support let subset values modified goal stated attempting find subsets variable values probability variables simultaneously assuming value within respective subset   relatively large intersection subsets called conjunctive rule quantitative variables subsets contiguous intervals categorical variables subsets delineated explicitly note subset fact entire set values often case variable said appear rule 
[unsupervised, learning, association, rules, market, basket, analysis] general approaches solving discussed section quite useful many applications however feasible large commercial data bases market basket analysis often applied several simplifications required first two types subsets considered either consists single value consists entire set values assume simplifies problem finding subsets integers corresponding values   large figure illustrates assumption one apply technique dummy variables turn problem involving binary valued variables assume support finite variable specifically new set variables created one variable values attainable original variables number dummy variables number distinct values attainable dummy variable assigned value variable sociated takes corresponding value assigned otherwise transforms finding subset integers association rules figure simplifications association rules two inputs taking four six distinct values respectively red squares indicate areas high density simplify computations assume derived subset corresponds either single value input values assumption could find either middle right pattern left one large standard formulation market basket problem set called item set number variables item set called size note size bigger estimated value taken fraction observations data base conjunction true value ith case called support prevalence item set observation said contain item set association rule mining lower support bound specified one seeks item sets formed variables support data base greater lower bound 
[unsupervised, learning, association, rules, apriori, algorithm] solution problem obtained feasible compu tation large data bases provided threshold adjusted consists small fraction possible item sets apriori algorithm agrawal exploits several aspects unsupervised learning curse dimensionality solve small number passes data specifically given support threshold cardinality relatively small item set consisting subset items must support greater equal first pass data computes support single item sets whose support less threshold discarded second pass computes support item sets size two formed pairs single items surviving first pass words generate frequent itemsets need consider candidates ancestral item sets size frequent size two item sets support less threshold discarded successive pass data considers item sets formed combining survived previous pass retained first pass passes data continue candidate rules previous pass support less specified threshold apriori algorithm requires one pass data value crucial since assume data cannot fitted computer main memory data sufficiently sparse threshold high enough process terminate reasonable time even huge data sets many additional tricks used part strat egy increase speed convergence agrawal apriori algorithm represents one major advances data mining technology high support item set returned apriori algorithm cast set association rules items partitioned two disjoint subsets written first item subset called antecedent second consequent association rules defined several properties based prevalence antecedent consequent item sets data base support rule fraction observations union antecedent consequent support item set derived viewed estimate probability simultaneously observing item sets randomly selected market basket confidence predictability rule support divided support antecedent viewed estimate notation probability item set occurring basket abbreviation association rules expected confidence defined support consequent estimate unconditional probability finally lift rule defined confidence divided expected confidence estimate association measure example suppose item set peanut butter jelly bread consider rule peanut butter jelly bread support value rule means peanut butter jelly bread appeared together market baskets confidence rule plies peanut butter jelly purchased time bread also purchased bread appeared market baskets rule peanut butter jelly bread would lift goal analysis produce association rules high values support confidence apriori algorithm returns item sets high support defined support threshold confidence threshold set rules formed item sets confidence greater value reported item set size rules form agrawal present variant apriori algorithm rapidly determine rules survive confidence threshold possible rules formed solution item sets output entire analysis collection association rules satisfy constraints generally stored data base queried user typical requests might display rules sorted order confidence lift support specifically one might request list conditioned particular items antecedent especially consequent example request might following display transactions ice skates consequent confidence support could provide information items antecedent predicate sales ice skates focusing particular consequent casts problem framework supervised learning association rules become popular tool analyzing large commercial data bases settings market basket relevant unsupervised learning data cast form multidimensional contingency table output form conjunctive rules easily understood interpreted apriori algorithm allows analysis applied huge data bases much larger amenable types analyses association rules among data mining biggest successes besides restrictive form data applied sociation rules limitations critical computational feasibility support threshold number solution item sets size number passes required data grow exponentially decreasing size lower bound thus rules high confidence lift low support discovered example high confi dence rule vodka caviar uncovered owing low sales volume consequent caviar 
[unsupervised, learning, association, rules, example, market, basket, analysis] illustrate use apriori moderately sized demographics data base data set consists questionnaires filled shop ping mall customers san francisco bay area impact resources inc columbus use answers first questions relat ing demographics illustration questions listed table data seen consist mixture ordinal unordered categorical variables many latter values many missing values used freeware implementation apriori algorithm due chris tian borgelt removing observations missing values ordinal predictor cut median coded two dummy variables categorical predictor categories coded dummy variables resulted matrix observations dummy variables algorithm found total association rules involving predictors support least understanding large set rules challenging data analysis task attempt illustrate figure relative frequency dummy variable data top association rules bottom prevalent categories tend appear often rules example first category language english however others occupation represented exception first fifth level three examples association rules found apriori algo rithm association rule support confidence lift see http fuzzy uni magdeburg borgelt ion attribute relative frequency data income sex marstat age educ occup yrs bay dualinc perhous peryoung house typehome ethnic language attribute relative frequency association rules income sex marstat age educ occup yrs bay dualinc perhous peryoung house typehome ethnic language tiv enc ble ing inp tio unsupervised learning table inputs demographic data feature demographic values type sex categorical marital status categorical age ordinal education ordinal occupation categorical income ordinal years bay area ordinal dual incomes categorical number household ordinal number children ordinal householder status categorical type home categorical ethnic classification categorical language home categorical number household number children language home english association rule support confidence lift  language home english householder status occupation professional managerial  income association rule support confidence lift    language home english income marital status married number children    education college graduate graduate study association rules chose first third rules based high support second rule association rule high income consequent could used try target high income individuals stated created dummy variables category input predictors example income income median income interested finding associations high income category would include often case actual market basket problems interested finding associations presence relatively rare item associations absence 
[unsupervised, learning, association, rules, unsupervised, supervised, learning] discuss technique transforming density estimation prob lem one supervised function approximation forms basis generalized association rules described next section let unknown data probability density estimated specified probability density function used reference ample might uniform density range variables possibilities discussed data set pre sumed random sample drawn sample size drawn using monte carlo methods pooling two data sets assigning mass drawn drawn results random sample drawn mixture density one assigns value sample point drawn drawn estimated supervised learning using combined sample training data resulting estimate inverted provide estimate generalized versions logistic regression section especially well suited application since log odds log estimated directly case one unsupervised learning figure density estimation via classification left panel training set data points right panel training set plus reference data points generated uniformly rectangle containing training data training sample labeled class reference sample class semipara metric logistic regression model fit data contours shown example shown figure generated training set size shown left panel right panel shows reference data blue generated uniformly rectangle containing training data training sample labeled class reference sample class logistic regression model using tensor product natural splines section fit data probability contours shown right panel also contours density estimate since monotone function contours roughly capture data density principle reference density used practice accuracy estimate depend greatly partic ular choices good choices depend data density procedure used estimate accuracy goal chosen resulting functions approx imated easily method used however accuracy always primary goal monotonic functions den sity ratio thus viewed contrast statistics provide information concerning departures data density chosen reference density therefore data analytic settings choice dictated types departures deemed interesting context specific problem hand example departures uniformity interest might uniform density range variables departures joint normality association rules interest good choice would gaussian distribution mean vector covariance matrix data departures independence could investigated using marginal data density jth coordinate sample independent density easily generated data applying different random permutation data values variables discussed unsupervised learning concerned revealing properties data density technique focuses particu lar property set properties although approach transforming problem one supervised learning seems part statistics folklore time appear much impact despite potential bring well developed pervised learning methodology bear unsupervised learning problems one reason may problem must enlarged simulated data set generated monte carlo techniques since size data set least large data sample compu tation memory requirements estimation procedure least doubled also substantial computation may required generate monte carlo sample although perhaps deterrent past increased computational requirements becoming much less burden increased resources become routinely available illustrate use supervising learning methods unsupervised learning next section 
[unsupervised, learning, association, rules, generalized, association, rules] general problem finding high density regions data space addressed using supervised learning approach described although applicable huge data bases market basket analysis feasible useful information obtained mod erately sized data sets problem formulated finding subsets integers corresponding value subsets corresponding variables     large following nomenclature association rule analysis called generalized item set subsets correspond ing quantitative variables taken contiguous intervals within unsupervised learning range values subsets categorical variables involve single value ambitious nature formulation precludes thorough search generalized item sets support greater specified minimum threshold possible restric tive setting market basket analysis heuristic search methods must employed one hope find useful collection generalized item sets market basket analysis generalized formulation implicitly reference uniform probability distribution one seeks item sets frequent would expected joint data values uniformly distributed favors discovery item sets whose marginal constituents individually frequent quantity large conjunctions frequent subsets tend appear often among item sets high support conjunctions margin ally less frequent subsets rule vodka caviar likely discovered spite high association lift neither item high marginal support joint support especially small reference uniform distribution cause highly frequent item sets low associations among constituents dominate collection highest support item sets highly frequent subsets formed disjunctions fre quent values using product variable marginal data densities reference distribution removes preference highly fre quent values individual variables discovered item sets density ratio uniform associations among variables complete independence regardless frequency distribution individual variable values rules like vodka caviar would chance emerge clear however incorporate reference distributions uniform apriori algorithm explained section straightforward generate sample product density given original data set choosing reference distribution drawing sample one supervised learning problem binary valued output variable goal use training data find regions target function relatively large addition one might wish require data support regions association rules small 
[unsupervised, learning, association, rules, choice, supervised, learning, method] regions defined conjunctive rules hence supervised methods learn rules would appropriate context terminal nodes cart decision tree defined rules precisely form applying cart pooled data pro duce decision tree attempts model target entire data space disjoint set regions terminal nodes region defined rule form terminal nodes high average values ave candidates high support generalized item sets actual data support given number pooled observations within region repre sented terminal node examining resulting decision tree one might discover interesting generalized item sets relatively high support partitioned antecedents consequents search generalized association rules high confidence lift another natural learning method purpose patient rule induction method prim described section prim also produces rules precisely form especially designed finding high support regions maximize average target value within rather trying model target function entire data space also provides control support average pagevalue tradeoff exercise addresses issue arises either methods generate random data product marginal distribu tions 
[unsupervised, learning, association, rules, example, market, basket, analysis, continued] illustrate use prim demographics data table three high support generalized item sets emerging prim analysis following item set support unsupervised learning  marital status married householder status type home apartment  item set support    age marital status living together married single occupation professional homemaker retired householder status rent live family    item set support        householder status rent type home house number household number children occupation homemaker student unemployed income        generalized association rules derived item sets confidence greater following association rule support confidence lift marital status married householder status type home apartment association rule support confidence lift  age occupation professional homemaker retired householder status rent live family  marital status single living together married association rule support confidence lift householder status type home apartment marital status married cluster analysis association rule support confidence lift      householder status rent type home house number household occupation homemaker student unemployed income      number children great surprises among particular rules part verify intuition contexts less prior formation available unexpected results greater chance emerge results illustrate type information generalized association rules provide supervised learning approach coupled ruled induction method cart prim uncover item sets exhibiting high associations among constituents generalized association rules compare found earlier apriori algorithm since apriori procedure gives thousands rules difficult compare however general points made apriori algorithm exhaustive finds rules support greater specified amount contrast prim greedy algorithm guaranteed give optimal set rules hand apriori algorithm deal dummy variables hence could find rules example since type home categorical input dummy variable level apriori could find rule involving set type home apartment find set would code dummy variable apartment versus categories type home generally feasible precode potentially interesting comparisons 
[unsupervised, learning, cluster, analysis] cluster analysis also called data segmentation variety goals relate grouping segmenting collection objects subsets clusters within cluster closely related one another objects assigned different clusters object described set measurements relation objects addition goal sometimes arrange clusters natural hierarchy involves successively grouping clusters unsupervised learning figure simulated data plane clustered three classes repre sented orange blue green means clustering algorithm level hierarchy clusters within group similar different groups cluster analysis also used form descriptive statistics ascertain whether data consists set distinct subgroups group representing objects substantially different properties latter goal requires assessment degree difference objects signed respective clusters central goals cluster analysis notion degree similarity dissimilarity individual objects clustered clustering method attempts group objects based definition similarity supplied come subject matter consid erations situation somewhat similar specification loss cost function prediction problems supervised learning cost associated inaccurate prediction depends considerations outside data figure shows simulated data clustered three groups via popular means algorithm case two clusters well separated segmentation accurately describes part process clustering means clustering starts guesses three cluster centers alternates following steps convergence data point closest cluster center euclidean distance identified cluster analysis cluster center replaced coordinate wise average data points closest describe means clustering detail later including prob lem choose number clusters three example means clustering top procedure cluster approaches discuss bottom fundamental clustering techniques choice distance dissimilarity measure two objects first discuss distance measures describing variety algorithms clustering 
[unsupervised, learning, cluster, analysis, proximity, matrices] sometimes data represented directly terms proximity alike ness affinity pairs objects either similarities dissimilarities difference lack affinity example social science experiments participants asked judge much certain objects differ one another dissimilarities computed averaging collection judgments type data represented matrix number objects element records proximity ith objects matrix provided input clustering algorithm algorithms presume matrix dissimilarities nonnegative entries zero diagonal elements original data collected similarities suitable monotone decreasing function used convert dissimilarities also algorithms sume symmetric dissimilarity matrices original matrix symmetric must replaced subjectively judged dissimi larities seldom distances strict sense since triangle inequality hold thus algorithms assume distances cannot used data 
[unsupervised, learning, cluster, analysis, dissimilarities, based, attributes] often measurements variables also called attributes since popular clustering algorithms take dissimilarity matrix input must first construct pairwise dissimilarities observations common case define dissimilarity values jth attribute define dissimilarity objects far common choice squared distance unsupervised learning however choices possible lead potentially different results nonquantitative attributes categorical data squared dis tance may appropriate addition sometimes desirable weigh attributes differently rather giving equal weight first discuss alternatives terms attribute type quantitative variables measurements type variable attribute represented continuous real valued numbers natural define error monotone increasing function absolute difference besides squared error loss common choice identity absolute error former places emphasis larger differ ences smaller ones alternatively clustering based correlation note averaged variables servations observations first standardized hence clustering based correlation simi larity equivalent based squared distance dissimilarity ordinal variables values type variable often represented contiguous integers realizable values considered ordered set examples academic grades degree preference stand dislike like terrific rank data special kind ordinal data error measures ordinal variables generally defined replacing original values prescribed order original values treated quantitative variables scale categorical variables unordered categorical also called nominal variables degree difference pairs values must delineated explicitly variable assumes distinct values arranged symmetric matrix elements common choice unequal losses used emphasize errors others cluster analysis 
[unsupervised, learning, cluster, analysis, object, dissimilarity] next define procedure combining individual attribute dissim ilarities single overall measure dissim ilarity two objects observations possessing respective attribute values nearly always done means weighted average convex combination weight assigned jth attribute regulating relative influence variable determining overall dissimilarity objects choice based subject matter considerations important realize setting weight value variable say necessarily give attributes equal influence influence jth attribute object dissimilarity depends upon relative contribution average object dissimilarity measure pairs observations data set average dissimilarity jth attribute thus relative fluence jth variable setting would give attributes equal influence characterizing overall dissimilarity jects example quantitative variables squared error distance used coordinate becomes weighted squared clidean distance pairs points quantitative variables axes case becomes var var sample estimate var thus relative impor tance variable proportional variance data unsupervised learning figure simulated data left means clustering applied raw data two colors indicate cluster memberships right features first standardized clustering equivalent using feature weights var standardization obscured two well separated groups note plot uses units horizontal vertical axes set general setting attributes irrespective type cause one equally influence overall dissimilarity pairs objects although may seem reasonable often recommended highly counterproductive goal segment data groups similar objects attributes may con tribute equally problem dependent notion dissimilarity objects attribute value differences may reflect greater actual object dissimilarity context problem domain goal discover natural groupings data attributes may exhibit grouping tendency others variables relevant separating groups assigned higher influ ence defining object dissimilarity giving attributes equal influence case tend obscure groups point clustering algorithm cannot uncover figure shows example although simple generic prescriptions choosing individual tribute dissimilarities weights comforting substitute careful thought context individ ual problem specifying appropriate dissimilarity measure far important obtaining success clustering choice clustering algorithm aspect problem emphasized less cluster ing literature algorithms since depends domain knowledge specifics less amenable general research cluster analysis finally often observations missing values one attributes common method incorporating missing values dissimilarity calculations omit observation pair least one value missing computing dissimilarity tween observations method fail circumstance observations measured values common case observations could deleted analysis alternatively missing values could imputed using mean median attribute nonmissing data categorical variables one could consider value missing another categorical value reasonable consider two objects similar missing values variables 
[unsupervised, learning, cluster, analysis, clustering, algorithms] goal cluster analysis partition observations groups clusters pairwise dissimilarities assigned cluster tend smaller different clusters clus tering algorithms fall three distinct types combinatorial algorithms mixture modeling mode seeking combinatorial algorithms work directly observed data direct reference underlying probability model mixture modeling sup poses data sample population described probability density function density function characterized rameterized model taken mixture component density functions component density describes one clusters model fit data maximum likelihood corresponding bayesian approaches mode seekers bump hunters take nonparametric perspective attempt ing directly estimate distinct modes probability density function observations closest respective mode define individual clusters mixture modeling described section prim algorithm dis cussed sections example mode seeking bump hunting discuss combinatorial algorithms next 
[unsupervised, learning, cluster, analysis, combinatorial, algorithms] popular clustering algorithms directly assign observation group cluster without regard probability model describing data observation uniquely labeled integer prespecified number clusters postulated one labeled integer observation assigned one one cluster assignments characterized many one mapping encoder assigns ith observation kth cluster one seeks particular encoder achieves unsupervised learning required goal details based dissimilarities every pair observations specified user described generally encoder explicitly delineated giving value cluster assignment observation thus parameters procedure individual cluster assignments observations adjusted minimize loss function characterizes degree clustering goal met one approach directly specify mathematical loss function attempt minimize combinatorial optimization algorithm since goal assign close points cluster natural loss energy function would criterion characterizes extent observations assigned cluster tend close one another sometimes referred within cluster point scatter since   total point scatter constant given data independent cluster assignment quantity cluster point scatter tend large obser vations assigned different clusters far apart thus one minimizing equivalent maximizing cluster analysis combinatorial optimization straightforward prin ciple one simply minimizes equivalently maximizes pos sible assignments data points clusters unfortunately optimization complete enumeration feasible small data sets number distinct assignments jain dubes example quite feasible grows rapidly increasing values arguments already cluster analysis clustering problems involve much larger data sets reason practical clustering algorithms able examine small fraction possible encoders goal identify small subset likely contain optimal one least good suboptimal partition feasible strategies based iterative greedy descent initial partition specified iterative step cluster assignments changed way value criterion improved previous value clustering algorithms type differ pre scriptions modifying cluster assignments iteration prescription unable provide improvement algorithm ter minates current assignments solution since assignment observations clusters iteration perturbation previous iteration small fraction possible assignments examined however algorithms converge local optima may highly suboptimal compared global optimum 
[unsupervised, learning, cluster, analysis, k-means] means algorithm one popular iterative descent clus tering methods intended situations variables quantitative type squared euclidean distance chosen dissimilarity measure note weighted euclidean dis tance used redefining values exercise within point scatter written mean vector associated kth clus ter thus criterion minimized assigning observations clusters way within cluster average dissimilarity observations cluster mean defined points cluster minimized iterative descent algorithm solving unsupervised learning algorithm means clustering given cluster assignment total cluster variance minimized respect yielding means currently assigned clusters given current set means minimized assigning observation closest current cluster mean argmin steps iterated assignments change min obtained noting set observations argmin hence obtain solving enlarged optimization problem min minimized alternating optimization procedure given algorithm steps reduces value criterion convergence assured however result may represent suboptimal local minimum algorithm hartigan wong goes ensures single switch observation one group another group decrease objective addition one start algorithm many different random choices starting means choose solution smallest value objective func tion figure shows means iterations simulated data figure centroids depicted straight lines show partitioning points sector set points closest centroid partitioning called voronoi tessellation iterations procedure converged 
[unsupervised, learning, cluster, analysis, gaussian, mixtures, soft, k-means, clustering] means clustering procedure closely related algorithm estimating certain gaussian mixture model sections cluster analysis initial centroids initial partition iteration number iteration number figure successive iterations means clustering algorithm simulated data figure unsupervised learning responsibilities responsibilities figure left panels two gaussian densities blue orange real line single data point green dot colored squares plotted means density right panels relative densities called responsibilities cluster data point top panels gaussian standard deviation bottom panels algorithm uses responsibilities make soft assignment data point two clusters fairly large responsibilities near top right panel responsibilities cluster center closest target point clusters hard assignment seen bottom right panel step algorithm assigns responsibilities data point based relative density mixture component step recomputes component density parameters based current responsibilities suppose specify mixture components gaussian density scalar covariance matrix relative density mixture component monotone function euclidean distance data point mixture center hence setup soft version means clustering making probabilistic rather deterministic assignments points cluster centers variance probabilities become two methods coincide details given exercise figure illustrates result two clusters real line 
[unsupervised, learning, cluster, analysis, example, human, tumor, microarray, data] apply means clustering human tumor microarray data scribed chapter example high dimensional clustering cluster analysis number clusters sum squares figure total within cluster sum squares means clustering plied human tumor microarray data table human tumor data number cancer cases type three clusters means clustering cluster breast cns colon leukemia mcf cluster melanoma nsclc ovarian prostate renal unknown data matrix real numbers representing expression measurement gene row sample column cluster samples vector length correspond ing expression values genes sample label breast breast cancer melanoma use bels clustering examine posthoc labels fall clusters applied means clustering running com puted total within sum squares clustering shown fig ure typically one looks kink sum squares curve logarithm locate optimal number clusters see section clear indication illustration chose giving three clusters shown table unsupervised learning figure sir ronald fisher one founders modern day statistics owe maximum likelihood sufficiency many fundamental concepts image left grayscale image bits per pixel center image result block using code vectors compression rate bits pixel right image uses four code vectors compression rate bits pixel see procedure successful grouping together samples cancer fact two breast cancers second cluster later found misdiagnosed melanomas metastasized however means clustering shortcomings application one give linear ordering objects within cluster simply listed alphabetic order secondly number clusters changed cluster memberships change arbitrary ways say four clusters clusters need nested within three clusters reasons hierarchical clustering described later probably preferable application 
[unsupervised, learning, cluster, analysis, vector, quantization] means clustering algorithm represents key tool apparently unrelated area image signal compression particularly vector quan tization gersho gray left image figure digitized photograph famous statistician sir ronald fisher consists pixels pixel grayscale value ranging hence requires bits storage per pixel entire image cupies megabyte storage center image compressed version left panel requires storage loss quality right image compressed even requires storage considerable loss quality version implemented first breaks image small blocks case blocks pixels blocks four example prepared maya gupta cluster analysis numbers regarded vector means clustering algorithm also known lloyd algorithm context run space center image uses right image pixel blocks points approximated closest cluster centroid known codeword clustering process called encoding step collection centroids called codebook represent approximated image need supply block identity codebook entry approximates require log bits per block also need supply codebook real numbers typically negligible overall storage compressed image amounts log original typically expressed rate bits per pixel log respectively process constructing approximate image centroids called decoding step expect work reason typical everyday images like photographs many blocks look case many almost pure white blocks similarly pure gray blocks various shades require one block represent multiple pointers block described known lossy compression since ages degraded versions original degradation distortion usually measured terms mean squared error case generally rate distortion curve would used assess tradeoff one also perform lossless compression using block clustering still capitalize repeated pat terns took original image losslessly compressed best would bits per pixel claimed log bits needed identify codewords codebook uses fixed length code inefficient codewords occur many times others image using shannon coding theory know general variable length code better rate becomes log term numerator entropy distribution codewords image using variable length coding rates come respectively finally many generalizations developed example tree structured finds centroids top means style algorithm alluded section allows successive refinement compression details may found gersho gray 
[unsupervised, learning, cluster, analysis, k-medoids] discussed means algorithm appropriate dis similarity measure taken squared euclidean distance unsupervised learning algorithm medoids clustering given cluster assignment find observation cluster minimizing total distance points cluster argmin current estimates cluster centers given current set cluster centers minimize tal error assigning observation closest current cluster center argmin iterate steps assignments change requires variables quantitative type addition using squared euclidean distance places highest influence largest distances causes procedure lack robustness outliers produce large distances restrictions moved expense computation part means algorithm assumes squared clidean distance minimization step cluster representatives taken means currently assigned clusters algorithm generalized use arbitrarily defined dissimilarities replacing step explicit optimization respect common form cen ters cluster restricted one observations assigned cluster summarized algorithm algorithm assumes attribute data approach also applied data described proximity matrices section need explicitly compute cluster centers rather keep track indices solving provisional cluster requires amount com putation proportional number observations assigned whereas solving computation increases given set clus ter centers obtaining new assignments argmin requires computation proportional thus medoids far computationally intensive means alternating represents particular heuristic search strategy trying solve cluster analysis table data political science survey values average pairwise dissimilarities countries questionnaire given political science students bel bra chi cub egy fra ind isr usa uss yug bra chi cub egy fra ind isr usa uss yug zai min kaufman rousseeuw propose alternative strategy directly solving provisionally exchanges center obser vation currently center selecting exchange produces greatest reduction value criterion repeated advantageous exchanges found massart derive branch bound combinatorial method finds global minimum practical small data sets example country dissimilarities example taken kaufman rousseeuw comes study political science students asked provide pairwise dis similarity measures countries belgium brazil chile cuba egypt france india israel united states union soviet socialist republics yugoslavia zaire average dissimilarity scores given ble applied medoid clustering dissimilarities note means clustering could applied distances rather raw observations left panel figure shows dissimilarities reordered blocked according medoid clustering right panel two dimensional multidimensional scaling plot medoid clusters assignments indicated colors multidimensional scaling discussed section plots show three well separated clusters mds display indicates egypt falls halfway two clusters unsupervised learning chi cub uss yug bra ind zai bel egy fra isr cub uss yug bra ind zai bel egy fra isr usa reordered dissimilarity matrix first mds coordinate second mds coordinate chi cub uss yug bra ind zai bel egy fra isr usa figure survey country dissimilarities left panel dissimilarities reordered blocked according medoid clustering heat map coded similar dark red least similar bright red right panel two dimen sional multidimensional scaling plot medoid clusters indicated different colors 
[unsupervised, learning, cluster, analysis, practical, issues] order apply means medoids one must select number clusters initialization latter defined specifying initial set centers initial encoder usually specifying centers convenient suggestions range simple random selection deliberate strategy based forward stepwise assignment step new center chosen minimize criterion given centers chosen previous steps continues steps thereby producing initial centers begin optimization algorithm choice number clusters depends goal data segmentation usually defined part problem example company may employ sales people goal partition customer database segments one sales person customers assigned one similar possible often however cluster analysis used provide descriptive statistic ascertaining extent observations comprising data base fall natural distinct groupings number groups unknown one requires well groupings estimated data data based methods estimating typically examine within cluster dissimilarity function number clusters separate solutions obtained max corresponding values cluster analysis max generally decrease increasing case even criterion evaluated independent test set since large number cluster centers tend fill feature space densely thus close data points thus cross validation techniques useful model selection supervised learning cannot utilized context intuition underlying approach actually distinct groupings observations defined dissimilarity mea sure clusters returned algorithm contain subset true underlying groups solution assign observations naturally occurring group different estimated clusters extent case solution criterion value tend decrease substantially successive increase number specified clusters natural groups suc cessively assigned separate clusters one estimated clusters must partition least one natural groups two sub groups tend provide smaller decrease criterion increased splitting natural group within observations quite close reduces criterion less partitioning union two well separated groups proper constituents extent scenario realized sharp decrease successive differences criterion value estimate obtained identifying kink plot function aspects clustering procedures approach somewhat heuristic recently proposed gap statistic tibshirani compares curve log curve obtained data uniformly distributed rectangle containing data estimates optimal number clusters place gap two curves largest essentially automatic way locating aforementioned kink also works reasonably well data fall single cluster case tend estimate optimal number clusters one scenario competing methods fail figure shows result gap statistic applied simulated data figure left panel shows log clusters green curve expected value log simulations uniform data blue curve right panel shows gap curve expected curve minus observed curve shown also error bars half width standard deviation log simulations gap curve maximized clusters gap curve clusters formal rule estimating argmin unsupervised learning number clusters number clusters gap figure left panel observed green expected blue values log simulated data figure curves translated equal zero one cluster right panel gap curve equal difference observed expected values log gap estimate smallest producing gap within one standard deviation gap gives looks reasonable figure 
[unsupervised, learning, cluster, analysis, hierarchical, clustering] results applying means medoids clustering algorithms pend choice number clusters searched starting configuration assignment contrast hierarchical clustering methods require specifications instead require user specify measure dissimilarity disjoint groups observations based pairwise dissimilarities among observations two groups name suggests produce hierarchical representations clusters level hierarchy created merging clusters next lower level lowest level cluster contains single observation highest level one cluster containing data strategies hierarchical clustering divide two basic paradigms glomerative bottom divisive top agglomerative strategies start bottom level recursively merge selected pair clusters single cluster produces grouping next higher level one less cluster pair chosen merging consist two groups smallest intergroup dissimilarity divisive methods start top level recursively split one existing clusters cluster analysis level two new clusters split chosen produce two new groups largest group dissimilarity paradigms levels hierarchy level hierarchy represents particular grouping data disjoint clusters observations entire hierarchy represents ordered sequence groupings user decide level actually represents natural clustering sense observations within groups sufficiently similar observations assigned different groups level gap statistic described earlier used purpose recursive binary splitting agglomeration represented rooted binary tree nodes trees represent groups root node repre sents entire data set terminal nodes represent one individual observations singleton clusters nonterminal node par ent two daughter nodes divisive clustering two daughters represent two groups resulting split parent agglom erative clustering daughters represent two groups merged form parent agglomerative divisive methods viewed bottom possess monotonicity property dissimilarity merged clusters monotone increasing level merger thus binary tree plotted height node proportional value intergroup dissimilarity two daughters terminal nodes representing individual observations plotted zero height type graphical display called dendrogram dendrogram provides highly interpretable complete description hierarchical clustering graphical format one main reasons popularity hierarchical clustering methods microarray data figure shows dendrogram resulting agglomerative clustering average linkage agglomerative cluster ing example discussed detail later chapter cutting dendrogram horizontally particular height partitions data disjoint clusters represented vertical lines intersect clusters would produced terminating pro cedure optimal intergroup dissimilarity exceeds threshold cut value groups merge high values relative merger values subgroups contained within lower tree candidates natural clusters note may occur several different levels indicating clustering hierarchy clusters nested within clusters dendrogram often viewed graphical summary data rather description results algorithm however interpretations treated caution first different hierar chical methods see well small changes data lead quite different dendrograms also summary valid extent pairwise observation dissimilarities possess hierar unsupervised learning cns cns cns renal breast cns cns breast nsclc nsclc renal renal renal renal renal renal renal breast nsclc renal unknown ovarian melanoma prostate ovarian ovarian ovarian ovarian ovarian prostate nsclc nsclc nsclc leukemia repro repro leukemia leukemia leukemia leukemia leukemia colon colon colon colon colon colon colon mcfa repro breast mcfd repro breast nsclc nsclc nsclc melanoma breast breast melanoma melanoma melanoma melanoma melanoma melanoma figure dendrogram agglomerative hierarchical clustering average linkage human tumor microarray data chical structure produced algorithm hierarchical methods impose hierarchical structure whether structure actually exists data extent hierarchical structure produced dendro gram actually represents data judged cophenetic correlation coefficient correlation pair wise observation dissimilarities input algorithm corre sponding cophenetic dissimilarities derived dendrogram cophenetic dissimilarity two observations inter group dissimilarity observations first joined together cluster cophenetic dissimilarity restrictive dissimilarity measure first observations must contain many ties since total values distinct also dissimilarities obey ultrametric inequality max cluster analysis three observations geometric example suppose data represented points euclidean coordinate system order set interpoint distances data conform triangles formed triples points must isosceles triangles unequal length longer length two equal sides jain dubes therefore unrealistic expect general dissimilarities arbitrary data sets closely resemble corresponding cophenetic dissimilarities calculated dendrogram especially many tied values thus dendrogram viewed mainly scription clustering structure data imposed particular algorithm employed agglomerative clustering agglomerative clustering algorithms begin every observation repre senting singleton cluster steps closest two least dissimilar clusters merged single cluster producing one less clus ter next higher level therefore measure dissimilarity two clusters groups observations must defined let represent two groups dissimilarity tween computed set pairwise observation dissim ilarities one member pair single linkage agglomerative clustering takes intergroup dissimilarity closest least dissimilar pair min also often called nearest neighbor technique complete linkage agglomerative clustering furthest neighbor technique takes tergroup dissimilarity furthest dissimilar pair max group average clustering uses average dissimilarity groups respective number observations group although many proposals defining intergroup dissimilarity context agglomerative clustering three ones commonly used figure shows examples three data dissimilarities exhibit strong clustering tendency clusters compact well separated others three methods produce similar results clusters compact unsupervised learning average linkage complete linkage single linkage figure dendrograms agglomerative hierarchical clustering man tumor microarray data observations within relatively close together small dissimilarities compared observations different clusters extent case results differ single linkage requires single dissimilarity small two groups considered close together irrespective observation dissimilarities groups therefore tendency combine relatively low thresholds observations linked series close intermediate observa tions phenomenon referred chaining often considered fect method clusters produced single linkage violate compactness property observations within cluster tend similar one another based supplied observation dissimilari ties define diameter group observations largest dissimilarity among members max single linkage produce clusters large diameters complete linkage represents opposite extreme two groups considered close observations union relatively similar tend produce compact clusters small diameters however produce clusters violate close ness property observations assigned cluster much cluster analysis closer members clusters members cluster group average clustering represents compromise two extremes single complete linkage attempts produce rel atively compact clusters relatively far apart however results depend numerical scale observation dissimilarities measured applying monotone strictly increasing transformation change result produced contrast depend ordering thus invariant monotone transformations invariance often used argument favor single complete linkage group average methods one argue group average clustering statistical consis tency property violated single complete linkage assume attribute value data cluster ran dom sample population joint density complete data set random sample mixture densities group average dissimilarity estimate dissimilarity points space attribute values sample size approaches infinity approaches characteristic relationship two densities single linkage approaches zero independent complete linkage becomes infinite independent two densities thus clear aspects population distribution estimated example human cancer microarray data continued left panel figure shows dendrogram resulting average linkage agglomerative clustering samples columns microarray data middle right panels show result using complete single linkage average complete linkage gave similar results single linkage produced unbalanced groups long thin clusters focus average linkage clustering like means clustering hierarchical clustering successful clustering simple cancers together however nice features cutting dendrogram various heights different numbers clusters emerge sets clusters nested within one another secondly gives partial ordering information samples figure arranged genes rows samples columns expression matrix orderings derived hierarchical clustering unsupervised learning note flip orientation branches dendrogram merge resulting dendrogram still consistent series hierar chical clustering operations hence determine ordering leaves must add constraint produce row ordering figure used default rule plus merge subtree tighter cluster placed left toward bottom rotated dendrogram figure individual genes tightest clusters possi ble merges involving two individual genes place order observation number rule used columns many rules possible example ordering multidimensional scaling genes see section two way rearrangement figure produces informative pic ture genes samples picture informative randomly ordered rows columns figure chapter dendrograms useful biologists example interpret gene clusters terms biological processes divisive clustering divisive clustering algorithms begin entire data set single cluster recursively divide one existing clusters two daugh ter clusters iteration top fashion approach studied nearly extensively agglomerative methods cluster ing literature explored somewhat engineering literature gersho gray context compression clustering setting potential advantage divisive agglomerative methods occur interest focused partitioning data relatively small number clusters divisive paradigm employed recursively applying combinatorial methods means section medoids section perform splits iteration ever approach would depend starting configuration specified step addition would necessarily produce splitting quence possesses monotonicity property required dendrogram representation divisive algorithm avoids problems proposed mac naughton smith begins placing observations single cluster chooses observation whose average dissimi larity observations largest observation forms first member second cluster successive step observation whose average distance minus remaining observations largest transferred continues corresponding difference averages becomes negative longer observations average closer result split original cluster two daughter clusters cluster analysis figure dna microarray data average linkage hierarchical clustering applied independently rows genes columns samples termining ordering rows columns see text colors range bright green negative expressed bright red positive expressed unsupervised learning observations transferred remaining two clusters represent second level hierarchy successive level produced applying splitting procedure one clusters previous level kaufman rousseeuw suggest choosing cluster level largest diameter splitting ternative would choose one largest average dissimilarity among members recursive splitting continues clusters either become singletons members one zero dissimilarity one another 
[unsupervised, learning, self-organizing, maps] method viewed constrained version means clustering prototypes encouraged lie one two dimensional manifold feature space resulting manifold also referred constrained topological map since original high dimensional obser vations mapped onto two dimensional coordinate system original som algorithm online observations processed one time later batch version proposed technique also bears close relationship principal curves surfaces discussed next section consider som two dimensional rectangular grid proto types choices hexagonal grids also used prototypes parametrized respect integer coordinate pair similarly initialized example lie two dimensional principal component plane data next section think prototypes buttons sewn principal component plane regular pattern som procedure tries bend plane buttons approximate data points well possible model fit observations mapped onto two dimensional grid observations processed one time find closest prototype euclidean distance neighbors move toward via update neighbors defined distance small simplest approach uses euclidean distance small determined threshold neighborhood always includes closest prototype self organizing maps notice distance defined space integer topological coordinates prototypes rather feature space effect update move prototypes closer data also maintain smooth two dimensional spatial relationship prototypes performance som algorithm depends learning rate distance threshold typically decreased say thousand iterations one per observation similarly decreased linearly starting value thousand iterations illustrate method choosing example described simplest version som sophisticated versions modify update step according distance neighborhood function gives weight prototypes indices closer away take distance small enough neighborhood contains one point spatial connection prototypes lost case one show som algorithm online version means clustering eventually stabilizes one local minima found means since som constrained version means clustering important check whether constraint reasonable given problem one computing reconstruction error summed observations methods necessarily smaller means much smaller som reasonable approximation illustrative example generated data points three dimen sions near surface half sphere radius points three clusters red green blue located near data shown figure design red cluster much tighter green blue ones full details data generation given exercise grid prototypes used initial grid size meant third prototypes initially neighborhood total passes dataset observations let decrease linearly iterations figure prototypes indicated circles points project prototype plotted randomly within correspond ing circle left panel shows initial configuration right panel shows final one algorithm succeeded separating clusters however separation red cluster indicates man ifold folded back see figure since distances two dimensional display used little indication som projection red cluster tighter others unsupervised learning figure simulated data three classes near surface half sphere figure self organizing map applied half sphere data example left panel initial configuration right panel final one grid prototypes indicated circles points project prototype plotted randomly within corresponding circle self organizing maps figure wiremesh representation fitted som model lines represent horizontal vertical edges topological lattice double lines indicate surface folded diagonally back order model red points cluster members jittered indicate color purple points node centers figure shows reconstruction error equal total sum squares data point around prototype comparison carried means clustering centroids indicate reconstruction error horizontal line graph see som significantly decreases error nearly level means solution pro vides evidence two dimensional constraint used som reasonable particular dataset batch version som update via sum points mapped closest neighbors weight function may rectangular equal neighbors may decrease smoothly distance neighborhood size chosen small enough consists rectangular weights reduces means clustering procedure described earlier also thought discrete version principal curves surfaces described section unsupervised learning iteration reconstruction error half sphere data reconstruction error som func tion iteration error means clustering indicated horizontal line example document organization retrieval document retrieval gained importance rapid development internet web soms proved useful organiz ing indexing large corpora example taken websom homepage http websom hut kohonen figure rep resents som fit newsgroup comp neural nets articles labels generated automatically websom software provide guide typical content node applications documents reprocessed order create feature vector term document matrix created row represents single document entries row relative frequency predefined set terms terms could large set dictionary entries words even larger set bigrams word pairs subsets matrices typically sparse often preprocessing done reduce number features columns sometimes svd next section used reduce matrix kohonen use randomized variant thereof reduced vectors input som self organizing maps figure heatmap representation som model fit corpus newsgroup comp neural nets contributions courtesy websom homepage lighter areas indicate higher density areas populated nodes automatically labeled according typical content unsupervised learning figure first linear principal component set data line minimizes total squared distance point orthogonal projection onto line application authors developed zoom feature allows one interact map order get detail final level zooming retrieves actual news articles read 
[unsupervised, learning, principal, components, curves, surfaces] principal components discussed sections shed light shrinkage mechanism ridge regression principal components sequence projections data mutually uncorrelated ordered variance next section present principal components linear manifolds approximating set points present nonlinear generalizations section recent proposals nonlinear approximating manifolds discussed section 
[unsupervised, learning, principal, components, curves, surfaces, principal, components] principal components set data provide sequence best linear approximations data ranks denote observations consider rank linear model representing principal components curves surfaces location vector matrix orthogonal unit vectors columns vector parameters parametric representation affine hyperplane rank figures illustrate respectively fitting model data least squares amounts minimizing reconstruction error min partially optimize exercise obtain leaves find orthogonal matrix min convenience assume otherwise simply replace observations centered versions matrix projection matrix maps point onto rank reconstruction orthogonal projection onto subspace spanned columns solution expressed follows stack centered observations rows matrix construct singular value decomposition udv standard decomposition numerical analysis many algo rithms exist computation golub van loan example orthogonal matrix whose columns called left singular vectors orthogonal matrix columns called right singular vectors diagonal matrix diagonal elements known sin gular values rank solution consists first columns columns called principal components see section optimal given first principal components rows matrix one dimensional principal component line illustrated fig ure data point closest point line given direction line measures distance along line origin similarly figure shows unsupervised learning first principal component second principal component figure best rank two linear approximation half sphere data right panel shows projected points coordinates given first two principal components data two dimensional principal component surface fit half sphere data left panel right panel shows projection data onto first two principal components projection basis initial configuration som method shown earlier procedure quite successful separating clusters since half sphere nonlinear nonlinear projection better job topic next section principal components many nice properties example linear combination highest variance among linear com binations features highest variance among linear combinations satisfying orthogonal example handwritten digits principal components useful tool dimension reduction com pression illustrate feature handwritten digits data described chapter figure shows sample handwritten digitized grayscale image total see considerable variation writing styles character thickness orienta tion consider images points compute principal components via svd figure shows first two principal components data first two principal components computed quantile points used define rectangular grid superimposed plot circled points indicate principal components curves surfaces figure sample handwritten shows variety writing styles images close vertices grid distance measure focuses mainly projected coordinates gives weight components orthogonal subspace right plot shows images corresponding circled points allows visualize nature first two principal components see horizontal move ment mainly accounts lengthening lower tail three vertical movement accounts character thickness terms parametrized model two component model form displayed first two principal component directions images although possible principal components approximately account variation threes count figure compares singular values obtained equivalent uncorrelated data obtained randomly scrambling column pixels digitized image inherently correlated since digit correlations even stronger unsupervised learning first principal component second principal component figure left panel first two principal components hand written threes circled points closest projected images vertices grid defined marginal quantiles principal components right panel images corresponding circled points show nature first two principal components dimension singular values real trace randomized trace figure singular values digitized threes compared randomized version data column scrambled principal components curves surfaces relatively small subset principal components serve excellent lower dimensional features representing high dimensional data example procrustes transformations shape averaging figure left panel two different digitized handwritten rep resented corresponding points green deliberately rotated translated visual effect right panel procrustes transforma tion applies translation rotation best match two set points figure represents two sets points orange green plot instance points represent two digitized versions handwritten extracted signature subject suresh figure shows entire signatures extracted third fourth panels signatures recorded dynamically using touch screen devices familiar sights modern supermarkets points representing denote matrices correspondence points ith rows meant represent positions along two language morphometrics points represent landmarks two objects one finds corresponding landmarks general difficult subject specific particular case used dynamic time warping speed signal along signature hastie details right panel applied translation rotation green points best match orange called procrustes transforma tion mardia example consider problem min procrustes african bandit greek mythology stretched squashed visitors fit iron bed eventually killing unsupervised learning matrices corresponding points thonormal matrix vector location coordinates trace squared frobenius matrix norm let column mean vectors matrices versions matrices means removed consider svd udv solution given exer cise minimal distances referred procrustes distance form solution center matrix column centroid ignore location completely hereafter assume case procrustes distance scaling solves slightly general problem min positive scalar solution trace related procrustes distance procrustes average collection shapes solves problem min find shape closest average squared procrustes distance shapes solved simple alternating algorithm initialize example solve procrustes rotation problems fixed yielding let steps repeated criterion converges figure shows simple example three shapes note expect solution rotation alternatively impose constraint upper triangular force uniqueness one easily incorporate scaling definition see exercise generally define affine invariant average set shapes via simplify matters consider orthogonal matrices include reflections well rotations group although reflections unlikely methods restricted allow rotations group principal components curves surfaces figure procrustes average three versions leading suresh signatures left panel shows preshape average shapes preshape space superimposed right three panels map pre shape separately match original min nonsingular matrices require stan dardization avoid trivial solution solution attractive computed without iteration exercise let rank projection matrix defined matrix formed largest eigenvectors 
[unsupervised, learning, principal, components, curves, surfaces, principal, curves, surfaces] principal curves generalize principal component line providing smooth one dimensional curved approximation set data points prin cipal surface general providing curved manifold approximation dimension first define principal curves random variables move finite data case let parameterized smooth curve hence vector function coordinates smooth function single parameter parameter chosen example arc length along curve fixed origin data value let define closest point curve called principal curve distribution random vector says average data points project points responsible also known self consistency property although practice continuous multivariate distributes infinitely many principal curves duchamp stuetzle unsupervised learning figure principal curve set data point curve average data points project interested mainly smooth ones principal curve illustrated figure principal points interesting related concept consider set prototypes point support distribution identify closest prototype prototype responsible induces partition feature space called voronoi regions set points minimize expected distance prototype called principal points distribution principal point self consistent equals mean voronoi region example principal point circular normal distribution mean vector pair points symmetrically placed ray mean vector principal points distributional analogs centroids found means clustering principal curves viewed principal points constrained lie smooth curve similar way som constrains means cluster centers fall smooth manifold find principal curve distribution consider coordinate functions let consider following alternating steps argmin first equation fixes enforces self consistency requirement second equation fixes curve finds closest point principal components curves surfaces figure principal surface fit half sphere data left panel fitted two dimensional surface right panel projections data points onto sur face resulting coordinates curve data point finite data principal curve algorithm starts linear principal component iterates two steps convergence scatterplot smoother used estimate conditional expectations step smoothing function arc length projection done served data points proving convergence general difficult one show linear least squares fit used scatterplot smoothing procedure converges first linear principal component equivalent power method finding largest eigenvector matrix principal surfaces exactly form principal curves higher dimension mostly commonly used two dimensional principal surface coordinate functions estimates step obtained two dimensional surface smoothers principal surfaces dimension greater two rarely used since visualization aspect less attractive smoothing high dimensions figure shows result principal surface fit half sphere data plotted data points function estimated nonlinear coordinates class separation evident principal surfaces similar self organizing maps use kernel surface smoother estimate coordinate function form batch version soms som weights weights kernel difference however unsupervised learning principal surface estimates separate prototype data point som shares smaller number prototypes data points result som principal surface agree number som prototypes grows large also conceptual difference two principal sur faces provide smooth parameterization entire manifold terms coordinate functions soms discrete produce estimated prototypes approximating data smooth parameter ization principal surfaces preserves distance locally figure reveals red cluster tighter green blue clusters simple examples estimates coordinate functions formative see exercise 
[unsupervised, learning, principal, components, curves, surfaces, spectral, clustering] traditional clustering methods like means use spherical elliptical metric group data points hence work well clus ters non convex concentric circles top left panel figure spectral clustering generalization standard clustering methods designed situations close connections local multidimensional scaling techniques section generalize mds starting point matrix pairwise similarities tween observation pairs represent observations undirected similarity graph vertices represent observations pairs vertices connected edge similarity positive exceeds threshold edges weighted clustering rephrased graph partition problem identify connected components clusters wish partition graph edges different groups low weight within group high weight idea spectral clustering construct similarity graphs represent local neighborhood relationships observations make things concrete consider set points let euclidean distance use similarity matrix radial kernel gram matrix exp scale parameter many ways define similarity matrix associated similarity graph reflect local behavior popular mutual nearest neighbor graph define symmetric set nearby pairs points specifically pair point among nearest neighbors vice versa connect symmetric nearest neighbors give edge weight otherwise edge weight zero equivalently set zero pairwise similarities draw graph modified similarity matrix principal components curves surfaces alternatively fully connected graph includes pairwise edges weights local behavior controlled scale param eter matrix edge weights similarity graph called adjacency matrix degree vertex sum weights edges connected let diagonal matrix diagonal elements finally graph laplacian defined called unnormalized graph laplacian number normalized versions proposed standardize laplacian respect node degrees example spectral clustering finds eigenvectors corresponding smallest eigenvalues ignoring trivial constant eigenvector using standard method like means cluster rows yield clustering original data points example presented figure top left panel shows simulated data points three circular clusters indicated colors means clustering would clearly difficulty identifying outer clusters applied spectral clustering using nearest neighbor similarity graph display eigenvector corresponding second third smallest eigenvalue graph laplacian lower left smallest eigen values shown top right panel two eigenvectors shown identified three clusters scatterplot rows eigenvector matrix bottom right clearly separates clusters procedure means clustering applied transformed points would eas ily identify three groups spectral clustering work vector formula suggests small value achieved pairs points large adjacencies coordinates close together since graph constant vector trivial eigenvector eigenvalue zero obvious fact graph con nected zero eigenvector exercise generalizing argument easy show graph connected components graph connected two nodes reached via path connected nodes unsupervised learning number eigenvalue eigenvectors index smallest smallest second smallest eigenvector third smallest eigenvector spectral clustering figure toy example illustrating spectral clustering data top left points falling three concentric clusters points points uniformly distributed angle radius three groups gaussian noise standard deviation added point using nearest neighbor similarity graph eigenvector corresponding second third smallest eigenvalues shown bottom left smallest eigen vector constant data points colored way top left smallest eigenvalues shown top right panel coordinates eigenvectors rows plotted bottom right panel spectral clustering standard means clustering points easily recover three original clusters principal components curves surfaces nodes reordered block diagonal block connected component eigenvectors eigenvalue zero eigenspace eigenvalue zero spanned indicator vectors connected components practice one strong weak connections zero eigenvalues approximated small eigenvalues spectral clustering interesting approach finding non convex clus ters normalized graph laplacian used another way view method defining consider random walk graph transition probability matrix spectral clustering yields groups nodes random walk seldom transitions one group another number issues one must deal applying spec tral clustering practice must choose type similarity graph fully connected nearest neighbors associated parameters number nearest neighbors scale parameter kernel must also choose number eigenvectors extract finally clustering methods number clusters toy example figure obtained good results value cor responding fully connected graph results deteriorated looking top right panel figure see strong separation smallest three eigenvalues rest hence clear many eigenvectors select 
[unsupervised, learning, principal, components, curves, surfaces, kernel, principal, components] spectral clustering related kernel principal components non linear version linear principal components standard linear principal compo nents pca obtained eigenvectors covariance matrix give directions data maximal variance kernel pca sch olkopf expand scope pca mimicking would obtain expand features non linear transformations apply pca transformed feature space show section principal components variables data matrix computed inner product gram matrix detail compute eigen decomposition double centered version gram matrix exercise shows com pute projections new observations space kernel pca simply mimics procedure interpreting kernel trix inner product matrix implicit fea tures finding eigenvectors elements mth component mth column written centering exercise unsupervised learning gain insight kernel pca viewing sam ple evaluations principal component functions reproducing kernel hilbert space generated see section first principal component function solves max var subject var refers sample variance training data norm constraint controls size roughness function dictated kernel regression case shown solution finite dimensional representation exercise shows solution defined second principal component function fined similar way additional constraint sch olkopf demonstrate use kernel principal compo nents features handwritten digit classification show improve performance classifier used instead linear principal components note use radial kernel exp kernel matrix form similarity matrix spectral clustering matrix edge weights localized version setting zero similarities pairs points nearest neighbors kernel pca finds eigenvectors corresponding largest eigenval ues equivalent finding eigenvectors corresponding smallest eigenvalues almost laplacian differences centering fact degrees nodes along diagonal figure examines performance kernel principal components toy example figure upper left panel used dial kernel value used spectral clustering separate groups upper right panel first component separates groups well lower left panel plied kernel pca using nearest neighbor radial kernel spectral clustering lower right panel use kernel matrix section benefited helpful discussions jonathan taylor principal components curves surfaces first largest eigenvector second largest eigenvector radial kernel first largest eigenvector second largest eigenvector radial kernel first largest eigenvector second largest eigenvector radial kernel second smallest eigenvector third smallest eigenvector radial kernel laplacian figure kernel principal components applied toy example fig ure using different kernels top left radial kernel top right radial kernel bottom left nearest neighbor radial ker nel spectral clustering bottom right spectral clustering laplacian constructed radial kernel unsupervised learning similarity matrix constructing laplacian spectral cluster ing neither case projections separate two groups adjusting help either toy example see kernel pca quite sensitive scale nature kernel also see nearest neighbor truncation kernel important success spectral clustering 
[unsupervised, learning, principal, components, curves, surfaces, sparse, principal, components] often interpret principal components examining direction vectors also known loadings see variables play role image loadings often interpretation made easier loadings sparse section briefly discuss methods deriving principal components sparse loadings based lasso penalties start data matrix centered columns proposed methods focus either maximum variance property prin cipal components minimum reconstruction error scotlass procedure joliffe takes first approach solving max subject absolute value constraint encourages loadings zero hence sparse sparse principal components found way forcing kth component orthogonal first components unfortunately problem convex computations difficult zou start instead regression reconstruction prop erty pca similar approach section let ith row single component sparse principal component technique solves min subject lets examine formulation detail zero easy show largest principal component direction solution necessarily unique unless solution proportional largest principal component direction second penalty encourages sparseness loadings principal components curves surfaces walking speed verbal fluency principal components sparse principal components figure standard sparse principal components study corpus callosum variation shape variations corresponding significant principal components red curves overlaid mean shape black curves multiple components sparse principal components procedures minimizes subject matrix columns also criterion jointly convex convex parameter parameter fixed minimization fixed equivalent elastic net problems section done efficiently hand minimization fixed version procrustes problem solved simple svd calculation exercise steps alternated convergence figure shows example sparse principal components analysis using taken ostrand shape mid sagittal cross section corpus callosum related various clinical parameters study involving elderly persons exam note usual principal component criterion example jointly convex parameters either nevertheless solution well defined efficient algorithm available thank rasmus larsen karl ostrand suggesting application supplying postscript figures reproduced unsupervised learning figure example mid saggital brain slice corpus col losum annotated landmarks ple pca applied shape data popular tool morphometrics applications number landmarks identified along cir cumference shape example given figure aligned procrustes analysis allow rotations case scal ing well see section features used pca sequence coordinate pairs landmark unpacked single vector analysis standard sparse principal components computed components significantly associated various clinical parameters identified figure shape variations cor responding significant principal components red curves overlaid mean shape black curves low walking speed relates ccs thinner displaying atrophy regions connecting motor control cognitive centers brain low verbal fluency relates ccs thinner regions connecting auditory visual cognitive centers sparse principal components procedure gives parsimonious tentially informative picture important differences non negative matrix factorization 
[unsupervised, learning, non-negative, matrix, factorization] non negative matrix factorization lee seung recent ternative approach principal components analysis data components assumed non negative useful modeling non negative data images data matrix approximated max assume matrices found maximizing log log likelihood model poisson dis tribution mean quite reasonable positive data following alternating algorithm lee seung converges local maximum algorithm derived minorization procedure maximizing exercise also related iterative proportional scaling algorithm log linear models exercise figure shows example taken lee seung com paring non negative matrix factorization nmf vector quantization equivalent means clustering principal components analysis pca three learning methods applied database cial images consisting pixels resulting matrix shown array montages image method learned set basis images positive values illustrated black pixels negative values red pixels par ticular instance face shown top right approximated linear superposition basis images coefficients linear superposition shown next montage array resulting perpositions shown right equality sign authors point thank sebastian seung providing image arrangements allow compact display structural significance unsupervised learning unlike pca nmf learns represent faces set basis images resembling parts faces donoho stodden point potentially serious problem non negative matrix factorization even situations holds exactly decomposition may unique figure illustrates problem data points lie dimensions open space data coordinate axes choose basis vectors anywhere open space represent data point exactly nonnegative linear combination vectors non uniqueness means solution found algorithm depends starting values would seem hamper interpretability factorization despite interpretational drawback non negative matrix factorization applications attracted lot interest 
[unsupervised, learning, non-negative, matrix, factorization, archetypal, analysis] method due cutler breiman approximates data points prototypes linear combinations data points sense similar flavor means clustering however rather approximating data point single nearby prototype archety pal analysis approximates data point convex combination collection prototypes use convex combination forces proto types lie convex hull data cloud sense prototypes pure archetypal data matrix modeled assume hence data points rows dimensional space represented convex combinations archetypes rows also assume thus archetypes convex combinations data points using minimize wbx weights function minimized alternating fashion separate minimization involving convex optimization overall problem convex however algorithm converges local minimum criterion non negative matrix factorization original figure non negative matrix factorization nmf vector quantization equivalent means clustering principal components analysis pca applied database facial images details given text unlike pca nmf learns represent faces set basis images resembling parts faces unsupervised learning figure non uniqueness non negative matrix factorization data points two dimensions choice basis vectors open space coordinate axes data gives exact reconstruction data figure shows example simulated data two dimensions top panel displays results archetypal analysis bottom panel shows results means clustering order best recon struct data convex combinations prototypes pays locate prototypes convex hull data seen top panels figure case general proven cutler breiman means clustering shown bottom panels chooses prototypes middle data cloud think means clustering special case archetypal model row single one rest entries zero notice also archetypal model general form non negative matrix factorization model however two models applied different settings somewhat different goals non negative matrix factorization aims approximate columns data matrix main output interest columns representing primary non negative components data archetypal analysis focuses instead approximation rows using rows represent archetypal data points non negative matrix factorization also assumes get exact reconstruction simply choosing data columns scaled sum contrast archetypal analysis requires allows figure example additional constraint implies archetypal approximation perfect even figure shows results archetypal analysis applied database displayed figure three rows figure resulting archetypes three runs specifying two three four independent component analysisand exploratory projection pursuit prototypes prototypes prototypes figure archetypal analysis top panels means clustering bot tom panels applied data points drawn bivariate gaussian distribu tion colored points show positions prototypes case archetypes respectively expected algorithm produced extreme size shape 
[unsupervised, learning, independent, component, analysis, exploratory, projection, pursuit] multivariate data often viewed multiple indirect measurements aris ing underlying source typically cannot directly measured examples include following educational psychological tests use answers questionnaires measure underlying intelligence mental abilities subjects eeg brain scans measure neuronal activity various parts brain indirectly via electromagnetic signals recorded sensors placed various positions head trading prices stocks change constantly time reflect various unmeasured factors market confidence external unsupervised learning figure archetypal analysis applied database digitized rows figure show resulting archetypes three runs specifying two three four archetypes respectively fluences driving forces may hard identify measure factor analysis classical technique developed statistical liter ature aims identify latent sources factor analysis models typically wed gaussian distributions extent hin dered usefulness recently independent component analysis emerged strong competitor factor analysis see relies non gaussian nature underlying sources success 
[unsupervised, learning, independent, component, analysis, exploratory, projection, pursuit, latent, variables, factor, analysis] singular value decomposition udv latent variable representation writing hence columns linear combination columns since orthogonal assuming columns hence mean zero implies columns zero mean uncorrelated unit variance terms random variables interpret svd corresponding principal component analysis pca estimate latent variable model independent component analysis exploratory projection pursuit simply correlated represented linear expansion uncorrelated unit variance variables satisfactory though given orthogonal matrix write cov cov hence many decom positions therefore impossible identify particular latent variables unique underlying sources svd decomposition property rank truncated decomposition approximates optimal way classical factor analysis model developed primarily researchers psychometrics alleviates problems extent see example mardia factor analysis model form vector underlying latent variables factors matrix factor loadings uncorrelated zero mean disturbances idea latent variables com mon sources variation amongst account correlation structure uncorrelated unique pick remaining unaccounted variation typically modeled gaussian random variables model fit maximum likelihood parameters reside covariance matrix diag var var gaussian correlated makes statistically independent random variables thus battery educational test scores would thought driven independent underlying factors intelligence drive columns referred factor loadings used name interpret factors unsupervised learning unfortunately identifiability issue remains since equivalent orthogonal leaves certain subjectivity use factor analysis since user search tated versions factors easily interpretable aspect left many analysts skeptical factor analysis may account lack popularity contemporary statistics although details svd plays key role estimation ample var assumed equal leading components svd identify subspace determined separate disturbances factor analysis seen modeling correlation structure rather covariance structure easily seen standardizing covari ance structure exercise important distinction factor analysis pca although central discussion exercise discusses simple example solutions factor analysis pca differ dramatically distinction 
[unsupervised, learning, independent, component, analysis, exploratory, projection, pursuit, independent, component, analysis] independent component analysis ica model exactly form except assumed statistically indepen dent rather uncorrelated intuitively lack correlation determines second degree cross moments covariances multivariate distribu tion general statistical independence determines cross moments extra moment conditions allow identify elements uniquely since multivariate gaussian distribution determined second moments alone exception gaussian inde pendent components determined rotation hence identifiability problems avoided assume independent non gaussian discuss full component model independent unit variance ica versions factor analysis model exist well treatment based survey article hyv arinen oja wish recover mixing matrix without loss generality assume already whitened cov typically achieved via svd described turn implies orthogonal since also covariance solving ica problem amounts finding orthogonal components vector random variable independent non gaussian figure shows power ica separating two mixed signals example classical cocktail party problem differ ent microphones pick mixtures different independent sources music speech different speakers etc ica able perform blind independent component analysis exploratory projection pursuit source signals measured signals pca solution ica solution figure illustration ica pca artificial time series data upper left panel shows two source signals measured uniformly spaced time points upper right panel shows observed mixed signals lower two panels show principal components independent component solutions source separation exploiting independence non gaussianity original sources many popular approaches ica based entropy dif ferential entropy random variable density given log well known result information theory says among random variables equal variance gaussian variables maximum tropy finally mutual information components random vector natural measure dependence quantity called kullback leibler distance density independence version marginal density covariance orthogonal easy show log det finding minimize looks orthogonal trans formation leads independence components unsupervised learning source data pca solution ica solution figure mixtures independent uniform random variables upper left panel shows realizations two independent uniform sources upper right panel mixed versions lower two panels show pca ica solutions respectively light equivalent minimizing sum entropies separate components turn amounts maximizing departures gaussianity convenience rather using entropy hyv arinen oja use negentropy measure defined gaussian random variable variance gentropy non negative measures departure gaussian ity propose simple approximations negentropy com puted optimized data ica solutions shown figures use approximation log cosh applied sample expectations replaced data averages one options fastica software provided authors classical less robust measures based fourth moments hence look departures gaussian via kurtosis see hyv arinen oja details section describe approximate newton algorithm finding optimal directions summary ica applied multivariate data looks sequence orthogonal projections projected data look far independent component analysis exploratory projection pursuit component component pca components ica components figure comparison first five ica components computed using fastica diagonal first five pca components diagonal component standardized unit variance gaussian possible pre whitened data amounts looking components independent possible ica starts essentially factor analysis solution looks rota tions lead independent components point view ica another factor rotation method along traditional varimax quartimax methods used psychometrics example handwritten digits revisit handwritten threes analyzed pca section fig ure compares first five standardized principal components first five ica components shown standardized units note plot two dimensional projection dimensional unsupervised learning mean ica ica ica ica ica figure highlighted digits figure comparing mean digits see nature ica component space pca components appear joint gaussian distri butions ica components long tailed distributions surprising since pca focuses variance ica specifically looks non gaussian distributions components standardized see decreasing variances principal components ica component highlighted two extreme digits well pair central digits displayed figure illustrates nature components example ica component five picks long sweeping tailed threes example eeg time courses ica become important tool study brain dynamics example present uses ica untangle components signals multi channel electroencephalographic eeg data onton makeig subjects wear cap embedded lattice eeg electrodes record brain activity different locations scalp figure top panel shows seconds output subset nine elec trodes subject performing standard two back learning task minute period subject presented letter roughly intervals responds pressing one two tons indicate whether letter presented different presented two steps back depending answer subject earns loses points occasionally earns bonus loses penalty points time course data show spatial correlation eeg signals signals nearby sensors look similar key assumption signals recorded scalp electrode mixture independent potentials arising different cortical reprinted progress brain research vol julie onton scott makeig information based modeling event related brain dynamics page copyright permission elsevier thank julie onton scott makeig supplying electronic version image independent component analysis exploratory projection pursuit tivities well non cortical artifact domains see reference detailed overview ica domain lower part figure shows selection ica components colored images represent estimated unmixing coefficient vectors heatmap images superimposed scalp indicating location activity corresponding time courses show activity learned ica components example subject blinked performance feedback signal colored vertical lines accounts location artifact signal artifact associated cardiac pulse account frontal theta band activities appear stretch correct performance see onton makeig detailed discussion example use ica eeg modeling 
[unsupervised, learning, independent, component, analysis, exploratory, projection, pursuit, exploratory, projection, pursuit] friedman tukey proposed exploratory projection pursuit graphical exploration technique visualizing high dimensional data view low one two dimensional projections high dimensional data look gaussian interesting structure clusters long tails would revealed non gaussian projections proposed number projection indices optimization focusing differ ent departure gaussianity since initial proposal variety improvements suggested huber friedman variety indices including entropy implemented interactive graphics package xgobi swayne called ggobi projection indices exactly form normalized linear combination components fact approximations substitutions cross entropy coin cide indices proposed projection pursuit typically projection pursuit directions constrained orthogonal friedman transforms data look gaussian chosen projection searches subsequent directions despite different origins ica exploratory projection pursuit quite similar least repre sentation described 
[unsupervised, learning, independent, component, analysis, exploratory, projection, pursuit, direct, approach, ica] independent components definition joint product density present approach estimates density directly ing generalized additive models section full details found unsupervised learning figure fifteen seconds eeg data seconds nine scalp channels top panel well nine ica components lower panel nearby electrodes record nearly identical mixtures brain non brain activity ica components temporally distinct colored scalps represent ica unmixing coefficients heatmap showing brain scalp location source independent component analysis exploratory projection pursuit hastie tibshirani method implemented package prodenica available cran spirit representing departures gaussianity represent tilted gaussian density standard gaussian density satisfies normalization conditions required density assuming pre whitened log likelihood observed data log wish maximize subject constraints orthogonal result densities without imposing restrictions model parametrized instead maximize regularized version log subtracted two penalty terms inspired silverman section first enforces density constraint solution second roughness penalty guarantees solution quartic spline knots observed values shown solution densities mean zero variance one exercise increase solutions approach standard gaussian algorithm product density ica algorithm prodenica initialize random gaussian matrix followed orthogonalization alternate convergence given optimize separately given perform one step fixed point algo rithm towards finding optimal fit functions directions optimizing alternating fashion described algorithm unsupervised learning step amounts semi parametric density estimation solved using novel application generalized additive models convenience extract one separate problems log although second integral leads smoothing spline first integral problematic requires approximation construct fine grid values increments covering observed values count number resulting bins typically pick adequate approximate log last expression seen proportional penalized poisson log likelihood response penalty parameter mean generalized additive spline model hastie tibshirani efron tibshirani offset term log fit using newton algorithm operations although quartic spline called find practice cubic spline adequate tuning parameters set practice make specify amount smoothing via effective degrees freedom software uses default value step algorithm requires optimizing respect holding fixed first terms sum involve since orthogonal collection terms involving depend exercise hence need maximize log likelihood ratio fitted density gaussian seen estimate negentropy con trast function fixed point update step modified newton step exercise independent component analysis exploratory projection pursuit update represents expectation sample since fitted quartic cubic spline first second derivatives readily available orthogonalize using symmetric square root transformation udv svd easy show leads update prodenica algorithm works well fastica artificial time series data figure mixture uniforms data figure digit data figure example simulations distribution amari distance true fastica kernelica proddenica figure left panel shows distributions used comparisons include uniform exponential mixtures exponentials symmetric asymmetric gaussian mixtures right panel shows log scale average amari metric method distribution based simulations distribution figure shows results simulation comparing prodenica fastica another semi parametric competitor kernelica bach jordan left panel shows distributions used basis comparison distribution generated pair independent components random mixing matrix condition number used implementations fastica using negentropy criterion prodenica kernelica used unsupervised learning authors matlab code since search criteria nonconvex used five random starts method algorithms delivers orthogonal mixing matrix data pre whitened available comparison generating orthogonalized mixing matrix used amari metric bach jordan measure closeness two frames max max right panel figure compares averages log scale amari metric truth estimated mixing matrices prodenica competitive fastica kernelica situations dominates mixture simulations 
[unsupervised, learning, multidimensional, scaling] self organizing maps principal curves surfaces map data points lower dimensional manifold multidimensional scaling mds similar goal approaches problem somewhat dif ferent way start observations let dis tance observations often choose euclidean distance distances may used plications may even available data points dissimilarity measure see section example wine tasting experiment might measure different sub ject judged wines subject provides measure pairs wines mds requires dissimilarities contrast som principal curves surfaces need data points multidimensional scaling seeks values minimize called stress function known least squares kruskal shephard scaling idea find lower dimensional representation data preserves pairwise distances well possible notice approximation francis bach kindly supplied code helped set simulations authors define stress square root since affect optimization leave squared make comparisons criteria simpler multidimensional scaling terms distances rather squared distances results slightly messier algebra gradient descent algorithm used minimize variation least squares scaling called sammon mapping minimizes emphasis put preserving smaller pairwise distances classical scaling instead start similarities often use centered inner product problem minimize attractive explicit solution terms eigenvectors see exercise distances rather inner products convert centered inner products distances euclidean see page chapter similarities fact centered inner products classical scaling exactly equivalent principal components inherently linear dimension reduction technique classical scaling equivalent least squares scaling loss functions different mapping nonlinear least squares classical scaling referred metric scaling meth ods sense actual dissimilarities similarities approx imated shephard kruskal nonmetric scaling effectively uses ranks nonmetric scaling seeks minimize stress function arbitrary increasing function fixed min imize gradient descent fixed method iso tonic regression used find best monotonic approximation steps iterated solutions stabilize like self organizing map principal surfaces multidimensional scaling represents high dimensional data low dimensional coordinate system principal surfaces soms step approximate original data low dimensional manifold parametrized low dimensional coordinate system principal surface som points distance matrix euclidean entries represent pairwise euclidean distances points dimensional space unsupervised learning first mds coordinate second mds coordinate figure first two coordinates half sphere data classical multi dimensional scaling close together original feature space map close together manifold points far apart feature space might also map close together less likely multidimensional scaling since explicitly tries preserve pairwise distances figure shows first two mds coordinates classical scaling half sphere example clear separation clusters tighter nature red cluster apparent 
[unsupervised, learning, nonlinear, dimension, reduction, local, multidimensional, scaling] several methods recently proposed nonlinear dimension duction similar spirit principal surfaces idea data lie close intrinsically low dimensional nonlinear manifold embedded high dimensional space methods thought flattening manifold hence reducing data set low dimensional ordinates represent relative positions manifold useful problems signal noise ratio high physical systems probably useful observational data lower signal noise ratios basic goal illustrated left panel figure data lie near parabola substantial curvature classical mds pre nonlinear dimension reduction local multidimensional scaling classical mds local mds figure orange points show data lying parabola blue points shows multidimensional scaling representations one dimension classical multidimensional scaling left panel preserve ordering points along curve judges points opposite ends curve close together contrast local multidimensional scaling right panel good job preserving ordering points along curve serve ordering points along curve judges points opposite ends curve close together right panel shows results local multi dimensional scaling one three methods non linear multi dimensional scaling discuss meth ods use coordinates points dimensions information manifold local mds done good job preserving ordering points along curve briefly describe three new approaches nonlinear dimension reduction manifold mapping isometric feature mapping isomap tenenbaum con structs graph approximate geodesic distance points along manifold specifically data point find neighbors points within small euclidean distance point construct graph edge two neighboring points geodesic distance two points approximated shortest path tween points graph finally classical scaling applied graph distances produce low dimensional mapping local linear embedding roweis saul takes different proach trying preserve local affine structure high dimensional data data point approximated linear combination neigh boring points lower dimensional representation constructed unsupervised learning best preserves local approximations details interesting give data point dimensions find nearest neigh bors euclidean distance approximate point affine mixture points neighborhood min weights satisfying contribution point reconstruction point note hope unique solution must finally find points space dimension minimize fixed step minimize small solutions trailing eigenvectors since trivial eigenvector eigenvalue discard keep next side effect hence embedding coordinates mean centered local mds chen buja takes simplest arguably direct approach define symmetric set nearby pairs points specifically pair point among nearest neighbors vice versa construct stress function large constant weight idea points neighbors considered far apart pairs given small weight dominate overall stress func tion simplify expression take let expanding gives nonlinear dimension reduction local multidimensional scaling figure images faces mapped embedding space described first two coordinates lle next circled points representative faces shown different parts space images bottom plot correspond points along top right path linked solid line illustrate one particular mode variability pose expression unsupervised learning first term tries preserve local structure data second term encourages representations pairs non neighbors farther apart local mds minimizes stress function fixed values number neighbors tuning parameter right panel figure shows result local mds using neighbors used coordinate descent multiple starting values find good minimum nonconvex stress function ordering points along curve largely preserved figure shows interesting application one meth ods lle data consist photographs digitized grayscale images result first two coordinates lle shown reveal variability pose expression similar pictures produced local mds experiments reported chen buja local mds shows perior performance compared isomap lle also demon strate usefulness local mds graph layout also close connections methods discussed spectral clustering sec tion kernel pca section 
[unsupervised, learning, google, pagerank, algorithm] section give brief description original pagerank algo rithm used google search engine interesting recent application unsupervised learning methods suppose web pages wish rank terms importance example pages might contain string match statistical learning might wish rank pages terms likely relevance websurfer pagerank algorithm considers webpage important many webpages point however linking webpages point given page treated equally algorithm also takes account importance pagerank linking pages number outgoing links linking pages higher pagerank given weight pages outgoing links given less weight ideas lead recursive definition pagerank detailed next sam roweis lawrence saul kindly provided figure google pagerank algorithm let page points page zero otherwise let equal number pages pointed page number links google pageranks defined recursive rela tionship positive constant apparently set idea importance page sum importances pages point page sums weighted page distributes total vote pages constant ensures page gets pagerank least matrix notation vector ones diag diagonal matrix diagonal elements introducing normalization average pagerank write dld matrix expression square braces exploiting connection markov chains see shown matrix real eigenvalue equal one one largest eigenvalue means find power method starting iterate fixed points desired pageranks original paper page authors considered pager ank model user behavior random web surfer clicks links random without regard content surfer random walk web choosing among available outgoing links random factor probability click link jumps instead random webpage descriptions pagerank first term def inition would better coincide random surfer terpretation page rank solution divided stationary distribution irreducible aperiodic markov chain webpages definition also corresponds irreducible aperiodic markov chain different transition probabilities version viewing pagerank markov chain makes clear matrix maximal real eigenvalue since positive entries unsupervised learning page page page page figure pagerank algorithm example small network column summing one markov chain theory tells unique eigenvector eigenvalue one corresponding stationary distribution chain bremaud small network shown illustration figure link matrix     number outlinks pagerank solution notice page incoming links hence gets minimum pagerank 
[unsupervised, learning, bibliographic, notes] many books clustering including hartigan gordon kaufman rousseeuw means clustering goes back least lloyd forgy jancey macqueen applications engineering especially image compression via vector quantization found gersho gray medoid pro cedure described kaufman rousseeuw association rules outlined agrawal self organizing map proposed kohonen kohonen kohonen give recent account principal components analysis multidimensional scal ing described standard books multivariate analysis example mardia buja implemented powerful vironment called ggvis multidimensional scaling user manual exercises contains lucid overview subject figures left panel left panel produced xgobi multidimensional data visualization package authors ggobi recent plementation cook swayne goodall gives technical overview procrustes methods statistics ramsay silverman discuss shape registration problem principal curves surfaces proposed hastie hastie stuetzle idea principal points formulated flury tarpey flury give exposition general concept self consistency excellent tutorial spectral clustering found von luxburg main source section luxborg credits donath hoffman fiedler earliest work subject history spectral clustering found spielman teng indepen dent component analysis proposed comon subsequent developments bell sejnowski treatment section based hyv arinen oja projection pursuit proposed friedman tukey discussed detail huber dynamic projection pursuit algorithm implemented ggobi 
[unsupervised, learning, exercises] weights clustering show weighted euclidean distance satisfies thus weighted euclidean distance based equivalent unweighted euclidean distance based consider mixture model density dimensional feature space unknown parameters unsupervised learning suppose data wish fit mix ture model write log likelihood data derive algorithm computing maximum likelihood timates see section show known value mixture model take sense algorithm coincides means clustering section discuss use cart prim con structing generalized association rules show problem occurs ther methods generate random data product marginal distribution randomly permuting values variables propose ways overcome problem cluster demographic data table using classification tree specifically generate reference sample size train ing set randomly permuting values within feature build classification tree training sample class reference sample class describe terminal nodes highest estimated class probability compare results prim results near table also results means clustering applied data generate data three features data points three classes follows sin cos sin sin cos sin cos sin sin cos sin cos sin sin cos indicates uniform variate range independent normal variates standard deviation hence data exercises lie near surface sphere three clusters centered write program fit som data using learning rates given text carry means clustering data compare results text write programs implement means clustering self organizing map som prototype lying two dimensional grid apply columns human tumor microarray data ing centroids demonstrate size som neighborhood taken smaller smaller som solution becomes similar means solution derive section show unique characterize family equivalent solutions derive solution procrustes problem derive also solution procrustes problem scaling write algorithm solve min apply three compare results shown fig ure derive solution affine invariant average problem apply three compare results computed exercise classical multidimensional scaling let centered ner product matrix elements let largest eigenvalues associated eigenvectors let diagonal matrix diagonal entries show solutions classical scaling problem rows consider sparse pca criterion show fixed solving amounts separate elastic net regression problems responses elements show fixed solving amounts reduced rank version procrustes problem reduces max trace subject udq svd show optimal unsupervised learning generate data points three features lying close helix detail define cos sin takes equally spaced values independent standard gaussian distributions fit principal curve data plot estimated coordinate functions compare underlying functions cos sin fit self organizing map data see discover helical shape original point cloud pre post multiply equation diagonal matrix containing inverse variances hence obtain equivalent decomposition correlation matrix sense simple scaling applied matrix generate observations three variates according independent standard normal variates compute leading principal component factor analysis directions hence show leading principal component aligns maximal variance direction leading factor essentially ignores uncorrelated component picks correlated component geoffrey hinton personal communication consider kernel principal component procedure outlined section argue number principal components equal rank number non zero elements show mth component mth column written centering show mapping new observation mth component given show solution given first column first diagonal element show second subsequent principal component functions defined similar manner hint see section consider regularized log likelihood density estimation problem arising ica exercises log solution quartic smoothing spline written quadratic function null space penalty let examining stationarity condi tions show solution density mean zero variance one used second derivative penalty instead simple modification could make problem maintain three moment conditions orthogonal show first term page log jth column depend fixed point algorithm ica hyv arinen consider maximizing respect cov use lagrange multiplier enforce norm constraint write first two derivatives modified criterion use approximation show newton update written fixed point update consider undirected graph non negative edge weights graph laplacian suppose connected components graph show eigenvectors corre sponding eigenvalue zero indicator vectors components span zero eigenspace show definition implies sum pageranks number web pages write program compute pagerank solutions power method using formulation apply network fig ure algorithm non negative matrix factorization lange function said minorize function unsupervised learning page page page page page page figure example small network domain useful maximizing since easy show nondecreasing update argmax analogous definitions majorization minimizing function resulting algorithms known algorithms minorize maximize majorize minimize lange also shown algorithm example algorithm see sec tion exercise details consider maximization function written without matrix notation log using concavity log show set values log log hence log ikj log ikj ikj indicates current iteration exercises hence show ignoring constants function ikj log log minorizes set partial derivatives zero hence derive updating steps consider non negative matrix factorization rank one case show updates reduce example iterative pro portional scaling procedure applied independence model two way contingency table fienberg example show final iterates explicit form constant equivalent usual row column estimates two way independence model fit non negative matrix factorization model collection two digits database use basis elements compare component plus mean pca model cases display matrices figure unsupervised learning 
[random, forests, introduction] bagging bootstrap aggregation section technique reducing variance estimated prediction function bagging seems work especially well high variance low bias procedures trees regression simply fit regression tree many times bootstrap sampled versions training data average result classifi cation committee trees cast vote predicted class boosting chapter initially proposed committee method well although unlike bagging committee weak learners evolves time members cast weighted vote boosting appears dominate bagging problems became preferred choice random forests breiman substantial modification bagging builds large collection correlated trees averages many problems performance random forests similar boosting simpler train tune consequence random forests popular implemented variety packages 
[random, forests, definition, random, forests] essential idea bagging section average many noisy approximately unbiased models hence reduce variance trees ideal candidates bagging since capture complex interaction random forests algorithm random forest regression classification draw bootstrap sample size training data grow random forest tree bootstrapped data cursively repeating following steps terminal node tree minimum node size min reached select variables random variables pick best variable split point among iii split node two daughter nodes output ensemble trees make prediction new point regression classification let class prediction bth random forest tree majority vote structures data grown sufficiently deep relatively low bias since trees notoriously noisy benefit greatly averag ing moreover since tree generated bagging identically distributed expectation average trees pectation one means bias bagged trees individual trees hope improvement variance reduction contrast boosting trees grown adaptive way remove bias hence average random variables variance vari ance variables simply identically distributed necessarily independent positive pairwise correlation variance average exercise increases second term disappears first remains hence size correlation pairs bagged trees limits benefits averaging idea random forests algorithm improve variance reduction bagging reducing correlation trees without increasing variance much achieved tree growing process random selection input variables specifically growing tree bootstrapped dataset split select input variables random candidates splitting definition random forests typically values even low trees grown random forest regression predictor section page characterizes bth random forest tree terms split variables cutpoints node terminal node values intuitively reducing reduce correlation pair trees ensemble hence reduce variance average spam data number trees test error bagging random forest gradient boosting node figure bagging random forest gradient boosting applied spam data boosting node trees used number trees chosen fold cross validation trees step figure corre sponds change single misclassification test set estimators improved shaking data like seems highly nonlinear estimators trees benefit bootstrapped trees typically small lower typical see figure much larger variance original tree hand bagging change linear estimates sample mean hence variance either pairwise correlation bootstrapped means exercise random forests random forests popular leo breiman collaborator adele cutler maintains random forest website software freely available downloads reported randomforest package maintained andy liaw available cran website authors make grand claims success random forests accurate interpretable like experience ran dom forests remarkably well little tuning required ran dom forest classifier achieves misclassification error spam test data compares well methods significantly worse gradient boosting bagging achieves significantly worse either using mcnemar test outlined ercise appears example additional randomization helps bagging gbm gbm nested spheres test misclassification error bayes error figure results simulations nested spheres model bayes decision boundary surface sphere additive refers random forest gbm gradient boosted model interaction order six similarly gbm training sets size test sets figure shows test error progression trees three methods case evidence gradient boosting started overfit although fold cross validation chose trees sadly leo breiman died july http www math usu edu adele forests definition random forests california housing data number trees test average absolute error gbm depth gbm depth figure random forests compared gradient boosting california housing data curves represent mean absolute error test data function number trees models two random forests shown two gradient boosted models use shrinkage parameter interaction depths boosted models outperform random forests figure shows results simulation comparing random forests gradient boosting nested spheres problem equation chapter boosting easily outperforms random forests notice smaller better although part reason could true decision boundary additive figure compares random forests boosting shrinkage regression problem using california housing data section two strong features emerge random forests stabilize trees trees boost ing continues improve boosting slowed shrinkage well fact trees much smaller boosting outperforms random forests terms weaker boosting model gbm depth smaller error stronger details random forests fit using package randomforest trees gradient boosting models fit using package gbm shrinkage parameter set trees random forests number trees misclassification error oob error test error figure oob error computed spam training data compared test error computed test set random forest wilcoxon test mean differences absolute errors value larger random forests performed better 
[random, forests, details, random, forests] glossed distinction random forests classifica tion versus regression used classification random forest obtains class vote tree classifies using majority vote see sec tion bagging similar discussion used regression predictions tree target point simply averaged addition inventors make following recommendations classification default value minimum node size one regression default value minimum node size five practice best values parameters depend problem treated tuning parameters figure performs much better default value 
[random, forests, details, random, forests, bag, samples] important feature random forests use bag oob sam ples details random forests observation construct random forest predictor averaging trees corresponding boot strap samples appear oob error estimate almost identical obtained fold cross validation see exercise hence unlike many nonlinear estimators random forests fit one sequence cross validation per formed along way oob error stabilizes training terminated figure shows oob misclassification error spam data com pared test error although trees averaged appears plot would sufficient 
[random, forests, details, random, forests, variable, importance] variable importance plots constructed random forests exactly way gradient boosted models section split tree improvement split criterion importance measure attributed splitting variable accumulated trees forest separately variable left plot figure shows variable importances computed way spam data compare corresponding figure page gradient boosting boosting ignores variables completely random forest candidate split variable selection increases chance single variable gets included random forest selection occurs boosting random forests also use oob samples construct different variable importance measure apparently measure prediction strength variable bth tree grown oob samples passed tree prediction accuracy recorded values jth variable randomly permuted oob samples accuracy computed decrease accuracy result permuting averaged trees used measure importance variable random forest expressed percent maximum right plot figure although rankings two methods similar importances right plot uni form variables randomization effectively voids effect variable much like setting coefficient zero linear model exer cise measure effect prediction variable available model refitted without variable variables could used surrogates random forests remove free capave capmax captot money george edu hpl business internet email receive mail meeting labs order address people make credit font data technology lab telnet report original project conference direct addresses parts table gini variable importance remove capave free capmax edu george captot hpl business meeting money internet receive email font mail technology order labs address original lab telnet people project data credit conference make report direct addresses parts table randomization variable importance figure variable importance plots classification random forest grown spam data left plot bases importance gini split ting index gradient boosting rankings compare well rankings produced gradient boosting figure page right plot uses oob randomization compute variable importances tends spread impor tances uniformly details random forests proximity plot random forest classifier dimension figure left proximity plot random forest classifier grown mixture data right decision boundary training data random forest mixture data six points identified plot 
[random, forests, details, random, forests, proximity, plots] one advertised outputs random forest proximity plot fig ure shows proximity plot mixture data defined section chapter growing random forest proximity matrix accumulated training data every tree pair oob obser vations sharing terminal node proximity increased one proximity matrix represented two dimensions using multidimen sional scaling section idea even though data may high dimensional involving mixed variables etc proximity plot gives indication observations effectively close together eyes random forest classifier proximity plots random forests often look similar irrespective data casts doubt utility tend star shape one arm per class pronounced better classification performance since mixture data two dimensional map points proximity plot original coordinates get better understanding represent seems points pure regions class wise map extremities star points nearer decision boundaries map nearer center surprising consider construction proximity matrices neighboring points pure regions often end sharing bucket since terminal node pure longer random forests split random forest tree growing algorithm hand pairs points close belong different classes sometimes share terminal node always 
[random, forests, details, random, forests, random, forests, overfitting] number variables large fraction relevant variables small random forests likely perform poorly small split chance small relevant variables selected figure shows results simulation supports claim tails given figure caption exercise top pair see hyper geometric probability relevant variable selected split random forest tree simulation relevant variables equal stature probability gets small gap boosting random forests increases number rele vant variables increases performance random forests surprisingly robust increase number noise variables example relevant noise variables probability relevant variable selected split assuming according figure hurt performance random forests compared boosting robustness largely due relative insensitivity misclassification cost bias variance probability estimates tree consider random forests regression next section another claim random forests cannot overfit data certainly true increasing cause random forest sequence overfit like bagging random forest estimate approximates expectation lim average realizations distribution con ditional training data however limit overfit data average fully grown trees result rich model incur unnec essary variance segal demonstrates small gains performance controlling depths individual trees grown random forests experience using full grown trees seldom costs much results one less tuning parameter figure shows modest effect depth control simple regression example classifiers less sensitive variance effect fitting seldom seen random forest classification analysis random forests test misclassification error bayes error number relevant noise variables random forest gradient boosting figure comparison random forests gradient boosting prob lems increasing numbers noise variables case true decision boundary depends two variables increasing number noise variables included random forests uses default value top pair probability one relevant variables chosen split results based simulations pair training sample test sample 
[random, forests, analysis, random, forests] section analyze mechanisms play additional randomization employed random forests discussion focus regression squared error loss since gets main points bias variance complex loss see section furthermore even case classification problem consider random forest average estimate class posterior probabilities bias variance appropriate descriptors 
[random, forests, analysis, random, forests, variance, de-correlation, effect] limiting form random forest regression estimator made explicit dependence training data consider estimation single target point see random forests minimum node size mean squared test error shallow deep figure effect tree size error random forest regres sion example true surface additive two variables plus additive unit variance gaussian noise tree depth controlled minimum node size smaller minimum node size deeper trees var sampling correlation pair trees used averaging corr randomly drawn pair random forest trees grown randomly sampled sampling variance single randomly drawn tree var easy confuse average correlation fitted trees given random forest ensemble think fitted trees vectors compute average pairwise correlation vec tors conditioned data case conditional corre lation directly relevant averaging process dependence warns distinction rather theoretical correlation pair random forest trees evaluated induced repeatedly making training sample draws population drawing pair random forest trees statistical jargon correlation induced sampling distribution precisely variability averaged calculations analysis random forests conditional due bootstrap sampling feature sampling split result sampling variability fact conditional covariance pair tree fits zero bootstrap feature sampling see exercise number randomly selected splitting variables correlation trees figure correlations pairs trees drawn random forest regression algorithm function boxplots represent correlations randomly chosen prediction points following demonstrations based simulation model iid gaussian use training sets size single set test locations size since regression trees nonlinear patterns see differ somewhat depending structure model figure shows correlation pairs trees creases decreases pairs tree predictions different training sets likely less similar use splitting variables left panel figure consider variances single tree predictors vart averaged prediction points drawn randomly simulation model total variance random forests decomposed two parts using standard conditional variance arguments see exercise var var var total variance var within variance second term within variance result randomization increases decreases first term fact sampling vari ance random forest ensemble shown right panel creases decreases variance individual trees change appreciably much range hence light vari ance ensemble dramatically lower tree variance single tree variance within total random forest ensemble mean squared error squared bias variance mean squared error squared bias variance figure simulation results left panel shows average variance single random forest tree function within refers average within sample contribution variance resulting bootstrap sampling split variable sampling total includes sampling variability horizontal line average variance single fully grown tree bootstrap sampling right panel shows average mean squared error squared bias variance ensemble function note variance axis right scale different level horizontal line average squared bias fully grown tree 
[random, forests, analysis, random, forests, bias] bagging bias random forest bias individual sampled trees analysis random forests bias also typically greater absolute terms bias pruned tree grown since randomization reduced sample space impose restrictions hence improvements prediction obtained bag ging random forests solely result variance reduction discussion bias depends unknown true function fig ure right panel shows squared bias additive model simu lation estimated realizations although different models shape rate bias curves may differ general trend decreases bias increases shown figure mean squared error see classical bias variance trade choice squared bias random forest greater single tree horizontal line patterns suggest similarity ridge regression section ridge regression useful linear models one large number variables similarly sized coefficients ridge shrinks coefficients toward zero strongly correlated variables toward although size training sample might permit variables model regularization via ridge stabilizes model lows variables say albeit diminished random forests small perform similar averaging relevant variables get turn primary split ensemble averaging reduces contribution individual variable since simulation exam ple based linear model variables ridge regression achieves lower mean squared error opt 
[random, forests, analysis, random, forests, adaptive, nearest, neighbors] random forest classifier much common nearest neigh bor classifier section fact weighted version thereof since tree grown maximal size particular sponse value one training samples tree growing algorithm finds optimal path observation choosing informative predictors disposal averaging process assigns weights training responses ultimately vote prediction hence via random forest voting mechanism observations close target point get assigned weights equivalent kernel combine form classification decision figure demonstrates similarity decision boundary nearest neighbors random forests mixture data gloss fact pure nodes split hence one observation terminal node random forests random forest classifier training error test error bayes error nearest neighbors training error test error bayes error figure random forests versus mixture data axis ori ented nature individual trees random forest lead decision regions axis oriented flavor 
[random, forests, bibliographic, notes] random forests described introduced breiman though many ideas cropped earlier literature dif ferent forms notably introduced term random forest used consensus trees grown random subspaces features idea using stochastic perturbation averaging avoid overfitting introduced kleinberg later kleinberg amit geman used randomized trees grown image features image classification problems breiman introduced bagging precursor version random forests dietterich also proposed provement bagging using additional randomization approach rank top candidate splits node select list random showed simulations real examples additional randomization improved performance bagging fried man hall showed sub sampling without replacement effective alternative bagging showed growing aver aging trees samples size approximately equivalent terms bias variance considerations bagging using smaller fractions reduces variance even decorrelation several free software implementations random forests chapter used randomforest package maintained andy liaw available cran website allows split variable lection well sub sampling adele cutler maintains random forest website http www math usu edu adele forests gust software written leo breiman adele cutler freely exercises available code name random forests exclusively censed salford systems commercial release weka machine learn ing archive http www waikato weka waikato univer sity new zealand offers free java implementation random forests 
[random, forests, exercises] derive variance formula appears fail negative diagnose problem case show number bootstrap samples gets large oob error estimate random forest approaches fold error estimate limit identity exact consider simulation model used figure mease wyner binary observations generated probabilities   predefined even number describe probability surface give bayes error rate suppose iid let two bootstrap realizations sample mean show sampling cor relation corr along way derive var variance bagged mean bag linear statistic bagging produces reduction variance linear statistics show sampling correlation pair random forest trees point given var var var term numerator var second term denominator expected conditional variance due randomization random forests fit series random forest classifiers spam data explore sensitivity parameter plot oob error well test error suitably chosen range values random forests suppose fit linear regression model observations response predictors assume variables stan dardized mean zero standard deviation one let rss mean squared residual training data estimated coefficient denote rss mean squared residual training data using values jth variable randomly permuted predictions calculated show rss rss denotes expectation respect permutation distribution argue approximately true evaluations done using independent test set 
[ensemble, learning, introduction] idea ensemble learning build prediction model combining strengths collection simpler base models already seen number examples fall category bagging section random forests chapter ensemble methods classification committee trees cast vote predicted class boosting chapter initially proposed committee method well although unlike random forests committee weak learners evolves time members cast weighted vote stacking section novel approach combining strengths number fitted models fact one could characterize dictionary method regression splines ensemble method basis functions serving role weak learners bayesian methods nonparametric regression also viewed ensemble methods large number candidate models averaged respect posterior distribution parameter settings neal zhang ensemble learning broken two tasks developing pop ulation base learners training data combining form composite predictor chapter discuss boosting tech nology goes step builds ensemble model conducting regularized supervised search high dimensional space weak learners ensemble learning early example learning ensemble method designed multi class classification using error correcting output codes dietterich bakiri ecoc consider class digit classification problem coding matrix given table table part bit error correcting coding matrix class digit classification problem column defines two class classification prob lem digit note th column coding matrix defines two class variable merges original classes two groups method works follows learn separate classifier two class problems defined columns coding matrix test point let predicted probability one th response define discriminant function kth class entry row column table row binary code representing class rows bits necessary idea redundant error correcting bits allow inaccuracies improve performance fact full code matrix minimum hamming distance pair rows note even indicator response coding section redundant since classes require log bits unique representation dietterich bakiri showed impressive improvements performance variety multiclass problems classification trees used base classifier james hastie analyzed ecoc approach showed random code assignment worked well optimally constructed error correcting codes also argued main benefit coding variance reduction bagging random forests different coded problems resulted different trees decoding step similar effect averaging hamming distance two vectors number mismatches corresponding entries boosting regularization paths 
[ensemble, learning, boosting, regularization, paths] section first edition book suggested analogy sequence models produced gradient boosting algorithm regularized model fitting high dimensional feature spaces primarily motivated observing close connection boosted version linear regression lasso section connec tions pursued others present current thinking area start original motivation fits naturally chapter ensemble learning 
[ensemble, learning, boosting, regularization, paths, penalized, regression] intuition success shrinkage strategy gradient boost ing page chapter obtained drawing analogies penalized linear regression large basis expansion consider dic tionary possible terminal node regression trees could realized training data basis functions linear model card suppose coefficients estimated least squares since number trees likely much larger even largest training data sets form regularization required let solve min    function coefficients generally penalizes larger values examples ridge regression lasso covered section discussed solution lasso problem moderate large tends sparse many small fraction possible trees enter model ensemble learning algorithm forward stagewise linear regression initialize set small constant large arg min sign output seems reasonable since likely small fraction pos sible trees relevant approximating particular target function however relevant subset different different targets coefficients set zero shrunk lasso absolute values smaller corresponding least squares values increases coefficients shrink one ultimately becoming zero owing large number basis functions directly solving lasso penalty possible however feasible forward stagewise strategy exists closely approximates effect lasso similar boosting forward stagewise algo rithm algorithm gives details although phrased terms tree basis functions algorithm used set sis functions initially coefficients zero line corresponds successive step tree selected best fits current residuals line corresponding coefficient incremented decremented infinitesimal amount coefficients left unchanged principle process could iterated either residuals zero latter case occur point coefficient values represent least squares solution corresponds applying algorithm iterations many coef ficients zero namely yet incremented oth ers tend absolute values smaller corresponding least squares solution values therefore iteration solution qualitatively resembles lasso inversely related figure shows example using prostate data studied chap ter instead using trees basis functions use origi general unique least squares value since infinitely many solutions exist fit data perfectly pick minimum norm solution amongst unique lasso solution boosting regularization paths lcavol lweight age lbph svi lcp gleason pgg lcavol lweight age lbph svi lcp gleason pgg effi effi lasso forward stagewise iteration figure profiles estimated coefficients linear regression prostate data studied chapter left panel shows results lasso different values bound parameter right panel shows results stagewise linear regression algorithm using consecutive steps size nal variables multiple linear regression model left panel displays profiles estimated coefficients lasso different values bound parameter right panel shows results stagewise algorithm left right panels figure figure left panel figure respectively similarity two graphs striking situations resemblance qualitative example basis functions mutually uncorrelated algorithm yields exactly solution lasso bound parameter likewise solutions along path course tree based regressors uncorrelated however solution sets also identical coefficients monotone functions often case correlation variables low monotone solution sets identical solution sets algorithm tend change less rapidly changing values regularization parameter lasso ensemble learning efron make connections precise characterizing exact solution paths limiting case show coeffi cient paths piece wise linear functions lasso forward stagewise facilitates efficient algorithms allow entire paths computed cost single least squares fit least angle regression algorithm described detail section hastie show infinitesimal forward stagewise algo rithm fits monotone version lasso optimally reduces step loss function given increase arc length coefficient path see sections arc length case hence proportional number steps tree boosting algorithm shrinkage closely resembles algorithm learning rate parameter corresponding squared error loss difference optimal tree selected iteration approximated standard top greedy tree induction algorithm loss functions exponential loss adaboost binomial deviance rosset show similar results see thus one view tree boosting shrinkage form monotone ill posed regression possible terminal node trees lasso penalty regularizer return topic section choice shrinkage equation analogous forward stepwise regression aggressive cousin best subset lection penalizes number non zero coefficients small fraction dominant variables best subset approaches often work well moderate fraction strong variables well known subset selection excessively greedy copas often yielding poor results compared less aggressive strategies lasso ridge regression dramatic improvements often seen shrinkage used boosting yet another confirmation approach 
[ensemble, learning, boosting, regularization, paths, bet, sparsity, principle] shown previous section boosting forward stagewise strategy shrinkage approximately minimizes loss function lasso style penalty model built slowly searching model space adding shrunken basis functions derived impor tant predictors contrast penalty computationally much easier deal shown section basis functions penalty chosen match particular positive definite kernel one solve corresponding optimization problem without explicitly searching individual basis functions however sometimes superior performance boosting proce dures support vector machine may largely due plicit use versus penalty shrinkage resulting boosting regularization paths penalty better suited sparse situations basis functions nonzero coefficients among possible choices strengthen argument simple example taken friedman suppose data points model linear combination million trees true population coefficients trees arose gaussian distribution know bayesian sense best predictor ridge regression exercise use rather penalty fitting coefficients hand small number coefficients nonzero lasso penalty work better think sparse scenario first case gaussian coefficients dense note however dense scenario although penalty best neither method well since little data estimate large number nonzero coefficients curse dimensionality taking toll sparse setting potentially well penalty since number nonzero coefficients small penalty fails words use penalty follows call bet sparsity principle high dimensional problems use procedure well sparse problems since pro cedure well dense problems comments need qualification given application degree sparseness denseness depends unknown true target function chosen dictionary notion sparse versus dense relative size train ing data set noise signal ratio nsr larger training sets allow estimate coefficients smaller standard errors likewise situations small nsr identify nonzero coefficients given sample size situations nsr larger size dictionary plays role well increasing size dictionary may lead sparser representation function search problem becomes difficult leading higher variance figure illustrates points context linear models ing simulation compare ridge regression lasso classifi cation regression problems run observations independent gaussian predictors top row coefficients nonzero generated gaussian distribution middle row nonzero generated gaussian last row non zero gaussian coefficients regression standard gaussian noise ensemble learning lasso gaussian ridge gaussian lasso subset ridge subset lasso subset ridge subset percentage squared prediction error explained noise signal ratio regression lasso gaussian ridge gaussian lasso subset ridge subset lasso subset ridge subset percentage misclassification error explained noise signal ratio classification figure simulations show superiority lasso penalty ridge regression classification run observations independent gaussian predictors top row coefficients nonzero generated gaussian distribution middle row nonzero last row nonzero gaussian errors added linear predictor regression problems binary responses generated via inverse logit transform classification problems scaling resulted noise signal ratios shown lasso used left sub columns ridge right report optimal percentage error explained test data relative error constant model displayed boxplots realizations combination situation ridge beats lasso top row neither well boosting regularization paths added linear predictor produce continuous sponse classification linear predictor transformed via inverse logit probability binary response generated five differ ent noise signal ratios presented obtained scaling prior generating response cases defined nsr var var ridge regression lasso coefficient paths fit using series values corresponding range see chapter details models evaluated large test set infinite gaussian binary case value chosen minimize test set error report percentage variance explained regression problems percentage misclassifi cation error explained classification problems relative baseline error simulation runs scenario note classification problems using squared error loss fit binary response note also using training data select rather reporting best possible behavior method different scenarios penalty performs poorly everywhere lasso performs reasonably well two situations sparse coefficients expected performance gets worse nsr increases less classification model becomes denser differences less marked classification regression empirical results supported large body theoretical results donoho johnstone donoho elad donoho candes tao support superiority estimation sparse settings 
[ensemble, learning, boosting, regularization, paths, regularization, paths, over-fitting, margins] often observed boosting overfit tutely slow overfit part explanation phenomenon made earlier random forests misclassification error less sensitive variance mean squared error classification major focus boosting community section show regulariza tion paths boosted models well behaved certain loss functions appealing limiting form figure shows coefficient paths lasso infinitesimal forward stagewise simulated regression setting data consists dictionary gaussian variables strongly correlated within blocks uncorrelated blocks generating model nonzero coefficients variables one drawn block coefficient values drawn standard gaussian finally gaussian noise added noise signal ratio exercise algorithm limiting form algorithm step size shrunk zero section grouping variables intended mimic correlations nearby trees forward stagewise ensemble learning standardized coefficients lasso standardized coefficients forward stagewise figure comparison lasso infinitesimal forward stagewise paths simulated regression data number samples number variables forward stagewise paths fluctuate less lasso final stages algorithms algorithm setup intended idealized version gradient boosting shrinkage algorithms coefficient paths computed exactly since piecewise linear see lars algorithm section coefficient profiles similar early stages paths later stages forward stagewise paths tend mono tone smoother lasso fluctuate widely due strong correlations among subsets variables lasso suffers somewhat multi collinearity problem exercise performance two models rather similar figure achieve minimum later stages forward stagewise takes longer overfit likely consequence smoother paths hastie show solves monotone version lasso problem squared error loss let augmented dictionary obtained including negative copy every basis element consider models non negative efficients expanded space lasso coefficient paths positive monotone nondecreasing monotone lasso path characterized differential equation boosting regularization paths oooo ooooooo oooooooooooooooooooooooooooo ooooo oooooo ooo ooo oooo ooo ooo ooo oooo ooo ooo ooo ooo ooo ooo ooooo ooooo oooo oooo ooo oooo oooooo ooo ooooooo oooo ooo ooo oooo ooo oooo oooo oooooo ooo ooo ooo ooo ooo ooo ooo lasso forward stagewise figure mean squared error lasso infinitesimal forward stagewise simulated data despite difference coefficient paths two models perform similarly critical part regularization path right tail lasso appears overfit rapidly initial condition arc length path exercise monotone lasso move direction velocity vector decreases loss optimal quadratic rate per unit increase arc length path since solution paths monotone lasso similarly characterized solution differential equation except move directions decrease loss optimally per unit increase norm path consequence necessarily positive hence lasso paths need monotone augmented dictionary restricting coefficients positive natural since avoids obvious ambiguity also ties naturally tree boosting always find trees positively correlated current residual suggestions boosting performs well two class classification exhibits maximal margin properties much like support vector machines chapters schapire define normalized margin fitted model min minimum taken training sample unlike margin support vector machines margin measures distance closest training point units max imum coordinate distance ensemble learning number trees margin number trees test error figure left panel shows margin adaboost clas sifier mixture data function number node trees model fit using package gbm shrinkage factor trees settled note margin crosses zero training error becomes zero right panel shows test error minimized trees case adaboost overfits dramatically run convergence schapire prove separable data adaboost creases iteration converging margin symmetric lution atsch warmuth prove asymptotic convergence adaboost shrinkage margin maximizing solution rosset consider regularized models form general loss functions show particular loss functions solution converges margin maximizing configuration particular show case exponential loss adaboost well binomial deviance collecting together results section reach following summary boosted classifiers sequence boosted classifiers form regularized mono tone path margin maximizing solution course margin maximizing end path poor overfit solution example figure early stopping amounts picking point along path done aid validation dataset 
[ensemble, learning, learning, ensembles] insights learned previous sections harnessed produce effective efficient ensemble model consider functions learning ensembles form dictionary basis functions typically trees gradient boosting random forests large quite typical final model involve many thousands trees previous section argue gradient boosting shrinkage fits regularized monotone path space trees friedman popescu propose hybrid approach breaks process two stages finite dictionary basis functions induced training data family functions built fitting lasso path dictionary arg min simplest form model could seen way post processing boosting random forests taking collection trees produced gradient boosting random forest algorithms fitting lasso path trees would typically use much reduced set would save computations storage future predictions next section describe modifications prescription reduce correlations ensemble improve performance lasso post processor initial illustration apply procedure random forest ensemble grown spam data figure shows lasso post processing offers modest improve ment random forest blue curve reduces forest trees rather original post processed performance matches gradient boosting orange curves represent modified version random forests designed reduce correlations trees even random sub sample without replacement training sample used grow tree trees restricted shallow six terminal nodes post processing offers dra matic improvements training costs reduced factor however performance post processed model falls somewhat short blue curves 
[ensemble, learning, learning, ensembles, learning, good, ensemble] ensembles perform well post processing terms basis functions want collection covers space well places ensemble learning spam data number trees test error random forest random forest gradient boost node figure application lasso post processing spam data horizontal blue line test error random forest fit spam data using trees grown maximum depth see algorithm jagged blue curve test error post processing first trees using lasso function number trees nonzero coefficients orange curve line use modified form random forest random draw data used grow tree trees forced shallow typically six terminal nodes post processing offers much greater improvement random forest generated ensemble needed sufficiently different post processor effective friedman popescu gain insights numerical quadrature importance sampling view unknown function integral indexes basis functions example basis functions trees indexes splitting variables split points values terminal nodes numerical quadrature amounts finding set evaluation points corresponding weights approximates well domain importance sampling amounts sampling random giving weight relevant regions space friedman popescu suggest measure lack relevance uses loss function learning ensembles min evaluated training data single basis function selected tree would global minimizer arg min introducing randomness selection would necessarily produce less optimal values propose natural measure characteristic width sampling scheme narrow suggests many look alike similar wide implies large spread possibly con sisting many irrelevant cases friedman popescu use sub sampling mechanism intro ducing randomness leading ensemble generation algorithm algorithm isle ensemble generation arg min arg min isle refers subsample training obser vations typically without replacement simulations suggest picking large picking reducing increases randomness hence width parameter introduces memory randomization process larger pro cedure avoids similar found number familiar randomization schemes special cases algorithm bagging samples replacement fried man hall argue sampling without replacement equivalent sampling replacement former much efficient ensemble learning random forest sampling similar randomness introduced selection splitting variable reducing algo rithm similar effect reducing random forests suffer potential biases discussed section gradient boosting shrinkage uses typically produce sufficient width stochastic gradient boosting friedman follows recipe exactly authors recommend values call combined procedure ensemble generation post processing importance sampled learning ensemble isle figure shows performance isle spam data spam data number trees test error gradient boosting node lasso post processed figure importance sampling learning ensemble isle fit spam data used trees five terminal nodes lasso post processed ensemble improve prediction error case reduces number trees factor five improve predictive performance able produce parsimonious model note practice post processing includes selection regularization parameter would learning ensembles chosen cross validation simply demonstrate effects post processing showing entire path test data figure shows various isles regression example generating number trees mean squared error gbm gbm isle isle random forest figure demonstration ensemble methods regression simulation example notation gbm refers gradient boosted model parameters report mean squared error true known function note sub sampled gbm model green outperforms full gbm model orange lasso post processed version achieves similar error random forest outperformed post processed version fall short models function last elements noise variables sponse chose resulting signal noise ratio approximately used training sample size estimated mean squared error averaging test set samples sub sampled gbm curve light blue instance stochastic gradient boosting friedman discussed section outperforms gradient boosting example ensemble learning 
[ensemble, learning, learning, ensembles, rule, ensembles] describe modification tree ensemble method focuses individual rules friedman popescu encountered rules section discussion prim method idea enlarge ensemble trees constructing set rules trees collection figure typical tree ensemble rules derived figure depicts small tree numbered nodes following rules derived tree linear expansion rules equivalent tree exercise hence complete basis tree tree ensemble construct mini ensemble rules rule combine form larger ensemble rule rule treated like ensemble post processed via lasso similar regularized procedure several advantages approach deriving rules complex trees space models enlarged lead improved perfor mance learning ensembles rules rules linear mean squared error figure mean squared error rule ensembles using realizations simulation example rules easier interpret trees potential simplified model often natural augment rule including variable separately well thus allowing ensemble model linear func tions well friedman popescu demonstrate power procedure number illustrative examples including simulation example figure shows boxplots mean squared error true model twenty realizations model models fit using rulefit software available esl homepage runs auto matic mode training set used figure rule based model achieved mean squared error although slightly worse best achieved figure results comparable cross validation used select final model 
[ensemble, learning, bibliographic, notes] noted introduction many new methods machine learning dubbed ensemble methods include neural networks boosting bagging random forests dietterich gives survey tree based ensemble methods neural networks chapter perhaps deserving name since simultaneously learn parameters esl homepage www stat stanford edu elemstatlearn ensemble learning hidden units basis functions along combine bishop discusses neural networks detail along bayesian perspective mackay neal support vector machines chapter also regarded ensemble method perform regularized model fitting high dimensional feature spaces boosting lasso exploit sparsity regularization overcome high dimensionality svms rely kernel trick characteristic regularization quinlan commercial tree rule generation package goals common rulefit vast varied literature often referred combining clas sifiers abounds hoc schemes mixing methods different types achieve better performance principled approach see kittler 
[ensemble, learning, exercises] describe exactly generate block correlated data used simulation section let piecewise differentiable continuous coef ficient profile arc length time defined show equality iff monotone show fitting linear regression model using rules equation gives fit regression tree corresponding tree show true classification logistic regression model fit program run simulation study described figure 
[undirected, graphical, models, introduction] graph consists set vertices nodes along set edges join ing pairs vertices graphical models vertex represents random variable graph gives visual way understanding joint distribution entire set random variables use ful either unsupervised supervised learning undirected graph edges directional arrows restrict discussion undi rected graphical models also known markov random fields markov networks graphs absence edge two vertices special meaning corresponding random variables conditionally independent given variables figure shows example graphical model flow cytometry dataset proteins measured cells sachs vertex graph corresponds real valued pression level protein network structure estimated assuming multivariate gaussian distribution using graphical lasso procedure discussed later chapter sparse graphs relatively small number edges convenient interpretation useful variety domains including nomics proteomics provide rough models cell pathways much work done defining understanding structure graphical models see bibliographic notes references undirected graphical models raf mek plcg pip pip erk akt pka pkc jnk figure example sparse undirected graph estimated flow cytometry dataset proteins measured cells net work structure estimated using graphical lasso procedure discussed chapter see edges graph parametrized values tentials encode strength conditional dependence random variables corresponding vertices main challenges working graphical models model selection choosing structure graph estimation edge parameters data compu tation marginal vertex probabilities expectations joint distribution last two tasks sometimes called learning inference computer science literature attempt comprehensive treatment interesting area instead introduce basic concepts discuss sim ple methods estimation parameters structure undirected graphical models methods relate techniques already discussed book estimation approaches present continuous discrete valued vertices different treat separately sec tions may particular interest describe new regression based procedures estimating graphical models large active literature directed graphical models bayesian networks graphical models edges directional arrows directed cycles directed graphical models rep resent probability distributions factored products condi tional distributions potential causal interpretations refer reader wasserman brief overview undi rected directed graphs next section follows closely chapter markov graphs properties figure examples undirected graphical models markov networks node vertex represents random variable lack edge two nodes indicates conditional independence example graph conditionally independent given graph independent longer list useful references given bibliographic notes page 
[undirected, graphical, models, markov, graphs, properties] section discuss basic properties graphs models joint distribution set random variables defer discussion parametrization estimation edge parameters data estimation topology graph later sections figure shows four examples undirected graphs graph consists pair set vertices set edges defined pairs vertices two vertices called adjacent edge joining denoted path set vertices joined complete graph graph every pair vertices joined edge subgraph subset vertices together edges example figure form path complete graph suppose graph whose vertex set represents set random variables joint distribution markov graph absence edge implies corresponding random variables conditionally independent given variables vertices expressed following notation undirected graphical models edge joining rest rest refers vertices graph example figure known pairwise markov independencies subgraphs said separate every path intersects node example separates figures separates figure connected say two sets separated empty set figure separates separators nice property break graph con ditionally independent pieces specifically markov graph sub graphs separates known global markov properties turns pairwise global markov properties graph equivalent graphs positive distributions set graphs associated prob ability distributions satisfy pairwise markov independencies global markov assumptions result useful inferring global independence relations simple pairwise properties example figure since markov graph link joining also separates hence global markov assumption conclude similarly global markov property allows decompose graphs smaller manageable pieces thus leads essential simplifications com putation interpretation purpose separate graph cliques clique complete subgraph set vertices adjacent one another called maximal clique vertices added still yield clique maximal cliques graphs figure although following applies continuous discrete distri butions much development latter probability density function markov graph represented markov graphs properties set maximal cliques positive functions called clique potentials general density functions rather affinities capture dependence scoring certain instances higher others quantity normalizing constant also known partition function alterna tively representation implies graph independence prop erties defined cliques product result holds markov networks positive distributions known hammersley clifford theorem hammersley clifford clifford many methods estimation computation graphs first compose graph maximal cliques relevant quantities com puted individual cliques accumulated across entire graph prominent example join tree junction tree algorithm computing marginal low order probabilities joint distribution graph details found pearl lauritzen spiegel halter pearl shenoy shafer jensen koller friedman figure complete graph uniquely specify higher order dependence structure joint distribution variables graphical model always uniquely specify higher order dependence structure joint probability distribution consider com plete three node graph figure could represent dependence structure either following distributions first specifies second order dependence represented fewer parameters graphical models discrete data special cliques separated potentials densities general case undirected graphical models case loglinear models multiway contingency tables bishop language referred second order interaction model remainder chapter focus pairwise markov graphs koller friedman potential function edge pair variables second order interac tions represented parsimonious terms parameters easier work give minimal complexity implied graph structure models continuous discrete data functions pairwise marginal distributions variables represented edge set 
[undirected, graphical, models, undirected, graphical, models, continuous, variables] consider markov networks variables continuous gaussian distribution almost always used graphical models convenient analytical properties assume observa tions multivariate gaussian distribution mean covariance matrix since gaussian distribution represents second order relationships automatically encodes pairwise markov graph graph figure example gaussian graphical model gaussian distribution property conditional distri butions also gaussian inverse covariance matrix contains information partial covariances variables covariances pairs conditioned variables particular ijth component zero variables conditionally independent given variables exercise instructive examine conditional distribution one variable versus rest role explicit suppose partition consists first variables last conditional distribution give mardia partitioned conditional mean exactly form pop ulation multiple linear regression regression coefficient see page partition way since standard formulas partitioned inverses give undirected graphical models continuous variables hence learned two things dependence mean term alone see explicitly zero elements hence mean corresponding elements conditionally independent given rest learn dependence structure multiple linear regression thus captures second order information structural quantitative needed describe conditional distribution node given rest called natural parameter gaussian graphical model another different kind graphical model covariance graph rel evance network vertices connected bidirectional edges covariance rather partial covariance corresponding variables nonzero popular genomics see especially butte negative log likelihood models convex making computations challenging chaudhuri 
[undirected, graphical, models, undirected, graphical, models, continuous, variables, estimation, parameters, graph, structure, known] given realizations would like estimate parameters undirected graph approximates joint distribution suppose first graph complete fully connected assume multivariate normal realizations population mean covariance let empirical covariance matrix sample mean vector ignoring constants log likelihood data written distribution arising gaussian graphical model wishart distribution member exponential family canonical natural parameter indeed partially maximized log likelihood constants wishart log likelihood undirected graphical models log det trace partially maximized respect mean parameter quantity convex function easy show maximum likelihood estimate simply make graph useful especially high dimensional set tings let assume edges missing example edge pip erk one several missing figure seen gaussian distribution implies correspond ing entries zero hence would like maximize constraints pre defined subset parame ters zero equality constrained convex optimization problem number methods proposed solving particular iterative proportional fitting procedure speed kiiveri methods summarized example whittaker lauritzen methods exploit simplifications arise decomposing graph maximal cliques described previ ous section outline simple alternate approach exploits sparsity different way fruits approach become apparent later discuss problem estimation graph structure idea based linear regression inspired particular suppose want estimate edge parameters vertices joined given vertex restricting joined zero would seem linear regression node values relevant vertices might provide reasonable estimate ignores dependence structure among predictors regression turns instead use current model based estimate cross product matrix predictors perform regressions gives correct solutions solves constrained maximum likelihood problem exactly give details constrain log likelihood add lagrange constants missing edges log det trace gradient equation maximizing written using fact derivative log det equals boyd van denberghe example page matrix lagrange param eters nonzero values pairs edges absent show use regression solve inverse one row column time simplicity let focus last row column upper right block equation written undirected graphical models continuous variables partitioned matrices two parts part first rows columns part pth row column inverse partitioned similar fashion implies substituting gives interpreted estimating equations con strained regression predictors except observed mean cross products matrix replaced current estimated covariance matrix model solve simple subset regression suppose nonzero elements edges constrained zero rows carry information removed furthermore reduce removing zero elements yielding reduced system equations solution padded zeros give although appears recover elements scale factor easy show using partitioned inverse formulas also since diagonal zero leads simple iterative procedure given algorithm estimating inverse subject constraints missing edges note algorithm makes conceptual sense graph estimation problem separate regression problems rather coupled prob lems use common step place observed cross products matrix couples problems together appropriate fashion surprisingly able find procedure lit erature however related covariance selection procedures undirected graphical models algorithm modified regression algorithm estimation undirected gaussian graphical model known structure initialize repeat convergence partition matrix part jth row column part jth row column solve unconstrained edge parameters using reduced system equations obtain padding zeros appropriate positions update final cycle solve     figure simple graph illustration along empirical covari ance matrix dempster similar flavor iterative conditional fitting procedure covariance graphs proposed chaudhuri little example borrowed whittaker suppose model depicted figure along empirical covariance matrix apply algorithm problem example modified regression variable step variable left procedure quickly converged solutions       note zeroes corresponding missing edges note also corresponding elements elements dif ferent estimation example sometimes called positive definite completion undirected graphical models continuous variables 
[undirected, graphical, models, undirected, graphical, models, continuous, variables, estimation, graph, structure] cases know edges omit graph would like try discover data recent years number authors proposed use lasso regularization purpose meinshausen uhlmann take simple approach prob lem rather trying fully estimate estimate components nonzero fit lasso regression using variable response others predictors com ponent estimated nonzero either estimated coefficient variable nonzero estimated coefficient variable nonzero alternatively use rule show asymp totically procedure consistently estimates set nonzero elements take systematic approach lasso penalty following development previous section consider maximizing penalized log likelihood log det trace norm sum absolute values elements ignored constants negative penalized likelihood convex function turns one adapt lasso give exact maximizer penalized log likelihood particular simply replace modified regression step algorithm modified lasso step details analog gradient equation sign use sub gradient notation sign sign else sign continuing development previous section reach analog sign recall opposite signs see system exactly equivalent estimating equations lasso regression consider usual regression setup outcome variables pre dictor matrix lasso minimizes see page added factor convenience gradient expression undirected graphical models algorithm graphical lasso initialize diagonal remains unchanged follows repeat convergence partition matrix part jth row column part jth row column solve estimating equations sign using cyclical coordinate descent algorithm modified lasso update final cycle solve sign factor analog replace estimated cross product matrix current model resulting procedure called graphical lasso proposed fried man building work banerjee summarized algorithm friedman use pathwise coordinate descent method section solve modified lasso problem stage details pathwise coordinate descent graphical lasso algorithm letting update form soft threshold operator sign procedure cycles predictors convergence easy show diagonal elements solution matrix simply fixed step algorithm graphical lasso algorithm extremely fast solve moder ately sparse problem nodes less minute easy modify algorithm edge specific penalty parameters since alternative formulation problem posed penalize diagonal diagonal elements solution matrix rest algorithm unchanged undirected graphical models continuous variables force zero algorithm subsumes algorithm casting sparse inverse covariance problem series regressions one also quickly compute examine solution paths function penalty parameter details found friedman raf mek plcg pip pip erk akt pka pkc jnk raf mek plcg pip pip erk akt pka pkc jnk raf mek plcg pip pip erk akt pka pkc jnk raf mek plcg pip pip erk akt pka pkc jnk figure four different graphical lasso solutions flow cytometry data figure shows result applying graphical lasso flow cytometry dataset lasso penalty parameter set practice informative examine different sets graphs obtained varied figure shows four different solutions graph becomes sparse penalty parameter increased finally note values nodes graphical model unobserved missing hidden values missing node algorithm used impute missing values undirected graphical models exercise however sometimes entire node hidden latent gaussian model node missing values due linearity one simply average missing nodes yield another gaussian model observed nodes hence inclusion hidden nodes enrich resulting model observed nodes fact imposes additional structure covariance matrix however discrete model described next inherent nonlinearities make hidden units powerful way expanding model 
[undirected, graphical, models, undirected, graphical, models, discrete, variables] undirected markov networks discrete variables popular particular pairwise markov networks binary variables common sometimes called ising models statistical mechanics literature boltzmann machines machine learning lit erature vertices referred nodes units binary valued addition values node observed visible observed hidden nodes often organized layers similar neural network boltzmann machines useful unsupervised supervised learning especially structured input data images hampered computational difficulties figure shows restricted boltzmann machine discussed later variables hidden pairs nodes connected first consider simpler case nodes visible edge pairs enu merated denoting binary valued variable node ising model joint probabilities given exp gaussian model previous section pairwise interactions modeled ising model developed statistical mechanics used generally model joint effects pairwise interactions log partition function defined log exp partition function ensures probabilities add one sample space terms represent particular parametrization undirected graphical models discrete variables log potential functions technical reasons requires constant node included exercise edges nodes statistics literature model equivalent first order interaction poisson log linear model multiway tables counts bishop mccullagh nelder agresti ising model implies logistic form node conditional others exercise exp denotes nodes except hence parameter measures dependence conditional nodes 
[undirected, graphical, models, undirected, graphical, models, discrete, variables, estimation, parameters, graph, structure, known] given data model estimate parameters suppose observations log likelihood log   gradient log likelihood setting gradient zero gives defined undirected graphical models expectation taken respect empirical distribution data looking see maximum likelihood estimates simply match estimated inner products nodes observed inner products standard form score gradient equation exponential family models sufficient statistics set equal expectations model find maximum likelihood estimates use gradient search newton methods however computation involves enu meration possible values generally feasible large larger smaller number standard statistical approaches available poisson log linear modeling treat problem large regres sion problem exercise response vector vector counts cells multiway tabulation data predictor matrix rows columns characterize cells although number depends sparsity graph computational cost essentially regression problem size manageable newton updates typically computed iteratively reweighted least squares number steps usually single digits see agresti mccullagh nelder details standard software package glm used fit model gradient descent requires computations compute gradient may require many gradient steps second order newton methods nevertheless handle slightly larger problems computations reduced exploiting special clique structure sparse graphs using junction tree algorithm details given iterative proportional fitting ipf performs cyclical coordinate descent gradient equations step parameter updated gradient equation exactly zero done cyclical fashion gradients zero one complete cycle costs gradient evaluation may efficient jirou sek preucil implement efficient version ipf using junction trees cell counts treated independent poisson variable get multinomial model corresponding conditioning total count also poisson framework undirected graphical models discrete variables large approaches used approximate gradient mean field approximation peterson anderson esti mates replaces input vari ables means leading set nonlinear equations parameters obtain near exact solutions gibbs sampling section used approximate successively sampling esti mated model probabilities see ripley discussed decomposable models maximum likelihood estimates found closed form without iteration whatsoever models arise example trees special graphs tree structured topology computational tractability concern trees represent useful class models sidestep computational concerns raised section details see example chapter whittaker 
[undirected, graphical, models, undirected, graphical, models, discrete, variables, hidden, nodes] increase complexity discrete markov network including latent hidden nodes suppose subset variables unobserved hidden remainder observed visible log likelihood observed data log log exp sum means summing possible values hidden units gradient works first term empirical average visible one hidden first imputed given visible data averaged hidden variables second term unconditional expectation inner expectation first term evaluated using basic rules conditional expectation properties bernoulli random variables detail observation undirected graphical models two separate runs gibbs sampling required first estimate sampling model second esti mate latter run visible units fixed clamped observed values hidden variables sampled gibbs sampling must done observation training set stage gradient search result procedure slow even moderate sized models section consider model restrictions make computations manageable 
[undirected, graphical, models, undirected, graphical, models, discrete, variables, estimation, graph, structure] use lasso penalty binary pairwise markov networks suggested lee wainwright first thors investigate conjugate gradient procedure exact maximization penalized log likelihood bottleneck computation gradient exact computation via junction tree algorithm man ageable sparse graphs becomes unwieldy dense graphs second authors propose approximate solution analogous meinshausen uhlmann approach gaussian graphical model fit penalized logistic regression model node function nodes symmetrize edge parameter estimates fashion example estimate edge parameter logistic model outcome node min symmetrization sets either whichever smallest abso lute value max criterion defined similarly show certain conditions either approximation estimates nonzero edges cor rectly sample size goes infinity hoefling tibshirani extend graphical lasso discrete markov networks obtaining pro cedure somewhat faster conjugate gradients still must deal computation also compare exact approximate solutions extensive simulation study find min max approximations slightly less accurate exact pro cedure estimating nonzero edges estimating actual values edge parameters much faster furthermore handle denser graphs never need compute quantities finally point key difference gaussian binary models gaussian case inverse often interest graphical lasso procedure delivers estimates quan tities however approximation meinshausen uhlmann gaussian graphical models analogous wainwright undirected graphical models discrete variables visible visible hidden figure restricted boltzmann machine rbm connections nodes layer visible units subdivided allow rbm model joint density feature labels approximation binary case yields estimate con trast markov model binary data object interest inverse interest approximate method wainwright estimates efficiently hence attractive solution binary problem 
[undirected, graphical, models, undirected, graphical, models, discrete, variables, restricted, boltzmann, machines] section consider particular architecture graphical models inspired neural networks units organized layers restricted boltzmann machine rbm consists one layer visible units one layer hidden units connections within layer much simpler compute conditional expectations connections hidden units removed figure shows example visible layer divided input variables output variables hidden layer denote network example could binary pixels image handwritten digit could units one observed class labels restricted form model simplifies gibbs sampling timating expectations since variables layer independent one another given variables layers hence sampled together using conditional probabilities given expression resulting model less general boltzmann machine still useful example learn extract interesting features images thank geoffrey hinton assistance preparation material rbms undirected graphical models alternately sampling variables layer rbm shown figure possible generate samples joint density model part visible layer clamped particular feature vector alternating sampling possible sample distribution labels given alternatively classification test items also achieved comparing unnormalized joint densities label category observed features need compute partition function combinations noted restricted boltzmann machine generic form single hidden layer neural network section edges latter model directed hidden units usually real valued fitting criterion different neural network minimizes error cross entropy targets model predictions conditional input features contrast restricted boltzmann machine maxi mizes log likelihood joint distribution visible units features targets extract information input fea tures useful predicting labels unlike supervised learning methods may also use hidden units model structure feature vectors immediately relevant predicting labels features may turn useful however combined features derived hidden layers unfortunately gibbs sampling restricted boltzmann machine slow take long time reach stationarity net work weights get larger chain mixes slowly need run steps get unconditional estimates hinton noticed pirically learning still works well estimate second expectation starting markov chain data running steps instead convergence calls contrastive divergence sample given given finally given idea parameters far solution may wasteful iterate gibbs sampler stationarity single iteration reveal good direction moving estimates give example illustrate use rbm using con trastive divergence possible train rbm recognize hand written digits mnist dataset lecun hidden units visible units representing binary pixel intensities one way multinomial visible unit representing labels rbm achieves error rate test set little higher achieved support vector machine comparable error rate achieved neural network trained backpropagation error rate rbm however reduced replacing pixel intensities features produced images without using label information first rbm visible units hidden units trained using contrastive divergence model set images hidden states first rbm used data training exercises figure example restricted boltzmann machine handwritten digit classification network depicted schematic left displayed right difficult test images model classifies correctly second rbm visible units hidden units finally hidden states second rbm used features training rbm hidden units joint density model details justification learning features greedy layer layer way scribed hinton figure gives representation composite model learned way also shows examples types distortion cope 
[undirected, graphical, models, bibliographic, notes] much work done defining understanding structure graphical models comprehensive treatments graphical models found whittaker lauritzen cox wermuth edwards pearl anderson jordan koller friedman wasserman gives brief introduction chapter bishop gives detailed overview boltzmann machines proposed ackley ripley detailed chapter topics graphical models relate machine learning found particularly useful discussion boltzmann machines 
[undirected, graphical, models, exercises] markov graph figure list implied condi tional independence relations find maximal cliques undirected graphical models figure consider random variables following cases draw graph given independence relations let covariance matrix set variables consider partial covariance matrix two subsets variables consisting first two rest covariance matrix two variables linear adjustment rest gaussian distribution covariance matrix conditional distribution partial correlation coefficient rest pair conditional rest simply computed partial covariance define show show diagonal element zero partial correlation coefficient corresponding variables zero show treat covariance matrix compute corresponding correlation matrix diag diag rest denote conditional density given show exercises consider setup section missing edges show estimating equations multiple regression coefficients last variable rest recovery algorithm use expression derive standard partitioned inverse expressions since show thus simply rescaling write program implement modified regression procedure fitting gaussian graphical model pre specified edges missing test flow cytometry data book website using graph figure write program fit lasso using coordinate descent procedure compare results lars program convex optimizer check working correctly using program write code implement graphical lasso algorithm apply flow cytometry data book website vary regularization parameter examine resulting networks suppose gaussian graphical model data vertices missing consider algorithm dataset multivariate servations mean covariance matrix sample let index predictors observed missing respectively show step observations imputed current estimates step estimated empirical mean modified covariance imputed data undirected graphical models zero otherwise explain reason correction term little rubin implement algorithm gaussian graphical model using modified regression procedure exercise step flow cytometry data book website set data last protein jnk first observations missing fit model figure compare predicted values actual values jnk compare results obtained regression jnk vertices edges jnk figure using non missing data using simple binary graphical model two variables show essential include constant node model show ising model joint probabili ties discrete graphical model implies conditional distributions logistic form consider poisson regression problem binary variables response variable measures number observations predictor design balanced possible combinations measured assume log linear model poisson mean cell log using notation section including constant variable assume response distributed write conditional log likelihood observed responses compute gradient show gradient equation computes partition func tion show gradient equations remainder parameters equivalent gradient 
[much, bigger] chapter discuss prediction problems number features much larger number observations often written problems become increasing importance especially genomics areas computational biology see high variance overfitting major concern setting result simple highly regularized approaches often become methods choice first part chapter focuses prediction classification regression settings second part discusses basic problem feature selection assessment get started figure summarizes small simulation study demonstrates less fitting better principle applies samples generated standard gaussian features pairwise correlation outcome generated according linear model generated standard gaussian distribution dataset set coefficients also generated standard gaus sian distribution investigated three cases standard deviation chosen case signal noise ratio var equaled result number significant uni high dimensional problems relative error features test error features features effective degrees freedom figure test error results simulation experiments shown box plots relative test errors simulations three different values number features relative error test error divided bayes error left right results shown ridge regression three different values regularization parameter average effective degrees freedom fit indicated plot variate regression coefficients respectively averaged simulation runs case designed mimic kind data might see high dimensional genomic proteomic dataset example fit ridge regression data three different values regularization parameter nearly least squares regression little regularization ensure problem non singular figure shows boxplots relative test error achieved different estimators scenario corresponding average degrees freedom used ridge regression fit indicated computed using formula page degrees freedom interpretable parameter see ridge regression wins wins wins explanation results fit way identify many significant coefficients possible call regression coefficient significant estimated univariate coefficient estimated standard error fixed value regularization parameter degrees freedom depends observed predictor values simulation hence compute average degrees freedom simulations nearest shrunken centroids low bias identify non zero coefficients using moderate shrinkage finally even though many nonzero coefficients hope finding need shrink way evidence let ridge regression estimate estimated standard error using optimal ridge parameter three cases median value average number values exceeding equal ridge regression successfully exploits correlation features cannot latter case enough information relatively small number samples efficiently estimate high dimensional covariance matrix case regularization leads superior prediction performance thus surprising analysis high dimensional data quires either modification procedures designed scenario entirely new procedures chapter discuss examples kinds approaches high dimensional classification regression meth ods tend regularize quite heavily using scientific contextual knowledge suggest appropriate form regularization chapter ends discussion feature selection multiple testing 
[diagonal, linear, discriminant, analysis, nearest, shrunken, centroids] gene expression arrays important new technology biology discussed chapters data next example form matrix genes columns samples rows set microarray experiments expression value log ratio log amount gene specific rna target sample hybridizes particular gene specific spot microarray corre sponding amount rna reference sample samples arose small round blue cell tumors srbct found children classified four major types burkitt lymphoma ews ewing sarcoma neuroblastoma rms rhabdomyosarcoma addi tional test data set observations scientific background since cannot fit full linear discriminant analysis lda data sort regularization needed method describe similar methods section important modifi cations achieve feature selection simplest form regularization assumes features independent within class within class covariance matrix diagonal despite fact features rarely independent within class high dimensional problems enough data estimate dependencies assumption indepen dence greatly reduces number parameters model often results effective interpretable classifier thus consider diagonal covariance lda rule classifying classes discriminant score see page class log vector expression values test servation pooled within class standard deviation jth gene mean values gene class index set class call centroid class first part simply negative standardized squared distance kth centroid second part correction based class prior probability classification rule max see diagonal lda classifier equivalent nearest centroid classifier appropriate standardization also special case naive bayes classifier described section assumes features class independent gaussian distributions variance diagonal lda classifier often effective high dimensional set tings also called independence rule bickel levina demonstrate theoretically often outperform standard lin ear discriminant analysis high dimensional problems diagonal lda classifier yielded five misclassification errors test samples one drawback diagonal lda classifier uses fea tures genes hence convenient interpretation regularization better terms test error inter pretability would like regularize way automatically drops fea tures contributing class predictions shrinking classwise mean toward overall mean feature separately result regularized version nearest centroid clas sifier equivalently regularized version diagonal covariance form lda call procedure nearest shrunken centroids nsc shrinkage procedure defined follows let overall mean gene small positive constant typically chosen median values nearest shrunken centroids figure soft thresholding function sign shown orange along line red constant guards large values arise expression values near zero constant within class variance variance contrast numerator hence form standardization denominator shrink toward zero using soft thresholding sign see figure parameter determined used fold cross validation example see top panel figure reduced amount absolute value set zero absolute value less zero soft thresholding function shown figure thresholding applied wavelet coefficients section alternative use hard thresholding prefer soft thresholding smoother operation typically works better shrunken versions obtained reversing transformation use shrunken centroids place original discriminant score estimator also viewed lasso style estimator class means exercise notice genes nonzero least one classes play role classification rule hence vast majority genes often discarded example genes discarded leaving small interpretable set genes characterize class figure represents genes heatmap figure top panel demonstrates effectiveness shrinkage shrinkage make errors test data several errors high dimensional problems training data shrunken centroids achieve zero test rors fairly broad band values bottom panel figure shows four centroids srbct data gray relative overall centroid blue bars shrunken versions centroids obtained soft thresholding gray bars using discriminant scores used construct class probability estimates used rate classifications decide classify particular sample note forms feature selection used setting including hard thresholding fan fan show theoretically importance carrying kind feature selection diagonal linear discriminant analysis high dimensional problems 
[linear, classifiers, quadratic, regularization] ramaswamy present difficult microarray classification problem involving training set patients different types cancer test set patients gene expression measurements available genes table shows prediction results eight different classification methods data patient first standardized mean variance seems improve prediction accuracy overall example suggesting shape gene expression profile important rather absolute expression levels case ews rms figure heat map chosen genes within horizontal partitions ordered genes hierarchical clustering similarly samples within vertical partition yellow represents blue expression linear classifiers quadratic regularization misclassification error number genes training fold test amount shrinkage ews rms gene centroids average expression centered overall centroid figure top error curves srbct data shown train ing fold cross validation test misclassification errors threshold parameter varied value chosen resulting sub set selected genes bottom four centroids profiles srbct data gray relative overall centroid centroid components see considerable noise blue bars shrunken versions centroids obtained soft thresholding gray bars using high dimensional problems table prediction results microarray data cancer classes method described section methods discussed sec tion discussed section method described section elastic net penalized multinomial best test data standard error test error estimate comparisons inconclusive methods errors test errors number genes used nearest shrunken centroids penalized discriminant analysis support vector classifier lasso regression one nearest neighbors penalized multinomial penalized multinomial elastic net penalized multinomial regularization parameter chosen minimize cross validation error test error value parameter shown one value regularization parameter yields minimal cross validation error average test error values reported rda regularized discriminant analysis regularized multinomial logistic regression support vector machine complex methods try exploit multivariate information data describe turn well variety regularization methods including 
[linear, classifiers, quadratic, regularization, regularized, discriminant, analysis] regularized discriminant analysis rda described section lin ear discriminant analysis involves inversion within covariance matrix matrix huge rank hence singular rda overcomes singularity issues regulariz ing within covariance estimate use version rda shrinks towards diagonal diag note corresponds diagonal lda shrinkage version nearest shrunken centroids form shrinkage linear classifiers quadratic regularization much like ridge regression section shrinks total covariance matrix features towards diagonal scalar matrix fact viewing linear discriminant analysis linear regression optimal scoring categorical response see section equivalence becomes precise computational burden inverting large matrix overcome using methods discussed section value chosen cross validation line table values gave test error development rda including shrinkage centroids addition covariance matrix found guo 
[linear, classifiers, quadratic, regularization, logistic, regression, quadratic, regularization] logistic regression section modified similar way deal case classes use symmetric version multiclass logistic model page exp exp coefficient vectors log odds parameters regularize fitting maximizing penalized log likelihood max log regularization automatically resolves redundancy paramet rization forces exercise note constant terms regularized one set zero resulting optimization problem convex solved newton algorithm numerical techniques details given zhu hastie friedman provide software computing regularization path two multiclass logistic regression mod els table line reports results multiclass logistic regres sion model referred multinomial shown rosset separable data regularized two class logistic regression estimate renormalized converges maximal margin classifier section gives attractive alternative support vector machine discussed next especially multiclass case 
[linear, classifiers, quadratic, regularization, support, vector, classifier] support vector classifier described two class case sec tion especially attractive general high dimensional problems classes perfectly separable hyperplane unless identical feature vectors different classes without regularization support vector classifier finds separating hyperplane largest margin hyperplane yielding biggest gap classes training data somewhat surprisingly unregularized support vector classifier often works well best regularized version overfitting often seem problem partly insensitivity misclassification loss many different methods generalizing two class support vector classifier classes one versus one ovo approach compute pairwise classifiers test point predicted class one wins pairwise contests one versus ova approach class compared others two class comparisons classify test point compute confidences signed distance hyperplane classifiers winner class highest confidence finally vapnik weston watkins suggested somewhat complex multiclass criteria generalize two class criterion tibshirani hastie propose margin tree classifier support vector classifiers used binary tree much cart chapter classes organized hierarchical manner useful classifying patients different cancer types example line table shows results support vector classifier using ova method ramaswamy reported con firmed approach worked best problem errors similar line might expect comments end previous section error rates insensitive choice regularization parameter page values since support vector hyperplane perfectly separate training data setting 
[linear, classifiers, quadratic, regularization, feature, selection] feature selection important scientific requirement classifier large neither discriminant analysis logistic regression support vector classifier perform feature selection automatically use quadratic regularization features nonzero weights models hoc methods feature selection proposed example removing genes small coefficients refitting classifier done backward stepwise manner starting smallest weights moving larger weights known recursive feature elimination guyon successful example ramaswamy report example accuracy support vector classifier starts degrade number genes reduced full linear classifiers quadratic regularization set rather remarkable number training samples explanation behavior three methods discussed section rda svm modified fit nonlinear decision boundaries using kernels usually motivation approach increase model complexity models already sufficiently complex overfitting always danger yet despite high dimensionality radial kernels section sometimes deliver superior results high dimensional problems radial kernel tends dampen inner products points far away turn leads robustness outliers occurs often high dimensions may explain positive results tried radial kernel svm table case performance inferior 
[linear, classifiers, quadratic, regularization, computational, shortcuts] computational techniques discussed section apply method fits linear model quadratic regularization coefficients includes methods discussed section many computations carried dimensional space rather via singular value decomposition introduced section geometric intuition like two points three dimensional space always lie line points dimensional space lie dimensional affine subspace given data matrix let udv singular value decomposition svd orthonormal columns orthogonal diagonal matrix elements matrix rows simple example let first consider estimates ridge gression replacing manipulations shown equal exercise thus ridge regression estimate using observations words simply reduce data matrix work rows trick reduces computational cost high dimensional problems results generalized models linear parameters quadratic penalties consider supervised learning problem use linear function model parameter conditional distribution fit parameters minimizing loss function data quadratic penalty logistic regression useful example mind following simple theorem let defined consider pair optimization problems arg min arg min theorem says simply replace vectors vectors perform penalized fit far fewer predictors vector solution transformed back vector solution via simple matrix multiplication result part statistics folklore deserves known widely see hastie tibshirani details geometrically rotating features coordinate system first coordinates zero rotations allowed since quadratic penalty invariant rotations linear models equivariant result applied many learning methods discussed chapter regularized multiclass logistic regression linear discriminant analysis exercise support vector machines also applies neural networks quadratic regularization section note however apply methods lasso uses nonquadratic penalties coefficients typically use cross validation select parameter seen exercise need construct original data use data folds support vector kernel trick section exploits duction used section slightly different context suppose disposal gram inner product matrix captures information exercise shows exploit ideas section fit ridged logistic regression using svd linear classifiers regularization 
[linear, classifiers, regularization] methods previous chapter use penalty regularize parameters ridge regression estimated coefficients nonzero hence feature selection performed section discuss methods use penalties instead hence provide automatic feature selection recall lasso section min written lagrange form discussed use penalty causes subset solution coefficients exactly zero sufficiently large value tuning parameter section discussed lars algorithm efficient procedure computing lasso solution chapter approaches zero lasso fits training data exactly fact convex duality one show number non zero coefficients values rosset zhu example thus lasso provides severe form feature selection lasso regression applied two class classification problem coding outcome applying cutoff usually predictions two classes many possible approaches including ova ovo methods discussed section tried ova approach cancer data section results shown line table performance among best natural approach classification problems use lasso penalty regularize logistic regression several implementations proposed literature including path algorithms similar lars park hastie paths piecewise smooth nonlinear exact methods slower lars algorithm less feasible large friedman provide fast algorithms fitting pen alized logistic multinomial regression models use symmetric multinomial logistic regression model section maximize penalized log likelihood max  log  compare algorithm computes exact solution pre chosen sequence values cyclical coordinate descent sec tion exploits fact solutions sparse high dimensional problems well fact solutions neighboring values tend similar method used line table tuning parameter chosen cross validation performance similar best methods except automatic feature lection chose genes altogether similar approach used genkin although present model bayesian point view fact compute posterior mode solves penalized maximum likelihood problem ffic ffic log log lasso elastic net figure regularized logistic regression paths leukemia data left panel lasso path right panel elastic net path ends path extreme left nonzero coefficients lasso elastic net averaging effect elastic net results non zero coefficients lasso smaller magnitudes genomic applications often strong correlations among variables genes tend operate molecular pathways lasso penalty somewhat indifferent choice among set strong corre lated variables exercise ridge penalty hand tends shrink coefficients correlated variables toward exer cise page elastic net penalty zou hastie compromise form second term encourages highly correlated features averaged first term encourages sparse solution coefficients aver linear classifiers regularization aged features elastic net penalty used linear model particular regression classification hence multinomial problem elastic net penalty becomes max  log  parameter determines mix penalties often pre chosen qualitative grounds elastic net yield non zero coefficients potential advantage lasso line table uses model chosen cross validation used sequence values values uniform log scale covering entire range values gave minimum error values tied solutions although lowest test error among methods margin small significant interestingly performed separately value minimum test error achieved value chosen two dimensional misclassification error training test fold deviance log log figure training test fold cross validation curves lasso logis tic regression leukemia data left panel shows misclassification errors right panel shows deviance figure shows lasso elastic net coefficient paths two class leukemia data golub gene expression measurements samples class acute lymphocytic leukemia class aml acute myelogenous leukemia also test set samples since data linearly separa ble solution undefined exercise degrades small values hence paths truncated fitted probabilities approach non zero coefficients left plot right figure left panel shows misclas high dimensional problems sification errors lasso logistic regression training test data well fold cross validation training data right panel uses binomial deviance measure errors much smoother small sample sizes lead considerable sampling variance curves even though individual curves relatively smooth see example fig ure page plots suggest limiting solution adequate leading misclassifications test set corresponding figures elastic net qualitatively similar shown limiting coefficients diverge regularized logistic regression models practical software implementations minimum value either explicitly implicitly set however renormalized versions coefficients converge limiting solutions thought interesting alternatives linear optimal separating perplane svm limiting solution coincides svm see end section genes selected limiting solution coincides separating hyperplane rosset includes genes decreases elastic net solutions include genes separating hyperplane 
[linear, classifiers, regularization, application, lasso, protein, mass, spectroscopy] protein mass spectrometry become popular technology analyzing proteins blood used diagnose disease understand processes underlying blood serum sample observe intensity many time flight values intensity related number particles observed take approximately time pass emitter detector cycle operation machine time flight known relationship mass charge ratio constituent proteins blood hence identification peak spectrum certain tells protein corresponding mass charge identity protein determined means figure shows example taken adam shows average spectra healthy patients prostate cancer sites total ranging value full dataset consists healthy patients cancer goal find sites discriminate two groups example functional data predictors viewed function much interest problem past years see petricoin data first standardized baseline subtraction normaliza tion restricted attention values spectra outside range interest applied near linear classifiers regularization intensity normal cancer figure protein mass spectrometry data average profiles normal prostate cancer patients est shrunken centroids lasso regression data results methods shown table fitting harder data lasso achieves considerably lower test error rate however may provide scientifically useful solu tion ideally protein mass spectrometry resolves biological sample constituent proteins appear peaks spectra lasso treat peaks special way surprisingly non zero lasso weights situated near peaks spectra furthermore protein may yield peak slightly different values different spectra order identify common peaks kind warping needed sample sample address applied standard peak extraction algorithm spectrum yielding total peaks training spectra idea pool collection peaks patients hence con struct set common peaks purpose applied hierarchical clustering positions peaks along log axis cut resulting dendrogram horizontally height log computed averages peak positions resulting cluster process yielded common clusters corresponding peak centers given common peaks determined present individual spectrum present height peak peak height zero assigned peak found pro duced matrix peak heights features used lasso regression scored test spectra peaks use value means peaks positions less apart considered peak fairly common assumption high dimensional problems table results prostate data example standard deviation test errors method test errors number sites nearest shrunken centroids lasso lasso peaks prediction results application lasso peaks shown last line table fairly well well lasso raw spectra however fitted model may useful biologist yields peak positions study hand results suggest may useful discriminatory information peaks spectra positions lasso sites line table also deserve examination 
[linear, classifiers, regularization, fused, lasso, functional, data] previous example features natural order determined mass charge ratio generally may functional fea tures ordered according index variable already discussed several approaches exploiting structure represent coefficients basis functions splines wavelets fourier bases apply regression using coefficients predictors equivalently one instead represent coefficients original features bases approaches described section classification setting discuss analogous approach penal ized discriminant analysis section uses penalty explicitly controls resulting smoothness coefficient vector methods tend smooth coefficients uniformly present adaptive strategy modifies lasso penalty take account ordering features fused lasso tibshirani solves min criterion strictly convex unique solution exists first penalty encourages solution sparse second encourages smooth index difference penalty assumes uniformly spaced index instead underlying index variable nonuniform values natural generalization would based divided differences linear classifiers regularization genome order log rat figure fused lasso applied cgh data point represents copy number gene tumor sample relative control log base scale amounts penalty modifier terms series particularly useful special case arises predictor matrix identity matrix special case fused lasso used approximate sequence fused lasso signal approximator solves min figure shows example taken tibshirani wang data panel come comparative genomic hybridization cgh array measuring approximate log base two ratio number copies gene tumor sample compared normal sample horizontal axis represents chromosomal location gene idea cancer cells genes often amplified duplicated deleted interest detect events furthermore events tend occur contiguous regions smoothed signal estimate fused lasso signal approximator shown dark red appropriately chosen values significantly nonzero regions used detect locations gains losses genes tumor also two dimensional version fused lasso parameters laid grid pixels penalty applied high dimensional problems first differences left right target pixel useful denoising classifying images friedman develop fast generalized coordinate descent algorithms one two dimensional fused lasso 
[classification, features, unavailable] applications objects study abstract nature obvious define feature vector long fill proximity matrix similarities pairs objects database turns put use many classifiers arsenal interpreting proximities inner products protein structures fall category explore example section applications document classification feature vectors available extremely high dimensional may wish compute high dimensional data rather store inner products pairs documents often inner products approximated sampling techniques pairwise distances serve similar purpose turned centered inner products proximity matrices discussed tail chapter 
[classification, features, unavailable, example, string, kernels, protein, classification] important problem computational biology classify proteins functional structural classes based sequence similarities pro tein molecules strings amino acids differing length com position example consider lengths vary amino acid molecules one different types labeled using letters two examples length respectively iptsalvketlallsthrtllianetlripvpvhknhqlcteeifqgigtlesqtvqggtv erlfknlslikkyidgqkkkcgeerrrvnqfldy lqe flgvmntewi phrrdlcsrsiwlarkirsdltaltesyvkhqglwselteaer lqe nlqayrtfhvlla rlledqqvhftptegdfhqaihtlllqvaafayqieelmilleykiprneadgmlfekk lwglkv lqe lsqwtvrsihdlrfisshqtgip many proposals measuring similarity pair protein molecules focus measure based count matching substrings leslie lqe construct features count number times given sequence length occurs string compute number classification features unavailable possible sequences length formally string define feature map set subsequences length number times occurs string using define inner product measures similarity two strings used drive example support vector classifier classifying strings different protein classes number possible sequences large moderate vast majority subsequences match strings training set turns compute inner product matrix string kernel efficiently using tree structures without actually computing individual vectors methodology data follow come leslie data consist proteins two classes negative positive two examples call set marked occurrences subsequence lqe appears proteins possible subsequences vector length example lqe lqe using software leslie computed string kernel used support vector classifier find maximal margin solution dimensional feature space used fold cross validation compute svm predictions training data orange curve figure shows cross validated roc curve support vector classifier computed varying cut point real valued predictions cross validated support vector classifier area curve leslie show string kernel method competitive perhaps accurate specialized methods protein string matching many classifiers computed using information kernel matrix details given next section results nearest centroid classifier green distance weighted one nearest neighbors blue shown figure performance similar support vector classifier thank christina leslie help providing data available book website high dimensional problems roc curves string kernel specificity sensitivity svm nearest centroid one nearest neighbor figure cross validated roc curves protein example using string kernel numbers next method legend give area curve overall measure accuracy svm achieves better sensitivities two achieve better specificities 
[classification, features, unavailable, classification, models, using, inner-product, kernels, pairwise, distances] number classifiers besides support vector chine implemented using inner product matrices also implies kernelized like svm obvious example nearest neighbor classification since trans form pairwise inner products pairwise distances variation classification used figure produces continuous discriminant score needed construct roc curve distance weighted makes use distance test points closest member class see exercise nearest centroid classification follows easily well training pairs test point class centroids write classification features unavailable hence compute distance test point cen troids perform nearest centroid classification also implies methods like means clustering also implemented using inner products data points logistic multinomial regression quadratic regularization also implemented inner product kernels see section exercise exercise derives linear discriminant analysis using inner product kernel principal components computed using inner product kernels well since frequently useful give details suppose first centered data matrix let udv svd matrix principal component variables see section follows hence compute eigen decomposition centered center using mean operator thus compute eigenvectors double centered kernel principal components uncentered inner product matrix exercise explores section discusses detail kernel pca general kernels radial kernel used svms instead available pairwise squared euclidean dis tances observations turns well trick convert pairwise distances centered inner products proceed write defining double center easy check centered inner product matrix distances inner products also allow compute medoid class observation smallest average distance observations class used classification closest medoids well drive medoids clustering section abstract data objects like proteins medoids practical advantage means medoid one training examples displayed tried closest medoids example next section see table performance disappointing useful consider cannot inner product kernels distances high dimensional problems table cross validated error rates abstracts example nearest shrunken centroids ended using shrinkage use word word standardization section standardization gives distinct advantage methods method error nearest shrunken centroids svm nearest medoids nearest centroids cannot standardize variables standardization significantly proves performance example next section cannot assess directly contributions individual variables particular cannot perform individual tests fit nearest shrunken centroids model fit model uses lasso penalty cannot separate good variables noise variables get equal say often case ratio relevant irrelevant variables small methods use kernels likely work well methods feature selection 
[classification, features, unavailable, example, abstracts, classification] somewhat whimsical example serves illustrate limitation ker nel approaches collected abstracts papers bradley efron trevor hastie rob tibshirani frequent authors jerome friedman extracted unique words abstracts defined features number times word appears abstract called bag words representation quotations parentheses special characters first removed abstracts characters converted lower case also removed word could unfairly discriminate abstracts others total words unique sought classify documents basis features although artificial example allows assess possible degradation performance information specific raw features used first applied nearest shrunken centroid classifier data using fold cross validation essentially chose shrinkage used features see first line table error rate number features reduced without much loss accuracy classification features unavailable note nearest shrunken classifier requires raw feature matrix order standardize features individually figure shows predictive bayes using algorithm procedure technology values accuracy variables inference bayesian frequentist propose presented method problems figure abstracts example top scores nearest shrunken cen troids score standardized difference frequency word given class versus classes thus positive score right vertical grey zero lines indicates higher frequency class negative score indicates lower relative frequency top discriminating words positive score indicating word appears class classes terms make sense example frequentist bayesian reflect efron greater emphasis statistical inference however many oth ers surprising reflect personal writing styles example fried man use presented use propose applied support vector classifier linear kernel regularization using pairs ovo method handle three classes regularization svm improve performance result shown table somewhat worse nearest shrunken centroid classifier mentioned first line table represents nearest shrunken cen troids shrinkage denote pooled within class standard deviation feature median values line also corresponds nearest centroid classification first standardizing feature recall page line shows performance nearest medoids poor something surprised perhaps due small sample sizes high dimensional problems high dimensions medoids much higher variance means performance one nearest neighbor classifier also poor performance nearest centroid classifier also shown ble line better nearest medoids worse nearest shrunken centroids even shrinkage difference seems standardization feature done nearest shrunken centroids standardization important requires access individual feature values nearest centroids uses spherical metric relies fact features similar units support vector machine estimates linear combination features better deal unstandardized features 
[high-dimensional, regression, supervised, principal, components] section describe simple approach regression generalized regression especially useful illustrate method another microarray data example data taken rosenwald consists samples patients diffuse large cell lymphoma dlbcl gene expression measurements genes outcome survival time either observed right censored randomly divided lymphoma samples training set size test set size although supervised principal components useful linear regression interesting applications may survival studies focus example yet discussed regression censored survival data book represents generalized form regression outcome variable survival time partly observed individuals sup pose example carry medical study lasts days simplicity subjects recruited day one might observe one individual die days start study another individ ual might still alive days study ends individual said right censored days know lived least days although know long past days individual actually lived censored observation still informative illustrated figure figure shows survival curve estimated kaplan meier method patients test set see example kalbfleisch prentice description kaplan meier method objective example find set features genes predict survival independent set patients could high dimensional regression supervised principal components time days patient figure censored survival data illustration four patients first third patients die study ends second patient alive end study days fourth patient lost follow study ends example patient might moved country survival times patients two four said censored lll lll lllll months survival function figure lymphoma data kaplan meier estimate survival function patients test set along one standard error curves curve estimates probability surviving past months ticks indicate censored observations high dimensional problems probability density poor cell type good cell type survival time figure underlying conceptual model supervised principal compo nents two cell types patients good cell type live longer average supervised principal components estimate cell type averaging expression genes reflect useful prognostic indicator aid choosing treatments help understand biological basis disease underlying conceptual model supervised principal components shown figure imagine two cell types patients good cell type live longer average however considerable overlap two sets survival times might think survival time noisy surrogate cell type fully supervised approach would give weight genes strongest relationship survival genes partially perfectly lated cell type could instead discover underlying cell types patients often reflected sizable signature genes acting together pathways might better job predicting patient survival although cell type figure discrete useful imagine continuous cell type define linear combination features estimate cell type continuous quantity discretize display interpretation find linear combination defines important lying cell types principal components analysis section effective method finding linear combinations features exhibit large varia tion dataset seek linear combinations high variance significant correlation outcome lower right panel figure shows result applying standard principal com ponents example leading component correlate strongly survival details given figure caption hence want encourage principal component analysis find linear combinations features high correlation outcome restrict attention features siz able correlation outcome summarized supervised principal components algorithm illustrated figure details steps depend type outcome variable standard regression problem use univariate linear least squares coefficients step linear least squares model high dimensional regression supervised principal components genes patients supervised absolute cox score probability survival low score high score best single gene probability survival supervised principal component genes months probability survival principal component genes figure supervised principal components lymphoma data left panel shows heatmap subset gene expression training data rows ordered magnitude univariate cox score shown mid dle vertical column top bottom genes shown supervised principal component uses top genes chosen fold repre sented bar top heatmap used order columns expression matrix addition row multiplied sign cox score middle panel right shows survival curves test data create low high group splitting supervised zero training data mean curves well separated indicated value log rank test top panel using top scoring gene training data curves somewhat separated significantly bottom panel uses first principal component genes separa tion also poor top genes interpreted noisy surrogates latent underlying cell type characteristic supervised principal components uses estimate latent factor high dimensional problems algorithm supervised principal components compute standardized univariate regression coefficients outcome function feature separately value threshold list form reduced data matrix consisting features whose univariate coefficient exceeds absolute value compute first principal components matrix use principal components regression model predict outcome pick cross validation step survival problems cox proportional hazards regression model widely used hence use score test model step multivariate cox model step details essential understanding basic method may found bair figure shows results supervised principal components example used cox score cutoff yielding genes value found fold cross validation computed first principal component using subset data well value test observations included quantitative predictor cox regression model likelihood ratio significance dichotomized using mean score training data threshold clearly separates patients test set low high risk groups middle right panel figure top right panel figure uses top scoring gene dichot omized alone predictor survival significant test set likewise lower right panel shows dichotomized principal compo nent using training data also significant procedure allows principal components step however supervision step encourages principal components align outcome thus cases first first com ponents tend useful prediction mathematical development consider first component extensions one component derived similar way 
[high-dimensional, regression, supervised, principal, components, connection, latent-variable, modeling] formal connection supervised principal components derlying cell type model figure seen latent variable model data suppose response variable related high dimensional regression supervised principal components underlying latent variable linear model addition measurements set features indexed pathway errors assumed mean zero independent random variables respective models also many additional features independent would like identify estimate hence fit predic tion model special case latent structure model single component factor analysis model mardia see also sec tion latent factor continuous version cell type conceptualized figure supervised principal component algorithm seen method fitting model screening step estimates set given largest principal component step estimates latent factor finally regression fit step estimates coefficient model step natural since average regression coefficient nonzero non zero hence step select features step natural assume errors gaussian dis tribution variance case principal component maximum likelihood estimate single factor model mardia regression obvious final step suppose total features features relevant set grow small relative one show reasonable conditions leading supervised principal component consistent underlying latent factor usual leading principal component may consistent since contaminated presence large number noise features finally suppose threshold used step supervised principal component procedure yields large number features com putation principal component interpretational purposes well practical uses would like way finding reduced set features approximates model pre conditioning section one way high dimensional problems 
[high-dimensional, regression, supervised, principal, components, relationship, partial, least, squares] supervised principal components closely related partial least squares regression section bair found key good performance supervised principal components filtering noisy features step partial least squares section downweights noisy features throw away result large number noisy features contaminate predictions however modification partial least squares procedure proposed similar flavor supervised principal components brown nadler coifman example select features steps supervised principal components apply pls rather principal components features current discussion call thresholded pls thresholded pls viewed noisy version supervised principal components hence might expect work well practice assume variables standardized first pls variate form thought estimate latent factor model contrast supervised principal components direction satisfies hu leading singular value follows definition leading principal component hence thresholded pls uses weights inner product features supervised principal components uses features derive self consistent estimate since many features contribute estimate rather single outcome expect less noisy fact features set infinity shown using techniques bair true unobservable latent variable model present simulation example compare methods numeri cally samples genes generated data follows high dimensional regression supervised principal components figure heatmap outcome left column first genes realization model genes columns samples rows independent normal random variables mean standard deviations respectively thus first genes average difference unit samples difference correlates outcome next genes large average difference units samples difference uncorrelated outcome rest genes noise figure shows heatmap typical realization outcome left first genes right generated simulations model summarize test error results figure test errors principal components partial least squares shown right plot badly affected noisy features data supervised principal components thresholded pls work best wide range number selected features former showing consistently lower test errors example seems tailor made supervised principal com ponents good performance seems hold simulated real datasets bair 
[high-dimensional, regression, supervised, principal, components, pre-conditioning, feature, selection] supervised principal components yield lower test errors competing methods shown figure however always produce sparse model involving small number features genes even thresholding step algorithm yields relatively small number high dimensional problems number features relative root mean square test error thresholded pls supervised principal components figure root mean squared test error one standard error supervised principal components thresholded pls realizations model methods use one component errors relative noise standard deviation bayes error methods different values filtering threshold tried number features retained shown horizontal axis extreme right points correspond regular principal components partial least squares using genes features may omitted features sizable inner products supervised principal component could act good surrogate addition highly correlated features tend chosen together may great deal redundancy set selected features lasso sections hand produces sparse model data test errors two methods compare simulated example last section figure shows test errors one realization model lasso supervised principal components pre conditioned lasso described see supervised principal components orange curve reaches lowest error features included model correct number simulation although linear model first features optimal lasso green adversely affected large number noisy features starts overfitting far fewer model get low test error supervised principal components along sparsity lasso goal pre conditioning paul approach one first computes supervised principal component predictor observation training set feature assessment multiple testing problem number features model mean test error lasso supervised principal components preconditioned lasso figure test errors lasso supervised principal components pre conditioned lasso one realization model model indexed number non zero features supervised principal component path truncated features lasso self truncates sample size see section case pre conditioned lasso achieves lowest error features threshold selected cross validation apply lasso outcome variable place usual outcome features used lasso fit retained thresholding step supervised principal components idea first denoising outcome variable lasso adversely affected large number noise features figure shows pre conditioning purple curve successful yielding much lower test error usual lasso low case supervised principal components also achieve using less features usual lasso applied raw outcome starts overfit quickly pre conditioned version overfitting problem since outcome variable denoised usually select tuning parameter pre conditioned lasso subjective grounds like parsimony pre conditioning applied variety settings using initial estimates supervised principal components post processors lasso details may found paul 
[feature, assessment, multiple-testing, problem] first part chapter discuss prediction models setting consider basic problem assessing signif high dimensional problems icance features consider protein mass spectrometry example section problem scientist might inter ested predicting whether given patient prostate cancer rather goal might identify proteins whose abundance differs mal cancer samples order enhance understanding disease suggest targets drug development thus goal assess significance individual features assessment usually done without use multivariate predictive model like first part chapter feature assessment problem moves focus prediction traditional statistical topic multiple hypothesis testing remainder chapter use instead denote number features since frequently referring values table subset genes microarray study radiation sensitivity total samples normal group radiation sensitive group show three samples group normal radiation sensitive gene gene gene gene gene consider example microarray data table taken study sensitivity cancer patients ionizing radiation treatment rieger row consists expression genes patient samples samples patients normal reaction patients severe reaction radiation measurements made oligo nucleotide microarrays object experiment find genes whose expression different radiation sensitive group patients genes altogether table shows data genes samples illustration identify informative genes construct two sample statistic gene indices samples group normal group sensitive group quantity pooled within group standard error gene feature assessment multiple testing problem statistics figure radiation sensitivity microarray example histogram statistics comparing radiation sensitive versus insensitive groups overlaid blue histogram statistics permutations sample labels histogram statistics shown orange figure ranging value values normally distributed could consider value greater two absolute value sig nificantly large would correspond significance level genes however genes would expect many large values occur chance even group ing unrelated gene example genes independent surely number falsely significant genes would binomial distribution mean standard deviation actual way range assess results genes called mul tiple testing problem start computing value gene done using theoretical distribution probabil ities assumes features normally distributed attractive alternative approach use permutation distribution since avoids assumptions distribution data compute principle permutations sample labels permutation compute statistics value gene high dimensional problems course large number around enumer ate possible permutations instead take random sample possible permutations took random sample permutations exploit fact genes similar measured scale instead pool results genes computing values also gives granular values since many values pooled null distribution individual null distribution using set values would like test hypotheses treatment effect gene versus treatment effect gene reject level test type error equal probability falsely rejecting many tests consider clear use overall measure error let event falsely rejected definition family wise error rate fwer probability least one false rejection commonly used overall measure error detail event least one false rejection fwer generally large depends correlation tests tests independent type error rate family wise error rate collection tests hand tests positive dependence fwer less positive dependence tests often occurs practice particular genomic studies one simplest approaches multiple testing bonferroni method makes individual test stringent order make fwer equal reject easy show resulting fwer exercise bonferroni method useful relatively small large conservative calls genes significant example test level say must use threshold none genes value small feature assessment multiple testing problem variations approach adjust individual values achieve fwer approaches avoiding assumption independence see dudoit 
[feature, assessment, multiple-testing, problem, false, discovery, rate] different approach multiple testing try control fwer focuses instead proportion falsely significant genes see approach strong practical appeal table summarizes theoretical outcomes hypothesis tests note family wise error rate instead focus table possible outcomes hypothesis tests note number false positive tests type error rate type error rate power called called significant significant total true false total false discovery rate fdr microarray setting expected proportion genes incorrectly called significant among genes called signif icant expectation taken population data generated benjamini hochberg first proposed notion false discovery rate gave testing procedure algorithm whose fdr bounded user defined level benjamini hochberg procedure based values obtained asymptotic approximation test statistic gaussian permutation dis tribution done hypotheses independent benjamini hochberg show regardless many null hypotheses true regardless distribution values null hypothesis false procedure property fdr illustration chose figure shows plot dered values line slope high dimensional problems algorithm benjamini hochberg method fix false discovery rate let denote ordered values define max reject hypotheses rejection threshold genes ordered value value figure microarray example continued shown plot ordered values line benjamini hochberg method largest value falls line gives threshold occurs indicated vertical line thus method calls significant genes red smallest values feature assessment multiple testing problem algorithm plug estimate false discovery rate create permutations data producing statistics fea tures permutations range values cut point let obs estimate fdr fdr obs starting left moving right method finds last time values fall line occurs reject genes smallest values note cutoff occurs smallest value largest values thus reject genes brief description clear procedure works corresponding fdr value used indeed proof fact quite complicated benjamini hochberg direct way proceed plug approach rather starting value fix cut point statistics say value appeared number observed values equal greater total number permutation values equal greater average per permutation thus direct estimate false discovery rate fdr note approximately equal value used difference due discreteness procedure summarized algorithm recap plug estimate fdr algorithm equivalent procedure algorithm using permutation values correspondence method plug estimate coincidence exercise shows equivalent general note procedure makes reference values rather works directly test statistics plug estimate based approximation general fdr consistent estimate fdr storey storey note numerator actually estimates high dimensional problems since permutation distribution uses rather null hypotheses hence estimate available better estimate fdr obtained fdr exercise shows way estimate conservative upwardly biased estimate fdr uses equivalently estimate used improve method relation reader might surprised chose value large fdr bound must remember fdr type error customary choice scientist false discovery rate expected proportion false positive genes among list genes statistician tells significant microarray experiments fdrs high might still useful especially exploratory nature 
[feature, assessment, multiple-testing, problem, asymmetric, cutpoints, sam, procedure] testing methods described used absolute value test statistic hence applied cut points positive negative values statistic experiments might happen differentially expressed genes change positive direc tion negative direction situation advantageous derive separate cut points two cases significance analysis microarrays sam approach offers way basis sam method shown figure vertical axis plotted ordered test statistics horizontal axis shows expected order statistics permutations data ordered test statistics permutation two lines drawn parallel line units away starting origin moving right find first place genes leave band defines upper cutpoint genes beyond point called significant marked red similarly find lower cutpoint low genes bottom left corner thus value tuning parameter defines upper lower cutpoints plug estimate fdr cutpoints estimated typically range values associated fdr values computed particular pair chosen subjective grounds advantage sam approach lies possible asymmetry cutpoints example figure obtain significant genes upper right data points bottom left never leave band hence low hence value genes called significant left negative side impose symmetry cutpoints done section reason assume similar behavior two ends feature assessment multiple testing problem expected order statistics statistic figure sam plot radiation sensitivity microarray data vertical axis plotted ordered test statistics horizontal axis shows expected order statistics test statistics permutations data two lines drawn parallel line units away starting origin moving right find first place genes leave band defines upper cut point genes beyond point called significant marked red similarly define lower cutpoint low particular value plot genes called significant bottom left high dimensional problems similarity approach asymmetry possi ble likelihood ratio tests suppose log likelihood null hypothesis effect log likelihood alterna tive likelihood ratio test amounts rejecting null hypothesis depending likelihoods particularly relative values result different threshold sam procedure rejects null hypothesis threshold depends corresponding value null value 
[feature, assessment, multiple-testing, problem, bayesian, interpretation, fdr] interesting bayesian view fdr developed storey efron tibshirani first need define positive false discovery rate pfdr pfdr additional term positive refers fact interested estimating error rate positive findings occurred slightly modified version fdr clean bayesian inter pretation note usual fdr expression defined let rejection region single test example used suppose identical simple hypothe sis tests performed statistics rejection region define random variable equals jth null hypothesis true otherwise assume pair random variables distributions says test statistic comes one two distributions null hypothesis true otherwise letting marginally shown efron storey bibliographic notes pfdr hence mixture model pfdr posterior proba bility null hypothesis true given test statistic falls rejection region test given reject null hypothesis exercise false discovery rate provides measure accuracy tests based entire rejection region fdr test say gene say significant gene thus interest derive local gene specific version fdr value storey test statistic defined smallest fdr rejection regions reject symmetric rejection regions value defined fdr rejection region thus value smaller reflecting fact significant local false discovery rate efron tibshirani defined positive fdr infinitesimal rejection region surrounding value 
[bibliographic, notes] many references given specific points chapter give additional ones dudoit give overview compar ison discrimination methods gene expression data levina mathematical analysis comparing diagonal lda full lda shows reasonable assumptions diago nal lda lower asymptotic error rate full lda tibshirani tibshirani proposed nearest shrunken centroid classifier zhu hastie study regularized logistic regression high dimensional regression lasso active areas research many references given section fused lasso proposed tibshirani zou hastie introduced elastic net supervised principal components discussed bair tib shirani bair introduction analysis censored survival data see kalbfleisch prentice microarray technology led flurry statistical research see example books speed parmigiani simon lee false discovery rate proposed benjamini hochberg studied generalized subsequent papers authors high dimensional problems many others partial list papers fdr may found yoav ben jamini homepage recent papers include efron tibshirani storey genovese wasserman storey tib shirani benjamini yekutieli dudoit review methods identifying differentially expressed genes microarray studies 
[exercises] coefficient estimate let normalized ver sion show normalized ridge regression estimates con verge renormalized partial least squares one component estimates nearest shrunken centroids lasso consider naive bayes gaussian model classification features assumed independent within class servations equal set indices observations class observe set pooled within class variance feature consider lasso style minimization problem min    show solution equivalent nearest shrunken centroid timator set zero equal instead show fitted coefficients regularized multiclass logistic regression problem satisfy discuss issues constant parameters resolved derive computational formula ridge regression hint use first derivative penalized sum squares criterion show prove theorem section decom posing rows projections column space complement show theorem section applied regu larized discriminant analysis section equation exercises consider linear regression problem assume rank let svd udv nonsingular orthonormal columns show infinitely many least squares solutions zero residuals show ridge regression estimate written show solution residuals equal zero unique smallest euclidean norm amongst zero residual solutions data piling exercise shows two class lda solution obtained linear regression binary response vector con sisting prediction scale shift lda score suppose consider linear regression model fit binary response using exercise show infinitely many directions defined onto data project exactly two points one class known data piling directions ahn marron show distance projected points hence directions define separating hyperplanes mar gin argue single maximal data piling direction distance largest defined udv svd compare data piling direction exercise direction optimal separating hyperplane section qualitatively makes widest margin use small simulation demonstrate difference linear discriminant analysis see section degenerate within class covariance matrix singular one version regularized discriminant analysis replaces ridged version leading regularized discriminant function show lim corresponds maximal data piling direction defined exercise suppose sample pairs binary suppose also two classes separable high dimensional problems pair wish fit linear logistic regression model logitpr maximum likelihood show undefined suppose wish select ridge parameter fold cross validation situation linear model wish use computational shortcuts described section show need reduce matrix matrix use cross validation runs suppose predictors presented inner product matrix wish fit equivalent linear logistic regression model original features quadratic regulariza tion predictions also made using inner products new presented let eigen decomposition show predictions given ridged logistic regression estimate input matrix argue approach used appropriate kernel matrix distance weighted classification consider nearest neighbor method section two class classification problem let shortest distance training observation class likewise shortest distance class let number samples class number class show log viewed nonparametric discriminant function correspond ing classification hint show viewed nonparametric estimate density class would modify function introduce class prior probabil ities different sample priors would generalize approach classification kernel pca section show compute principal component variables uncentered inner product matrix compute eigen decomposition suppose inner product exercises vector containing inner products new point training set show centered projections onto principal component directions given bonferroni method multiple comparisons suppose multiple testing scenario null hypotheses corresponding values let event least one null hypothesis falsely rejected let event jth null hypothesis falsely rejected suppose use bonferroni method rejecting jth null hypothesis show hint hypotheses independent use show case equivalence benjamini hochberg plug methods notation algorithm show rejection threshold proportion permuted values exceed lth largest value among hence show plug fdr estimate fdr less equal show cut point produces test estimated fdr greater use result show pfdr type error type error power storey consider data table section available book website using symmetric two sided rejection region based statistic compute plug estimate fdr various values cut point carry procedure various fdr levels show equivalence results part high dimensional problems let quartiles statistics permuted datasets let set min multiply fdr estimates examine results give motivation estimate part storey proof result write pfdr use fact given binomial random variable trials probability success complete proof 
