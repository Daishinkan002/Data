[introduction, overview, statistical, learning] statistical learning refers vast set tools understanding data tools classiﬁed supervised unsupervised broadly speaking supervised statistical learning involves building statistical model pre dicting estimating output based one inputs problems nature occur ﬁelds diverse business medicine astrophysics public policy unsupervised statistical learning inputs supervising output nevertheless learn relationships struc ture data provide illustration applications statistical learning brieﬂy discuss three real world data sets considered book 
[introduction, overview, statistical, learning, wage, data] application refer wage data set throughout book examine number factors relate wages group males atlantic region united states particular wish understand association employee age education well calendar year wage consider example left hand panel figure displays wage versus age individu als data set evidence wage increases age decreases approximately age blue line provides estimate average wage given age makes trend clearer james introduction statistical learning applications springer texts statistics doi springer science business media new york introduction age year education level figure wage data contains income survey information males central atlantic region united states left wage function age average wage increases age years age point begins decline center wage function year slow steady increase approximately average wage right boxplots displaying wage function education indicating lowest level high school diploma highest level advanced graduate degree average wage increases level education given employee age use curve predict wage however also clear figure signiﬁcant amount vari ability associated average value age alone unlikely provide accurate prediction particular man wage also information regarding employee education level year wage earned center right hand panels figure display wage function year education dicate factors associated wage wages increase approximately roughly linear straight line fashion though rise slight relative vari ability data wages also typically greater individuals higher education levels men lowest education level tend substantially lower wages highest education level clearly accurate prediction given man wage obtained combining age education year chapter discuss linear regression used predict wage data set ideally predict wage way accounts non linear relationship wage age chapter discuss class approaches addressing problem 
[introduction, overview, statistical, learning, stock, market, data] wage data involves predicting continuous quantitative output value often referred regression problem however certain cases may instead wish predict non numerical value categorical introduction yesterday today direction ercentage change samp two days previous ercentage change samp today direction today direction three days previous ercentage change samp figure left boxplots previous day percentage change samp index days market increased decreased obtained smarket data center right left panel percentage changes days previous shown qualitative output example chapter examine stock mar ket data set contains daily movements standard amp poor samp stock index year period refer smarket data goal predict whether index increase decrease given day using past days percentage changes index statistical learning problem volve predicting numerical value instead involves predicting whether given day stock market performance fall bucket bucket known classiﬁcation problem model could accurately predict direction market move would useful left hand panel figure displays two boxplots previous day percentage changes stock index one days market increased subsequent day one days market decreased two plots look almost identical suggest ing simple strategy using yesterday movement samp predict today returns remaining panels display box plots percentage changes days previous today similarly indicate little association past present returns course lack pattern expected presence strong correlations tween successive days returns one could adopt simple trading strategy generate proﬁts market nevertheless chapter explore data using several diﬀerent statistical learning methods interestingly hints weak trends data suggest least year period possible correctly predict direction movement market approximately time figure introduction today direction predicted probability figure quadratic discriminant analysis model subset smarket data corresponding time period predicted probability stock market decrease using data average predicted probability decrease higher days market decrease based results able correctly predict direction movement market time 
[introduction, overview, statistical, learning, gene, expression, data] previous two applications illustrate data sets input output variables however another important class problems involves situations observe input variables corresponding output example marketing setting might demographic information number current potential customers may wish understand types customers similar grouping individuals according observed characteristics known clustering problem unlike previous examples trying predict output variable devote chapter discussion statistical learning methods problems natural output variable available consider nci data set consists gene expression measurements cancer cell lines instead predicting particular output variable interested determining whether groups clusters among cell lines based gene expression measurements diﬃcult question address part thousands gene expression measurements per cell line making hard visualize data left hand panel figure addresses problem represent ing cell lines using two numbers ﬁrst two principal components data summarize expression measurements cell line two numbers dimensions likely dimension reduction resulted introduction figure left representation nci gene expression data set two dimensional space point corresponds one cell lines appear four groups cell lines represented using diﬀerent colors right left panel except represented diﬀerent types cancer using diﬀerent colored symbol cell lines corresponding cancer type tend nearby two dimensional space loss information possible visually examine data evidence clustering deciding number clusters often diﬃ cult problem left hand panel figure suggests least four groups cell lines represented using separate colors examine cell lines within cluster similarities types cancer order better understand relationship gene expression levels cancer particular data set turns cell lines correspond diﬀerent types cancer however information used create left hand panel figure right hand panel fig ure identical left hand panel except cancer types shown using distinct colored symbols clear evidence cell lines cancer type tend located near two dimensional representation addition even though cancer infor mation used produce left hand panel clustering obtained bear resemblance actual cancer types observed right hand panel provides independent veriﬁcation accuracy clustering analysis 
[introduction, brief, history, statistical, learning] though term statistical learning fairly new many concepts underlie ﬁeld developed long ago beginning nineteenth century legendre gauss published papers method introduction least squares implemented earliest form known linear regression approach ﬁrst successfully applied problems astronomy linear regression used predicting quantitative values individual salary order predict qualitative values whether patient survives dies whether stock market increases decreases fisher proposed linear discriminant analysis various authors put forth alternative approach logistic regression early nelder wedderburn coined term generalized linear models entire class statistical learning methods include linear logistic regression special cases end many techniques learning data available however almost exclusively linear methods cause ﬁtting non linear relationships computationally infeasible time computing technology ﬁnally improved suﬃciently non linear methods longer computationally prohibitive mid breiman friedman olshen stone introduced classiﬁcation regression trees among ﬁrst demonstrate power detailed practical implementation method including cross validation model selection hastie tibshirani coined term generalized addi tive models class non linear extensions generalized linear models also provided practical software implementation since time inspired advent machine learning disciplines statistical learning emerged new subﬁeld statistics focused supervised unsupervised modeling prediction recent years progress statistical learning marked increasing availability powerful relatively user friendly software popular freely available system potential continue transformation ﬁeld set techniques used developed statisticians computer scientists essential toolkit much broader community 
[introduction, book] elements statistical learning esl hastie tibshirani friedman ﬁrst published since time become important reference fundamentals statistical machine learning success derives comprehensive detailed treatment many important topics statistical learning well fact relative many upper level statistics textbooks accessible wide audience however greatest factor behind success esl topical nature time publication interest ﬁeld statistical introduction learning starting explode esl provided one ﬁrst accessible comprehensive introductions topic since esl ﬁrst published ﬁeld statistical learning con tinued ﬂourish ﬁeld expansion taken two forms obvious growth involved development new improved statis tical learning approaches aimed answering range scientiﬁc questions across number ﬁelds however ﬁeld statistical learning also expanded audience increases computational power generated surge interest ﬁeld non statisticians eager use cutting edge statistical tools analyze data unfortu nately highly technical nature approaches meant user community remained primarily restricted experts statistics computer science related ﬁelds training time understand implement recent years new improved software packages signiﬁcantly eased implementation burden many statistical learning methods time growing recognition across number ﬁelds business health care genetics social sciences beyond statistical learning powerful tool important practical applications result ﬁeld moved one primarily academic interest mainstream discipline enormous potential audience trend surely continue increasing availability enormous quantities data software analyze purpose introduction statistical learning isl facili tate transition statistical learning academic mainstream ﬁeld isl intended replace esl far comprehen sive text terms number approaches considered depth explored consider esl important companion professionals graduate degrees statistics machine learning related ﬁelds need understand technical details behind statistical learning approaches however community users statistical learning techniques expanded include individuals wider range interests backgrounds therefore believe place less technical accessible version esl teaching topics years discovered interest master phd students ﬁelds disparate business administration biology computer science well quantitatively oriented upper division undergraduates important diverse group able understand models intuitions strengths weaknesses various approaches audience many technical details behind statistical learning methods optimiza tion algorithms theoretical properties primary interest believe students need deep understanding aspects order become informed users various methodologies introduction order contribute chosen ﬁelds use statistical learning tools islr based following four premises many statistical learning methods relevant useful wide range academic non academic disciplines beyond sta tistical sciences believe many contemporary statistical learn ing procedures become widely available used currently case classical methods linear regres sion result rather attempting consider every possible approach impossible task concentrated presenting methods believe widely applicable statistical learning viewed series black boxes single approach perform well possible applications understanding cogs inside box interaction cogs impossible select best box hence attempted carefully describe model intuition assump tions trade oﬀs behind methods consider important know job performed cog necessary skills construct machine inside box thus minimized discussion technical details related ﬁtting procedures theoretical properties assume reader comfortable basic mathematical concepts assume graduate degree mathematical sciences stance almost completely avoided use matrix algebra possible understand entire book without detailed knowledge matrices vectors presume reader interested applying statistical learn ing methods real world problems order facilitate well motivate techniques discussed devoted section within chapter computer labs lab walk reader realistic application methods considered chapter taught material courses allocated roughly one third classroom time working labs found extremely useful many less computationally oriented students ini tially intimidated command level interface got hang things course quarter semester used freely available powerful enough implement methods discussed book also optional packages downloaded implement literally thousands addi tional methods importantly language choice academic statisticians new approaches often become available introduction years implemented commercial packages ever labs isl self contained skipped reader wishes use diﬀerent software package wish apply methods discussed real world problems 
[introduction, read, book] book intended anyone interested using modern statis tical methods modeling prediction data group includes scientists engineers data analysts quants also less technical indi viduals degrees non quantitative ﬁelds social sciences business expect reader least one elementary course statistics background linear regression also useful though required since review key concepts behind linear regression chapter mathematical level book modest detailed knowledge matrix operations required book provides troduction statistical programming language previous exposure programming language matlab python useful required successfully taught material level master phd students business computer science biology earth sciences psychology many areas physical social sciences book could also appropriate advanced undergraduates already taken course linear regression context mathematically rigorous course esl serves primary textbook isl could used supplementary text teaching computational aspects various approaches 
[introduction, notation, simple, matrix, algebra] choosing notation textbook always diﬃcult task part adopt notational conventions esl use represent number distinct data points observa tions sample let denote number variables available use making predictions example wage data set con sists variables people observations book indicate variable names using colored font variable name examples might quite large order thou sands even millions situation arises quite often example analysis modern biological data web based advertising data variables year age note throughout sex introduction general let represent value variable observation throughout book used index samples observations used index variables let denote matrix whose element readers unfamiliar matrices useful visualize spreadsheet numbers rows columns times interested rows write vector length containing variable measurements observation vectors default represented columns example wage data values individual times instead interested columns write vector length example wage data contains values year using notation matrix written vector length consisting year age sex introduction notation denotes transpose matrix vector example use denote observation variable wish make predictions wage hence write set observations vector form observed data consists vector length simply scalar text vector length always denoted lower case bold however vectors length feature vectors length denoted lower case normal font scalars also denoted lower case normal font rare cases two uses lower case normal font lead ambiguity clarify use intended matrices denoted using bold capitals random variables denoted using capital normal font regardless dimensions occasionally want indicate dimension particular ject indicate object scalar use notation indicate vector length use length indicate object matrix using avoided using matrix algebra whenever possible however instances becomes cumbersome avoid entirely rare instances important understand concept multiplying two matrices suppose product introduction denoted element computed multiplying element row corresponding element column example consider note operation produces matrix possible compute number columns number rows 
[introduction, organization, book] chapter introduces basic terminology concepts behind statistical learning chapter also presents nearest neighbor classiﬁer simple method works surprisingly well many problems chap ters cover classical linear methods regression classiﬁcation particular chapter reviews linear regression fundamental start ing point regression methods chapter discuss two important classical classiﬁcation methods logistic regression lin ear discriminant analysis central problem statistical learning situations involves choosing best method given application hence chapter intro duce cross validation bootstrap used estimate accuracy number diﬀerent methods order choose best one much recent research statistical learning concentrated non linear methods however linear methods often advantages non linear competitors terms interpretability sometimes also accuracy hence chapter consider host linear methods classical modern oﬀer potential improvements stan dard linear regression include stepwise selection ridge regression principal components regression partial least squares lasso remaining chapters move world non linear statistical learning ﬁrst introduce chapter number non linear methods work well problems single input variable show methods used non linear additive models one input chapter investigate tree based methods including bagging boosting random forests support vector machines set approaches performing linear non linear classiﬁcation introduction discussed chapter finally chapter consider setting input variables output variable particular present principal components analysis means clustering hierarchi cal clustering end chapter present one lab sections systematically work applications various meth ods discussed chapter labs demonstrate strengths weaknesses various approaches also provide useful reference syntax required implement various methods reader may choose work labs pace labs may focus group sessions part classroom environment within lab present results obtained performed lab time writing book however new versions continuously released time packages called labs updated therefore future possible results shown lab sections may longer correspond precisely results obtained reader performs labs necessary post updates labs book website use symbol denote sections exercises contain challenging concepts easily skipped readers wish delve deeply material lack mathematical background 
[introduction, data, sets, used, labs, exercises] textbook illustrate statistical learning methods using applications marketing ﬁnance biology areas islr package available book website contains number data sets required order perform labs exercises associated book one data set contained mass library yet another part base distribution table contains summary data sets required perform labs exercises couple data sets also available text ﬁles book website use chapter 
[introduction, book, website] website book located www statlearning com introduction name description auto gas mileage horsepower information cars boston housing values information boston suburbs caravan information individuals oﬀered caravan insurance carseats information car seat sales stores college demographic characteristics tuition usa colleges default customer default records credit card company hitters records salaries baseball players khan gene expression measurements four cancer types nci gene expression measurements cancer cell lines sales information citrus hill minute maid orange juice portfolio past values ﬁnancial assets use portfolio allocation smarket daily percentage returns samp year period usarrests crime statistics per residents states usa wage income survey data males central atlantic region usa weekly weekly stock market returns years table list data sets needed perform labs exercises textbook data sets available islr library exception boston part mass usarrests part base distribution contains number resources including package associated book additional data sets
[introduction, overview, statistical, learning] statistical learning refers vast set tools understanding data tools classiﬁed supervised unsupervised broadly speaking supervised statistical learning involves building statistical model pre dicting estimating output based one inputs problems nature occur ﬁelds diverse business medicine astrophysics public policy unsupervised statistical learning inputs supervising output nevertheless learn relationships struc ture data provide illustration applications statistical learning brieﬂy discuss three real world data sets considered book 
[introduction, overview, statistical, learning, wage, data] application refer wage data set throughout book examine number factors relate wages group males atlantic region united states particular wish understand association employee age education well calendar year wage consider example left hand panel figure displays wage versus age individu als data set evidence wage increases age decreases approximately age blue line provides estimate average wage given age makes trend clearer james introduction statistical learning applications springer texts statistics doi springer science business media new york introduction age year education level figure wage data contains income survey information males central atlantic region united states left wage function age average wage increases age years age point begins decline center wage function year slow steady increase approximately average wage right boxplots displaying wage function education indicating lowest level high school diploma highest level advanced graduate degree average wage increases level education given employee age use curve predict wage however also clear figure signiﬁcant amount vari ability associated average value age alone unlikely provide accurate prediction particular man wage also information regarding employee education level year wage earned center right hand panels figure display wage function year education dicate factors associated wage wages increase approximately roughly linear straight line fashion though rise slight relative vari ability data wages also typically greater individuals higher education levels men lowest education level tend substantially lower wages highest education level clearly accurate prediction given man wage obtained combining age education year chapter discuss linear regression used predict wage data set ideally predict wage way accounts non linear relationship wage age chapter discuss class approaches addressing problem 
[introduction, overview, statistical, learning, stock, market, data] wage data involves predicting continuous quantitative output value often referred regression problem however certain cases may instead wish predict non numerical value categorical introduction yesterday today direction ercentage change samp two days previous ercentage change samp today direction today direction three days previous ercentage change samp figure left boxplots previous day percentage change samp index days market increased decreased obtained smarket data center right left panel percentage changes days previous shown qualitative output example chapter examine stock mar ket data set contains daily movements standard amp poor samp stock index year period refer smarket data goal predict whether index increase decrease given day using past days percentage changes index statistical learning problem volve predicting numerical value instead involves predicting whether given day stock market performance fall bucket bucket known classiﬁcation problem model could accurately predict direction market move would useful left hand panel figure displays two boxplots previous day percentage changes stock index one days market increased subsequent day one days market decreased two plots look almost identical suggest ing simple strategy using yesterday movement samp predict today returns remaining panels display box plots percentage changes days previous today similarly indicate little association past present returns course lack pattern expected presence strong correlations tween successive days returns one could adopt simple trading strategy generate proﬁts market nevertheless chapter explore data using several diﬀerent statistical learning methods interestingly hints weak trends data suggest least year period possible correctly predict direction movement market approximately time figure introduction today direction predicted probability figure quadratic discriminant analysis model subset smarket data corresponding time period predicted probability stock market decrease using data average predicted probability decrease higher days market decrease based results able correctly predict direction movement market time 
[introduction, overview, statistical, learning, gene, expression, data] previous two applications illustrate data sets input output variables however another important class problems involves situations observe input variables corresponding output example marketing setting might demographic information number current potential customers may wish understand types customers similar grouping individuals according observed characteristics known clustering problem unlike previous examples trying predict output variable devote chapter discussion statistical learning methods problems natural output variable available consider nci data set consists gene expression measurements cancer cell lines instead predicting particular output variable interested determining whether groups clusters among cell lines based gene expression measurements diﬃcult question address part thousands gene expression measurements per cell line making hard visualize data left hand panel figure addresses problem represent ing cell lines using two numbers ﬁrst two principal components data summarize expression measurements cell line two numbers dimensions likely dimension reduction resulted introduction figure left representation nci gene expression data set two dimensional space point corresponds one cell lines appear four groups cell lines represented using diﬀerent colors right left panel except represented diﬀerent types cancer using diﬀerent colored symbol cell lines corresponding cancer type tend nearby two dimensional space loss information possible visually examine data evidence clustering deciding number clusters often diﬃ cult problem left hand panel figure suggests least four groups cell lines represented using separate colors examine cell lines within cluster similarities types cancer order better understand relationship gene expression levels cancer particular data set turns cell lines correspond diﬀerent types cancer however information used create left hand panel figure right hand panel fig ure identical left hand panel except cancer types shown using distinct colored symbols clear evidence cell lines cancer type tend located near two dimensional representation addition even though cancer infor mation used produce left hand panel clustering obtained bear resemblance actual cancer types observed right hand panel provides independent veriﬁcation accuracy clustering analysis 
[introduction, brief, history, statistical, learning] though term statistical learning fairly new many concepts underlie ﬁeld developed long ago beginning nineteenth century legendre gauss published papers method introduction least squares implemented earliest form known linear regression approach ﬁrst successfully applied problems astronomy linear regression used predicting quantitative values individual salary order predict qualitative values whether patient survives dies whether stock market increases decreases fisher proposed linear discriminant analysis various authors put forth alternative approach logistic regression early nelder wedderburn coined term generalized linear models entire class statistical learning methods include linear logistic regression special cases end many techniques learning data available however almost exclusively linear methods cause ﬁtting non linear relationships computationally infeasible time computing technology ﬁnally improved suﬃciently non linear methods longer computationally prohibitive mid breiman friedman olshen stone introduced classiﬁcation regression trees among ﬁrst demonstrate power detailed practical implementation method including cross validation model selection hastie tibshirani coined term generalized addi tive models class non linear extensions generalized linear models also provided practical software implementation since time inspired advent machine learning disciplines statistical learning emerged new subﬁeld statistics focused supervised unsupervised modeling prediction recent years progress statistical learning marked increasing availability powerful relatively user friendly software popular freely available system potential continue transformation ﬁeld set techniques used developed statisticians computer scientists essential toolkit much broader community 
[introduction, book] elements statistical learning esl hastie tibshirani friedman ﬁrst published since time become important reference fundamentals statistical machine learning success derives comprehensive detailed treatment many important topics statistical learning well fact relative many upper level statistics textbooks accessible wide audience however greatest factor behind success esl topical nature time publication interest ﬁeld statistical introduction learning starting explode esl provided one ﬁrst accessible comprehensive introductions topic since esl ﬁrst published ﬁeld statistical learning con tinued ﬂourish ﬁeld expansion taken two forms obvious growth involved development new improved statis tical learning approaches aimed answering range scientiﬁc questions across number ﬁelds however ﬁeld statistical learning also expanded audience increases computational power generated surge interest ﬁeld non statisticians eager use cutting edge statistical tools analyze data unfortu nately highly technical nature approaches meant user community remained primarily restricted experts statistics computer science related ﬁelds training time understand implement recent years new improved software packages signiﬁcantly eased implementation burden many statistical learning methods time growing recognition across number ﬁelds business health care genetics social sciences beyond statistical learning powerful tool important practical applications result ﬁeld moved one primarily academic interest mainstream discipline enormous potential audience trend surely continue increasing availability enormous quantities data software analyze purpose introduction statistical learning isl facili tate transition statistical learning academic mainstream ﬁeld isl intended replace esl far comprehen sive text terms number approaches considered depth explored consider esl important companion professionals graduate degrees statistics machine learning related ﬁelds need understand technical details behind statistical learning approaches however community users statistical learning techniques expanded include individuals wider range interests backgrounds therefore believe place less technical accessible version esl teaching topics years discovered interest master phd students ﬁelds disparate business administration biology computer science well quantitatively oriented upper division undergraduates important diverse group able understand models intuitions strengths weaknesses various approaches audience many technical details behind statistical learning methods optimiza tion algorithms theoretical properties primary interest believe students need deep understanding aspects order become informed users various methodologies introduction order contribute chosen ﬁelds use statistical learning tools islr based following four premises many statistical learning methods relevant useful wide range academic non academic disciplines beyond sta tistical sciences believe many contemporary statistical learn ing procedures become widely available used currently case classical methods linear regres sion result rather attempting consider every possible approach impossible task concentrated presenting methods believe widely applicable statistical learning viewed series black boxes single approach perform well possible applications understanding cogs inside box interaction cogs impossible select best box hence attempted carefully describe model intuition assump tions trade oﬀs behind methods consider important know job performed cog necessary skills construct machine inside box thus minimized discussion technical details related ﬁtting procedures theoretical properties assume reader comfortable basic mathematical concepts assume graduate degree mathematical sciences stance almost completely avoided use matrix algebra possible understand entire book without detailed knowledge matrices vectors presume reader interested applying statistical learn ing methods real world problems order facilitate well motivate techniques discussed devoted section within chapter computer labs lab walk reader realistic application methods considered chapter taught material courses allocated roughly one third classroom time working labs found extremely useful many less computationally oriented students ini tially intimidated command level interface got hang things course quarter semester used freely available powerful enough implement methods discussed book also optional packages downloaded implement literally thousands addi tional methods importantly language choice academic statisticians new approaches often become available introduction years implemented commercial packages ever labs isl self contained skipped reader wishes use diﬀerent software package wish apply methods discussed real world problems 
[introduction, read, book] book intended anyone interested using modern statis tical methods modeling prediction data group includes scientists engineers data analysts quants also less technical indi viduals degrees non quantitative ﬁelds social sciences business expect reader least one elementary course statistics background linear regression also useful though required since review key concepts behind linear regression chapter mathematical level book modest detailed knowledge matrix operations required book provides troduction statistical programming language previous exposure programming language matlab python useful required successfully taught material level master phd students business computer science biology earth sciences psychology many areas physical social sciences book could also appropriate advanced undergraduates already taken course linear regression context mathematically rigorous course esl serves primary textbook isl could used supplementary text teaching computational aspects various approaches 
[introduction, notation, simple, matrix, algebra] choosing notation textbook always diﬃcult task part adopt notational conventions esl use represent number distinct data points observa tions sample let denote number variables available use making predictions example wage data set con sists variables people observations book indicate variable names using colored font variable name examples might quite large order thou sands even millions situation arises quite often example analysis modern biological data web based advertising data variables year age note throughout sex introduction general let represent value variable observation throughout book used index samples observations used index variables let denote matrix whose element readers unfamiliar matrices useful visualize spreadsheet numbers rows columns times interested rows write vector length containing variable measurements observation vectors default represented columns example wage data values individual times instead interested columns write vector length example wage data contains values year using notation matrix written vector length consisting year age sex introduction notation denotes transpose matrix vector example use denote observation variable wish make predictions wage hence write set observations vector form observed data consists vector length simply scalar text vector length always denoted lower case bold however vectors length feature vectors length denoted lower case normal font scalars also denoted lower case normal font rare cases two uses lower case normal font lead ambiguity clarify use intended matrices denoted using bold capitals random variables denoted using capital normal font regardless dimensions occasionally want indicate dimension particular ject indicate object scalar use notation indicate vector length use length indicate object matrix using avoided using matrix algebra whenever possible however instances becomes cumbersome avoid entirely rare instances important understand concept multiplying two matrices suppose product introduction denoted element computed multiplying element row corresponding element column example consider note operation produces matrix possible compute number columns number rows 
[introduction, organization, book] chapter introduces basic terminology concepts behind statistical learning chapter also presents nearest neighbor classiﬁer simple method works surprisingly well many problems chap ters cover classical linear methods regression classiﬁcation particular chapter reviews linear regression fundamental start ing point regression methods chapter discuss two important classical classiﬁcation methods logistic regression lin ear discriminant analysis central problem statistical learning situations involves choosing best method given application hence chapter intro duce cross validation bootstrap used estimate accuracy number diﬀerent methods order choose best one much recent research statistical learning concentrated non linear methods however linear methods often advantages non linear competitors terms interpretability sometimes also accuracy hence chapter consider host linear methods classical modern oﬀer potential improvements stan dard linear regression include stepwise selection ridge regression principal components regression partial least squares lasso remaining chapters move world non linear statistical learning ﬁrst introduce chapter number non linear methods work well problems single input variable show methods used non linear additive models one input chapter investigate tree based methods including bagging boosting random forests support vector machines set approaches performing linear non linear classiﬁcation introduction discussed chapter finally chapter consider setting input variables output variable particular present principal components analysis means clustering hierarchi cal clustering end chapter present one lab sections systematically work applications various meth ods discussed chapter labs demonstrate strengths weaknesses various approaches also provide useful reference syntax required implement various methods reader may choose work labs pace labs may focus group sessions part classroom environment within lab present results obtained performed lab time writing book however new versions continuously released time packages called labs updated therefore future possible results shown lab sections may longer correspond precisely results obtained reader performs labs necessary post updates labs book website use symbol denote sections exercises contain challenging concepts easily skipped readers wish delve deeply material lack mathematical background 
[introduction, data, sets, used, labs, exercises] textbook illustrate statistical learning methods using applications marketing ﬁnance biology areas islr package available book website contains number data sets required order perform labs exercises associated book one data set contained mass library yet another part base distribution table contains summary data sets required perform labs exercises couple data sets also available text ﬁles book website use chapter 
[introduction, book, website] website book located www statlearning com introduction name description auto gas mileage horsepower information cars boston housing values information boston suburbs caravan information individuals oﬀered caravan insurance carseats information car seat sales stores college demographic characteristics tuition usa colleges default customer default records credit card company hitters records salaries baseball players khan gene expression measurements four cancer types nci gene expression measurements cancer cell lines sales information citrus hill minute maid orange juice portfolio past values ﬁnancial assets use portfolio allocation smarket daily percentage returns samp year period usarrests crime statistics per residents states usa wage income survey data males central atlantic region usa weekly weekly stock market returns years table list data sets needed perform labs exercises textbook data sets available islr library exception boston part mass usarrests part base distribution contains number resources including package associated book additional data sets
[statistical, learning, what, statistical, learning] order motivate study statistical learning begin simple example suppose statistical consultants hired client provide advice improve sales particular product advertising data set consists sales product diﬀerent markets along advertising budgets product markets three diﬀerent media radio newspaper data displayed figure possible client directly increase sales product hand control advertising expenditure three media therefore determine association advertising sales instruct client adjust advertising budgets thereby indirectly increasing sales words goal develop accurate model used predict sales basis three media budgets setting advertising budgets input variables sales input variable output variable input variables typically denoted using output variable symbol subscript distinguish might budget radio budget newspaper budget inputs diﬀerent names predictors independent variables features predictor independent variable feature sometimes variables output variable case sales variable often called response dependent variable typically denoted response dependent variable using symbol throughout book use terms interchangeably james introduction statistical learning applications springer texts statistics doi springer science business media new york statistical learning sales radio sales newspaper sales figure advertising data set plot displays sales thousands units function radio newspaper budgets thousands dollars diﬀerent markets plot show simple least squares sales variable described chapter words blue line represents simple model used predict sales using radio newspaper respectively generally suppose observe quantitative response diﬀerent predictors assume relationship written general form ﬁxed unknown function random error term independent mean zero formula error term tion represents systematic information provides systematic another example consider left hand panel figure plot income versus years education individuals income data set plot suggests one might able predict income using years education however function connects input variable output variable general unknown situation one must estimate based observed points since income simulated data set known shown blue curve right hand panel figure vertical lines represent error terms note observations lie blue curve lie overall errors approximately mean zero general function may involve one input variable figure plot income function years education seniority two dimensional surface must estimated based observed data statistical learning years education income years education income figure income data set left red dots observed values income tens thousands dollars years education indi viduals right blue curve represents true underlying relationship income years education generally unknown known case data simulated black lines represent error associated observation note errors positive servation lies blue curve negative observation lies curve overall errors approximately mean zero essence statistical learning refers set approaches estimating chapter outline key theoretical concepts arise estimating well tools evaluating estimates obtained 
[statistical, learning, what, statistical, learning, estimate] throughout book explore many linear non linear approaches estimating however methods generally share certain charac teristics provide overview shared characteristics section always assume observed set diﬀerent data points example figure observed data points observations called training data use training data observations train teach method estimate let represent value predictor input observation correspondingly let represent response variable observation training data consist goal apply statistical learning method training data order estimate unknown function words want ﬁnd function observation broadly speaking statistical learning methods task character ized either parametric non parametric brieﬂy discuss parametric non parametric two types approaches parametric methods parametric methods involve two step model based approach first make assumption functional form shape example one simple assumption linear linear model discussed extensively chap ter assumed linear problem estimat ing greatly simpliﬁed instead estimate entirely arbitrary dimensional function one needs estimate coeﬃcients model selected need procedure uses training data train model case linear model train need estimate parameters want ﬁnd values parameters common approach ﬁtting model referred ordinary least squares discuss chapter however least squares least squares one many possible ways linear model chapter discuss approaches estimating parameters model based approach described referred parametric reduces problem estimating one estimating set statistical learning ars education senior ity income figure linear model least squares income data fig ure observations shown red yellow plane indicates least squares data parameters assuming parametric form simpliﬁes problem estimating generally much easier estimate set rameters linear model entirely arbitrary function potential disadvantage paramet ric approach model choose usually match true unknown form chosen model far true estimate poor try address problem choos ing ﬂexible models many diﬀerent possible functional forms ﬂexible general ﬁtting ﬂexible model requires estimating greater number parameters complex models lead phenomenon known overﬁtting data essentially means overﬁtting follow errors noise closely issues discussed noise book figure shows example parametric approach applied income data figure linear model form income education seniority since assumed linear relationship response two predictors entire ﬁtting problem reduces estimating using least squares linear regression comparing figure figure see linear given figure quite right true curvature captured linear however linear still appears reasonable job capturing positive relationship years education income well statistical learning years education senio rity income figure smooth thin plate spline income data figure shown yellow observations displayed red splines discussed chapter slightly less positive relationship seniority income may small number observations best non parametric methods non parametric methods make explicit assumptions func tional form instead seek estimate gets close data points possible without rough wiggly approaches major advantage parametric approaches avoiding assumption particular functional form potential accurately wider range possible shapes parametric approach brings possibility functional form used estimate diﬀerent true case resulting model data well contrast non parametric approaches completely avoid danger since essentially assumption form made non parametric approaches suﬀer major disadvantage since reduce problem estimating small number parameters large number observations far typically needed parametric approach required order obtain accurate estimate example non parametric approach ﬁtting income data shown figure thin plate spline used estimate thin plate spline proach impose pre speciﬁed model instead attempts produce estimate close possible observed data subject yellow surface figure eing statistical learning ars education senior ity income figure rough thin plate spline income data figure makes zero errors training data smooth case non parametric produced remarkably curate estimate true shown figure order thin plate spline data analyst must select level smoothness figure shows thin plate spline using lower level smoothness allowing rougher resulting estimate ﬁts observed data perfectly however spline shown figure far variable true function figure example overﬁtting data discussed previously undesirable situation obtained yield accurate estimates response new observations part original training data set dis cuss methods choosing correct amount smoothness chapter splines discussed chapter seen advantages disadvantages parametric non parametric methods statistical learning explore types methods throughout book 
[statistical, learning, what, statistical, learning, trade-oﬀ, prediction, accuracy, model, interpretability] many methods examine book less ﬂexible restrictive sense produce relatively small range shapes estimate example linear regression relatively inﬂexible approach generate linear functions lines shown figure plane shown figure statistical learning flexibility inter pretability low high high low subset selection lasso least squares generalized additive models trees bagging boosting support vector machines figure representation tradeoﬀ ﬂexibility inter pretability using diﬀerent statistical learning methods general ﬂexibil ity method increases interpretability decreases methods thin plate splines shown figures considerably ﬂexible generate much wider range possible shapes estimate one might reasonably ask following question would ever choose use restrictive method instead ﬂexible approach several reasons might prefer restrictive model mainly interested inference restrictive models much interpretable instance inference goal linear model may good choice since quite easy understand relationship contrast ﬂexible approaches splines discussed chapter displayed figures boosting methods discussed chapter lead complicated estimates diﬃcult understand individual predictor associated response figure provides illustration trade ﬂexibility interpretability methods cover book least squares linear regression discussed chapter relatively inﬂexible quite interpretable lasso discussed chapter relies upon lasso linear model uses alternative ﬁtting procedure estimating coeﬃcients new procedure restrictive timating coeﬃcients sets number exactly zero hence sense lasso less ﬂexible approach linear regression also interpretable linear regression ﬁnal model response variable related small subset predictors namely nonzero coeﬃcient estimates generalized statistical learning additive models gams discussed chapter instead extend lin generalized additive model ear model allow certain non linear relationships consequently gams ﬂexible linear regression also somewhat less interpretable linear regression relationship predictor response modeled using curve finally fully non linear methods bagging boosting support vector machines bagging boosting non linear kernels discussed chapters highly ﬂexible support vector machine approaches harder interpret established inference goal clear vantages using simple relatively inﬂexible statistical learning meth ods settings however interested prediction interpretability predictive model simply interest instance seek develop algorithm predict price stock sole requirement algorithm predict accurately interpretability concern setting might expect best use ﬂexible model available surprisingly always case often obtain accurate predictions using less ﬂexible method phenomenon may seem counterintuitive ﬁrst glance potential overﬁtting highly ﬂexible methods saw example overﬁtting figure discuss important concept section throughout book 
[statistical, learning, what, statistical, learning, supervised, versus, unsupervised, learning] statistical learning problems fall one two categories supervised supervised unsupervised examples discussed far chap unsupervised ter fall supervised learning domain observation predictor measurement associated response measurement wish model relates response predictors aim accurately predicting response future observations prediction better understanding relationship response predictors inference many classical statistical learn ing methods linear regression logistic regression chapter logistic regression well modern approaches gam boosting support vec tor machines operate supervised learning domain vast majority book devoted setting contrast unsupervised learning describes somewhat chal lenging situation every observation observe vector measurements associated response pos sible linear regression model since response variable predict setting sense working blind sit uation referred unsupervised lack response vari able supervise analysis sort statistical analysis statistical learning figure clustering data set involving three groups group shown using diﬀerent colored symbol left three groups well separated setting clustering approach successfully identify three groups right overlap among groups clustering task challenging possible seek understand relationships variables observations one statistical learning tool may use setting cluster analysis clustering goal cluster analysis cluster analysis ascertain basis whether observations fall relatively distinct groups example market segmentation study might observe multiple characteristics variables potential customers zip code family income shopping habits might believe customers fall diﬀerent groups big spenders versus low spenders information customer spending patterns available supervised analysis would possible however information available know whether poten tial customer big spender setting try cluster customers basis variables measured order identify distinct groups potential customers identifying groups interest might groups diﬀer respect property interest spending habits figure provides simple illustration clustering problem plotted observations measurements two variables observation corresponds one three distinct groups illustrative purposes plotted members group using diﬀerent colors symbols however practice group memberships unknown goal determine group servation belongs left hand panel figure relatively easy task groups well separated contrast right hand panel illustrates challenging problem overlap statistical learning groups clustering method could expected assign overlapping points correct group blue green orange examples shown figure two variables one simply visually inspect scatterplots observations order identify clusters however practice often encounter data sets contain many two variables case cannot easily plot observations instance variables data set distinct scatterplots made visual inspection simply viable way identify clusters reason automated clustering methods important discuss clustering unsupervised learning approaches chapter many problems fall naturally supervised unsupervised learn ing paradigms however sometimes question whether analysis considered supervised unsupervised less clear cut stance suppose set observations observa tions predictor measurements response measurement remaining observations predictor measurements response measurement scenario arise predictors measured relatively cheaply corresponding responses much expensive collect refer setting semi supervised learning problem setting wish use sta semi supervised learning tistical learning method incorporate observations response measurements available well observations although interesting topic beyond scope book 
[statistical, learning, what, statistical, learning, regression, versus, classiﬁcation, problems] variables characterized either quantitative qualitative also quantitative qualitative known categorical quantitative variables take numerical values categorical examples include person age height income value house price stock contrast qualitative variables take val ues one diﬀerent classes categories examples qualitative class variables include person gender male female brand prod uct purchased brand whether person defaults debt yes cancer diagnosis acute myelogenous leukemia acute lymphoblastic leukemia leukemia tend refer problems quantitative response regression problems involv regression ing qualitative response often referred classiﬁcation problems classiﬁcation however distinction always crisp least squares linear gression chapter used quantitative response whereas logistic regression chapter typically used qualitative two class binary response often used classiﬁcation method binary since estimates class probabilities thought regression assessing model accuracy method well statistical methods nearest neighbors chapters boosting chapter used case either quantitative qualitative responses tend select statistical learning methods basis whether response quantitative qualitative might use linear regres sion quantitative logistic regression qualitative however whether predictors qualitative quantitative generally consid ered less important statistical learning methods discussed book applied regardless predictor variable type provided qualitative predictors properly coded analysis performed discussed chapter 
[statistical, learning, assessing, model, accuracy] one key aims book introduce reader wide range statistical learning methods extend far beyond standard linear regression approach necessary introduce many diﬀerent statistical learning approaches rather single best method free lunch statistics one method dominates others possible data sets particular data set one speciﬁc method may work best method may work better similar diﬀerent data set hence important task decide given set data method produces best results selecting best approach one challenging parts performing statistical learning practice section discuss important concepts arise selecting statistical learning procedure speciﬁc data set book progresses explain concepts presented applied practice 
[statistical, learning, assessing, model, accuracy, measuring, quality, fit] order evaluate performance statistical learning method given data set need way measure well predictions actually match observed data need quantify extent predicted response value given observation close true response value observation regression setting commonly used measure mean squared error mse given mean squared error statistical learning prediction gives observation mse small predicted responses close true responses large observations predicted true responses diﬀer substantially mse computed using training data used model accurately referred training mse general really care well method works training mse training data rather interested accuracy pre dictions obtain apply method previously unseen test data care suppose interested test data developing algorithm predict stock price based previous stock returns train method using stock returns past months really care well method predicts last week stock price instead care well predict tomorrow price next month price similar note suppose clinical measurements weight blood pressure height age family history disease number patients well information whether patient diabetes use patients train statistical learn ing method predict risk diabetes based clinical measurements practice want method accurately predict diabetes risk future patients based clinical measurements interested whether method accurately predicts diabetes risk patients used train model since already know patients diabetes state mathematically suppose statistical learn ing method training observations obtain estimate compute approximately equal training mse given small however really interested whether instead want know whether approximately equal previously unseen test observation used train statistical learning method want choose method gives lowest test mse opposed lowest training mse words test mse large number test observations could compute ave average squared prediction error test observations like select model average quantity test mse small possible trying select method minimizes test mse settings may test data set available may access set observations used train statistical learning method simply evaluate test observations select learning method test mse assessing model accuracy flexibility mean squared error figure left data simulated shown black three estimates shown linear regression line orange curve two smoothing spline ﬁts blue green curves right training mse grey curve test mse red curve minimum possible test mse methods dashed line squares represent training test mses three ﬁts shown left hand panel smallest test observations available case one might imagine simply selecting statistical learning method minimizes training mse seems like might sensible approach since training mse test mse appear closely related unfortunately fundamental problem strategy guarantee method lowest training mse also lowest test mse roughly speaking problem many statistical methods speciﬁcally estimate coeﬃcients minimize training set mse methods training set mse quite small test mse often much larger figure illustrates phenomenon simple example left hand panel figure true given black curve orange blue green curves illus trate three possible estimates obtained using methods increasing levels ﬂexibility orange line linear regression rela tively inﬂexible blue green curves produced using smoothing splines discussed chapter diﬀerent levels smoothness smoothing spline clear level ﬂexibility increases curves observed data closely green curve ﬂexible matches data well however observe ﬁts true shown black poorly wiggly adjusting level ﬂexibility smoothing spline produce many diﬀerent ﬁts data statistical learning move right hand panel figure grey curve displays average training mse function ﬂexibility mally degrees freedom number smoothing splines degrees freedom grees freedom quantity summarizes ﬂexibility curve discussed fully chapter orange blue green squares indicate mses associated corresponding curves left hand panel restricted hence smoother curve fewer degrees freedom wiggly curve note figure linear regression restrictive end two degrees freedom training mse declines monotonically ﬂexibility increases example true non linear orange linear ﬂexible enough estimate well green curve lowest training mse three methods since corresponds ﬂexible three curves left hand panel example know true function also com pute test mse large test set function ﬂexibility course general unknown possible test mse displayed using red curve right hand panel figure training mse test mse initially declines level ﬂex ibility increases however point test mse levels starts increase consequently orange green curves high test mse blue curve minimizes test mse surprising given visually appears estimate best left hand panel figure horizontal dashed line indicates var irreducible error corresponds lowest achievable test mse among possible methods hence smoothing spline repre sented blue curve close optimal right hand panel figure ﬂexibility statistical learning method increases observe monotone decrease training mse shape test mse fundamental property statistical learning holds regardless particular data set hand regardless statistical method used model ﬂexibility increases training mse decrease test mse may given method yields small training mse large test mse said overﬁtting data happens statistical learning procedure working hard ﬁnd patterns training data may picking patterns caused random chance rather true properties unknown function overﬁt training data test mse large supposed patterns method found training data simply exist test data note regardless whether overﬁtting occurred almost always expect training mse smaller test mse statistical learning methods either directly indirectly seek minimize training mse overﬁtting refers speciﬁcally case less ﬂexible model would yielded smaller test mse assessing model accuracy flexibility mean squared error figure details figure using diﬀerent true much closer linear setting linear regression provides good data figure provides another example true approxi mately linear observe training mse decreases mono tonically model ﬂexibility increases shape test mse however truth close linear test mse decreases slightly increasing orange least squares substantially better highly ﬂexible green curve nally figure displays example highly non linear training test mse curves still exhibit general patterns rapid decrease curves test mse starts increase slowly practice one usually compute training mse relative ease estimating test mse considerably diﬃcult usually test data available previous three examples illustrate ﬂexibility level corresponding model minimal test mse vary considerably among data sets throughout book discuss variety approaches used practice estimate minimum point one important method cross validation chapter cross validation method estimating test mse using training data 
[statistical, learning, assessing, model, accuracy, bias-variance, trade-oﬀ] shape observed test mse curves figures turns result two competing properties statistical learning methods though mathematical proof beyond scope book possible show expected test mse given value statistical learning flexibility mean squared error figure details figure using diﬀerent far linear setting linear regression provides poor data always decomposed sum three fundamental quantities variance squared bias variance error variance bias terms var bias var notation deﬁnes expected test mse refers expected test mse average test mse would obtain repeatedly estimated using large number training sets tested overall expected test mse computed averaging possible values test set equation tells order minimize expected test error need select statistical learning method simultaneously achieves low variance low bias note variance inherently nonnegative quantity squared bias also nonnegative hence see expected test mse never lie var irreducible error mean variance bias statistical learning method variance refers amount would change estimated using diﬀerent training data set since training data used statistical learning method diﬀerent training data sets result diﬀerent ideally estimate vary much training sets however method high variance small changes training data result large changes general ﬂexible statistical methods higher variance consider assessing model accuracy green orange curves figure ﬂexible green curve following observations closely high variance changing one data points may cause estimate change considerably contrast orange least squares line relatively inﬂexible low variance moving single observation likely cause small shift position line hand bias refers error introduced approxi mating real life problem may extremely complicated much simpler model example linear regression assumes linear relationship unlikely real life problem truly simple linear relationship performing lin ear regression undoubtedly result bias estimate figure true substantially non linear matter many training observations given possible produce accurate estimate using linear regression words linear regression results high bias example however figure true close linear given enough data possible linear regression produce accurate estimate generally ﬂexible methods result less bias general rule use ﬂexible methods variance increase bias decrease relative rate change two quantities determines whether test mse increases decreases increase ﬂexibility class methods bias tends initially decrease faster variance increases consequently expected test mse declines however point increasing ﬂexibility little impact bias starts signiﬁcantly increase variance happens test mse increases note observed pattern decreasing test mse followed increasing test mse right hand panels figures three plots figure illustrate equation examples figures case blue solid curve represents squared bias diﬀerent levels ﬂexibility orange curve corresponds variance horizontal dashed line represents var irreducible error finally red curve corresponding test set mse sum three quantities three cases variance increases bias decreases method ﬂexibility increases however ﬂexibility level corresponding optimal test mse diﬀers considerably among three data sets squared bias variance change diﬀerent rates data sets left hand panel figure bias initially decreases rapidly resulting initial sharp decrease expected test mse hand center panel figure true close linear small decrease bias ﬂex ibility increases test mse declines slightly increasing rapidly variance increases finally right hand panel fig ure ﬂexibility increases dramatic decline bias statistical learning flexibility flexibility flexibility mse bias var figure squared bias blue curve variance orange curve var dashed line test mse red curve three data sets figures vertical dotted line indicates ﬂexibility level corresponding smallest test mse true non linear also little increase variance ﬂexibility increases consequently test mse declines substantially experiencing small increase model ﬂexibility increases relationship bias variance test set mse given equa tion displayed figure referred bias variance trade good test set performance statistical learning method bias variance trade quires low variance well low squared bias referred trade easy obtain method extremely low bias high variance instance drawing curve passes every single training observation method low variance high bias ﬁtting horizontal line data challenge lies ﬁnding method variance squared bias low trade one important recurring themes book real life situation unobserved generally pos sible explicitly compute test mse bias variance statistical learning method nevertheless one always keep bias variance trade mind book explore methods extremely ﬂexible hence essentially eliminate bias however guarantee outperform much simpler method linear regression take extreme example suppose true linear situation linear regression bias making hard ﬂexible method compete contrast true highly non linear ample number training observations may better using highly ﬂexible approach figure chapter discuss cross validation way estimate test mse using training data assessing model accuracy 
[statistical, learning, assessing, model, accuracy, classiﬁcation, setting] thus far discussion model accuracy focused regres sion setting many concepts encountered bias variance trade transfer classiﬁcation setting modiﬁcations due fact longer numer ical suppose seek estimate basis training obser vations qualitative common approach quantifying accuracy estimate training error rate proportion mistakes made apply error rate estimate training observations predicted class label observation using indicator variable equals zero indicator variable observation classiﬁed correctly classiﬁcation method otherwise misclassiﬁed hence equation computes fraction incorrect classiﬁcations equation referred training error rate com training error puted based data used train classiﬁer regression setting interested error rates result applying classiﬁer test observations used training test error rate associated set test observations form test error given ave predicted class label results applying classiﬁer test observation predictor good classiﬁer one test error smallest bayes classiﬁer possible show though proof outside scope book test error rate given minimized average simple classiﬁer assigns observation likely class given predictor values words simply assign test observation predictor vector class largest note conditional probability probability conditional probability given observed predictor vector simple clas siﬁer called bayes classiﬁer two class problem bayes classiﬁer two possible response values say class class bayes classiﬁer statistical learning figure simulated data set consisting observations two groups indicated blue orange purple dashed line represents bayes decision boundary orange background grid indicates region test observation assigned orange class blue background grid indicates region test observation assigned blue class corresponds predicting class one class two otherwise figure provides example using simulated data set two dimensional space consisting predictors orange blue circles correspond training observations belong two diﬀerent classes value diﬀerent probability response orange blue since simulated data know data generated calculate conditional probabilities value orange shaded region reﬂects set points orange greater blue shaded region indicates set points probability purple dashed line represents points probability exactly called bayes decision boundary bayes bayes decision boundary classiﬁer prediction determined bayes decision boundary observation falls orange side boundary assigned orange class similarly observation blue side boundary assigned blue class bayes classiﬁer produces lowest possible test error rate called bayes error rate since bayes classiﬁer always choose class bayes error rate largest error rate max general overall bayes error rate given max assessing model accuracy expectation averages probability possible values simulated data bayes error rate greater zero classes overlap true population max values bayes error rate analogous irreducible error discussed earlier nearest neighbors theory would always like predict qualitative responses using bayes classiﬁer real data know conditional distri bution given computing bayes classiﬁer impossi ble therefore bayes classiﬁer serves unattainable gold standard compare methods many approaches attempt estimate conditional distribution given classify given observation class highest estimated probability one method nearest neighbors knn classiﬁer given positive nearest neighbors teger test observation knn classiﬁer ﬁrst identiﬁes points training data closest represented estimates conditional probability class fraction points whose response values equal finally knn applies bayes rule classiﬁes test observation class largest probability figure provides illustrative example knn approach left hand panel plotted small training data set consisting six blue six orange observations goal make prediction point labeled black cross suppose choose knn ﬁrst identify three observations closest cross neighborhood shown circle consists two blue points one orange point resulting estimated probabilities blue class orange class hence knn predict black cross belongs blue class right hand panel figure applied knn approach possible values drawn corresponding knn decision boundary despite fact simple approach knn often pro duce classiﬁers surprisingly close optimal bayes classiﬁer figure displays knn decision boundary using plied larger simulated data set figure notice even though true distribution known knn classiﬁer knn decision boundary close bayes classiﬁer test error rate using knn close bayes error rate statistical learning figure knn approach using illustrated simple situation six blue observations six orange observations left test servation predicted class label desired shown black cross three closest points test observation identiﬁed predicted test observation belongs commonly occurring class case blue right knn decision boundary example shown black blue grid indicates region test observation assigned blue class orange grid indicates region assigned orange class choice drastic eﬀect knn classiﬁer obtained figure displays two knn ﬁts simulated data figure using decision boundary overly ﬂexible ﬁnds patterns data correspond bayes decision boundary corresponds classiﬁer low bias high variance grows method becomes less ﬂexible produces decision boundary close linear corresponds low variance high bias classiﬁer simulated data set neither give good predictions test error rates respectively regression setting strong relationship tween training error rate test error rate knn training error rate test error rate may quite high general use ﬂexible classiﬁcation methods training error rate decline test error rate may figure plotted knn test training errors function creases method becomes ﬂexible regression setting training error rate consistently declines ﬂexibility increases however test error exhibits characteristic shape declining ﬁrst minimum approximately increasing method becomes excessively ﬂexible overﬁts assessing model accuracy knn figure black curve indicates knn decision boundary data figure using bayes decision boundary shown purple dashed line knn bayes decision boundaries similar knn knn figure comparison knn decision boundaries solid black curves obtained using data figure decision boundary overly ﬂexible suﬃciently ﬂexible bayes decision boundary shown purple dashed line statistical learning error rate training errors test errors figure knn training error rate blue observations test error rate orange observations data figure level ﬂexibility assessed using increases equivalently number neighbors decreases black dashed line indicates bayes error rate jumpiness curves due small size training data set regression classiﬁcation settings choosing correct level ﬂexibility critical success statistical learning method bias variance tradeoﬀ resulting shape test error make diﬃcult task chapter return topic discuss various methods estimating test error rates thereby choosing optimal level ﬂexibility given statistical learning method 
[statistical, learning, lab, introduction] lab introduce simple commands best way learn new language try commands downloaded http cran project org 
[statistical, learning, lab, introduction, basic, commands] uses functions perform operations run function called funcname function type funcname input input inputs arguments input argument lab introduction input tell run function function number inputs example create vector numbers use function concatenate numbers inside parentheses joined gether following command instructs join together numbers save vector named type vector gives back vector note part command rather printed indicate ready another command entered also save things using rather hitting arrow multiple times display previous commands edited useful since one often wishes repeat similar command addition typing funcname always cause open new help ﬁle window additional information function funcname tell add two sets numbers together add ﬁrst number ﬁrst number however length check length using length length function function allows look list objects data functions saved far function used delete want also possible remove objects statistical learning matrix function used create matrix numbers matrix use matrix function learn help ﬁle reveals matrix function takes number inputs focus ﬁrst three data entries matrix number rows number columns first create simple matrix note could well omit typing data nrow ncol matrix command could type would eﬀect however sometimes useful specify names arguments passed since otherwise assume function arguments passed function order given function help ﬁle example illustrates default creates matrices successively ﬁlling columns alternatively byrow true option used populate matrix order rows notice command assign matrix value case matrix printed screen saved future calculations sqrt function returns square root sqrt element vector matrix command raises element power powers possible including fractional negative powers rnorm function generates vector random normal variables rnorm ﬁrst argument sample size time call function get diﬀerent answer create two correlated sets numbers use cor function compute correlation cor lab introduction cor default rnorm creates standard normal random variables mean standard deviation however mean standard devi ation altered using mean arguments illustrated sometimes want code reproduce exact set random numbers use set seed function set seed set seed function takes arbitrary integer argument set use set seed throughout labs whenever perform calculations involving random quantities general allow user produce results however noted new versions become available possible small discrepancies may form book output mean var functions used compute mean mean var variance vector numbers applying sqrt output var give standard deviation simply use function set var var 
[statistical, learning, lab, introduction, graphics] plot function primary way plot data instance plot plot produces scatterplot numbers versus numbers many additional options passed plot function example passing argument xlab result label axis ﬁnd information plot function type plot statistical learning often want save output plot command use depend ﬁle type would like create instance create pdf use pdf function create jpeg pdf use jpeg function jpeg pdf pdf col dev function dev indicates done creating plot dev alternatively simply copy plot window paste appropriate ﬁle type word document function seq used create sequence numbers seq instance seq makes vector integers many options instance seq length makes sequence numbers equally spaced typing shorthand seq integer arguments seq seq create sophisticated plots contour func contour tion produces contour plot order represent three dimensional data contour plot like topographical map takes three arguments vector values ﬁrst dimension vector values second dimension matrix whose elements correspond value third dimen sion pair coordinates plot function many inputs used ﬁne tune output contour function learn take look help ﬁle typing contour cos add image function works way contour except image produces color coded plot whose colors depend value lab introduction known heatmap sometimes used plot temperature weather heatmap forecasts alternatively persp used produce three dimensional persp plot arguments theta phi control angles plot 
[statistical, learning, lab, introduction, indexing, data] often wish examine part set data suppose data stored matrix typing select element corresponding second row third col umn ﬁrst number open bracket symbol always refers row second number always refers column also select multiple rows columns time providing vectors indices statistical learning last two examples include either index columns index rows indicate include columns rows respectively treats single row column matrix vector use negative sign index tells keep rows columns except indicated index dim function outputs number rows followed number dim columns given matrix dim 
[statistical, learning, lab, introduction, loading, data] analyses ﬁrst step involves importing data set read table function one primary ways help ﬁle read table contains details use function use function write table export data write table attempting load data set must make sure knows search data proper directory example windows system one could select directory using change dir option file menu however details depend erating system windows mac unix used give details begin loading auto data set data part islr library discuss libraries chapter illustrate read table function load text ﬁle following command load auto data ﬁle store object called auto format referred data frame text ﬁle data frame obtained book website data loaded fix function used view spreadsheet like window however window must closed commands entered fix lab introduction note auto data simply text ﬁle could alternatively open computer using standard text editor often good idea view data set using text editor software excel loading particular data set loaded correctly assumed variable names part data included ﬁrst row data set also includes number missing observations indicated question mark missing values common occurrence real data sets using option header header true read table function tells ﬁrst line ﬁle contains variable names using option strings tells time sees particular character set characters question mark treated missing element data matrix fix excel common format data storage program easy way load data save csv comma separated value ﬁle use read csv function load csv csv fix dim dim function tells data observations rows dim nine variables columns various ways deal missing data case ﬁve rows contain missing observations choose use omit function simply remove rows omit dim data loaded correctly use names check names variable names mpg 
[statistical, learning, lab, introduction, additional, graphical, numerical, summaries] use plot function produce scatterplots quantitative scatterplot variables however simply typing variable names produce error message know look auto data set variables statistical learning mpg mpg refer variable must type data set variable name joined symbol alternatively use attach function attach order tell make variables data frame available name mpg cylinders variable stored numeric vector treated quantitative however since small number possible values cylinders one may prefer treat qualitative variable factor function converts quantitative variables qualitative factor variables variable plotted axis categorial boxplots boxplot automatically produced plot function usual number options speciﬁed order customize plots mpg mpg col red mpg col red mpg col red mpg col red mpg hist function used plot histogram note col hist histogram eﬀect col red mpg mpg col mpg col pairs function creates scatterplot matrix scatterplot every scatterplot matrix pair variables given data set also produce scatterplots subset variables mpg displacement horsepower weight conjunction plot function identify provides useful identify interactive method identifying value particular variable points plot pass three arguments identify axis variable axis variable variable whose values would like see printed point clicking given point plot cause print value variable interest right clicking plot exit identify function control click mac numbers printed identify function correspond rows selected points lab introduction mpg mpg summary function produces numerical summary variable summary particular data set mpg min min min max max max min min min max max max min min amc amc amc max max qualitative variables name list number observations fall category also produce summary single variable mpg min max ﬁnished using type order shut quit exiting option save current workspace workspace objects data sets created session available next time exiting may want save record commands typed recent session accomplished using savehistory function next time enter savehistory load history using loadhistory function loadhistory statistical learning 
[statistical, learning, exercises, conceptual] parts indicate whether would generally expect performance ﬂexible statistical learning method better worse inﬂexible method justify answer sample size extremely large number predic tors small number predictors extremely large number observations small relationship predictors response highly non linear variance error terms var extremely high explain whether scenario classiﬁcation regression prob lem indicate whether interested inference pre diction finally provide collect set data top ﬁrms ﬁrm record proﬁt number employees industry ceo salary interested understanding factors aﬀect ceo salary considering launching new product wish know whether success failure collect data similar products previously launched prod uct recorded whether success failure price charged product marketing budget competition price ten variables revisit bias variance decomposition provide sketch typical squared bias variance training ror test error bayes irreducible error curves sin gle plot less ﬂexible statistical learning methods towards ﬂexible approaches axis represent interest predicting change usd euro exchange rate relation weekly changes world stock markets hence collect weekly data week record change usd euro change market change british market change german market exercises amount ﬂexibility method axis represent values curve ﬁve curves make sure label one explain ﬁve curves shape displayed part think real life applications statistical learn ing describe three real life applications classiﬁcation might useful describe response well predictors goal application inference prediction explain answer describe three real life applications regression might useful describe response well predictors goal application inference prediction explain answer describe three real life applications cluster analysis might useful advantages disadvantages ﬂexible versus less ﬂexible approach regression classiﬁcation circumstances might ﬂexible approach preferred less ﬂexible approach might less ﬂexible approach preferred describe diﬀerences parametric non parametric statistical learning approach advantages para metric approach regression classiﬁcation opposed non parametric approach disadvantages table provides training data set containing six observa tions three predictors one qualitative response variable obs red red red green green red suppose wish use data set make prediction using nearest neighbors compute euclidean distance observation test point statistical learning prediction prediction bayes decision boundary problem highly non linear would expect best value large small 
[statistical, learning, exercises, applied] exercise relates college data set found ﬁle college csv contains number variables diﬀerent universities colleges variables private public private indicator apps number applications received accept number applicants accepted enroll number new students enrolled topperc new students top high school class topperc new students top high school class undergrad number full time undergraduates undergrad number part time undergraduates outstate state tuition room board room board costs books estimated book costs personal estimated personal spending phd percent faculty terminal percent faculty terminal degree ratio student faculty ratio perc alumni percent alumni donate expend instructional expenditure per student grad rate graduation rate reading data viewed excel text editor use read csv function read data call loaded data college make sure directory set correct location data look data using fix function notice ﬁrst column name university really want treat data however may handy names later try following commands exercises fix see row names column name university recorded means given row name corresponding appropriate university try perform calculations row names however still need eliminate ﬁrst column data names stored try fix see ﬁrst data column private note another column labeled row names appears private column however data column rather name giving row use summary function produce numerical summary variables data set use pairs function produce scatterplot matrix ﬁrst ten columns variables data recall reference ﬁrst ten columns matrix using iii use plot function produce side side boxplots outstate versus private create new qualitative variable called elite binning topperc variable going divide universities two groups based whether proportion students coming top high school classes exceeds rep yes college use summary function see many elite univer sities use plot function produce side side boxplots outstate versus elite use hist function produce histograms diﬀering numbers bins quantitative vari ables may ﬁnd command par mfrow useful divide print window four regions four plots made simultaneously modifying arguments function divide screen ways continue exploring data provide brief summary discover statistical learning exercise involves auto data set studied lab make sure missing values removed data predictors quantitative quali tative range quantitative predictor swer using range function range mean standard deviation quantitative predictor remove observations range mean standard deviation predictor subset data remains using full data set investigate predictors graphically using scatterplots tools choice create plots highlighting relationships among predictors comment ﬁndings suppose wish predict gas mileage mpg basis variables plots suggest variables might useful predicting mpg justify answer exercise involves boston housing data set begin load boston data set boston data set part mass library data set contained object boston read data set many rows data set many columns rows columns represent make pairwise scatterplots predictors columns data set describe ﬁndings predictors associated per capita crime rate explain relationship suburbs boston appear particularly high crime rates tax rates pupil teacher ratios comment range predictor many suburbs data set bound charles river exercises median pupil teacher ratio among towns data set suburb boston lowest median value owner occupied homes values predictors suburb values compare overall ranges predictors comment ﬁndings data set many suburbs average seven rooms per dwelling eight rooms per dwelling comment suburbs average eight rooms per dwelling 
[linear, regression] chapter linear regression simple approach supervised learning particular linear regression useful tool pre dicting quantitative response linear regression around long time topic innumerable textbooks though may seem somewhat dull compared modern statistical learning approaches described later chapters book linear regression still useful widely used statistical learning method moreover serves good jumping point newer approaches see later chapters many fancy statistical learning approaches seen gener alizations extensions linear regression consequently importance good understanding linear regression studying complex learning methods cannot overstated chapter review key ideas underlying linear regression model well least squares approach commonly used model recall advertising data chapter figure displays sales thousands units particular product function advertis ing budgets thousands dollars radio newspaper media suppose role statistical consultants asked suggest basis data marketing plan next year result high product sales information would useful order provide recommendation important questions might seek address relationship advertising budget sales ﬁrst goal determine whether data provide james introduction statistical learning applications springer texts statistics doi springer science business media new york linear regression evidence association advertising expenditure sales evidence weak one might argue money spent advertising strong relationship advertising budget sales assuming relationship advertising sales would like know strength relationship words given certain advertising budget predict sales high level accuracy would strong relationship prediction sales based advertising expenditure slightly better random guess would weak relationship media contribute sales three media radio newspaper contribute sales one two media contribute answer question must ﬁnd way separate individual eﬀects medium spent money three media accurately estimate eﬀect medium sales every dollar spent advertising particular medium amount sales increase accurately predict amount increase accurately predict future sales given level television radio newspaper advertising prediction sales accuracy prediction relationship linear approximately straight line relationship advertis ing expenditure various media sales linear regression appropriate tool may still possible trans form predictor response linear regression used synergy among advertising media perhaps spending television advertising radio advertising results sales allocating either television radio individually marketing known synergy eﬀect statistics called interaction eﬀect synergy interaction turns linear regression used answer questions ﬁrst discuss questions general context return speciﬁc context section simple linear regression 
[linear, regression, simple, linear, regression] simple linear regression lives name straightforward simple linear regression approach predicting quantitative response basis sin gle predictor variable assumes approximately linear relationship mathematically write linear relationship might read approximately modeled sometimes describe saying regressing onto example may represent advertising may represent sales regress sales onto ﬁtting model sales equation two unknown constants represent intercept slope terms linear model together intercept slope known model coeﬃcients parameters used coeﬃcient parameter training data produce estimates model coeﬃcients predict future sales basis particular value advertising computing indicates prediction basis use hat symbol denote estimated value unknown parameter coeﬃcient denote predicted value response 
[linear, regression, simple, linear, regression, estimating, coeﬃcients] practice unknown use make predictions must use data estimate coeﬃcients let represent observation pairs consists measurement measurement advertising example data set consists advertising budget product sales diﬀerent markets recall data displayed figure goal obtain coeﬃcient estimates linear model ﬁts available data well words want ﬁnd intercept slope resulting line close possible data points number ways measuring closeness however far common approach involves minimizing least squares criterion least squares take approach chapter alternative approaches considered chapter linear regression sales figure advertising data least squares regression sales onto shown found minimizing sum squared errors grey line segment represents error makes compro mise averaging squares case linear captures essence relationship although somewhat deﬁcient left plot let prediction based value represents residual diﬀerence residual observed response value response value predicted linear model deﬁne residual sum squares rss residual sum squares rss equivalently rss least squares approach chooses minimize rss using calculus one show minimizers sample means words deﬁnes least squares coeﬃcient estimates simple linear regression figure displays simple linear regression advertising data words according simple linear regression rss figure contour three dimensional plots rss advertising data using sales response predictor red dots correspond least squares estimates given approximation additional spent advertising asso ciated selling approximately additional units product figure computed rss number values using advertising data sales response predic tor plot red dot represents pair least squares estimates given values clearly minimize rss 
[linear, regression, simple, linear, regression, assessing, accuracy, coeﬃcient, estimates] recall assume true relationship takes form unknown function mean zero random error term approximated linear function write relationship intercept term expected value slope average increase associated one unit increase error term catch miss simple model true relationship probably linear may variables cause variation may measurement error typically assume error term independent model given deﬁnes population regression line population regression line best linear approximation true relationship least squares regression coeﬃcient estimates characterize least squares line left hand panel figure displays least squares line assumption linearity often useful working model however despite many textbooks might tell seldom believe true relationship linear linear regression figure simulated data set left red line represents true rela tionship known population regression line blue line least squares line least squares estimate based observed data shown black right population regression line shown red least squares line dark blue light blue ten least squares lines shown computed basis separate random set observations least squares line diﬀerent average least squares lines quite close population regression line two lines simple simulated example created random generated corresponding model generated normal distribution mean zero red line left hand panel figure displays true relationship blue line least squares estimate based observed data true relationship generally known real data least squares line always computed using coeﬃcient estimates given words real applications access set observations compute least squares line however population regression line unobserved right hand panel figure generated ten diﬀerent data sets model given plotted corresponding ten least squares lines notice diﬀerent data sets generated true model result slightly diﬀerent least squares lines unobserved population regression line change ﬁrst glance diﬀerence population regression line least squares line may seem subtle confusing one data set mean two diﬀerent lines describe relationship predictor response fundamentally simple linear regression concept two lines natural extension standard statistical approach using information sample estimate characteristics large population example suppose interested knowing population mean random variable unfortunately unknown access observations write use estimate reasonable estimate sample mean sample mean population mean diﬀerent general sample mean provide good estimate population mean way unknown coeﬃcients linear regression deﬁne population regression line seek estimate unknown coeﬃcients using given coeﬃcient estimates deﬁne least squares line analogy linear regression estimation mean random variable apt one based concept bias use bias sample mean estimate estimate unbiased sense unbiased average expect equal exactly mean means basis one particular set observations might overestimate basis another set observations might underestimate could average huge number estimates obtained huge number sets observations average would exactly equal hence unbiased estimator systematically estimate true parameter property unbiasedness holds least squares coeﬃcient estimates given well estimate basis particular data set estimates exactly equal could average estimates obtained huge number data sets average estimates would spot fact see right hand panel figure average many least squares lines estimated separate data set pretty close true population regression line continue analogy estimation population mean random variable natural question follows accurate sample mean estimate established average many data sets close single estimate may substantial underestimate overestimate far single estimate general answer question computing standard error written standard error well known formula var linear regression standard deviation realizations roughly speaking standard error tells average amount estimate diﬀers actual value equation also tells deviation shrinks observations smaller standard error similar vein wonder close true values compute standard errors associated use following formulas var formulas strictly valid need sume errors observation uncorrelated common variance clearly true figure formula still turns good approximation notice formula smaller spread intuitively leverage estimate slope case also see would zero case would equal general known estimated data estimate known residual standard error given formula residual standard error rse rss strictly speaking estimated data write indicate estimate made simplicity notation drop extra hat standard errors used compute conﬁdence intervals conﬁdence interval conﬁdence interval deﬁned range values probability range contain true unknown value parameter range deﬁned terms lower upper limits computed sample data linear regression conﬁdence interval approximately takes form approximately chance interval contain true value similarly conﬁdence interval approximately takes form formula holds provided observations uncorrelated approximately several reasons equation relies assumption errors gaussian also factor front term vary slightly depending number observations linear regression precise rather number contain quantile distribution degrees freedom details compute conﬁdence interval precisely provided later chapter simple linear regression case advertising data conﬁdence interval conﬁdence interval therefore conclude absence advertising sales average fall somewhere units furthermore increase television advertising average increase sales units standard errors also used perform hypothesis tests hypothesis test coeﬃcients common hypothesis test involves testing null hypothesis null hypothesis relationship versus alternative hypothesis alternative hypothesis relationship mathematically corresponds testing versus since model reduces associated test null hypothesis need determine whether estimate suﬃciently far zero conﬁdent non zero far far enough course depends accuracy depends small even relatively small values may provide strong evidence hence relationship contrast large must large absolute value order reject null hypothesis practice compute statistic statistic given measures number standard deviations away really relationship expect distribution degrees freedom distribution bell shape values greater approximately quite similar normal distribution consequently simple matter compute probability observing number equal larger absolute value assuming call probability value value ial association pre predictor response hence see small value roughly speaking interpret value follows small value indicates unlikely observe substant dictor response due chance absence real association linear regression infer association predictor response reject null hypothesis declare relationship exist value small enough typical value cutoﬀs rejecting null hypothesis correspond statistics around respectively coeﬃcient std error statistic value intercept table advertising data coeﬃcients least squares model regression number units sold advertising budget increase advertising budget associated increase sales around units recall sales variable thousands units variable thousands dollars table provides details least squares model regression number units sold advertising budget advertising data notice coeﬃcients large relative standard errors statistics also large probabilities seeing values true virtually zero hence conclude 
[linear, regression, simple, linear, regression, assessing, accuracy, model] rejected null hypothesis favor alternative hypothesis natural want quantify extent model ﬁts data quality linear regression typically assessed using two related quantities residual standard error rse statistic table displays rse statistic statistic described section linear regression number units sold advertising budget residual standard error recall model associated observation error term due presence error terms even knew true regression line even known would able perfectly predict rse estimate standard table small value intercept indicates reject null hypothesis small value indicates reject null hypothesis rejecting latter null hypothesis allows conclude relationship sales rejecting former allows conclude absence expenditure sales non zero simple linear regression quantity value residual standard error statistic table advertising data information least squares model regression number units sold advertising budget deviation roughly speaking average amount response deviate true regression line computed using formula rse rss note rss deﬁned section given formula rss case advertising data see linear regression output table rse words actual sales market deviate true regression line approximately units average another way think even model correct true values unknown coeﬃcients known exactly prediction sales basis advertising would still units average course whether units acceptable prediction error depends problem context advertising data set mean value sales markets approximately units percentage error rse considered measure lack model data predictions obtained using model close true outcome values small conclude model ﬁts data well hand far one observations rse may quite large indicating model data well statistic rse provides absolute measure lack model data since measured units always clear constitutes good rse statistic provides alternative measure takes form proportion proportion variance explained always takes value independent scale linear regression calculate use formula tss rss tss rss tss tss total sum squares rss deﬁned total sum squares tss measures total variance response thought amount variability inherent response regression performed contrast rss measures amount variability left unexplained performing regression hence tss rss measures amount variability response explained removed performing regression measures proportion variability explained using statistic close indicates large proportion variability response explained regression number near indicates regression explain much variability response might occur linear model wrong inherent error high table two thirds variability sales explained linear regression statistic interpretational advantage rse since unlike rse always lies however still challenging determine good value general depend application instance certain problems physics may know data truly comes linear model small residual error case would expect see value extremely close substantially smaller value might indicate serious problem experiment data generated hand typical applications biology psychology marketing domains linear model best extremely rough approximation data residual errors due unmeasured factors often large setting would expect small proportion variance response explained predictor value well might realistic statistic measure linear relationship recall correlation deﬁned correlation cor also measure linear relationship sug gests might able use cor instead order assess linear model fact shown simple linear regression setting words squared correlation note fact right hand side sample correlation thus would correct write cor however omit hat ease notation multiple linear regression statistic identical however next section discuss multiple linear regression problem use several pre dictors simultaneously predict response concept correlation predictors response extend automatically setting since correlation quantiﬁes association single pair variables rather larger number variables see ﬁlls role 
[linear, regression, multiple, linear, regression] simple linear regression useful approach predicting response basis single predictor variable however practice often one predictor example advertising data examined relationship sales advertising also data amount money spent advertising radio newspapers may want know whether either two media associated sales extend analysis advertising data order accommodate two additional predictors one option run three separate simple linear regressions uses diﬀerent advertising medium predictor instance simple linear regression predict sales basis amount spent radio advertisements results shown table top table ﬁnd increase spending radio advertising associated increase sales around units table bottom table contains least squares coeﬃcients simple linear regression sales onto newspaper advertising budget increase newspaper advertising budget associated increase sales approximately units however approach ﬁtting separate simple linear regression model predictor entirely satisfactory first unclear make single prediction sales given levels three advertising media budgets since budgets associated separate regression equation second three regression equations ignores two media forming estimates regression coeﬃcients see shortly media budgets correlated markets constitute data set lead misleading estimates individual media eﬀects sales instead ﬁtting separate simple linear regression model pre dictor better approach extend simple linear regression model directly accommodate multiple predictors giving predictor separate slope coeﬃcient single model general suppose distinct predictors multiple linear regression model takes form linear regression simple regression sales radio coeﬃcient std error statistic value intercept radio simple regression sales newspaper coeﬃcient std error statistic value intercept newspaper table simple linear regression models advertising data eﬃcients simple linear regression model number units sold top radio advertising budget bottom newspaper advertising budget crease spending radio advertising associated average increase sales around units increase spending newspaper vertising associated average increase sales around units note sales variable thousands units radio newspaper variables thousands dollars represents predictor quantiﬁes association variable response interpret average eﬀect one unit increase holding predictors ﬁxed advertising example becomes sales radio newspaper 
[linear, regression, multiple, linear, regression, estimating, regression, coeﬃcients] case simple linear regression setting regression coef ﬁcients unknown must estimated given estimates make predictions using formula parameters estimated using least squares approach saw context simple linear regression choose minimize sum squared residuals rss multiple linear regression figure three dimensional setting two predictors one sponse least squares regression line becomes plane plane chosen minimize sum squared vertical distances observation shown red plane values minimize multiple least squares regression coeﬃcient estimates unlike simple linear regression estimates given multiple regression coeﬃcient estimates somewhat complicated forms easily represented using trix algebra reason provide statistical software package used compute coeﬃcient estimates later chapter show done figure illustrates example least squares toy data set predictors table displays multiple regression coeﬃcient estimates radio newspaper advertising budgets used predict product sales using advertising data interpret results follows given amount newspaper advertising spending additional radio advertising leads increase sales approximately units comparing coeﬃcient estimates displayed tables notice multiple regression coeﬃcient estimates radio pretty similar simple linear regression coeﬃcient estimates however newspaper regression coeﬃcient estimate table signiﬁcantly non zero coeﬃcient estimate newspaper multiple regression model close zero corresponding value longer signiﬁcant value around illustrates linear regression coeﬃcient std error statistic value intercept radio newspaper table advertising data least squares coeﬃcient estimates multiple linear regression number units sold radio newspaper advertising budgets simple multiple regression coeﬃcients quite diﬀerent diﬀerence stems fact simple regression case slope term represents average eﬀect increase newspaper advertising ignoring predictors radio contrast multiple regression setting coeﬃcient newspaper represents average eﬀect increasing newspaper spending holding radio ﬁxed make sense multiple regression suggest relationship sales newspaper simple linear regression implies opposite fact consider correlation matrix three predictor variables response variable displayed table notice correlation radio newspaper reveals tendency spend newspaper advertising markets spent radio advertising suppose multiple regression correct newspaper advertising direct impact sales radio advertising increase sales markets spend radio sales tend higher correlation matrix shows also tend spend newspaper advertising markets hence simple linear regression examines sales versus newspaper observe higher values newspaper tend associated higher values sales even though newspaper advertising actually aﬀect sales newspaper sales surrogate radio advertising newspaper gets credit eﬀect radio sales slightly counterintuitive result common many real life situations consider absurd example illustrate point running regression shark attacks versus ice cream sales data collected given beach community period time would show positive relationship similar seen sales newspaper course one yet suggested ice creams banned beaches reduce shark attacks reality higher temperatures cause people visit beach turn results ice cream sales shark attacks multiple regression attacks versus ice cream sales temperature reveals intuition implies former predictor longer signiﬁcant adjusting temperature multiple linear regression radio newspaper sales radio newspaper sales table correlation matrix radio newspaper sales advertising data 
[linear, regression, multiple, linear, regression, important, questions] perform multiple linear regression usually interested answering important questions least one predictors useful predicting response predictors help explain subset predictors useful well model data given set predictor values response value predict accurate prediction address questions turn one relationship response predictors recall simple linear regression setting order determine whether relationship response predictor simply check whether multiple regression setting predictors need ask whether regression coeﬃcients zero whether simple linear regression setting use hypothesis test answer question test null hypothesis versus alternative least one non zero hypothesis test performed computing statistic statistic tss rss rss linear regression quantity value residual standard error statistic table information least squares model regression number units sold newspaper radio advertising budgets advertising data information model displayed table simple linear regression tss rss linear model assumptions correct one show rss provided true tss rss hence relationship response predictors one would expect statistic take value close hand true tss rss expect greater statistic multiple linear regression model obtained gressing sales onto radio newspaper shown table example statistic since far larger provides compelling evidence null hypothesis words large statistic suggests least one advertising media must related sales however statistic closer large statistic need reject conclude relationship turns answer depends values large statistic little larger might still provide evidence contrast larger statistic needed reject small true errors normal distribution statistic follows distribution given value statistical software package used compute value associated statistic using distribution based value determine whether reject advertising data value associated statistic table essentially zero extremely strong evidence least one media associated increased sales testing coeﬃcients zero sometimes want test particular subset coeﬃcients zero corresponds null hypothesis even errors normally distributed statistic approximately follows distribution provided sample size large multiple linear regression convenience put variables chosen omission end list case second model uses variables except last suppose residual sum squares model rss appropriate statistic rss rss rss notice table individual predictor statistic value reported provide information whether individual predictor related response adjusting predictors turns exactly equivalent test omits single variable model leaving others reports partial eﬀect adding variable model instance discussed earlier values indicate radio related sales evidence newspaper associated sales presence two given individual values variable need look overall statistic seems likely one values individual variables small least one predictors related response however logic ﬂawed especially number predictors large instance consider example true variable truly associated response situation values associated variable type shown table chance words expect see approximately ﬁve small values even absence true association predictors response fact almost guaranteed observe least one value chance hence use individual statistics associated values order decide whether association variables response high chance incorrectly conclude relationship however statistic suﬀer problem adjusts number predictors hence true chance statistic result value regardless number predictors number observations approach using statistic test association predictors response works relatively small cer tainly small compared however sometimes large num ber variables coeﬃcients estimate observations estimate case cannot even multiple linear regression model using least squares square statistic corresponding statistic linear regression statistic cannot used neither concepts seen far chapter large approaches discussed next section forward selection used high dimensional setting discussed greater detail chapter high dimensional two deciding important variables discussed previous section ﬁrst step multiple regression analysis compute statistic examine associated value conclude basis value least one predictors related response natural wonder guilty ones could look individual values table discussed large likely make false discoveries possible predictors associated response often case response related subset predictors task determining predictors associated response order single model involving predictors referred variable selection variable selection problem studied variable selection extensively chapter provide brief outline classical approaches ideally would like perform variable selection trying lot diﬀerent models containing diﬀerent subset predictors instance consider four models model contain ing variables model containing model containing model containing lect best model models considered determine model best various statistics used judge quality model include mallow akaike informa mallow tion criterion aic bayesian information criterion bic adjusted akaike information criterion bayesian information criterion discussed detail chapter also deter adjusted mine model best plotting various model outputs residuals order search patterns unfortunately total models contain subsets variables means even moderate trying every possible subset predictors infeasible instance saw models consider must consider models practical therefore unless small cannot consider models instead need automated eﬃcient approach choose smaller set models consider three classical approaches task forward selection begin null model model con forward selection null model tains intercept predictors simple linear gressions add null model variable results lowest rss add model variable results multiple linear regression lowest rss new two variable model approach continued stopping rule satisﬁed backward selection start variables model backward selection remove variable largest value variable least statistically signiﬁcant new variable model variable largest value removed procedure continues stopping rule reached instance may stop remaining variables value threshold mixed selection combination forward backward mixed selection lection start variables model forward selection add variable provides best con tinue add variables one one course noted advertising example values variables become larger new predictors added model hence point value one variables model rises certain threshold remove variable model con tinue perform forward backward steps variables model suﬃciently low value variables outside model would large value added model backward selection cannot used forward selection always used forward selection greedy approach might include variables early later become redundant mixed selection remedy three model fit two common numerical measures model rse fraction variance explained quantities computed interpreted fashion simple linear regression recall simple regression square correlation response variable multiple linear regression turns equals cor square correlation response ﬁtted linear model fact one property ﬁtted linear model maximizes correlation among possible linear models value close indicates model explains large portion variance response variable example saw table advertising data model uses three advertising dia predict sales hand model uses radio predict sales value words small increase include newspaper advertising model already contains radio advertising even though saw earlier value newspaper advertising table signiﬁcant turns always increase variables linear regression added model even variables weakly associated response due fact adding another variable least squares equations must allow training data though necessarily testing data accurately thus statistic also computed training data must increase fact adding newspaper advertising model containing radio advertising leads tiny increase provides additional evidence newspaper dropped model essentially newspaper pro vides real improvement model training samples inclusion likely lead poor results independent test samples due overﬁtting contrast model containing predictor table adding radio model leads substantial improvement implies model uses radio expenditures predict sales substantially better one uses advertis ing could quantify improvement looking value radio coeﬃcient model contains radio predictors model contains radio predictors rse model also contains newspaper predictor rse table contrast model contains rse table corroborates previous conclusion model uses radio expenditures predict sales much accurate training data one uses spending furthermore given radio expenditures used predictors point also using newspaper spending predictor model observant reader may wonder rse increase newspaper added model given rss must decrease general rse deﬁned rse rss simpliﬁes simple linear regression thus models variables higher rse decrease rss small relative increase addition looking rse statistics discussed useful plot data graphical summaries reveal problems model visible numerical statistics example figure displays three dimensional plot radio versus sales see observations lie observations lie least squares regression plane particular linear model seems overestimate sales instances advertising money spent exclusively either radio underestimates sales instances budget split two media pro nounced non linear pattern cannot modeled accurately using linear multiple linear regression sales radio figure advertising data linear regression sales using radio predictors pattern residuals see pronounced non linear relationship data positive residuals visible surface tend lie along degree line radio budgets split evenly negative residuals visible tend lie away line budgets lopsided gression suggests synergy interaction eﬀect advertising media whereby combining media together results bigger boost sales using single medium section discuss tending linear model accommodate synergistic eﬀects use interaction terms four predictions multiple regression model straightforward apply order predict response basis set values predictors however three sorts uncertainty associated prediction coeﬃcient estimates estimates least squares plane estimate true population regression plane inaccuracy coeﬃcient estimates related reducible error chapter compute conﬁdence interval order determine close linear regression course practice assuming linear model almost always approximation reality additional source potentially reducible error call model bias use linear model fact estimating best linear approximation true surface however ignore discrepancy operate linear model correct even knew even knew true values response value cannot predicted perfectly random error model chapter referred irreducible error much vary use prediction intervals answer question prediction intervals always wider conﬁdence intervals incorporate error estimate reducible error uncertainty much individual point diﬀer population regression plane irreducible error use conﬁdence interval quantify uncertainty surrounding conﬁdence interval average sales large number cities example given spent advertising spent radio advertising city conﬁdence interval interpret mean intervals form contain true value hand prediction interval used quantify prediction interval uncertainty surrounding sales particular city given spent advertising spent radio advertising city prediction interval interpret mean intervals form contain true value city note intervals centered prediction interval substantially wider conﬁdence interval reﬂecting increased uncertainty sales given city comparison average sales many locations 
[linear, regression, considerations, regression, model, qualitative, predictors] discussion far assumed variables linear regression model quantitative practice necessarily case often predictors qualitative words collect large number data sets like advertising data set construct conﬁdence interval average sales basis data set given radio advertising conﬁdence intervals contain true value average sales considerations regression model example credit data set displayed figure records balance average credit card debt number individuals well several quantitative predictors age cards number credit cards education years education income thousands dollars limit credit limit rating credit rating panel figure scatterplot pair variables whose identities given corresponding row column labels example scatterplot directly right word balance depicts balance versus age plot directly right age corresponds age versus cards addition quantitative variables also four qualitative variables gender student student status status marital status ethnicity caucasian african amer ican asian balance age cards education income limit rating figure credit data set contains information balance age cards education income limit rating number potential cus tomers linear regression coeﬃcient std error statistic value intercept gender female table least squares coeﬃcient estimates associated regression balance onto gender credit data set linear model given gender encoded dummy variable predictors two levels suppose wish investigate diﬀerences credit card balance tween males females ignoring variables moment qualitative predictor also known factor two levels possi factor level ble values incorporating regression model simple simply create indicator dummy variable takes two possible dummy variable numerical values example based gender variable create new variable takes form person female person male use variable predictor regression equation results model person female person male interpreted average credit card balance among males average credit card balance among females average diﬀerence credit card balance females males table displays coeﬃcient estimates information asso ciated model average credit card debt males estimated whereas females estimated carry additional debt total however notice value dummy variable high indicates statistical evidence diﬀerence average credit card balance genders decision code females males arbitrary eﬀect regression alter interpretation coeﬃcients coded males females estimates would respectively leading prediction credit card debt males prediction females alternatively instead coding scheme could create dummy variable considerations regression model person female person male use variable regression equation results model person female person male interpreted overall average credit card balance noring gender eﬀect amount females average males average example estimate would halfway male female averages estimate would half average diﬀerence females males important note ﬁnal predictions credit balances males females identical regardless coding scheme used diﬀerence way coeﬃcients interpreted qualitative predictors two levels qualitative predictor two levels single dummy variable cannot represent possible values situation create additional dummy variables example ethnicity variable create two dummy variables ﬁrst could person asian person asian second could person caucasian person caucasian variables used regression equation order obtain model person asian person caucasian person african american interpreted average credit card balance african americans interpreted diﬀerence average balance asian african american categories inter preted diﬀerence average balance caucasian linear regression coeﬃcient std error statistic value intercept ethnicity asian ethnicity caucasian table least squares coeﬃcient estimates associated regression balance onto ethnicity credit data set linear model given african american categories always one fewer dummy vari able number levels level dummy variable african american example known baseline baseline table see estimated balance baseline african american estimated asian category less debt african american category caucasian category less debt african american category however values associated coeﬃcient estimates two dummy variables large suggesting statistical evidence real diﬀerence credit card balance ethnicities level selected baseline category arbitrary ﬁnal predictions group regardless choice ever coeﬃcients values depend choice dummy variable coding rather rely individual coeﬃcients use test test depend coding test value indicating cannot reject null hypothesis relationship balance ethnicity using dummy variable approach presents diﬃculties corporating quantitative qualitative predictors example regress balance quantitative variable income qual itative variable student must simply create dummy variable student multiple regression model using income dummy variable predictors credit card balance many diﬀerent ways coding qualitative variables besides dummy variable approach taken approaches lead equivalent model ﬁts coeﬃcients diﬀerent diﬀerent interpretations designed measure particular contrasts topic contrast beyond scope book pursue 
[linear, regression, considerations, regression, model, extensions, linear, model] standard linear regression model provides interpretable results works quite well many real world problems however makes sev eral highly restrictive assumptions often violated practice two important assumptions state relationship predictors response additive linear additive assumption additive linear considerations regression model means eﬀect changes predictor response independent values predictors linear assumption states change response due one unit change constant regardless value book examine number sophisticated methods relax two assumptions brieﬂy examine common classical approaches extending linear model removing additive assumption previous analysis advertising data concluded radio seem associated sales linear models formed basis conclusion assumed eﬀect sales increasing one advertising medium independent amount spent media example linear model states average eﬀect sales one unit increase always regardless amount spent radio however simple model may incorrect suppose spending money radio advertising actually increases eﬀectiveness vertising slope term increase radio increases situation given ﬁxed budget spending half radio half may increase sales allocating entire amount either radio marketing known synergy eﬀect statistics referred interaction eﬀect figure sug gests eﬀect may present advertising data notice levels either radio low true sales lower predicted linear model advertising split two media model tends underestimate sales consider standard linear regression model two variables according model increase one unit increase average units notice presence alter statement regardless value one unit increase lead unit increase one way extending model allow interaction eﬀects include third predictor called interaction term constructed computing product results model inclusion interaction term relax additive assumption notice rewritten linear regression coeﬃcient std error statistic value intercept radio radio table advertising data least squares coeﬃcient estimates asso ciated regression sales onto radio interaction term since changes eﬀect longer constant adjusting change impact example suppose interested studying productiv ity factory wish predict number units produced basis number production lines total number workers seems likely eﬀect increasing number production lines depend number workers since workers available operate lines increasing number lines increase production suggests would appropriate include inter action term lines workers linear model predict units suppose model obtain units lines workers lines workers workers lines workers words adding additional line increase number units produced workers hence workers stronger eﬀect lines return advertising example linear model uses radio interaction two predict sales takes form sales radio radio radio radio interpret increase eﬀectiveness advertising one unit increase radio advertising vice versa coeﬃcients result ﬁtting model given table results table strongly suggest model includes interaction term superior model contains main eﬀects main eﬀect value interaction term radio extremely low indicating strong evidence words clear true relationship additive model compared model predicts sales using radio without interaction term means variability sales remains ﬁtting ditive model explained interaction term coeﬃcient considerations regression model estimates table suggest increase advertising associated increased sales radio radio units increase radio advertising associated increase sales units example values associated radio interac tion term statistically signiﬁcant table obvious three variables included model however sometimes case interaction term small value associated main eﬀects case radio hier archical principle states include interaction model hierarchical principle also include main eﬀects even values associated coeﬃcients signiﬁcant words interaction tween seems important include model even coeﬃcient estimates large values rationale principle related response whether coeﬃcients exactly zero lit tle interest also typically correlated leaving tends alter meaning interaction previous example considered interaction radio quantitative variables however concept interactions applies well qualitative variables combination quantitative qualitative variables fact interaction qualitative variable quantitative variable particularly nice interpretation consider credit data set section suppose wish predict balance using income quantitative student qualitative variables absence interaction term model takes form balance income person student person student income person student person student notice amounts ﬁtting two parallel lines data one students one non students lines students non students diﬀerent intercepts versus slope illustrated left hand panel figure fact lines parallel means average eﬀect balance one unit increase income depend whether individual student represents potentially serious limitation model since fact change income may diﬀerent eﬀect credit card balance student versus non student limitation addressed adding interaction variable cre ated multiplying income dummy variable student linear regression income balance income balance student non student figure credit data least squares lines shown pre diction balance income students non students left model interaction income student right model interaction term income student model becomes balance income income student student income student income student two diﬀerent regression lines students non students regression lines diﬀerent intercepts versus well diﬀerent slopes versus allows possibility changes income may aﬀect credit card balances students non students diﬀerently right hand panel figure shows estimated relationships income balance students non students model note slope students lower slope non students suggests increases income associated smaller increases credit card balance among students compared non students non linear relationships discussed previously linear regression model assumes linear relationship response predictors cases true relationship response predictors may non linear present simple way directly extend linear model accommodate non linear relationships using polynomial regression polynomial regression later chapters present complex approaches performing non linear ﬁts general settings consider figure mpg gas mileage miles per gallon versus horsepower shown number cars auto data set considerations regression model horsepower miles per gallon linear degree degree figure auto data set number cars mpg horsepower shown linear regression shown orange linear regression model includes horsepower shown blue curve linear regression model includes polynomials horsepower ﬁfth degree shown green orange line represents linear regression pronounced rela tionship mpg horsepower seems clear relation ship fact non linear data suggest curved relationship simple approach incorporating non linear associations linear model include transformed versions predictors model example points figure seem quadratic shape suggesting quadratic model form mpg horsepower horsepower may provide better equation involves predicting mpg using non linear function horsepower still linear model simply multiple linear regression model horsepower horsepower use standard linear regression software estimate order produce non linear blue curve figure shows resulting quadratic data quadratic appears substantially better obtained linear term included quadratic compared linear value table quadratic term highly signiﬁcant including horsepower led big improvement model include horsepower horsepower even horsepower green curve linear regression coeﬃcient std error statistic value intercept horsepower horsepower table auto data set least squares coeﬃcient estimates associated regression mpg onto horsepower horsepower figure displays results including polynomials ﬁfth degree model resulting seems unnecessarily wiggly unclear including additional terms really led better data approach described extending linear model accommodate non linear relationships known polynomial regres sion since included polynomial functions predictors regression model explore approach non linear extensions linear model chapter 
[linear, regression, considerations, regression, model, potential, problems] linear regression model particular data set many prob lems may occur common among following non linearity response predictor relationships correlation error terms non constant variance error terms outliers high leverage points collinearity practice identifying overcoming problems much art science many pages countless books written topic since linear regression model primary focus provide brief summary key points non linearity data linear regression model assumes straight line relation ship predictors response true relationship far linear virtually conclusions draw suspect addition prediction accuracy model signiﬁcantly reduced residual plots useful graphical tool identifying non linearity residual plot given simple linear regression model plot residuals versus predictor case multiple regression model considerations regression model fitted values residuals residual plot linear fit fitted values residuals residual plot quadratic fit figure plots residuals versus predicted ﬁtted values auto data set plot red line smooth residuals intended make easier identify trend left linear regression mpg horsepower strong pattern residuals indicates non linearity data right linear regression mpg horsepower horsepower little pattern residuals since multiple predictors instead plot residuals versus predicted ﬁtted values ideally residual plot show ﬁtted discernible pattern presence pattern may indicate problem aspect linear model left panel figure displays residual plot linear regression mpg onto horsepower auto data set illustrated figure red line smooth residuals displayed order make easier identify trends residuals exhibit clear shape provides strong indication non linearity data contrast right hand panel figure displays residual plot results model contains quadratic term appears little pattern residuals suggesting quadratic term improves data residual plot indicates non linear associations data simple approach use non linear transformations predictors log regression model later chapters book discuss advanced non linear approaches addressing issue correlation error terms important assumption linear regression model error terms uncorrelated mean instance errors uncorrelated fact positive provides little information sign standard errors computed estimated regression coeﬃcients ﬁtted values linear regression based assumption uncorrelated error terms fact correlation among error terms estimated standard errors tend underestimate true standard errors result conﬁ dence prediction intervals narrower example conﬁdence interval may reality much lower prob ability containing true value parameter addition values associated model lower could cause erroneously conclude parameter statistically signiﬁcant short error terms correlated may unwarranted sense conﬁdence model extreme example suppose accidentally doubled data lead ing observations error terms identical pairs ignored standard error calculations would sample size fact samples estimated parameters would samples samples conﬁdence intervals would narrower factor might correlations among error terms occur correlations frequently occur context time series data consists time series servations measurements obtained discrete points time many cases observations obtained adjacent time points positively correlated errors order determine case given data set plot residuals model function time errors uncorrelated discernible pat tern hand error terms positively correlated may see tracking residuals adjacent residuals may tracking similar values figure provides illustration top panel see residuals linear regression data generated uncorre lated errors evidence time related trend residuals contrast residuals bottom panel data set adjacent errors correlation clear pattern residuals adjacent residuals tend take similar values finally center panel illustrates moderate case residuals correlation still evidence tracking pattern less clear many methods developed properly take account corre lations error terms time series data correlation among error terms also occur outside time series data instance consider study individuals heights predicted weights assumption uncorrelated errors could violated individ uals study members family eat diet exposed environmental factors general assumption uncorrelated errors extremely important linear regres sion well statistical methods good experimental design crucial order mitigate risk correlations considerations regression model residual residual residual observation figure plots residuals simulated time series data sets generated diﬀering levels correlation error terms adjacent time points non constant variance error terms another important assumption linear regression model error terms constant variance var standard errors conﬁdence intervals hypothesis tests associated linear model rely upon assumption unfortunately often case variances error terms non constant instance variances error terms may increase value response one identify non constant variances errors heteroscedasticity presence funnel shape heterosceda sticity residual plot example shown left hand panel figure magnitude residuals tends increase ﬁtted values faced problem one possible solution trans form response using concave function log transformation results greater amount shrinkage larger sponses leading reduction heteroscedasticity right hand panel figure displays residual plot transforming response linear regression fitted values residuals response fitted values residuals response log figure residual plots plot red line smooth residuals intended make easier identify trend blue lines track outer quantiles residuals emphasize patterns left funnel shape indicates heteroscedasticity right response log transformed evidence heteroscedasticity using log residuals appear constant variance though evidence slight non linear relationship data sometimes good idea variance response example response could average raw observations raw observations uncorrelated variance average variance case simple remedy model weighted least squares weights proportional inverse weighted least squares variances case linear regression software allows observation weights outliers outlier point far value predicted outlier model outliers arise variety reasons incorrect recording observation data collection red point observation left hand panel figure illustrates typical outlier red solid line least squares regression blue dashed line least squares removal outlier case removing outlier little eﬀect least squares line leads almost change slope miniscule reduction intercept typical outlier unusual predictor value little eﬀect least squares however even outlier much eﬀect least squares cause problems instance example rse outlier included regression outlier removed since rse used compute conﬁdence intervals considerations regression model fitted values residuals fitted values studentiz residuals figure left least squares regression line shown red regression line removing outlier shown blue center residual plot clearly identiﬁes outlier right outlier studentized residual typically expect values values dramatic increase caused single data point implications interpretation similarly inclusion outlier causes decline residual plots used identify outliers example lier clearly visible residual plot illustrated center panel figure practice diﬃcult decide large resid ual needs consider point outlier address problem instead plotting residuals plot studentized residuals computed dividing residual estimated standard studentized residual error observations whose studentized residuals greater abso lute value possible outliers right hand panel figure outlier studentized residual exceeds observations studentized residuals believe outlier occurred due error data collec tion recording one solution simply remove observation however care taken since outlier may instead indicate deﬁciency model missing predictor high leverage points saw outliers observations response unusual given predictor contrast observations high leverage high leverage unusual value example observation left hand panel figure high leverage predictor value observation large relative observations note data displayed figure data displayed figure addition single high leverage observation red solid line least squares data blue dashed line produced observation removed comparing left hand panels figures observe removing high leverage observation much substantial impact least squares line linear regression leverage studentiz residuals figure left observation high leverage point red line data blue line observation removed center red observation unusual terms value value still falls outside bulk data hence high leverage right observation high leverage high residual removing outlier fact high leverage observations tend sizable impact estimated regression line cause concern least squares line heavily aﬀected couple observations problems points may invalidate entire reason important identify high leverage observations simple linear regression high leverage observations fairly easy identify since simply look observations predictor value outside normal range observations multiple linear regression many predictors possible observation well within range individual predictor values unusual terms full set predictors example shown center panel figure data set two predictors observations predictor values fall within blue dashed ellipse red observation well outside range neither value value unusual examine fail notice high leverage point problem pronounced multiple regression settings two predictors simple way plot dimensions data simultaneously order quantify observation leverage compute leverage statistic large value statistic indicates observation high leverage statistic leverage simple linear regression clear equation increases distance simple extension case multiple predictors though provide formula leverage statistic always average leverage observations always equal given observation leverage statistic considerations regression model limit age limit rating figure scatterplots observations credit data set left plot age versus limit two variables collinear right plot rating versus limit high collinearity greatly exceeds may suspect corresponding point high leverage right hand panel figure provides plot studentized residuals versus data left hand panel figure servation stands high leverage statistic well high studentized residual words outlier well high leverage observation particularly dangerous combination plot also reveals reason observation relatively little eﬀect least squares figure low leverage collinearity collinearity refers situation two predictor variables collinearity closely related one another concept collinearity illustrated figure using credit data set left hand panel fig ure two predictors limit age appear obvious rela tionship contrast right hand panel figure predictors limit rating highly correlated say collinear presence collinearity pose problems regression context since diﬃcult separate indi vidual eﬀects collinear variables response words since limit rating tend increase decrease together diﬃcult determine one separately associated response balance figure illustrates diﬃculties result collinear ity left hand panel figure contour plot rss associated diﬀerent possible coeﬃcient estimates regression balance limit age ellipse represents set coeﬃcients correspond rss ellipses nearest center tak ing lowest values rss black dots associated dashed linear regression limit limit age rating figure contour plots rss values function parameters various regressions involving credit data set plot black dots represent coeﬃcient values corresponding minimum rss left contour plot rss regression balance onto age limit minimum value well deﬁned right contour plot rss regression balance onto rating limit collinearity many pairs limit rating similar value rss lines represent coeﬃcient estimates result smallest possible rss words least squares estimates axes limit age scaled plot includes possible coeﬃ cient estimates four standard errors either side least squares estimates thus plot includes plausible values coeﬃcients example see true limit coeﬃcient almost certainly somewhere contrast right hand panel figure displays contour plots rss associated possible coeﬃcient estimates regression balance onto limit rating know highly collinear contours run along narrow valley broad range values coeﬃcient estimates result equal values rss hence small change data could cause pair coeﬃcient values yield smallest rss least squares estimates move anywhere along valley results great deal uncertainty coeﬃcient estimates notice scale limit coeﬃcient runs roughly eight fold increase plausible range limit coeﬃcient regression age interestingly even though limit rating coeﬃcients much individual uncertainty almost certainly lie somewhere contour valley example would expect true value limit rating coeﬃcients respectively even though value plausible coeﬃcient individually considerations regression model coeﬃcient std error statistic value intercept model age limit intercept model rating limit table results two multiple regression models involving credit data set shown model regression balance age limit model regression balance rating limit standard error limit increases fold second regression due collinearity since collinearity reduces accuracy estimates regression coeﬃcients causes standard error grow recall statistic predictor calculated dividing standard error consequently collinearity results decline statistic result presence collinearity may fail reject means power hypothesis test probability correctly power detecting non zero coeﬃcient reduced collinearity table compares coeﬃcient estimates obtained two separate multiple regression models ﬁrst regression balance age limit second regression balance rating limit ﬁrst regression age limit highly signiﬁcant small values second collinearity limit rating caused standard error limit coeﬃcient estimate increase factor value increase words importance limit variable masked due presence collinearity avoid situation desirable identify address potential collinearity problems ﬁtting model simple way detect collinearity look correlation matrix predictors element matrix large absolute value indicates pair highly correlated variables therefore collinearity problem data unfortunately collinearity problems detected inspection correlation matrix possible collinear ity exist three variables even pair variables particularly high correlation call situation multicollinearity multi collinearity instead inspecting correlation matrix better way assess multi collinearity compute variance inﬂation factor vif vif variance inﬂation factor ratio variance ﬁtting full model divided variance smallest possible value vif indicates complete absence collinearity typically practice small amount collinearity among predictors rule thumb vif value exceeds indicates problematic amount linear regression collinearity vif variable computed using formula vif regression onto predictors close one collinearity present vif large credit data regression balance age rating limit indicates predictors vif values suspected considerable collinearity data faced problem collinearity two simple solu tions ﬁrst drop one problematic variables regres sion usually done without much compromise regression since presence collinearity implies information variable provides response redundant presence variables instance regress balance onto age limit without rating predictor resulting vif values close minimum possible value drops dropping rating set predictors eﬀectively solved collinearity problem without compromising second solution combine collinear variables together single predictor stance might take average standardized versions limit rating order create new variable measures credit worthiness 
[linear, regression, marketing, plan] brieﬂy return seven questions advertising data set answer beginning chapter relationship advertising sales budget question answered ﬁtting multiple regression model sales onto radio newspaper testing hypothesis radio newspaper section showed statistic used determine whether reject null hypothesis case value corresponding statistic table low indicating clear evidence relationship advertising sales strong relationship discussed two measures model accuracy section first rse estimates standard deviation response population regression line advertising data rse marketing plan units mean value response indicating percentage error roughly second statistic records percentage variability response explained predictors predictors explain almost variance sales rse statistics displayed table media contribute sales answer question examine values associated predictor statistic section multiple linear gression displayed table values radio low value newspaper suggests radio related sales chapter explore question greater detail large eﬀect medium sales saw section standard error used construct conﬁdence intervals advertising data conﬁdence intervals follows radio newspaper conﬁ dence intervals radio narrow far zero provid ing evidence media related sales interval newspaper includes zero indicating variable statis tically signiﬁcant given values radio saw section collinearity result wide stan dard errors could collinearity reason conﬁdence terval associated newspaper wide vif scores radio newspaper suggesting evidence collinearity order assess association medium individually sales perform three separate simple linear regressions sults shown tables evidence tremely strong association sales radio sales evidence mild association newspaper sales values radio ignored accurately predict future sales response predicted using accuracy associ ated estimate depends whether wish predict individual response average response section former use prediction interval latter use conﬁdence interval prediction intervals always wider conﬁdence intervals account certainty associated irreducible error linear regression relationship linear section saw residual plots used order identify non linearity relationships linear residual plots display pattern case advertising data observe non linear eﬀect figure though eﬀect could also observed residual plot section discussed inclusion transformations predictors linear regression model order accommodate non linear relationships synergy among advertising media standard linear regression model assumes additive relation ship predictors response additive model easy interpret eﬀect predictor response unrelated values predictors however additive assumption may unrealistic certain data sets section showed include interaction term regression model order accommodate non additive relationships small value associated interaction term indicates presence relationships figure suggested advertising data may additive including interaction term model results substantial increase around almost 
[linear, regression, k-nearest, neighbors] discussed chapter linear regression example parametric approach assumes linear functional form parametric methods several advantages often easy one need estimate small number coeﬃcients case linear gression coeﬃcients simple interpretations tests statistical signiﬁcance easily performed parametric methods disadvantage construction make strong assumptions form speciﬁed functional form far truth prediction accuracy goal parametric method perform poorly instance assume linear relationship true relationship far linear resulting model provide poor data conclusions drawn suspect contrast non parametric methods explicitly assume para metric form thereby provide alternative ﬂexi ble approach performing regression discuss various non parametric methods book consider one simplest best known non parametric methods nearest neighbors regression knn regression nearest neighbors regression comparison linear regression nearest neighbors figure plots using knn regression two dimensional data set observations orange dots left results rough step func tion right produces much smoother knn regression method closely related knn classiﬁer dis cussed chapter given value prediction point knn regression ﬁrst identiﬁes training observations closest represented estimates using average training responses words figure illustrates two knn ﬁts data set predictors shown left hand panel right hand panel corresponds see knn perfectly interpolates training observations consequently takes form step function knn still step function averaging nine observations results much smaller regions constant prediction consequently smoother general optimal value depend bias variance tradeoﬀ introduced chapter small value provides ﬂexible low bias high variance variance due fact prediction given region entirely dependent one observation contrast larger values provide smoother less variable prediction region average several points changing one observation smaller eﬀect however smoothing may cause bias masking structure chapter introduce several approaches estimating test error rates methods used identify optimal value knn regression linear regression setting parametric approach least squares linear gression outperform non parametric approach knn regression answer simple parametric approach outperform non parametric approach parametric form selected close true form figure provides example data generated one dimensional linear regression model black solid lines rep resent blue curves correspond knn ﬁts using case predictions far variable smoother much closer however since true relationship linear hard non parametric approach compete linear regression non parametric approach incurs cost variance oﬀset reduction bias blue dashed line left hand panel figure represents linear regression data almost perfect right hand panel figure reveals linear regression outperforms knn data green solid line plot ted function represents test set mean squared error mse knn knn errors well black dashed line test mse linear regression value large knn performs little worse least squares regression terms mse performs far worse small practice true relationship rarely exactly lin ear figure examines relative performances least squares regres sion knn increasing levels non linearity relationship top row true relationship nearly linear case see test mse linear regression still superior knn low values however knn performs linear regression second row illustrates substantial deviation linearity situation knn substantially outperforms linear regression values note extent non linearity increases little change test set mse non parametric knn method large increase test set mse linear regression figures display situations knn performs slightly worse linear regression relationship linear much better linear regression non linear situations real life situation true relationship unknown one might draw conclusion knn favored linear regression worst slightly inferior linear regression true relationship linear may give substantially better results true relationship non linear reality even true relationship highly non linear knn may still provide inferior results linear regression particular figures illustrate settings predictor higher dimensions knn often performs worse linear regression figure considers strongly non linear situation second row figure except added additional noise comparison linear regression nearest neighbors figure plots using knn regression one dimensional data set observations true relationship given black solid line left blue curve corresponds interpolates passes directly training data right blue curve corresponds represents smoother mean squared error figure data set shown figure investigated left blue dashed line least squares data since fact linear displayed black line least squares regression line provides good estimate right dashed horizontal line represents least squares test set mse green solid line corresponds mse knn function log scale linear regression achieves lower test mse knn regression since fact linear knn regression best results occur large value corresponding small value linear regression mean squared error mean squared error figure top left setting slightly non linear relationship solid black line knn ﬁts blue red displayed top right slightly non linear data test set mse least squares regression horizontal black knn various values green displayed bottom left bottom right top panel strongly non linear relationship predictors associated response knn outperforms linear regression results mixed linear regression superior knn fact increase dimension caused small deterioration linear regression test set mse caused ten fold increase mse knn decrease performance dimension increases common problem knn results fact higher dimensions eﬀectively reduction sample size data set training observations provides enough information accurately estimate however spreading observations dimensions results phenomenon given observation nearby neighbors called curse dimensionality curse mensionality observations nearest given test observation may far away dimensional space large leading lab linear regression mean squared error figure test mse linear regression black dashed lines knn green curves number variables increases true function non linear ﬁrst variable lower panel figure depend additional variables performance linear regression deteri orates slowly presence additional noise variables whereas knn performance degrades much quickly increases poor prediction hence poor knn general rule parametric methods tend outperform non parametric approaches small number observations per predictor even problems dimension small might prefer linear regression knn interpretability standpoint test mse knn slightly lower linear regression might willing forego little bit prediction accuracy sake simple model described terms coeﬃcients values available 
[linear, regression, lab, linear, regression, libraries] library function used load libraries groups functions library data sets included base distribution basic functions perform least squares linear regression simple analyses come standard base distribution exotic functions require ditional libraries load mass package large collection data sets functions also load islr package includes data sets associated book receive error message loading libraries likely indicates corresponding library yet installed system libraries mass come need separately installed computer however packages linear regression islr must downloaded ﬁrst time used done rectly within example windows system select install package option packages tab select mirror site list available packages appear simply select package wish install automatically download package alternatively done command line via install packages islr stallation needs done ﬁrst time use package however library function must called time wish use given package 
[linear, regression, lab, linear, regression, simple, linear, regression] mass library contains boston data set records medv median house value neighborhoods around boston seek predict medv using predictors average number rooms per house age average age houses lstat percent households low socioeconomic status fix nox age dis rad tax ﬁnd data set type boston start using function simple linear regression model medv response lstat predictor basic syntax data response predictor data data set two variables kept fit lstat expr envir command causes error know ﬁnd variables medv lstat next line tells variables boston attach boston ﬁrst line works ﬁne recognizes variables fit lstat data boston fit lstat type fit basic information model output detailed information use summary fit gives values standard errors coeﬃcients well statistic statistic model fit lstat lab linear regression fit lstat min max std use names function order ﬁnd pieces names information stored fit although extract quan tities name fit coefficients safer use extractor functions like coef access coef fit fit order obtain conﬁdence interval coeﬃcient estimates use confint command confint fit predict function used produce conﬁdence intervals predict prediction intervals prediction medv given value lstat fit fit lwr upr linear regression fit fit lwr upr instance conﬁdence interval associated lstat value prediction interval expected conﬁdence prediction intervals centered around point predicted value medv lstat equals latter substantially wider plot medv lstat along least squares regression line using plot abline functions abline lstat fit evidence non linearity relationship lstat medv explore issue later lab abline function used draw line least squares regression line draw line intercept slope type abline experiment additional settings plotting lines points lwd command causes width regression line increased factor works plot lines functions also also use pch option create diﬀerent plotting symbols fit lwd fit lwd col red lstat medv col red lstat medv pch lstat medv pch pch next examine diagnostic plots several discussed section four diagnostic plots automatically produced plying plot function directly output general command produce one plot time hitting enter generate next plot however often convenient view four plots together achieve using par function tells split par display screen separate panels multiple plots viewed multaneously example par mfrow divides plotting region grid panels par fit alternatively compute residuals linear regression using residuals function function rstudent return residuals rstudent studentized residuals use function plot residuals ﬁtted values lab linear regression fit fit fit fit basis residual plots evidence non linearity leverage statistics computed number predictors using hatvalues function hatvalues fit max fit max function identiﬁes index largest element max vector case tells observation largest leverage statistic 
[linear, regression, lab, linear, regression, multiple, linear, regression] order multiple linear regression model using least squares use function syntax used model three predictors summary function outputs regression coeﬃcients predictors fit lstat age data boston fit lstat age data boston min max std age boston data set contains variables would cumbersome type order perform regression using predictors instead use following short hand fit data boston fit data boston linear regression min max std nox age dis rad tax access individual components summary object name type summary see available hence summary fit gives summary fit sigma gives rse vif vif function part car package used compute variance inﬂation factors vif low moderate data car package part base installation must downloaded ﬁrst time use via install packages option car vif fit nox age dis rad tax would like perform regression using variables one example regression output age high value may wish run regression excluding predictor following syntax results regression using predictors except age age data boston alternatively update function used update lab linear regression fit age 
[linear, regression, lab, linear, regression, interaction, terms] easy include interaction terms linear model using func tion syntax lstat black tells include interaction term lstat black syntax lstat age simultaneously includes lstat age interaction term lstat age predictors shorthand lstat age lstat age lstat age data boston lstat age data boston min max std age age 
[linear, regression, lab, linear, regression, non-linear, transformations, predictors] function also accommodate non linear transformations predictors instance given predictor create predictor using function needed since special meaning formula wrapping allows standard usage raise power perform regression medv onto lstat lstat lstat lstat lstat lstat min max linear regression std near zero value associated quadratic term suggests leads improved model use anova function anova quantify extent quadratic superior linear fit lstat fit lstat lstat lstat res rss sum model represents linear submodel containing one predictor lstat model corresponds larger quadratic model two predictors lstat lstat anova function performs hypothesis test comparing two models null hypothesis two models data equally well alternative hypothesis full model superior statistic associated value virtually zero provides clear evidence model containing predictors lstat lstat far superior model contains predictor lstat surprising since earlier saw evidence non linearity relationship medv lstat type par see lstat term included model little discernible pattern residuals order create cubic include predictor form however approach start get cumbersome higher order polynomials better approach involves using poly function poly create polynomial within example following command produces ﬁfth order polynomial lab linear regression poly lstat poly lstat min max std lstat lstat lstat lstat lstat suggests including additional polynomial terms ﬁfth order leads improvement model however investigation data reveals polynomial terms beyond ﬁfth order signiﬁ cant values regression course way restricted using polynomial transforma tions predictors try log transformation log data boston 
[linear, regression, lab, linear, regression, qualitative, predictors] examine carseats data part islr library attempt predict sales child car seat sales locations based number predictors fix age carseats data includes qualitative predictors shelveloc dicator quality shelving location space within store car seat displayed location pre dictor shelveloc takes three possible values bad medium good linear regression given qualitative variable shelveloc generates dummy variables automatically multiple regression model includes interaction terms fit income advertising price age data carseats fit income advertising price age data min max std age age contrasts function returns coding uses dummy contrasts variables bad use contrasts learn contrasts set created shelvelocgood dummy variable takes value shelving location good otherwise also created shelvelocmedium dummy variable equals shelving location medium otherwise bad shelving location corresponds zero two dummy variables fact coeﬃcient lab linear regression shelvelocgood regression output positive indicates good shelving location associated high sales relative bad location shelvelocmedium smaller positive coeﬃcient indicating medium shelving location leads higher sales bad shelving location lower sales good shelving location 
[linear, regression, lab, linear, regression, writing, functions] seen comes many useful functions still func tions available way libraries however often inter ested performing operation function available setting may want write function instance provide simple function reads islr mass libraries called loadlibraries created function returns error try call create function note symbols printed typed symbol informs multiple commands input hitting enter typing cause print symbol input many commands wish hitting enter one finally symbol informs commands entered type loadlibraries tell function call function libraries loaded print statement output linear regression 
[linear, regression, exercises, conceptual] describe null hypotheses values given table correspond explain conclusions draw based values explanation phrased terms sales radio newspaper rather terms coeﬃcients linear model carefully explain diﬀerences knn classiﬁer knn regression methods suppose data set ﬁve predictors gpa gender female male interaction gpa interaction gpa gender response starting salary graduation thousands dollars suppose use least squares model get answer correct ﬁxed value gpa males earn average females ﬁxed value gpa females earn average males iii ﬁxed value gpa males earn average females provided gpa high enough ﬁxed value gpa females earn average males provided gpa high enough predict salary female gpa true false since coeﬃcient gpa interaction term small little evidence interaction eﬀect justify answer collect set data observations containing single predictor quantitative response linear regression model data well separate cubic regression suppose true relationship linear consider training residual sum squares rss linear regression also training rss cubic regression would expect one lower would expect enough information tell justify answer exercises answer using test rather training rss suppose true relationship linear know far linear consider training rss linear regression also training rss cubic regression would expect one lower would expect enough information tell justify answer answer using test rather training rss consider ﬁtted values result performing linear regres sion without intercept setting ﬁtted value takes form show write note interpret result saying ﬁtted values linear regression linear combinations response values using argue case simple linear regression least squares line always passes point claimed text case simple linear regression onto statistic equal square correlation prove case simplicity may assume 
[linear, regression, exercises, applied] question involves use simple linear regression auto data set use function perform simple linear regression mpg response horsepower predictor use summary function print results comment output example linear regression relationship predictor sponse strong relationship predictor response iii relationship predictor response positive negative predicted mpg associated horsepower associated conﬁdence prediction intervals plot response predictor use abline function display least squares regression line use plot function produce diagnostic plots least squares regression comment problems see question involves use multiple linear regression auto data set produce scatterplot matrix includes variables data set compute matrix correlations variables using function cor need exclude name variable cor qualitative use function perform multiple linear regression mpg response variables except name predictors use summary function print results comment output instance relationship predictors sponse predictors appear statistically signiﬁcant relationship response iii coeﬃcient year variable suggest use plot function produce diagnostic plots linear regression comment problems see residual plots suggest unusually large outliers leverage plot identify observations unusually high leverage use symbols linear regression models interaction eﬀects interactions appear statistically signiﬁcant try diﬀerent transformations variables log comment ﬁndings exercises question answered using carseats data set fit multiple regression model predict sales using price urban provide interpretation coeﬃcient model careful variables model qualitative write model equation form careful handle qualitative variables properly predictors reject null hypothesis basis response previous question smaller model uses predictors evidence association outcome well models data using model obtain conﬁdence intervals coeﬃcient evidence outliers high leverage observations model problem investigate statistic null hypoth esis simple linear regression without intercept begin generate predictor response follows set perform simple linear regression onto without tercept report coeﬃcient estimate standard error coeﬃcient estimate statistic value associ ated null hypothesis comment results perform regression without intercept using command perform simple linear regression onto without intercept report coeﬃcient estimate standard error corresponding statistic values associated null hypothesis comment results relationship results obtained regression onto without intercept statistic takes form given linear regression formulas slightly diﬀerent given sec tions since performing regression without intercept show algebraically conﬁrm numeri cally statistic written using results argue statistic gression onto statistic regression onto show regression performed intercept statistic regression onto regression onto problem involves simple linear regression without intercept recall coeﬃcient estimate linear regression onto without intercept given circumstance coeﬃcient estimate regression onto coeﬃcient estimate regression onto generate example observations coeﬃcient estimate regression onto diﬀerent coeﬃcient estimate regression onto generate example observations coeﬃcient estimate regression onto coeﬃcient estimate regression onto exercise create simulated data simple linear regression models make sure use set seed prior starting part ensure consistent results using rnorm function create vector containing observations drawn distribution represents feature using rnorm function create vector eps containing observations drawn distribution normal distribution mean zero variance using eps generate vector according model length vector values linear model exercises create scatterplot displaying relationship comment observe fit least squares linear model predict using comment model obtained compare display least squares line scatterplot obtained draw population regression line plot diﬀerent color use legend command create appropriate leg end polynomial regression model predicts using evidence quadratic term improves model explain answer repeat modifying data generation process way less noise data model remain decreasing vari ance normal distribution used generate error term describe results repeat modifying data generation process way noise data model remain increasing variance normal distribution used generate error term describe results conﬁdence intervals based original data set noisier data set less noisy data set comment results problem focuses collinearity problem perform following commands set last line corresponds creating linear model function write form linear model regression coeﬃcients correlation create scatterplot displaying relationship variables using data least squares regression predict using describe results obtained relate true reject null hypothesis null hypothesis linear regression least squares regression predict using comment results reject null hypothesis least squares regression predict using comment results reject null hypothesis results obtained contradict explain answer suppose obtain one additional observation unfortunately mismeasured linear models using new data eﬀect new observation models model observation outlier high leverage point explain answers problem involves boston data set saw lab chapter try predict per capita crime rate using variables data set words per capita crime rate response variables predictors predictor simple linear regression model predict response describe results models statistically signiﬁcant association predictor response create plots back assertions fit multiple regression model predict response using predictors describe results predictors reject null hypothesis results compare results create plot displaying univariate regression coeﬃcients axis multiple regression coeﬃcients axis predictor displayed single point plot coeﬃcient simple linear regres sion model shown axis coeﬃcient estimate multiple linear regression model shown axis evidence non linear association predictors response answer question predictor model form 
[classification] linear regression model discussed chapter assumes sponse variable quantitative many situations response variable instead qualitative example eye color qualitative taking qualitative values blue brown green often qualitative variables referred categorical use terms interchangeably chapter study approaches predicting qualitative responses process known classiﬁcation predicting qualitative response obser classiﬁcation vation referred classifying observation since involves assigning observation category class hand often methods used classiﬁcation ﬁrst predict probability categories qualitative variable basis making classi ﬁcation sense also behave like regression methods many possible classiﬁcation techniques classiﬁers one classiﬁer might use predict qualitative response touched sections chapter discuss three widely used classiﬁers logistic regression linear discriminant analysis logistic regression linear discriminant analysis nearest neighbors discuss computer intensive methods later nearest neighbors chapters generalized additive models chapter trees random forests boosting chapter support vector machines chap ter james introduction statistical learning applications springer texts statistics doi springer science business media new york classiﬁcation 
[classification, overview, classification] classiﬁcation problems occur often perhaps even regression problems examples include person arrives emergency room set symptoms could possibly attributed one three medical conditions three conditions individual online banking service must able determine whether transaction performed site fraudulent basis user address past transaction history forth basis dna sequence data number patients without given disease biologist would like ﬁgure dna mutations deleterious disease causing regression setting classiﬁcation setting set training observations use build classiﬁer want classiﬁer perform well training data also test observations used train classiﬁer chapter illustrate concept classiﬁcation using simulated default data set interested predicting whether individual default credit card payment basis annual income monthly credit card balance data set displayed figure plotted annual income monthly credit card balance subset individuals left hand panel figure displays individuals defaulted given month orange blue overall default rate plotted fraction individuals default appears individuals defaulted tended higher credit card balances right hand panel figure two pairs boxplots shown ﬁrst shows distribution balance split binary default variable second similar plot income chapter learn build model predict default given value balance income since quantitative simple linear regression model chapter appropriate worth noting figure displays pronounced relation ship predictor balance response default real applications relationship predictor response nearly strong however sake illustrating classiﬁca tion procedures discussed chapter use example relationship predictor response somewhat exagger ated linear regression balance income default default yes balance yes income figure default data set left annual incomes monthly credit card balances number individuals individuals defaulted credit card payments shown orange shown blue center boxplots balance function default status right boxplots income function default status 
[classification, linear, regression] stated linear regression appropriate case qualitative response suppose trying predict medical condition patient emergency room basis symptoms simpliﬁed example three possible diagnoses stroke drug overdose epileptic seizure could consider encoding values quantita tive response variable follows stroke drug overdose epileptic seizure using coding least squares could used linear regression model predict basis set predictors unfortunately coding implies ordering outcomes putting drug overdose stroke epileptic seizure insisting diﬀerence stroke drug overdose diﬀerence drug overdose epileptic seizure practice particular reason needs case instance one could choose equally reasonable coding epileptic seizure stroke drug overdose classiﬁcation would imply totally diﬀerent relationship among three condi tions codings would produce fundamentally diﬀerent linear models would ultimately lead diﬀerent sets predictions test observations response variable values take natural ordering mild moderate severe felt gap mild moderate similar gap moderate severe coding would reasonable unfortunately general natural way convert qualitative response variable two levels quantitative response ready linear regression binary two level qualitative response situation better binary instance perhaps two possibilities patient med ical condition stroke drug overdose could potentially use dummy variable approach section code response follows stroke drug overdose could linear regression binary response predict drug overdose stroke otherwise binary case hard show even ﬂip coding linear regression produce ﬁnal predictions binary response coding regression least squares make sense shown obtained using linear regression fact estimate drug overdose special case however use linear regression estimates might outside interval see figure making hard interpret probabilities nevertheless predictions provide ordering interpreted crude probability estimates curiously turns classiﬁcations get use linear regression predict binary response linear discriminant analysis lda procedure discuss section however dummy variable approach cannot easily extended accommodate qualitative responses two levels reasons preferable use classiﬁcation method truly suited qualitative response values ones presented next 
[classification, logistic, regression] consider default data set response default falls one two categories yes rather modeling response directly logistic regression models probability belongs par ticular category logistic regression balance probability default figure classiﬁcation using default data left estimated probabil ity default using linear regression estimated probabilities negative orange ticks indicate values coded default yes right predicted probabilities default using logistic regression probabilities lie default data logistic regression models probability default example probability default given balance written default yes balance values default yes balance abbreviate balance range given value balance prediction made default example one might predict default yes individual balance alterna tively company wishes conservative predicting individuals risk default may choose use lower threshold balance 
[classification, logistic, regression, logistic, model] model relationship convenience using generic coding response section talked using linear regression model represent probabilities use approach predict default yes using balance obtain model shown left hand panel figure see problem approach balances close zero predict negative probability default predict large balances would get values bigger predictions sensible since course true probability default regardless credit card balance must fall problem unique credit default data time straight line binary response coded classiﬁcation principle always predict values others unless range limited avoid problem must model using function gives outputs values many functions meet description logistic regression use logistic function logistic function model use method called maximum likelihood maximum likelihood discuss next section right hand panel figure illustrates logistic regression model default data notice low balances predict probability default close never zero likewise high balances predict default probability close never one logistic function always produce shaped curve form regardless value obtain sensible prediction also see logistic model better able capture range probabilities linear regression model left hand plot average ﬁtted probability cases averaged training data overall proportion defaulters data set bit manipulation ﬁnd quantity called odds take value odds values odds close indicate low high probabilities default respectively example average people odds default since implies odds likewise average nine every ten people odds default since implies odds odds traditionally used instead probabilities horse racing since relate naturally correct betting strategy taking logarithm sides arrive log left hand side called log odds logit see logistic log odds logit regression model logit linear recall chapter linear regression model gives average change associated one unit increase contrast logistic regression model increasing one unit changes log odds equivalently multiplies odds however relationship straight line logistic regression correspond change associated one unit increase amount changes due one unit change depend current value regardless value positive increasing associated increasing negative increasing associated decreasing fact straight line relationship fact rate change per unit change depends current value also seen inspection right hand panel figure 
[classification, logistic, regression, estimating, regression, coeﬃcients] coeﬃcients unknown must estimated based available training data chapter used least squares approach estimate unknown linear regression coeﬃcients although could use non linear least squares model general method maximum likelihood preferred since better sta tistical properties basic intuition behind using maximum likelihood logistic regression model follows seek estimates predicted probability default individual using corresponds closely possible individual observed default status words try ﬁnd plugging estimates model given yields number close one individuals defaulted number close zero individuals intuition formalized using mathematical equation called likelihood function likelihood function amp amp estimates chosen maximize likelihood function maximum likelihood general approach used many non linear models examine throughout book linear regression setting least squares approach fact special case maximum likelihood mathematical details maximum likelihood beyond scope book however general logistic regression models easily using statistical software package need concern details maximum likelihood ﬁtting procedure table shows coeﬃcient estimates related information result ﬁtting logistic regression model default data order predict probability default yes using balance see indicates increase balance associated increase probability default precise one unit increase balance associated increase log odds default units classiﬁcation coeﬃcient std error statistic value intercept balance table default data estimated coeﬃcients logistic regres sion model predicts probability default using balance one unit increase balance associated increase log odds default units many aspects logistic regression output shown table similar linear regression output chapter example measure accuracy coeﬃcient estimates computing stan dard errors statistic table plays role statistic linear regression output example table page instance statistic associated equal large absolute value statistic indicates evidence null hypothesis null hypothesis implies words probability default depend balance since value associated balance table tiny reject words conclude indeed association balance probability default estimated intercept table typically interest main purpose adjust average ﬁtted probabilities proportion ones data 
[classification, logistic, regression, making, predictions] coeﬃcients estimated simple matter compute probability default given credit card balance example using coeﬃcient estimates given table predict default probability individual balance contrast predicted probability default individual balance much higher equals one use qualitative predictors logistic regression model using dummy variable approach section example default data set contains qualitative variable student model simply create dummy variable takes value students non students logistic regression model results predicting probability default student status seen table coeﬃcient associated dummy variable positive logistic regression coeﬃcient std error statistic value intercept student yes table default data estimated coeﬃcients logistic regres sion model predicts probability default using student status student status encoded dummy variable value student value non student represented variable student yes table associated value statistically signiﬁcant indicates students tend higher default probabilities non students default yes student yes default yes student 
[classification, logistic, regression, multiple, logistic, regression] consider problem predicting binary response using multiple predictors analogy extension simple multiple linear regression chapter follows log predictors equation rewritten section use maximum likelihood method estimate table shows coeﬃcient estimates logistic regression model uses balance income thousands dollars student status predict probability default surprising result values associated balance dummy variable student status small indicating variables associated probability default however coeﬃcient dummy variable negative indicating students less likely default non students contrast coeﬃcient dummy variable positive table possible student status associated increase probability default table decrease probability default table left hand panel figure provides graph ical illustration apparent paradox orange blue solid lines show average default rates students non students respectively classiﬁcation coeﬃcient std error statistic value intercept balance income student yes table default data estimated coeﬃcients logistic regres sion model predicts probability default using balance income student status student status encoded dummy variable student yes value student value non student ﬁtting model income measured thousands dollars function credit card balance negative coeﬃcient student multiple logistic regression indicates ﬁxed value balance income student less likely default non student indeed observe left hand panel figure student default rate non student default rate every value balance horizontal broken lines near base plot show default rates students non students averaged val ues balance income suggest opposite eﬀect overall student default rate higher non student default rate consequently positive coeﬃcient student single variable logistic regression output shown table right hand panel figure provides explanation dis crepancy variables student balance correlated students tend hold higher levels debt turn associated higher prob ability default words students likely large credit card balances know left hand panel fig ure tend associated high default rates thus even though individual student given credit card balance tend lower probability default non student credit card balance fact students whole tend higher credit card balances means overall students tend default higher rate non students important distinction credit card company trying determine oﬀer credit student riskier non student information student credit card balance available however student less risky non student credit card balance simple example illustrates dangers subtleties associated performing regressions involving single predictor predictors may also relevant linear regression setting results obtained using one predictor may quite diﬀerent tained using multiple predictors especially correlation among predictors general phenomenon seen figure known confounding confounding logistic regression credit card balance def ult rate yes student status credit card balance figure confounding default data left default rates shown students orange non students blue solid lines display default rate function balance horizontal broken lines display overall default rates right boxplots balance students orange non students blue shown substituting estimates regression coeﬃcients table make predictions example student credit card balance income estimated proba bility default non student balance income estimated prob ability default multiply income coeﬃcient estimate table rather table model income measured units 
[classification, logistic, regression, response, classes] sometimes wish classify response variable two classes example section three categories medical con dition emergency room stroke drug overdose epileptic seizure setting wish model stroke drug overdose remaining epileptic seizure stroke drug overdose two class logis tic regression models discussed previous sections multiple class extensions practice tend used often one reasons method discuss next section discriminant classiﬁcation analysis popular multiple class classiﬁcation details multiple class logistic regression simply note approach possible software available 
[classification, linear, discriminant, analysis] extend lda classiﬁer case multiple predictors assume drawn multi variate gaussian multivariate normal distribution class speciﬁc multivariate gaussian mean vector common covariance matrix begin brief review distribution multivariate gaussian distribution assumes individual pre dictor follows one dimensional normal distribution correlation pair predictors two examples multivariate gaussian distributions shown figure height surface particular point represents probability fall small region around point either panel sur face cut along axis along axis resulting cross section shape one dimensional normal distribution left hand panel figure illustrates example var var cor surface characteristic bell shape however bell shape distorted predictors correlated unequal variances illustrated right hand panel figure situation base bell elliptical rather circular linear discriminant analysis figure example three classes observations class drawn multivariate gaussian distribution class spe ciﬁc mean vector common covariance matrix left ellipses contain probability three classes shown dashed lines bayes decision boundaries right observations generated class corresponding lda decision boundaries indicated using solid black lines bayes decision boundaries shown dashed lines shape indicate dimensional random variable multi variate gaussian distribution write mean vector components cov covariance matrix formally multivariate gaussian density deﬁned exp case predictors lda classiﬁer assumes observations class drawn multivariate gaussian dis tribution class speciﬁc mean vector covariance matrix common classes plugging density function class performing little bit algebra reveals bayes classiﬁer assigns observation class log largest vector matrix version example shown left hand panel figure three equally sized gaussian classes shown class speciﬁc mean vectors common covariance matrix three ellipses represent regions con tain probability three classes dashed lines classiﬁcation bayes decision boundaries words represent set values log term disappeared three classes number training observations class note three lines representing bayes decision boundaries three pairs classes among three classes one bayes decision boundary separates class class one separates class class one separates class class three bayes decision boundaries divide predictor space three regions bayes classiﬁer classify observation according region located need estimate unknown parameters formulas similar used one dimensional case given assign new observation lda plugs estimates classiﬁes class largest note linear function lda decision rule depends linear combination elements reason word linear lda right hand panel figure observations drawn three classes displayed resulting lda decision boundaries shown solid black lines overall lda decision boundaries pretty close bayes decision boundaries shown dashed lines test error rates bayes lda classiﬁers respectively indicates lda performing well data perform lda default data order predict whether individual default basis credit card balance student status lda model training samples results training error rate sounds like low error rate two caveats must noted first training error rates usually lower test error rates real quantity interest words might expect classiﬁer perform worse use predict whether new set individuals default reason speciﬁcally adjust parameters model well training data higher ratio parameters number samples expect overﬁtting play role overﬁtting data expect problem since second since individuals training sample defaulted simple useless classiﬁer always predicts linear discriminant analysis true default status yes total predicted default status yes total table confusion matrix compares lda predictions true fault statuses training observations default data set ele ments diagonal matrix represent individuals whose default statuses correctly predicted diagonal elements represent individuals misclassiﬁed lda made incorrect predictions individuals default individuals default individual default regardless credit card balance student status result error rate words trivial null classiﬁer achieve error rate null bit higher lda training set error rate practice binary classiﬁer one make two types errors incorrectly assign individual defaults default category incorrectly assign individual default default category often interest determine two types errors made confusion matrix shown default confusion matrix data table convenient way display information table reveals lda predicted total people would default people actually defaulted hence individuals default incorrectly labeled looks like pretty low error rate however individuals defaulted missed lda overall error rate low error rate among individuals defaulted high perspective credit card company trying identify high risk individuals error rate among individuals default may well unacceptable class speciﬁc performance also important medicine biology terms sensitivity speciﬁcity characterize performance sensitivity speciﬁcity classiﬁer screening test case sensitivity percentage true defaulters identiﬁed low case speciﬁcity percentage non defaulters correctly identiﬁed lda poor job classifying customers fault words low sensitivity seen lda trying approximate bayes classiﬁer low est total error rate classiﬁers gaussian model correct bayes classiﬁer yield smallest possible total number misclassiﬁed observations irrespective class errors come misclassiﬁcations result incorrectly assigning classiﬁcation true default status yes total predicted default status yes total table confusion matrix compares lda predictions true fault statuses training observations default data set using modiﬁed threshold value predicts default individuals whose posterior default probability exceeds customer default default class others sult incorrectly assigning customer defaults non default class contrast credit card company might particularly wish avoid incorrectly classifying individual default whereas incorrectly classifying individual default though still avoided less problematic see possible modify lda order develop classiﬁer better meets credit card company needs bayes classiﬁer works assigning observation class posterior probability greatest two class case amounts assigning observation default class default yes thus bayes classiﬁer extension lda uses threshold posterior probability default order assign observation default class however concerned incorrectly pre dicting default status individuals default consider lowering threshold instance might label customer posterior probability default default class words instead assigning observation default class holds could instead assign observation class default yes error rates result taking approach shown table lda predicts individuals default individuals default lda correctly predicts vast improvement error rate resulted using threshold however improvement comes cost individuals default incorrectly classiﬁed result overall error rate increased slightly credit card company may consider slight increase total error rate small price pay accurate identiﬁcation individuals indeed default figure illustrates trade results modifying thresh old value posterior probability default various error rates linear discriminant analysis threshold error rate figure default data set error rates shown function threshold value posterior probability used perform assign ment black solid line displays overall error rate blue dashed line represents fraction defaulting customers incorrectly classiﬁed orange dotted line indicates fraction errors among non defaulting customers shown function threshold value using threshold minimizes overall error rate shown black solid line expected since bayes classiﬁer uses threshold known lowest overall error rate threshold used error rate among individuals default quite high blue dashed line threshold reduced error rate among individuals default decreases steadily error rate among individuals default increases decide threshold value best decision must based domain knowledge detailed information costs associated default roc curve popular graphic simultaneously displaying roc curve two types errors possible thresholds name roc toric comes communications theory acronym receiver operating characteristics figure displays roc curve lda classiﬁer training data overall performance classiﬁer sum marized possible thresholds given area roc curve auc ideal roc curve hug top left corner larger area roc curve auc better classiﬁer data auc close maximum one would considered good expect classiﬁer performs better chance auc evaluated independent test set used model training roc curves useful comparing diﬀerent classiﬁers since take account possible thresholds turns roc curve logistic regression model section data virtually indis tinguishable one lda model display seen varying classiﬁer threshold changes true positive false positive rate also called sensitivity one sensitivity classiﬁcation roc curve false positive rate rue positiv ate figure roc curve lda classiﬁer default data traces two types error vary threshold value posterior probability default actual thresholds shown true positive rate sensitivity fraction defaulters correctly identiﬁed using given threshold value false positive rate speciﬁcity fraction non defaulters classify incorrectly defaulters using threshold value ideal roc curve hugs top left corner indicating high true positive rate low false positive rate dotted line represents information classiﬁer would expect student status credit card balance associated probability default predicted class null non null total true null true neg false pos class non null false neg true pos total table possible results applying classiﬁer diagnostic test population minus speciﬁcity classiﬁer since almost bewildering speciﬁcity array terms used context give summary table shows possible results applying classiﬁer diagnostic test population make connection epidemiology literature think disease trying detect non disease state make connection classical hypothesis testing literature think null hypothesis alternative non null hypothesis context default data indicates individual defaults indicates one linear discriminant analysis name deﬁnition synonyms false pos rate type error speciﬁcity true pos rate type error power sensitivity recall pos pred value precision false discovery proportion neg pred value table important measures classiﬁcation diagnostic testing derived quantities table table lists many popular performance measures used context denominators false positive true positive rates actual population counts class contrast denominators positive predictive value negative predictive value total predicted counts class 
[classification, linear, discriminant, analysis, using, bayes, theorem, classiﬁcation] suppose wish classify observation one classes words qualitative response variable take possible distinct unordered values let represent overall prior prior probability randomly chosen observation comes class probability given observation associated category response variable let denote density function observation comes class density function words relatively large high probability observation class small technically definition correct discrete random variabl would correspond probability ling small region around continuous linear discriminant analysis unlikely observation class bayes theorem states bayes theorem accordance earlier notation use abbreviation suggests instead directly computing section simply plug estimates general estimating easy random sample population simply compute fraction training observations belong class however estimating tends challenging unless assume simple forms densities refer posterior probability observation posterior belongs class probability observation belongs class given predictor value observation know chapter bayes classiﬁer classiﬁes observation class largest lowest possible error rate classiﬁers course true terms correctly speciﬁed therefore ﬁnd way estimate develop classiﬁer approximates bayes classiﬁer approach topic following sections 
[classification, linear, discriminant, analysis, quadratic, discriminant, analysis] discussed lda assumes observations within class drawn multivariate gaussian distribution class speciﬁc mean vector covariance matrix common classes quadratic discriminant analysis qda provides alternative quadratic discriminant analysis approach like lda qda classiﬁer results assuming observations class drawn gaussian distribution plugging estimates parameters bayes theorem order per form prediction however unlike lda qda assumes class covariance matrix assumes observation class form covariance matrix class assumption bayes classiﬁer assigns observation class log log log log largest qda classiﬁer involves plugging estimates assigning observation class quantity largest unlike quantity appears quadratic function qda gets name matter whether assume classes share common covariance matrix words would one prefer lda qda vice versa answer lies bias variance trade predictors estimating covariance matrix requires esti mating parameters qda estimates separate covariance matrix class total parameters predictors classiﬁcation figure left bayes purple dashed lda black dotted qda green solid decision boundaries two class problem shading indicates qda decision rule since bayes decision boundary linear accurately approximated lda qda right details given left hand panel except since bayes decision boundary non linear accurately approximated qda lda multiple lot parameters instead assum ing classes share common covariance matrix lda model becomes linear means linear coeﬃcients esti mate consequently lda much less ﬂexible classiﬁer qda substantially lower variance potentially lead improved prediction performance trade lda assumption classes share common covariance matrix badly lda suﬀer high bias roughly speaking lda tends better bet qda relatively training observations reducing variance crucial contrast qda recommended training set large variance classiﬁer major concern assumption common covariance matrix classes clearly untenable figure illustrates performances lda qda two scenarios left hand panel two gaussian classes common correla tion result bayes decision boundary linear accurately approximated lda decision boundary qda decision boundary inferior suﬀers higher vari ance without corresponding decrease bias contrast right hand panel displays situation orange class correlation variables blue class correlation bayes decision boundary quadratic qda accurately approximates boundary lda comparison classiﬁcation methods 
[classification, comparison, classification, methods] chapter considered three diﬀerent classiﬁcation approaches logistic regression lda qda chapter also discussed nearest neighbors knn method consider types scenarios one approach might dominate others though motivations diﬀer logistic regression lda methods closely connected consider two class setting predictor let probabilities observation belongs class class respectively lda framework see bit simple algebra log odds given log log functions know logistic regression log linear functions hence logistic gression lda produce linear decision boundaries diﬀerence two approaches lies fact estimated using maximum likelihood whereas computed using esti mated mean variance normal distribution connection lda logistic regression also holds multidimensional data since logistic regression lda diﬀer ﬁtting procedures one might expect two approaches give similar results often always case lda assumes observations drawn gaussian distribution common covariance matrix class provide improvements logistic regression assumption approximately holds conversely logistic regression outperform lda gaussian assumptions met recall chapter knn takes completely diﬀerent approach classiﬁers seen chapter order make prediction observation training observations closest identiﬁed assigned class plurality observations belong hence knn completely non parametric approach assumptions made shape decision boundary fore expect approach dominate lda logistic regression decision boundary highly non linear hand knn tell predictors important get table coeﬃcients table classiﬁcation scenario scenario knn knn lda logistic qda knn knn lda logistic qda knn knn lda logistic qda scenario figure boxplots test error rates linear scenarios described main text knn knn lda logistic qda knn knn lda logistic qda knn knn lda logistic qda scenario scenario scenario figure boxplots test error rates non linear sce narios described main text finally qda serves compromise non parametric knn method linear lda logistic regression approaches since qda assumes quadratic decision boundary accurately model wider range problems linear methods though ﬂexible knn qda perform better presence limited number training observations make assumptions form decision boundary illustrate performances four classiﬁcation approaches generated data six diﬀerent scenarios three scenarios bayes decision boundary linear remaining scenarios non linear scenario produced random training data sets training sets method data computed resulting test error rate large test set results linear scenarios shown figure results non linear scenarios figure knn method requires selection number neighbors performed knn two values comparison classiﬁcation methods value chosen automatically using approach called cross validation discuss chapter six scenarios predictors scenarios follows scenario training observations two classes observations within class uncorrelated random normal variables diﬀerent mean class left hand panel figure shows lda performed well setting one would expect since model assumed lda knn performed poorly paid price terms variance oﬀset reduction bias qda also performed worse lda since ﬂexible classiﬁer necessary since logistic regression assumes linear decision boundary results slightly inferior lda scenario details scenario except within class two predictors correlation center panel figure indicates little change relative performances methods compared previous scenario scenario generated distribution distribution observations per class distribution similar shape normal distribution tendency yield extreme points points far mean set ting decision boundary still linear logistic regression framework set violated assumptions lda since observations drawn normal distribution right hand panel figure shows logistic regression outperformed lda though methods superior approaches particular qda results deteriorated considerably consequence non normality scenario data generated normal distribution correlation predictors ﬁrst class correlation predictors second class setup corresponded qda assumption resulted quadratic decision boundaries left hand panel figure shows qda outperformed approaches scenario within class observations generated normal distribution uncorrelated predictors however sponses sampled logistic function using predictors consequently quadratic decision boundary center panel figure indicates qda performed best followed closely knn linear meth ods poor performance classiﬁcation scenario details previous scenario responses sampled complicated non linear function sult even quadratic decision boundaries qda could ade quately model data right hand panel figure shows qda gave slightly better results linear methods much ﬂexible knn method gave best results knn gave worst results methods highlights fact even data exhibits complex non linear relationship non parametric method knn still give poor results level smoothness chosen correctly six examples illustrate one method dominate oth ers every situation true decision boundaries linear lda logistic regression approaches tend perform well boundaries moderately non linear qda may give better results finally much complicated decision boundaries non parametric approach knn superior level smoothness non parametric approach must chosen carefully next chapter examine number approaches choosing correct level smooth ness general selecting best overall method finally recall chapter regression setting accom modate non linear relationship predictors response performing regression using transformations predictors similar approach could taken classiﬁcation setting instance could create ﬂexible version logistic regression including even predictors may may improve logistic regres sion performance depending whether increase variance due added ﬂexibility oﬀset suﬃciently large reduction bias could lda added possible quadratic terms cross products lda form model would qda model although parameter estimates would diﬀerent device allows move somewhere lda qda model 
[classification, lab, logistic, regression, lda, qda, knn, stock, market, data] begin examining numerical graphical summaries smarket data part islr library data set consists percentage returns samp stock index days beginning end date recorded percentage returns ﬁve previous trading days lag lag also recorded volume number shares traded lab logistic regression lda qda knn previous day billions today percentage return date question direction whether market date dim min min min max max max min min min max max max min min max max cor function produces matrix contains pairwise correlations among predictors data set ﬁrst command gives error message direction variable qualitative cor cor cor classiﬁcation one would expect correlations lag variables day returns close zero words appears little correlation today returns previous days returns substantial correlation year volume plotting data see volume increasing time words average number shares traded daily increased 
[classification, lab, logistic, regression, lda, qda, knn, logistic, regression] next logistic regression model order predict direction using lag lag volume glm function ﬁts generalized glm linear models class models includes logistic regression syntax generalized linear model glm function similar except must pass argument family binomial order tell run logistic regression rather type generalized linear model glm lag lag lag lag lag volume min max std glm fit glm lag lag lag lag lag volume smarket glm fit lab logistic regression lda qda knn aic smallest value associated lag negative coeﬃcient predictor suggests market positive return yesterday less likely today however value value still relatively large clear evidence real association lag direction use coef function order access coeﬃcients ﬁtted model also use summary function access particular aspects ﬁtted model values coeﬃcients std predict function used predict probability market given values predictors type response option tells output probabilities form opposed information logit data set supplied predict function probabilities computed training data used logistic regression model printed ﬁrst ten probabilities know values correspond probability market going rather contrasts function indicates created dummy variable glm glm fit glm fit glm fit glm glm fit classiﬁcation order make prediction whether market particular day must convert predicted probabilities class labels following two commands create vector class predictions based whether predicted probability market increase greater less glm rep glm glm probs ﬁrst command creates vector elements second line transforms elements predicted probability market increase exceeds given predictions table function table used produce confusion matrix order determine many observations correctly incorrectly classiﬁed glm pred glm glm diagonal elements confusion matrix indicate correct predictions diagonals represent incorrect predictions hence model correctly predicted market would days would days total correct predictions mean function used compute fraction days prediction correct case logistic regression correctly predicted movement market time ﬁrst glance appears logistic regression model working little better random guessing however result misleading trained tested model set observa tions words training error rate seen previously training error rate often overly optimistic tends underestimate test error rate order better assess curacy logistic regression model setting model using part data examine well predicts held data yield realistic error rate sense prac tice interested model performance data used model rather days future market movements unknown lab logistic regression lda qda knn implement strategy ﬁrst create vector corresponding observations use vector create held data set observations year train dim object train vector elements corresponding servations data set elements vector correspond observations occurred set true whereas correspond observations set false object train boolean vector since elements true false boolean vectors boolean used obtain subset rows columns matrix instance command smarket train would pick submatrix stock market data set corresponding dates since ones elements train true symbol used reverse elements boolean vector train vector similar train except elements true train get swapped false train elements false train get swapped true train therefore smarket train yields submatrix stock market data containing observations train false observations dates output indicates observations logistic regression model using subset obser vations correspond dates using subset argument obtain predicted probabilities stock market going days test set days smarket notice trained tested model two completely sep arate data sets training performed using dates testing performed using dates finally com pute predictions compare actual movements market time period glm rep glm glm probs glm pred glm glm glm fit glm lag lag lag lag lag volume glm glm fit classiﬁcation glm notation means equal last command computes test set error rate results rather disappointing test error rate worse random guessing course result surprising given one would generally expect able use previous days returns predict future market performance possible authors book would striking rich rather writing statistics textbook recall logistic regression model underwhelming values associated predictors smallest value though small corresponded lag perhaps removing variables appear helpful predicting direction obtain eﬀective model using predictors relationship response tends cause deterioration test error rate since predictors cause increase variance without corresponding decrease bias removing predictors may turn yield improvement reﬁt logistic regression using lag lag seemed highest predictive power original logistic regression model glm rep glm glm probs glm pred glm glm results appear little better daily movements correctly predicted worth noting case much simpler strategy predicting market increase every day also correct time hence terms overall error rate logistic regression method better ıve approach however confusion matrix shows days logistic regression predicts increase market accuracy rate suggests possible trading strategy buying days model predicts creasing market avoiding trades days decrease predicted course one would need investigate carefully whether small improvement real due random chance glm fit glm lag lag data smarket family binomial glm glm fit lab logistic regression lda qda knn suppose want predict returns associated particular values lag lag particular want predict direction day lag lag equal respectively day equal using predict function 
[classification, lab, logistic, regression, lda, qda, knn, linear, discriminant, analysis] perform lda smarket data lda model using lda function part mass library notice lda syntax lda function identical glm except absence family option model using observations lda fit lda lag lag data smarket subset train lda fit lda lag lag data smarket subset train lda fit lda output indicates words training observations correspond days market went also provides group means average predictor within class used lda estimates suggest tendency previous days returns negative days market increases tendency previous days returns positive days market declines coeﬃcients linear discriminants output provides linear combination lag lag used form lda decision rule words multipliers elements lag lag large lda classiﬁer glm fit classiﬁcation predict market increase small lda classiﬁer predict market decline plot function produces plots linear discriminants obtained computing lag lag training observations predict function returns list three elements ﬁrst ele ment class contains lda predictions movement market second element posterior matrix whose column contains posterior probability corresponding observation belongs class computed finally contains linear discriminants described earlier lda lda fit lda observed section lda logistic regression predictions almost identical lda lda lda class lda lda applying threshold posterior probabilities allows recre ate predictions contained lda pred class sum lda sum lda notice posterior probability output model corresponds probability market decrease lda lda wanted use posterior probability threshold order make predictions could easily instance suppose wish predict market decrease certain market indeed decrease day say posterior probability least sum lda days meet threshold fact greatest posterior prob ability decrease lab logistic regression lda qda knn 
[classification, lab, logistic, regression, lda, qda, knn, quadratic, discriminant, analysis] qda model smarket data qda implemented using qda function also part mass library qda syntax identical lda qda fit qda lag lag data smarket subset train qda fit qda lag lag data smarket subset train output contains group means contain coef ﬁcients linear discriminants qda classiﬁer involves quadratic rather linear function predictors predict function works exactly fashion lda qda qda fit qda class qda qda interestingly qda predictions accurate almost time even though data used model level accu racy quite impressive stock market data known quite hard model accurately suggests quadratic form assumed qda may capture true relationship accurately linear forms assumed lda logistic regression however recommend evaluating method performance larger test set betting approach consistently beat market 
[classification, lab, logistic, regression, lda, qda, knn, k-nearest, neighbors] perform knn using knn function part knn class library function works rather diﬀerently model ﬁtting functions encountered thus far rather two step approach ﬁrst model use model make predictions knn forms predictions using single command function requires four inputs classiﬁcation matrix containing predictors associated training data labeled train matrix containing predictors associated data wish make predictions labeled test vector containing class labels training observations labeled train direction value number nearest neighbors used classiﬁer use cbind function short column bind bind lag cbind lag variables together two matrices one training set test set lag train lag train knn function used predict market movement dates set random seed apply knn several observations tied nearest neighbors randomly break tie therefore seed must set order ensure reproducibil ity results set knn knn knn pred knn results using good since observa tions correctly predicted course may results overly ﬂexible data repeat analysis using knn knn knn pred knn knn results improved slightly increasing turns provide improvements appears data qda provides best results methods examined far lab logistic regression lda qda knn 
[classification, lab, logistic, regression, lda, qda, knn, application, caravan, insurance, data] finally apply knn approach caravan data set part islr library data set includes predictors measure demographic characteristics individuals response variable purchase indicates whether given individual purchases caravan insurance policy data set people purchased caravan insurance dim yes knn classiﬁer predicts class given test observation identifying observations nearest scale variables matters variables large scale much larger eﬀect distance observations hence knn classiﬁer variables small scale instance imagine data set contains two variables salary age measured dollars years respectively far knn concerned diﬀerence salary enormous compared diﬀerence years age conse quently salary drive knn classiﬁcation results age almost eﬀect contrary intuition salary diﬀerence quite small compared age diﬀerence years importance scale knn classiﬁer leads another issue measured salary japanese yen measured age minutes get quite diﬀerent classiﬁcation results get two variables measured dollars years good way handle problem standardize data standardize variables given mean zero standard deviation one variables comparable scale scale function scale standardizing data exclude column qualitative purchase variable var var var var every column standardized standard deviation one mean zero classiﬁcation split observations test set containing ﬁrst observations training set containing remaining observations knn model training data using evaluate performance test data test test set knn knn knn vector test numeric values typing standardized test yields submatrix data containing servations whose indices range whereas typing standardized test yields submatrix containing observations whose indices range knn error rate test observations ﬁrst glance may pear fairly good however since customers purchased insurance could get error rate always predicting regardless values predictors suppose non trivial cost trying sell insurance given individual instance perhaps salesperson must visit potential customer company tries sell insurance random selection customers success rate may far low given costs involved instead company would like try sell insurance customers likely buy overall error rate interest instead fraction individuals correctly predicted buy insurance interest turns knn far better random guessing among customers predicted buy insurance among customers actually purchase insurance double rate one would obtain random guessing knn pred knn yes yes using success rate increases rate four times rate results random guessing appears knn ﬁnding real patterns diﬃcult data set lab logistic regression lda qda knn knn knn knn pred knn yes yes knn knn knn pred knn yes yes comparison also logistic regression model data use predicted probability cut classiﬁer problem seven test observations predicted purchase insurance even worse wrong however required use cut instead predict purchase time predicted probability purchase exceeds get much better results predict people purchase insurance correct people ﬁve times better random guessing glm rep glm glm probs yes glm pred glm yes yes glm rep glm glm probs yes glm pred glm yes yes glm fit glm data caravan family binomial glm test fit fit classiﬁcation 
[classification, exercises, conceptual] using little bit algebra prove words logistic function representation logit represen tation logistic regression model equivalent stated text classifying observation class largest equivalent classifying observation class largest prove case words assumption observations class drawn distribution bayes classiﬁer assigns observation class discriminant function maximized problem relates qda model observations within class drawn normal distribution class speciﬁc mean vector class speciﬁc covariance matrix con sider simple case one feature suppose classes observation belongs class comes one dimensional normal dis tribution recall density function one dimensional normal distribution given prove case bayes classiﬁer linear argue fact quadratic hint problem follow arguments laid section without making assumption number features large tends deteri oration performance knn local approaches perform prediction using observations near test servation prediction must made phenomenon known curse dimensionality ties fact curse mensionality non parametric approaches often perform poorly large investigate curse suppose set observations measure ments feature assume uniformly evenly distributed associated observation response value suppose wish predict test obser vation response using observations within range closest test observation instance order predict response test observation exercises use observations range average fraction available observations use make prediction suppose set observations measurements features assume uniformly distributed wish predict test observation response using observations within range within range closest test observation instance order predict response test observation use observations range range average fraction available observations use make prediction suppose set observations fea tures observations uniformly distributed feature feature ranges value wish predict test observation response using observations within feature range closest test observation fraction available observations use make prediction using answers parts argue drawback knn large training obser vations near given test observation suppose wish make prediction test obser vation creating dimensional hypercube centered around test observation contains average train ing observations length side hypercube comment answer note hypercube generalization cube arbitrary number dimensions hypercube simply line segment square dimensional cube examine diﬀerences lda qda bayes decision boundary linear expect lda qda perform better training set test set bayes decision boundary non linear expect lda qda perform better training set test set general sample size increases expect test prediction accuracy qda relative lda improve decline unchanged classiﬁcation true false even bayes decision boundary given problem linear probably achieve superior test ror rate using qda rather lda qda ﬂexible enough model linear decision boundary justify answer suppose collect data group students statistics class variables hours studied undergrad gpa receive logistic regression produce estimated coeﬃcient estimate probability student studies undergrad gpa gets class many hours would student part need study chance getting class suppose wish predict whether given stock issue dividend year yes based last year percent proﬁt examine large number companies discover mean value companies issued dividend mean addition variance two sets companies finally companies issued dividends assuming follows mal distribution predict probability company issue dividend year given percentage proﬁt last year hint recall density function normal random variable need use bayes theorem suppose take data set divide equally sized training test sets try two diﬀerent classiﬁcation procedures first use logistic regression get error rate training data test data next use nearest neigh bors get average error rate averaged test training data sets based results method prefer use classiﬁcation new observations problem odds average fraction people odds defaulting credit card payment fact default suppose individual chance defaulting credit card payment odds fault exercises 
[classification, exercises, applied] question answered using weekly data set part islr package data similar nature smarket data chapter lab except contains weekly returns years beginning end produce numerical graphical summaries weekly data appear patterns use full data set perform logistic regression direction response ﬁve lag variables plus volume predictors use summary function print results predictors appear statistically signiﬁcant ones compute confusion matrix overall fraction correct predictions explain confusion matrix telling types mistakes made logistic regression logistic regression model using training data period lag predictor compute confusion matrix overall fraction correct predictions held data data repeat using lda repeat using qda repeat using knn methods appears provide best results data experiment diﬀerent combinations predictors includ ing possible transformations interactions methods report variables method associated confu sion matrix appears provide best results held data note also experiment values knn classiﬁer problem develop model predict whether given car gets high low gas mileage based auto data set create binary variable mpg contains mpg contains value median mpg contains value median compute median using median function note may ﬁnd helpful use data frame function create single data set containing mpg auto variables classiﬁcation explore data graphically order investigate associ ation mpg features features seem likely useful predicting mpg scat terplots boxplots may useful tools answer ques tion describe ﬁndings split data training set test set perform lda training data order predict mpg using variables seemed associated mpg test error model obtained perform qda training data order predict mpg using variables seemed associated mpg test error model obtained perform logistic regression training data order pre dict mpg using variables seemed associated mpg test error model obtained perform knn training data several values order predict mpg use variables seemed associated mpg test errors obtain value seems perform best data set problem involves writing functions write function power prints result raising power words function compute print results hint recall raises power use print function output result create new function power allows pass two numbers prints value beginning function line able call function entering instance command line output value namely using power function wrote compute create new function power actually returns result object rather simply printing screen store value object called result within function simply return return result using following line exercises line last line function symbol using power function create plot axis display range integers axis display label axes appropriately use appropriate title ﬁgure consider displaying either axis axis log scale using log log log arguments plot function create function plotpower allows create plot ﬁxed range values instance call plot created axis taking values axis taking values using boston data set classiﬁcation models order predict whether given suburb crime rate median explore logistic regression lda knn models using various sub sets predictors describe ﬁndings 
[resampling, methods] resampling methods indispensable tool modern statistics involve repeatedly drawing samples training set reﬁtting model interest sample order obtain additional information ﬁtted model example order estimate variability linear regression repeatedly draw diﬀerent samples training data linear regression new sample examine extent resulting ﬁts diﬀer approach may allow obtain information would available ﬁtting model using original training sample resampling approaches computationally expensive involve ﬁtting statistical method multiple times using diﬀerent subsets training data however due recent advances computing power computational requirements resampling methods generally prohibitive chapter discuss two commonly used resampling methods cross validation bootstrap methods important tools practical application many statistical learning procedures example cross validation used estimate test error associated given statistical learning method order evaluate performance select appropriate level ﬂexibility process evaluating model performance known model assessment whereas model assessment process selecting proper level ﬂexibility model known model selection bootstrap used several contexts commonly model selection provide measure accuracy parameter estimate given statistical learning method james introduction statistical learning applications springer texts statistics doi springer science business media new york resampling methods 
[resampling, methods, cross-validation] chapter discuss distinction test error rate training error rate test error average error results using statistical learning method predict response new observation measurement used training method given data set use particular statistical learning method warranted results low test error test error easily calculated designated test set available unfortunately usually case contrast training error easily calculated applying statistical learning method observations used training saw chapter training error rate often quite diﬀerent test error rate particular former dramatically underestimate latter absence large designated test set used directly estimate test error rate number techniques used estimate quantity using available training data methods make mathematical adjustment training error rate order estimate test error rate approaches discussed chapter section instead consider class methods estimate test error rate holding subset training observations ﬁtting process applying statistical learning method held observations sections simplicity assume interested performing regression quantitative response section consider case classiﬁcation qualitative response see key concepts remain regardless whether response quantitative qualitative 
[resampling, methods, cross-validation, validation, set, approach] suppose would like estimate test error associated ting particular statistical learning method set observations validation set approach displayed figure simple strategy validation set approach task involves randomly dividing available set observa tions two parts training set validation set hold set validation set hold set model training set ﬁtted model used predict responses observations validation set resulting validation set error rate typically assessed using mse case quantitative response provides estimate test error rate illustrate validation set approach auto data set recall chapter appears non linear relationship mpg horsepower model predicts mpg using horsepower horsepower gives better results model uses linear term natural wonder whether cubic higher order might provide cross validation figure schematic display validation set approach set observations randomly split training set shown blue containing observations among others validation set shown beige containing observation among others statistical learning method training set performance evaluated validation set even better results answer question chapter looking values associated cubic term higher order polynomial terms linear regression could also answer question using validation method randomly split observations two sets training set containing data points validation set containing remaining observations validation set error rates result ﬁtting various regression models training sample evaluating performance validation sample using mse measure validation set error shown left hand panel figure validation set mse quadratic considerably smaller linear however validation set mse cubic actually slightly larger quadratic implies including cubic term regression lead better prediction simply using quadratic term recall order create left hand panel figure ran domly divided data set two parts training set validation set repeat process randomly splitting sample set two parts get somewhat diﬀerent estimate test mse illustration right hand panel figure displays ten diﬀerent vali dation set mse curves auto data set produced using ten diﬀerent random splits observations training validation sets ten curves indicate model quadratic term dramatically smaller validation set mse model linear term fur thermore ten curves indicate much beneﬁt including cubic higher order polynomial terms model worth noting ten curves results diﬀerent test mse estimate ten regression models considered consensus among curves model results smallest validation set mse based variability among curves conclude conﬁdence linear adequate data validation set approach conceptually simple easy imple ment two potential drawbacks resampling methods degree polynomial mean squared error degree polynomial mean squared error figure validation set approach used auto data set order estimate test error results predicting mpg using polynomial functions horsepower left validation error estimates single split training validation data sets right validation method repeated ten times time using diﬀerent random split observations training set validation set illustrates variability estimated test mse results approach shown right hand panel figure validation esti mate test error rate highly variable depending pre cisely observations included training set observations included validation set validation approach subset observations included training set rather validation set used model since statistical methods tend per form worse trained fewer observations suggests validation set error rate may tend overestimate test error rate model entire data set coming subsections present cross validation reﬁnement validation set approach addresses two issues 
[resampling, methods, cross-validation, leave-one-out, cross-validation] leave one cross validation loocv closely related validation leave one cross validation set approach section attempts address method drawbacks like validation set approach loocv involves splitting set observations two parts however instead creating two subsets comparable size single observation used validation set remaining observations make training set statistical learning method training observations prediction made excluded observation using value since used ﬁtting process mse cross validation figure schematic display loocv set data points repeat edly split training set shown blue containing one observation validation set contains observation shown beige test error estimated averaging resulting mse ﬁrst training set contains observation second training set contains observation forth provides approximately unbiased estimate test error even though mse unbiased test error poor estimate highly variable since based upon single observation repeat procedure selecting validation data training statistical learning procedure observations computing mse repeat ing approach times produces squared errors mse mse loocv estimate test mse average test error estimates mse schematic loocv approach illustrated figure loocv couple major advantages validation set proach first far less bias loocv repeatedly sta tistical learning method using training sets contain observa tions almost many entire data set contrast validation set approach training set typically around half size original data set consequently loocv approach tends overestimate test error rate much validation set approach second contrast validation approach yield diﬀerent results applied repeatedly due randomness training validation set splits performing loocv multiple times resampling methods loocv degree polynomial mean squared error fold degree polynomial mean squared error figure cross validation used auto data set order timate test error results predicting mpg using polynomial functions horsepower left loocv error curve right fold run nine separate times diﬀerent random split data ten parts ﬁgure shows nine slightly diﬀerent error curves always yield results randomness training vali dation set splits used loocv auto data set order obtain estimate test set mse results ﬁtting linear regression model predict mpg using polynomial functions horsepower results shown left hand panel figure loocv potential expensive implement since model times time consuming large individual model slow least squares linear polynomial regression amazing shortcut makes cost loocv single model following formula holds ﬁtted value original least squares leverage deﬁned page like ordinary mse except residual divided leverage lies reﬂects amount observation inﬂuences hence residuals high leverage points inﬂated formula exactly right amount equality hold loocv general method used kind predictive modeling example could use logistic regression linear discriminant analysis methods discussed later cross validation figure schematic display fold set observations randomly split ﬁve non overlapping groups ﬁfths acts validation set shown beige remainder training set shown blue test error estimated averaging ﬁve resulting mse estimates chapters magic formula hold general case model reﬁt times 
[resampling, methods, cross-validation, k-fold, cross-validation] mentioned section fold compu tational advantage loocv putting computational issues aside less obvious potentially important advantage fold often gives accurate estimates test error rate loocv bias variance trade mentioned section validation set approach lead overestimates test error rate since approach training set used statistical learning method contains half observations entire data set using logic hard see loocv give approximately unbiased estimates test error since training set contains observations almost many number observations full data set performing fold say lead intermediate level bias since training set contains observations fewer loocv approach substantially validation set approach therefore perspective bias reduction clear loocv preferred fold however know bias source concern esti mating procedure must also consider procedure variance turns loocv higher variance fold case perform loocv eﬀect averaging outputs ﬁtted models trained almost identical set observations therefore outputs highly positively corre lated contrast perform fold averaging outputs ﬁtted models somewhat less correlated since overlap training sets model smaller since mean many highly correlated quantities resampling methods higher variance mean many quantities highly correlated test error estimate resulting loocv tends higher variance test error estimate resulting fold summarize bias variance trade associated choice fold cross validation typically given considerations one performs fold cross validation using values shown empirically yield test error rate estimates suﬀer neither excessively high bias high variance 
[resampling, methods, cross-validation, cross-validation, classiﬁcation, problems] chapter far illustrated use cross validation regression setting outcome quantitative used mse quantify test error cross validation also useful approach classiﬁcation setting qualitative setting cross validation works described earlier chapter except rather using mse quantify test error instead use number misclassiﬁed observations instance classiﬁcation setting loocv error rate takes form err err fold error rate validation set error rates deﬁned analogously example various logistic regression models two dimensional classiﬁcation data displayed figure top left panel figure black solid line shows estimated decision bound ary resulting ﬁtting standard logistic regression model data set since simulated data compute true test error rate takes value substantially larger bayes error rate clearly logistic regression enough ﬂexi bility model bayes decision boundary setting easily extend logistic regression obtain non linear decision boundary using polynomial functions predictors regression setting section example quadratic logistic regression model given log top right panel figure displays resulting decision boundary curved however test error rate improved slightly much larger improvement apparent bottom left panel cross validation degree degree degree degree figure logistic regression ﬁts two dimensional classiﬁcation data displayed figure bayes decision boundary represented using purple dashed line estimated decision boundaries linear quadratic cubic quartic degrees logistic regressions displayed black test error rates four logistic regression ﬁts respectively bayes error rate figure logistic regression model involving cubic polynomials predictors test error rate decreased going quartic polynomial bottom right slightly increases test error practice real data bayes decision boundary test ror rates unknown might decide four logistic regression models displayed figure use cross validation order make decision left hand panel figure displays resampling methods order polynomials used error rate error rate figure test error brown training error blue fold error black two dimensional classiﬁcation data displayed figure left logistic regression using polynomial functions predictors order polynomials used displayed axis right knn classiﬁer diﬀerent values number neighbors used knn classiﬁer black fold error rates result ﬁtting ten logistic regres sion models data using polynomial functions predictors tenth order true test errors shown brown training errors shown blue seen previously training error tends decrease ﬂexibility increases ﬁgure indicates though training error rate quite decrease monotonically tends decrease whole model complexity increases contrast test error displays characteristic shape fold error rate provides pretty good approximation test error rate somewhat underestimates error rate reaches minimum fourth order polynomials used close min imum test curve occurs third order polynomials used fact using fourth order polynomials would likely lead good test set performance true test error rate approximately third fourth ﬁfth sixth order polynomials right hand panel figure displays three curves ing knn approach classiﬁcation function value context indicates number neighbors used knn classiﬁer rather number folds used training error rate declines method becomes ﬂexible see training error rate cannot used select optimal value though cross validation error curve slightly underestimates test error rate takes minimum close best value bootstrap 
[resampling, methods, bootstrap] bootstrap widely applicable extremely powerful statistical tool bootstrap used quantify uncertainty associated given esti mator statistical learning method simple example bootstrap used estimate standard errors coeﬃcients linear regression speciﬁc case linear regression particularly useful since saw chapter standard statistical software outputs standard errors automatically however power bootstrap lies fact easily applied wide range statistical learning methods including measure vari ability otherwise diﬃcult obtain automatically output statistical software section illustrate bootstrap toy example wish determine best investment allocation simple model section explore use bootstrap assess variability associated regression coeﬃcients linear model suppose wish invest ﬁxed sum money two ﬁnancial assets yield returns respectively random quantities invest fraction money invest remaining since variability associated returns two assets wish choose minimize total risk variance investment words want minimize var one show value minimizes risk given var var cov reality quantities unknown compute estimates quantities using data set contains past measurements estimate value minimizes variance investment using figure illustrates approach estimating simulated data set panel simulated pairs returns investments used returns estimate substituted order obtain estimates value resulting simulated data set ranges natural wish quantify accuracy estimate estimate standard deviation repeated process simu lating paired observations estimating using resampling methods figure panel displays simulated returns investments left right top bottom resulting estimates times thereby obtained estimates call left hand panel figure displays histogram resulting estimates simulations parameters set know true value indicated value using solid vertical line histogram mean estimates close standard deviation estimates gives good idea accuracy roughly speaking random sample population would expect diﬀer approximately average practice however procedure estimating outlined cannot applied real data cannot generate new samples original population however bootstrap approach allows use computer emulate process obtaining new sample sets bootstrap true bootstrap figure left histogram estimates obtained generating simulated data sets true population center histogram estimates obtained bootstrap samples single data set right estimates displayed left center panels shown boxplots panel pink line indicates true value estimate variability without generating additional samples rather repeatedly obtaining independent data sets population instead obtain distinct data sets repeatedly sampling observations original data set approach illustrated figure simple data set call contains observations randomly select observations data set order produce bootstrap data set sampling performed replacement means replacement observation occur bootstrap data set example contains third observation twice ﬁrst observation instances second observation note observation contained values included use produce new bootstrap estimate call procedure repeated times large value order produce diﬀerent bootstrap data sets corresponding estimates compute standard error bootstrap estimates using formula serves estimate standard error estimated original data set bootstrap approach illustrated center panel figure displays histogram bootstrap estimates com puted using distinct bootstrap data set panel constructed basis single data set hence could created using real data resampling methods obs obs original data obs obs figure graphical illustration bootstrap approach small sample containing observations bootstrap data set contains obser vations sampled replacement original data set bootstrap data set used obtain estimate note histogram looks similar left hand panel dis plays idealized histogram estimates obtained generating simulated data sets true population particular boot strap estimate close estimate obtained using simulated data sets right hand panel displays information center left panels diﬀerent way via boxplots estimates obtained generating simulated data sets true population using bootstrap approach boxplots quite similar indicating bootstrap approach used eﬀectively estimate variability associated 
[resampling, methods, lab, cross-validation, bootstrap] lab explore resampling techniques covered chapter commands lab may take run com puter lab cross validation bootstrap 
[resampling, methods, lab, cross-validation, bootstrap, validation, set, approach] explore use validation set approach order estimate test error rates result ﬁtting various linear models auto data set begin use set seed function order set seed seed random number generator reader book obtain precisely results shown generally good idea set random seed performing analysis cross validation contains element randomness results obtained reproduced precisely later time begin using sample function split set observations sample two halves selecting random subset observations original observations refer observations training set set use shortcut sample command see sample details use subset option linear regression using observations corresponding training set fit mpg horsepower data auto subset train use predict function estimate response observations use mean function calculate mse observations validation set note train index selects observations training set mpg fit therefore estimated test mse linear regression cubic regressions mpg poly horsepower data auto subset train mpg fit mpg poly horsepower data auto subset train mpg fit error rates respectively choose diﬀerent training set instead obtain somewhat diﬀerent errors validation set set fit mpg horsepower subset train use poly function estimate test error quadratic resampling methods mpg fit mpg poly horsepower data auto subset train mpg fit mpg poly horsepower data auto subset train mpg fit using split observations training set validation set ﬁnd validation set error rates models linear quadratic cubic terms respectively results consistent previous ﬁndings model predicts mpg using quadratic function horsepower performs better model involves linear function horsepower little evidence favor model uses cubic function horsepower 
[resampling, methods, lab, cross-validation, bootstrap, leave-one-out, cross-validation] loocv estimate automatically computed generalized linear model using glm glm functions lab chap glm ter used glm function perform logistic regression passing family binomial argument use glm model without passing family argument performs linear regression like function instance glm fit glm mpg horsepower data auto glm fit fit mpg horsepower data auto fit yield identical linear regression models lab perform linear regression using glm function rather function part boot library glm fit glm mpg horsepower data auto err glm auto glm fit glm function produces list several components two numbers delta vector contain cross validation results former used together glm glm function lab cross validation bootstrap case numbers identical two decimal places correspond loocv statistic given discuss situation two numbers diﬀer cross validation estimate test error approximately repeat procedure increasingly complex polynomial ﬁts automate process use function initiate loop loop iteratively ﬁts polynomial regressions polynomials order computes associated cross validation error stores element vector error begin initializing vector command likely take couple minutes run rep glm fit glm mpg poly horsepower data auto glm auto glm fit figure see sharp drop estimated test mse linear quadratic ﬁts clear improvement using higher order polynomials 
[resampling, methods, lab, cross-validation, bootstrap, k-fold, cross-validation] glm function also used implement fold use common choice auto data set set random seed initialize vector store errors corresponding polynomial ﬁts orders one ten set rep glm fit glm mpg poly horsepower data auto glm auto glm fit notice computation time much shorter loocv principle computation time loocv least squares linear model faster fold due availability formula loocv however unfortunately glm function make use formula still see little evidence using cubic higher order polynomial terms leads lower test error simply using quadratic saw section two numbers associated delta essentially loocv performed instead perform fold two numbers associated delta diﬀer slightly resampling methods ﬁrst standard fold estimate second bias corrected version data set two estimates similar 
[resampling, methods, lab, cross-validation, bootstrap, bootstrap] illustrate use bootstrap simple example section well example involving estimating accuracy linear regression model auto data set estimating accuracy statistic interest one great advantages bootstrap approach applied almost situations complicated mathematical calculations required performing bootstrap analysis entails two steps first must create function computes statistic interest second use boot function part boot library boot perform bootstrap repeatedly sampling observations data set replacement portfolio data set islr package described section illustrate use bootstrap data must ﬁrst create function alpha takes input data well vector indicating observations used estimate function outputs estimate based selected observations data var cov var var cov function returns outputs estimate based applying observations indexed argument index instance following command tells estimate using observations next command uses sample function randomly select servations range replacement equivalent constructing new bootstrap data set recomputing based new data set set implement bootstrap analysis performing command many times recording corresponding estimates computing lab cross validation bootstrap resulting standard deviation however boot function automates boot approach produce bootstrap estimates std ﬁnal output shows using original data bootstrap estimate estimating accuracy linear regression model bootstrap approach used assess variability coef ﬁcient estimates predictions statistical learning method use bootstrap approach order assess variability estimates intercept slope terms linear regres sion model uses horsepower predict mpg auto data set compare estimates obtained using bootstrap obtained using formulas described section ﬁrst create simple function boot takes auto data set well set indices observations returns intercept slope estimates linear regression model apply function full set observations order compute esti mates entire data set using usual linear regression coeﬃcient estimate formulas chapter note need beginning end function one line long data mpg horsepower data data subset index auto boot function also used order create bootstrap esti mates intercept slope terms randomly sampling among observations replacement give two examples set auto auto resampling methods next use boot function compute standard errors bootstrap estimates intercept slope terms auto auto std indicates bootstrap estimate bootstrap estimate discussed section standard formulas used compute standard errors regression coeﬃcients linear model obtained using summary function mpg horsepower data auto coef std standard error estimates obtained using formulas section intercept slope interestingly somewhat diﬀerent estimates obtained using bootstrap indicate problem bootstrap fact suggests opposite recall standard formulas given equation page rely certain assumptions example depend unknown parameter noise variance estimate using rss although formula standard errors rely linear model correct estimate see figure page non linear relationship data residuals linear inﬂated secondly standard formulas assume somewhat unrealistically ﬁxed variability comes variation errors bootstrap approach rely assumptions likely giving accurate estimate standard errors summary function compute bootstrap standard error estimates stan dard linear regression estimates result ﬁtting quadratic model data since model provides good data figure better correspondence bootstrap estimates standard estimates exercises data mpg horsepower horsepower data data set auto auto std mpg horsepower horsepower data auto coef std 
[resampling, methods, exercises, conceptual] using basic statistical properties variance well single variable calculus derive words prove given indeed minimize var derive probability given observation part bootstrap sample suppose obtain bootstrap sample set observations probability ﬁrst bootstrap observation observation original sample justify answer probability second bootstrap observation observation original sample argue probability observation bootstrap sample probability observation bootstrap sample probability observation bootstrap sample resampling methods probability observa tion bootstrap sample create plot displays integer value probability observation bootstrap sample comment observe investigate numerically probability boot strap sample size contains observation repeatedly create bootstrap samples time record whether fourth observation contained bootstrap sample rep sum rep comment results obtained review fold cross validation explain fold cross validation implemented advantages disadvantages fold cross validation relative validation set approach loocv suppose use statistical learning method make pre diction response particular value predictor carefully describe might estimate standard deviation prediction 
[resampling, methods, exercises, applied] chapter used logistic regression predict probability default using income balance default data set estimate test error logistic regression model using validation set approach forget set random seed beginning analysis fit logistic regression model uses income balance predict default using validation set approach estimate test error model order must perform following steps split sample set training set validation set exercises fit multiple logistic regression model using train ing observations iii obtain prediction default status individual validation set computing posterior probability default individual classifying individual default category posterior probability greater compute validation set error fraction observations validation set misclassiﬁed repeat process three times using three diﬀerent splits observations training set validation set com ment results obtained consider logistic regression model predicts prob ability default using income balance dummy variable student estimate test error model using val idation set approach comment whether including dummy variable student leads reduction test error rate continue consider use logistic regression model predict probability default using income balance default data set particular compute estimates standard errors income balance logistic regression eﬃcients two diﬀerent ways using bootstrap using standard formula computing standard errors glm function forget set random seed beginning analysis using summary glm functions determine esti mated standard errors coeﬃcients associated income balance multiple logistic regression model uses predictors write function boot takes input default data set well index observations outputs coeﬃcient estimates income balance multiple logistic regression model use boot function together boot function estimate standard errors logistic regression coeﬃcients income balance comment estimated standard errors obtained using glm function using bootstrap function sections saw glm function used order compute loocv test error estimate alterna tively one could compute quantities using glm resampling methods predict glm functions loop take proach order compute loocv error simple logistic regression model weekly data set recall context classiﬁcation problems loocv error given fit logistic regression model predicts direction using lag lag fit logistic regression model predicts direction using lag lag using ﬁrst observation use model predict direction ﬁrst obser vation predicting ﬁrst observation direction lag lag servation correctly classiﬁed write loop number observations data set performs following steps fit logistic regression model using obser vation predict direction using lag lag compute posterior probability market moving observation iii use posterior probability observation order predict whether market moves determine whether error made predicting direction observation error made indicate otherwise indicate take average numbers obtained order obtain loocv estimate test error comment results perform cross validation simulated data set generate simulated data set follows set data set write model used generate data equation form create scatterplot comment ﬁnd set random seed compute loocv errors result ﬁtting following four models using least squares exercises iii note may ﬁnd helpful use data frame function create single data set containing repeat using another random seed report results results got models smallest loocv error expected explain answer comment statistical signiﬁcance coeﬃcient esti mates results ﬁtting models using least squares results agree conclusions drawn based cross validation results consider boston housing data set mass library based data set provide estimate population mean medv call estimate provide estimate standard error interpret result hint compute standard error sample mean dividing sample standard deviation square root number observations estimate standard error using bootstrap compare answer based bootstrap estimate provide con ﬁdence interval mean medv compare results obtained using test boston medv hint approximate conﬁdence interval using formula based data set provide estimate med median value medv population would like estimate standard error med unfor tunately simple formula computing standard error median instead estimate standard error median using bootstrap comment ﬁndings based data set provide estimate tenth per centile medv boston suburbs call quantity use quantile function use bootstrap estimate standard error com ment ﬁndings 
[linear, model, selection, regularization] regression setting standard linear model commonly used describe relationship response set variables seen chapter one typically ﬁts model using least squares chapters follow consider approaches extending linear model framework chapter generalize order accommodate non linear still additive relationships chap ter consider even general non linear models however linear model distinct advantages terms inference real world prob lems often surprisingly competitive relation non linear methods hence moving non linear world discuss chapter ways simple linear model improved replacing plain least squares ﬁtting alternative ﬁtting procedures might want use another ﬁtting procedure instead least squares see alternative ﬁtting procedures yield better pre diction accuracy model interpretability prediction accuracy provided true relationship response predictors approximately linear least squares estimates low bias observations much larger number variables least squares estimates tend also low variance hence perform well test observations however much larger james introduction statistical learning applications springer texts statistics doi springer science business media new york linear model selection regularization lot variability least squares resulting overﬁtting consequently poor predictions future observations used model training longer unique least squares coeﬃcient estimate variance inﬁnite method cannot used constraining shrinking estimated coeﬃcients often substantially reduce variance cost negligible increase bias lead substantial improvements accuracy predict response observations used model training model interpretability often case many variables used multiple regression model fact associ ated response including irrelevant variables leads unnecessary complexity resulting model removing variables setting corresponding coeﬃcient estimates zero obtain model easily interpreted least squares extremely unlikely yield coeﬃcient estimates exactly zero chapter see approaches tomatically performing feature selection variable selection feature selection variable selection excluding irrelevant variables multiple regression model many alternatives classical modern using least squares chapter discuss three important classes methods subset selection approach involves identifying subset predictors believe related response model using least squares reduced set variables shrinkage approach involves ﬁtting model involving pre dictors however estimated coeﬃcients shrunken towards zero relative least squares estimates shrinkage also known regularization eﬀect reducing variance depending type shrinkage performed coeﬃcients may esti mated exactly zero hence shrinkage methods also perform variable selection dimension reduction approach involves projecting predic tors dimensional subspace achieved computing diﬀerent linear combinations projections variables projections used predictors linear regression model least squares following sections describe approaches greater tail along advantages disadvantages although chapter describes extensions modiﬁcations linear model regression seen chapter concepts apply methods classiﬁcation models seen chapter subset selection 
[linear, model, selection, regularization, subset, selection] section consider methods selecting subsets predictors include best subset stepwise model selection procedures 
[linear, model, selection, regularization, subset, selection, best, subset, selection] perform best subset selection separate least squares regression best subset selection possible combination predictors models contain exactly one predictor models contain exactly two predictors forth look resulting models goal identifying one best problem selecting best model among possibilities considered best subset selection trivial usually broken two stages described algorithm algorithm best subset selection let denote null model contains predictors model simply predicts sample mean observation fit models contain exactly predictors pick best among models call best deﬁned smallest rss equivalently largest select single best model among using cross validated prediction error aic bic adjusted algorithm step identiﬁes best model training data subset size order reduce problem one possible models one possible models figure models form lower frontier depicted red order select single best model must simply choose among options task must performed care rss models decreases monotonically increases monotonically number features included models increases therefore use statistics select best model always end model involving variables problem low rss high indicates model low training error whereas wish choose model low test error shown chapter figures training error tends quite bit smaller test error low training error means guarantees low test error therefore step use cross validated prediction linear model selection regularization number predictors residual sum squares number predictors figure possible model containing subset ten predictors credit data set rss displayed red frontier tracks best model given number predictors according rss though data set contains ten predictors axis ranges since one variables categorical takes three values leading creation two dummy variables error bic adjusted order select among approaches discussed section application best subset selection shown figure plotted point corresponds least squares regression model using diﬀerent subset predictors credit data set discussed chapter variable ethnicity three level qualitative variable represented two dummy variables selected separately case plotted rss statistics model function number variables red curves connect best models model size according rss ﬁgure shows expected quantities improve number variables increases however three variable model little improvement rss result including additional predictors although presented best subset selection least squares regression ideas apply types models logistic regression case logistic regression instead ordering models rss step algorithm instead use deviance measure deviance plays role rss broader class models deviance negative two times maximized log likelihood smaller deviance better best subset selection simple conceptually appealing proach suﬀers computational limitations number possible models must considered grows rapidly increases general models involve subsets predictors approximately possible models considered subset selection one million possibilities consequently best sub set selection becomes computationally infeasible values greater around even extremely fast modern computers compu tational shortcuts called branch bound techniques eliminat ing choices limitations gets large also work least squares linear regression present computationally eﬃcient alternatives best subset selection next 
[linear, model, selection, regularization, subset, selection, stepwise, selection] computational reasons best subset selection cannot applied large best subset selection may also suﬀer statistical problems large larger search space higher chance ﬁnding models look good training data even though might predictive power future data thus enormous search space lead overﬁtting high variance coeﬃcient estimates reasons stepwise methods explore far restricted set models attractive alternatives best subset selection forward stepwise selection forward stepwise selection computationally eﬃcient alternative best forward stepwise selection subset selection best subset selection procedure considers possible models containing subsets predictors forward step wise considers much smaller set models forward stepwise selection begins model containing predictors adds predictors model one time predictors model particular step variable gives greatest additional improvement added model formally forward stepwise selection procedure given algorithm algorithm forward stepwise selection let denote null model contains predictors consider models augment predictors one additional predictor choose best among models call best deﬁned smallest rss highest select single best model among using cross validated prediction error aic bic adjusted linear model selection regularization unlike best subset selection involved ﬁtting models forward stepwise selection involves ﬁtting one null model along models iteration amounts total models substantial diﬀerence best subset selection requires ﬁtting models whereas forward stepwise selection requires ﬁtting models step algorithm must identify best model among augment one additional predictor simply choosing model lowest rss highest however step must identify best model among set models diﬀerent numbers variables challenging discussed section forward stepwise selection computational advantage best subset selection clear though forward stepwise tends well practice guaranteed ﬁnd best possible model mod els containing subsets predictors instance suppose given data set predictors best possible one variable model contains best possible two variable model instead contains forward stepwise selection fail select best possible two variable model contain must also contain together one additional variable table shows ﬁrst four selected models best subset forward stepwise selection credit data set illustrates phe nomenon best subset selection forward stepwise selection choose rating best one variable model include income student two three variable models however best subset selection places rating cards four variable model forward stepwise selection must maintain rating four variable model example figure indicates much diﬀerence three four variable models terms rss either four variable models likely adequate forward stepwise selection applied even high dimensional setting however case possible construct sub models since submodel using least squares yield unique solution backward stepwise selection like forward stepwise selection backward stepwise selection provides backward stepwise selection eﬃcient alternative best subset selection however unlike forward though forward stepwise selection considers models performs guided search model space eﬀective model space considered contains substantially models subset selection variables best subset forward stepwise one rating rating two rating income rating income three rating income student rating income student four cards income rating income student limit student limit table ﬁrst four selected models best subset selection forward stepwise selection credit data set ﬁrst three models identical fourth models diﬀer stepwise selection begins full least squares model containing predictors iteratively removes least useful predictor one time details given algorithm algorithm backward stepwise selection let denote full model contains predictors consider models contain one predictors total predictors choose best among models call best deﬁned smallest rss highest select single best model among using cross validated prediction error aic bic adjusted like forward stepwise selection backward selection approach searches models applied settings large apply best subset selection also like forward stepwise selection backward stepwise selection guaranteed yield best model containing subset predictors backward selection requires number samples larger number variables full model contrast forward stepwise used even viable subset method large like forward stepwise selection backward stepwise selection performs guided search model space eﬀectively considers substantially models linear model selection regularization hybrid approaches best subset forward stepwise backward stepwise selection proaches generally give similar identical models another ternative hybrid versions forward backward stepwise selection available variables added model sequentially analogy forward selection however adding new variable method may also remove variables longer provide improvement model approach attempts closely mimic best sub set selection retaining computational advantages forward backward stepwise selection 
[linear, model, selection, regularization, subset, selection, choosing, optimal, model] best subset selection forward selection backward selection result creation set models contains subset pre dictors order implement methods need way determine models best discussed section model containing predictors always smallest rss largest since quantities related training error instead wish choose model low test error evident show chapter training error poor estimate test error therefore rss suitable selecting best model among collection models diﬀerent numbers predictors order select best model respect test error need estimate test error two common approaches indirectly estimate test error making adjustment training error account bias due overﬁtting directly estimate test error using either validation set approach cross validation approach discussed chapter consider approaches aic bic adjusted show chapter training set mse generally estimate test mse recall mse rss model training data using least squares speciﬁ cally estimate regression coeﬃcients training rss test rss small possible particular training error decrease variables included model test error may therefore training set rss training set cannot used select among set models diﬀerent numbers variables however number techniques adjusting training error model size available approaches used select among set subset selection number predictors number predictors bic number predictors adjusted figure bic adjusted shown best models size credit data set lower frontier figure bic estimates test mse middle plot see bic estimate test error shows increase four variables selected two plots rather ﬂat four variables included models diﬀerent numbers variables consider four approaches akaike information criterion aic bayesian information akaike information criterion criterion bic adjusted figure displays bic adjusted bayesian information criterion adjusted best model size produced best subset selection credit data set ﬁtted least squares model containing predictors estimate test mse computed using equation rss mallow sometimes deﬁned rss equivalent deﬁnition given sense model smallest also smallest estimate variance error associated response measurement essentially statistic adds penalty training rss order adjust fact training error tends underestimate test error clearly penalty increases number predictors model increases intended adjust corresponding decrease training rss though beyond scope book one show unbiased estimate unbiased estimate test mse consequence statistic tends take small value models low test error determining set models best choose model lowest value figure selects six variable model containing predictors income limit rating cards age student typically estimated using full model containing predictors linear model selection regularization aic criterion deﬁned large class models maximum likelihood case model gaussian errors maximum likelihood least squares thing case aic given aic rss simplicity omitted additive constant hence least squares models aic proportional displayed figure bic derived bayesian point view ends looking similar aic well least squares model predictors bic irrelevant constants given like bic tend take small value model low test error generally select model lowest bic value notice bic replaces used log term number observations since log bic statistic generally places heavier penalty models many variables hence results selection smaller models figure see indeed case credit data set bic chooses model contains four predictors income limit cards student case curves ﬂat appear much diﬀerence accuracy four variable six variable models adjusted statistic another popular approach selecting among set models contain diﬀerent numbers variables recall chapter usual deﬁned rss tss tss total sum squares response since rss always decreases variables added model always increases variables added least squares model variables adjusted statistic calculated adjusted rss tss unlike aic bic small value indicates model low test error large value adjusted indicates model small test error maximizing adjusted equivalent minimizing rss rss always decreases number variables model increases rss may increase decrease due presence denominator intuition behind adjusted correct variables included model adding additional noise variables bic rss log subset selection lead small decrease rss since adding noise variables leads increase variables lead increase rss consequently decrease adjusted therefore theory model largest adjusted correct variables noise variables unlike statistic adjusted statistic pays price inclusion unnecessary variables model figure displays adjusted credit data set using statistic results selection model contains seven variables adding gender model selected aic aic bic rigorous theoretical justiﬁcations beyond scope book justiﬁcations rely asymptotic guments scenarios sample size large despite pop ularity even though quite intuitive adjusted well motivated statistical theory aic bic measures simple use compute presented formulas aic bic case linear model using least squares however quantities also deﬁned general types models validation cross validation alternative approaches discussed directly esti mate test error using validation set cross validation methods discussed chapter compute validation set error cross validation error model consideration select model resulting estimated test error smallest pro cedure advantage relative aic bic adjusted provides direct estimate test error makes fewer assumptions true underlying model also used wider range model selection tasks even cases hard pinpoint model degrees freedom number predictors model hard estimate error variance past performing cross validation computationally prohibitive many problems large large aic bic adjusted attractive approaches choosing among set models however nowadays fast computers computations required perform cross validation hardly ever issue thus cross validation attractive approach selecting among number models consideration figure displays function bic validation set errors cross validation errors credit data best variable model validation errors calculated randomly selecting three quarters observations training set remainder valida tion set cross validation errors computed using folds case validation cross validation methods result linear model selection regularization number predictors square root bic number predictors alidation set error number predictors cross alidation error figure credit data set three quantities displayed best model containing predictors ranging overall best model based quantities shown blue cross left square root bic center validation set errors right cross validation errors six variable model however three approaches suggest four ﬁve six variable models roughly equivalent terms test errors fact estimated test error curves displayed center right hand panels figure quite ﬂat three variable model clearly lower estimated test error two variable model estimated test errors variable models quite similar furthermore repeated validation set approach using diﬀerent split data training set validation set repeated cross validation using diﬀerent set cross validation folds precise model lowest estimated test error would surely change setting select model using one standard error rule ﬁrst calculate one standard error rule standard error estimated test mse model size select smallest model estimated test error within one standard error lowest point curve rationale set models appear less equally good might well choose simplest model model smallest number predictors case applying one standard error rule validation set cross validation approach leads selection three variable model 
[linear, model, selection, regularization, shrinkage, methods] subset selection methods described section involve using least squares linear model contains subset predictors alternative model containing predictors using technique constrains regularizes coeﬃcient estimates equivalently shrinks coeﬃcient estimates towards zero may immediately shrinkage methods obvious constraint improve turns shrinking coeﬃcient estimates signiﬁcantly reduce variance two best known techniques shrinking regression coeﬃcients towards zero ridge regression lasso 
[linear, model, selection, regularization, shrinkage, methods, ridge, regression] recall chapter least squares ﬁtting procedure estimates using values minimize rss ridge regression similar least squares except coeﬃcients ridge regression estimated minimizing slightly diﬀerent quantity particular ridge regression coeﬃcient estimates values minimize rss tuning parameter determined separately equa tuning parameter tion trades two diﬀerent criteria least squares ridge regres sion seeks coeﬃcient estimates data well making rss small however second term called shrinkage penalty shrinkage penalty small close zero eﬀect shrinking estimates towards zero tuning parameter serves control relative impact two terms regression coeﬃcient esti mates penalty term eﬀect ridge regression produce least squares estimates however impact shrinkage penalty grows ridge regression coeﬃcient estimates approach zero unlike least squares generates one set eﬃcient estimates ridge regression produce diﬀerent set coeﬃcient estimates value selecting good value critical defer discussion section use cross validation note shrinkage penalty applied intercept want shrink estimated association variable response however want shrink intercept simply measure mean value response assume variables columns data matrix centered mean zero ridge regression performed estimated intercept take form linear model selection regularization standardiz coefficients income limit rating student standardiz coefficients figure standardized ridge regression coeﬃcients displayed credit data set function application credit data figure ridge regression coeﬃcient estimates credit data income coeﬃcient varied extreme left hand side plot essentially zero corresponding ridge coeﬃcient estimates usual least squares esti mates increases ridge coeﬃcient estimates shrink towards zero extremely large ridge coeﬃcient estimates basically zero corresponds null model contains pre dictors plot income limit rating student variables displayed distinct colors since variables tend far largest coeﬃcient estimates ridge coeﬃcient estimates tend decrease aggregate increases individual coeﬃcients rating income may occasionally increase increases right hand panel figure displays ridge coeﬃcient estimates left hand panel instead displaying axis display denotes vector least squares coeﬃcient estimates notation xcβ denotes norm pronounced norm ell vector deﬁned xcβ measures distance zero increases norm always decrease latter quantity ranges case ridge regression coeﬃcient estimate least squares estimate norms case ridge regression coeﬃcient estimate vector zeros norm equal zero therefore think axis right hand panel figure amount ridge shrinkage methods regression coeﬃcient estimates shrunken towards zero small value indicates shrunken close zero standard least squares coeﬃcient estimates discussed chapter scale equivariant multiplying constant simply leads scale equivariant scaling least squares coeﬃcient estimates factor words regardless predictor scaled remain contrast ridge regression coeﬃcient estimates change sub stantially multiplying given predictor constant instance consider income variable measured dollars one could rea sonably measured income thousands dollars would result reduction observed values income factor due sum squared coeﬃcients term ridge regression formulation change scale simply cause ridge regression eﬃcient estimate income change factor words depend value also scaling predictor fact value may even depend scaling predictors therefore best apply ridge regression standardizing predictors using formula scale denominator estimated standard deviation predictor consequently standardized predictors standard deviation one sult ﬁnal depend scale predictors measured figure axis displays standardized ridge regres sion coeﬃcient estimates coeﬃcient estimates result performing ridge regression using standardized predictors ridge regression improve least squares ridge regression advantage least squares rooted bias variance trade increases ﬂexibility ridge regression decreases leading decreased variance increased bias illustrated left hand panel figure using simulated data set containing predictors observations green curve left hand panel figure displays variance ridge regression predictions function least squares coeﬃcient estimates correspond ridge regression variance high bias increases shrinkage ridge coeﬃcient estimates leads substantial reduction variance predictions expense slight increase bias recall test mean squared error mse plot ted purple function variance plus squared bias values linear model selection regularization mean squared error mean squared error figure squared bias black variance green test mean squared error purple ridge regression predictions simulated data set function horizontal dashed lines indicate minimum possible mse purple crosses indicate ridge regression models mse smallest variance decreases rapidly little increase bias plotted black consequently mse drops considerably increases beyond point decrease variance due increasing slows shrinkage coeﬃcients causes signiﬁcantly underestimated resulting large increase bias minimum mse achieved approximately interestingly high variance mse associated least squares almost high null model coeﬃcient estimates zero however intermediate value mse considerably lower right hand panel figure displays curves left hand panel time plotted norm ridge regression coeﬃcient estimates divided norm least squares estimates move left right ﬁts become ﬂexible bias decreases variance increases general situations relationship response predictors close linear least squares estimates low bias may high variance means small change training data cause large change least squares coeﬃcient estimates particular number variables almost large number observations example figure least squares estimates extremely variable least squares estimates even unique solution whereas ridge regression still perform well trading small increase bias large decrease variance hence ridge regression works best situations least squares estimates high variance ridge regression also substantial computational advantages best subset selection requires searching models shrinkage methods discussed previously even moderate values search computationally infeasible contrast ﬁxed value ridge regression ﬁts single model model ﬁtting procedure performed quite quickly fact one show computations required solve simultaneously values almost iden tical ﬁtting model using least squares 
[linear, model, selection, regularization, shrinkage, methods, lasso] ridge regression one obvious disadvantage unlike best subset forward stepwise backward stepwise selection generally select models involve subset variables ridge regression include predictors ﬁnal model penalty shrink coeﬃcients towards zero set exactly zero unless may problem prediction accuracy create challenge model interpretation settings number variables quite large example credit data set appears important variables income limit rating student might wish build model including predictors however ridge regression always generate model involving ten predictors increasing value tend reduce magnitudes coeﬃcients result exclusion variables lasso relatively recent alternative ridge regression lasso comes disadvantage lasso coeﬃcients minimize quantity rss comparing see lasso ridge regression similar formulations diﬀerence term ridge regression penalty replaced lasso penalty statistical parlance lasso uses pronounced ell penalty instead penalty norm coeﬃcient vector given xcβ ridge regression lasso shrinks coeﬃcient estimates towards zero however case lasso penalty eﬀect forcing coeﬃcient estimates exactly equal zero tuning parameter suﬃciently large hence much like best subset lection lasso performs variable selection result models generated lasso generally much easier interpret produced ridge regression say lasso yields sparse models sparse models involve subset variables ridge regression selecting good value lasso critical defer discussion section use cross validation linear model selection regularization standardiz coefficients standardiz coefficients income limit rating student figure standardized lasso coeﬃcients credit data set shown function example consider coeﬃcient plots figure gen erated applying lasso credit data set lasso simply gives least squares becomes suﬃciently large lasso gives null model coeﬃcient estimates equal zero however two extremes ridge regression lasso models quite diﬀerent moving left right right hand panel figure observe ﬁrst lasso sults model contains rating predictor student limit enter model almost simultaneously shortly followed income eventually remaining variables enter model hence depending value lasso produce model involving number vari ables contrast ridge regression always include variables model although magnitude coeﬃcient estimates depend another formulation ridge regression lasso one show lasso ridge regression coeﬃcient estimates solve problems minimize subject minimize subject shrinkage methods respectively words every value equations give lasso coeﬃcient estimates similarly every value corresponding equa tions give ridge regression coeﬃcient estimates indicates lasso coeﬃcient estimates smallest rss points lie within diamond deﬁned similarly ridge regression estimates smallest rss points lie within circle deﬁned think follows perform lasso trying ﬁnd set coeﬃcient estimates lead smallest rss subject constraint budget large extremely large budget restrictive coeﬃcient estimates large fact large enough least squares solution falls within budget simply yield least squares solution contrast small must small order avoid violating budget similarly indicates perform ridge regression seek set coeﬃcient estimates rss small possible subject requirement exceed budget formulations reveal close connection lasso ridge regression best subset selection consider problem minimize subject indicator variable takes value equals zero otherwise amounts ﬁnding set coeﬃcient timates rss small possible subject constraint coeﬃcients nonzero problem equivalent best subset selection unfortunately solving computationally infeasible large since requires considering models con taining predictors therefore interpret ridge regression lasso computationally feasible alternatives best subset selection replace intractable form budget forms much easier solve course lasso much closely related best subset selection since lasso performs feature selection suﬃciently small variable selection property lasso lasso unlike ridge regression results coeﬃcient estimates exactly equal zero formulations used shed light issue figure illustrates situation least squares solution marked blue diamond linear model selection regularization figure contours error constraint functions lasso left ridge regression right solid blue areas constraint gions red ellipses contours rss circle represent lasso ridge regression constraints respectively suﬃciently large constraint regions con tain ridge regression lasso estimates least squares estimates large value corresponds however figure least squares estimates lie outside diamond circle least squares estimates lasso ridge regression estimates ellipses centered around represent regions constant rss words points given ellipse share common value rss ellipses expand away least squares eﬃcient estimates rss increases equations indicate lasso ridge regression coeﬃcient estimates given ﬁrst point ellipse contacts constraint region since ridge regression circular constraint sharp points intersection generally occur axis ridge regression coeﬃcient estimates exclusively non zero however lasso constraint corners axes ellipse often intersect con straint region axis occurs one coeﬃcients equal zero higher dimensions many coeﬃcient estimates may equal zero simultaneously figure intersection occurs resulting model include figure considered simple case constraint region ridge regression becomes sphere constraint region lasso becomes polyhedron shrinkage methods mean squared error training data mean squared error figure left plots squared bias black variance green test mse purple lasso simulated data set right comparison squared bias variance test mse lasso solid ridge dotted plotted training data common form indexing crosses plots indicate lasso model mse smallest constraint ridge regression becomes hypersphere constraint lasso becomes polytope however key ideas depicted fig ure still hold particular lasso leads feature selection due sharp corners polyhedron polytope comparing lasso ridge regression clear lasso major advantage ridge regression produces simpler interpretable models involve subset predictors however method leads better prediction accuracy figure displays variance squared bias test mse lasso applied simulated data figure clearly lasso leads qualitatively similar behavior ridge regression increases variance decreases bias increases right hand panel figure dotted lines represent ridge regression ﬁts plot training data another useful way index models used compare models diﬀerent types regularization case example lasso ridge regression result almost identical biases however variance ridge regression slightly lower variance lasso consequently minimum mse ridge regression slightly smaller lasso however data figure generated way predictors related response none true coeﬃcients equaled zero lasso implicitly assumes number coeﬃcients truly equal zero consequently surprising ridge regression outperforms lasso terms prediction error setting figure illustrates similar situation except response linear model selection regularization mean squared error training data mean squared error figure left plots squared bias black variance green test mse purple lasso simulated data similar figure except two predictors related response right comparison squared bias variance test mse lasso solid ridge dotted plotted training data common form indexing crosses plots indicate lasso model mse smallest function predictors lasso tends outperform ridge regression terms bias variance mse two examples illustrate neither ridge regression lasso universally dominate general one might expect lasso perform better setting relatively small number predictors substantial coeﬃcients remaining predictors coeﬃcients small equal zero ridge regression perform better response function many predictors coeﬃcients roughly equal size however number predictors related response never known priori real data sets technique cross validation used order determine approach better particular data set ridge regression least squares estimates exces sively high variance lasso solution yield reduction variance expense small increase bias consequently gener ate accurate predictions unlike ridge regression lasso performs variable selection hence results models easier interpret eﬃcient algorithms ﬁtting ridge lasso models cases entire coeﬃcient paths computed amount work single least squares explore lab end chapter simple special case ridge regression lasso order obtain better intuition behavior ridge regression lasso consider simple special case diag onal matrix diagonal diagonal elements simplify problem assume also performing regres shrinkage methods sion without intercept assumptions usual least squares problem simpliﬁes ﬁnding minimize case least squares solution given setting ridge regression amounts ﬁnding minimized lasso amounts ﬁnding coeﬃcients minimized one show setting ridge regression esti mates take form lasso estimates take form figure displays situation see ridge regression lasso perform two diﬀerent types shrinkage ridge regression least squares coeﬃcient estimate shrunken proportion contrast lasso shrinks least squares coeﬃcient towards zero constant amount least squares coeﬃcients less absolute value shrunken entirely zero type shrink age performed lasso simple setting known soft thresholding fact lasso coeﬃcients shrunken entirely soft thresholding zero explains lasso performs feature selection case general data matrix story little complicated depicted figure main ideas still hold approximately ridge regression less shrinks every dimension data proportion whereas lasso less shrinks coeﬃcients toward zero similar amount suﬃciently small eﬃcients shrunken way zero linear model selection regularization coefficient estimate ridge least squares coefficient estimate lasso least squares figure ridge regression lasso coeﬃcient estimates simple setting diagonal matrix diagonal left ridge regression coeﬃcient estimates shrunken proportionally towards zero relative least squares estimates right lasso coeﬃcient estimates soft thresholded towards zero bayesian interpretation ridge regression lasso show one view ridge regression lasso bayesian lens bayesian viewpoint regression assumes coeﬃcient vector prior distribution say likelihood data written multiplying prior distribution likeli hood gives proportionality constant posterior distribution posterior distribution takes form proportionality follows bayes theorem equality follows assumption ﬁxed assume usual linear model suppose errors independent drawn normal dis tribution furthermore assume density function turns ridge regression lasso follow naturally two special cases gaussian distribution mean zero standard deviation function follows posterior mode posterior mode likely value given data given ridge regression solution fact ridge regression solution also posterior mean shrinkage methods figure left ridge regression posterior mode gaus sian prior right lasso posterior mode double exponential prior double exponential laplace distribution mean zero scale parameter function follows posterior mode lasso solution however lasso solution posterior mean fact posterior mean yield sparse coeﬃcient vector gaussian double exponential priors displayed figure therefore bayesian viewpoint ridge regression lasso follow directly assuming usual linear model normal errors together simple prior distribution notice lasso prior steeply peaked zero gaussian ﬂatter fatter zero hence lasso expects priori many coeﬃcients exactly zero ridge assumes coeﬃcients randomly distributed zero 
[linear, model, selection, regularization, shrinkage, methods, selecting, tuning, parameter] subset selection approaches considered section require method determine models consideration best implementing ridge regression lasso requires method selecting value tuning parameter equivalently value constraint cross validation provides sim ple way tackle problem choose grid values compute cross validation error value described chapter select tuning parameter value cross validation error smallest finally model using available observations selected value tuning parameter figure displays choice results performing leave one cross validation ridge regression ﬁts credit data set dashed vertical lines indicate selected value case value relatively small indicating optimal involves linear model selection regularization cross alidation error standardiz coefficients figure left cross validation errors result applying ridge regression credit data set various value right coeﬃcient estimates function vertical dashed lines indicate value selected cross validation small amount shrinkage relative least squares solution addition dip pronounced rather wide range values would give similar error case like might simply use least squares solution figure provides illustration ten fold cross validation applied lasso ﬁts sparse simulated data figure left hand panel figure displays cross validation error right hand panel displays coeﬃcient estimates vertical dashed lines indicate point cross validation error smallest two colored lines right hand panel figure represent two predictors related response grey lines represent unre lated predictors often referred signal noise variables signal respectively lasso correctly given much larger coeﬃ cient estimates two signal predictors also minimum cross validation error corresponds set coeﬃcient estimates signal variables non zero hence cross validation together lasso correctly identiﬁed two signal variables model even though challenging setting variables observations contrast least squares solution displayed far right right hand panel figure ssigns large coeﬃcient estimate one two signal variables 
[linear, model, selection, regularization, dimension, reduction, methods] methods discussed far chapter controlled variance two diﬀerent ways either using subset original vari ables shrinking coeﬃcients toward zero methods dimension reduction methods cross alidation error standardiz coefficients figure left ten fold cross validation mse lasso applied sparse simulated data set figure right corresponding lasso coeﬃcient estimates displayed vertical dashed lines indicate lasso cross validation error smallest deﬁned using original predictors explore class approaches transform predictors least squares model using transformed variables refer tech niques dimension reduction methods dimension reduction let represent linear combinations original linear combination predictors constants linear regression model using least squares note regression coeﬃcients given constants chosen wisely dimension reduction approaches often outperform least squares regression words ﬁtting using least squares lead better results ﬁtting using least squares term dimension reduction comes fact approach reduces problem estimating coeﬃcients simpler problem estimating coeﬃcients words dimension problem reduced notice linear model selection regularization population spending figure population size pop spending diﬀerent cities shown purple circles green solid line indicates ﬁrst principal component blue dashed line indicates second principal component hence thought special case original linear regression model given dimension reduction serves constrain estimated coeﬃcients since must take form constraint form coeﬃcients potential bias coeﬃcient estimates however situations large relative selecting value signiﬁcantly reduce variance ﬁtted coeﬃcients linearly independent poses constraints case dimension reduction occurs ﬁtting equivalent performing least squares original predictors dimension reduction methods work two steps first trans formed predictors obtained second model using predictors however choice equiv alently selection achieved diﬀerent ways chapter consider two approaches task principal components partial least squares 
[linear, model, selection, regularization, dimension, reduction, methods, principal, components, regression] principal components analysis pca popular approach deriving principal components analysis low dimensional set features large set variables pca discussed greater detail tool unsupervised learning chapter describe use dimension reduction technique regression dimension reduction methods overview principal components analysis pca technique reducing dimension data matrix ﬁrst principal component direction data along observations vary instance consider figure shows population size pop tens thousands people spending particular company thousands dollars cities green solid line represents ﬁrst principal component direction data see eye direction along greatest variability data projected observations onto line shown left hand panel figure resulting projected observations would largest possible variance projecting observations onto line would yield projected observations lower variance projecting point onto line simply involves ﬁnding location line closest point ﬁrst principal component displayed graphically figure summarized mathematically given formula pop pop principal component loadings deﬁne direction referred pop indicates mean pop values data set indicates mean vertising spending idea every possible linear combination pop particular linear combination yields highest variance linear combination var pop pop maximized necessary consider linear combinations form since otherwise could increase arbitrarily order blow variance two loadings positive similar size almost average two variables since pop vectors length instance pop pop values known principal component scores seen right hand panel figure also another interpretation pca ﬁrst principal compo nent vector deﬁnes line close possible data instance figure ﬁrst principal component line minimizes sum squared perpendicular distances point line distances plotted dashed line segments left hand panel figure crosses represent projection point onto ﬁrst principal component line ﬁrst principal component chosen projected observations close possible original observations linear model selection regularization population spending principal component incipal component figure subset advertising data mean pop budgets indicated blue circle left ﬁrst principal component direction shown green dimension along data vary also deﬁnes line closest observations distances observation principal component represented using black dashed line segments blue dot represents pop right left hand panel rotated ﬁrst principal component direction coincides axis right hand panel figure left hand panel rotated ﬁrst principal component direction coincides axis possible show ﬁrst principal component score observation given distance direction cross zero example point bottom left corner left hand panel figure large negative principal component score point top right corner large positive score scores computed directly using think values principal component single number summaries joint pop budgets location example pop pop indicates city average population size average spending positive score suggests opposite well single number represent pop case figure indicates pop approximately linear relationship might expect single number summary work well figure displays ﬁrst principal component two features words ﬁrst principal component appears capture information contained pop predictors far concentrated ﬁrst principal component gen eral one construct distinct principal components second principal component linear combination variables correlated largest variance subject constraint second principal component direction illustrated dashed blue line figure turns zero correlation condition versus pop plots show strong relationship principal components calculated first standardizing pop common approach hence axes figures scale dimension reduction methods principal component pulation principal component spending figure plots ﬁrst principal component scores versus pop relationships strong equivalent condition direction must perpendicular perpendicular orthogonal ﬁrst principal component direction second principal orthogonal component given formula pop pop since advertising data two predictors ﬁrst two principal com ponents contain information pop however construction ﬁrst component contain information con sider example much larger variability axis versus axis right hand panel figure fact second principal component scores much closer zero indicates component captures far less information another illustration fig ure displays versus pop little relationship second principal component two predictors suggesting case one needs ﬁrst principal component order accurately represent pop budgets two dimensional data advertising example construct two principal components however predictors population age income level education forth additional components could constructed would successively maximize variance subject constraint uncorrelated preceding components principal components regression approach principal components regression pcr approach involves constructing principal components regression ﬁrst principal components using compo nents predictors linear regression model using least squares key idea often small number prin cipal components suﬃce explain variability data well relationship response words assume directions show variation direc tions associated assumption guaranteed linear model selection regularization principal component opulation principal component spending figure plots second principal component scores versus pop relationships weak number components mean squared error number components mean squared error squared bias test mse variance figure pcr applied two simulated data sets left simulated data figure right simulated data figure true often turns reasonable enough approximation give good results assumption underlying pcr holds ﬁtting least squares model lead better results ﬁtting least squares model since information data relates response contained estimating coeﬃcients mitigate overﬁtting advertising data ﬁrst principal component explains variance pop principal component regression uses single variable predict response interest sales likely perform quite well figure displays pcr ﬁts simulated data sets figures recall data sets generated using observations predictors however response ﬁrst data set function predictors response second data set generated using two predictors curves plotted function number principal components used predic tors regression model principal components used dimension reduction methods pcr number components mean squared error squared bias test mse variance ridge regression lasso shrinkage factor mean squared error figure pcr ridge regression lasso applied simulated data set ﬁrst ﬁve principal components contain informa tion response panel irreducible error var shown horizontal dashed line left results pcr right results lasso solid ridge regression dotted axis displays shrinkage factor eﬃcient estimates deﬁned norm shrunken coeﬃcient estimates divided norm least squares estimate regression model bias decreases variance increases results typical shape mean squared error pcr amounts simply least squares using original predictors ﬁgure indicates performing pcr appropriate choice result substantial improvement least squares pecially left hand panel however examining ridge regression lasso results figures see pcr perform well two shrinkage methods example relatively worse performance pcr figure consequence fact data generated way many princi pal components required order adequately model response contrast pcr tend well cases ﬁrst principal components suﬃcient capture variation predictors well relationship response left hand panel fig ure illustrates results another simulated data set designed favorable pcr response generated way depends exclusively ﬁrst ﬁve principal components bias drops zero rapidly number principal components used pcr increases mean squared error displays clear minimum right hand panel figure displays results data using ridge regression lasso three methods oﬀer signif icant improvement least squares however pcr ridge regression slightly outperform lasso note even though pcr provides simple way perform regression using predictors feature selection method principal components used regression linear model selection regularization number components standardiz coefficients income limit rating student number components cross alidation mse figure left pcr standardized coeﬃcient estimates credit data set diﬀerent values right ten fold cross validation mse obtained using pcr function linear combination original features instance linear combination pop therefore pcr often performs quite well many practical settings result development model relies upon small set original features sense pcr closely related ridge regression lasso fact one show pcr ridge regression closely related one even think ridge regression continuous ver sion pcr pcr number principal components typically chosen cross validation results applying pcr credit data set shown figure right hand panel displays cross validation errors obtained function data lowest cross validation error occurs components corre sponds almost dimension reduction since pcr equivalent simply performing least squares performing pcr generally recommend standardizing predictor using prior generating principal components standardization ensures variables scale absence standardization high variance variables tend play larger role principal components obtained scale variables measured ultimately eﬀect ﬁnal pcr model however variables measured units say kilograms inches one might choose standardize details found section elements statistical learning hastie tibshirani friedman dimension reduction methods population spending figure advertising data ﬁrst pls direction solid line ﬁrst pcr direction dotted line shown 
[linear, model, selection, regularization, dimension, reduction, methods, partial, least, squares] pcr approach described involves identifying linear combi nations directions best represent predictors directions identiﬁed unsupervised way since response used help determine principal component directions response supervise identiﬁcation principal components consequently pcr suﬀers drawback guarantee directions best explain predictors also best directions use predicting response unsupervised methods discussed chapter present partial least squares pls supervised alternative partial least squares pcr like pcr pls dimension reduction method ﬁrst identiﬁes new set features linear combinations original features ﬁts linear model via least squares using new features unlike pcr pls identiﬁes new features supervised way makes use response order identify new features approximate old features well also related response roughly speaking pls approach attempts ﬁnd directions help explain response predictors describe ﬁrst pls direction computed stan dardizing predictors pls computes ﬁrst direction setting equal coeﬃcient simple linear regression onto one show coeﬃcient proportional cor relation hence computing pls places highest weight variables strongly related response figure displays example pls data dimension per unit synthetic set sales regions response two predictors population size advertising spending solid green line indicates ﬁrst pls direction dotted line shows ﬁrst principal component direction pls chosen direction less change dataset distinct advertising data discussed chapter linear model selection regularization identify second pls direction ﬁrst adjust variables regressing variable taking residuals resid uals interpreted remaining information explained ﬁrst pls direction compute using thogonalized data exactly fashion computed based original data iterative approach repeated times identify multiple pls components finally end procedure use least squares linear model predict using exactly fashion pcr pcr number partial least squares directions used pls tuning parameter typically chosen cross validation generally standardize predictors response performing pls pls popular ﬁeld chemometrics many variables arise digitized spectrometry signals practice often performs better ridge regression pcr supervised dimension reduction pls reduce bias also potential increase variance overall beneﬁt pls relative pcr wash 
[linear, model, selection, regularization, considerations, high, dimensions, high-dimensional, data] traditional statistical techniques regression classiﬁcation intended low dimensional setting number low dimensional servations much greater number features due part fact throughout ﬁeld history bulk sci entiﬁc problems requiring use statistics low dimensional instance consider task developing model predict patient blood pressure basis age gender body mass index bmi three predictors four intercept included model perhaps several thousand patients blood pressure age gender bmi available hence low dimensional dimension referring size past years new technologies changed way data collected ﬁelds diverse ﬁnance marketing medicine commonplace collect almost unlimited number feature mea surements large extremely large number observations often limited due cost sample availability considerations two examples follows rather predicting blood pressure basis age gen der bmi one might also collect measurements half million pop pls direction change pop dimension relative pca suggests explaining response fit predictors closely pca better job highly correlated response considerations high dimensions single nucleotide polymorphisms snps individual dna mutations relatively common population inclu sion predictive model marketing analyst interested understanding people online shop ping patterns could treat features search terms entered users search engine sometimes known bag words model researcher might access search histories hundred thousand search engine users consented share information researcher given user search terms scored present absent creating large binary feature vector much larger data sets containing features observations often referred high dimensional classical approaches least squares linear high dimensional regression appropriate setting many issues arise analysis high dimensional data discussed earlier book since apply also include role bias variance trade danger overﬁtting though issues always rele vant become particularly important number features large relative number observations deﬁned high dimensional setting case num ber features larger number observations con siderations discuss certainly also apply slightly smaller best always kept mind performing super vised learning 
[linear, model, selection, regularization, considerations, high, dimensions, goes, wrong, high, dimensions] order illustrate need extra care specialized techniques regression classiﬁcation begin examining wrong apply statistical technique intended high dimensional setting purpose examine least squares regression concepts apply logistic regression linear discriminant anal ysis classical statistical approaches number features large larger number observations least squares described chapter cannot rather performed reason simple regardless whether truly relationship features response least squares yield set coeﬃcient estimates result perfect data residuals zero example shown figure feature plus intercept two cases observations two observations observations least linear model selection regularization figure left least squares regression low dimensional setting right least squares regression observations two parameters estimated intercept coeﬃcient squares regression line perfectly data instead regression line seeks approximate observations well possible hand two observations regardless values observations regression line data exactly problematic perfect almost certainly lead overﬁtting data words though possible perfectly training data high dimensional setting resulting linear model perform extremely poorly independent test set therefore constitute useful model fact see happened figure least squares line obtained right hand panel perform poorly test set comprised observations left hand panel problem simple simple least squares regression line ﬂexible hence overﬁts data figure illustrates risk carelessly applying least squares number features large data simulated observations regression performed features completely unrelated response shown ﬁgure model increases number features included model increases correspondingly training set mse decreases number features increases even though features completely unrelated response hand mse independent test set becomes extremely large number features included model increases including additional predictors leads vast increase variance coeﬃcient estimates looking test set mse clear best model contains variables however someone carelessly examines training set mse might erroneously conclude model greatest number variables best indicates importance applying extra care considerations high dimensions number variables number variables raining mse number variables est mse figure simulated example training observations features completely unrelated outcome added model left increases features included center training set mse decreases features included right test set mse increases features included analyzing data sets large number variables always evaluating model performance independent test set section saw number approaches adjusting training set rss order account number variables used least squares model unfortunately aic bic approaches appropriate high dimensional setting estimating problematic instance formula chapter yields estimate setting similarly problems arise application adjusted high dimensional setting since one easily obtain model adjusted value clearly alternative approaches better suited high dimensional setting required 
[linear, model, selection, regularization, considerations, high, dimensions, regression, high, dimensions] turns many methods seen chapter ﬁtting less ﬂexible least squares models forward stepwise selection ridge regression lasso principal components regression particularly useful performing regression high dimensional setting essentially approaches avoid overﬁtting using less ﬂexible ﬁtting approach least squares figure illustrates performance lasso simple simulated example features truly associated outcome lasso performed training observations mean squared error evaluated independent test set number features increases test set error increases lowest validation set error achieved small however larger lowest validation set error achieved using larger value boxplot rather reporting values used degrees freedom resulting linear model selection regularization degrees freedom degrees freedom degrees freedom figure lasso performed observations three values number features features associated response boxplots show test mses result using three diﬀerent values tuning parameter ease interpretation rather reporting degrees freedom reported lasso turns simply number estimated non zero coeﬃcients lowest test mse obtained smallest amount regularization lowest test mse achieved substantial amount regularization lasso performed poorly regardless amount regularization due fact features truly associated outcome lasso solution displayed simply number non zero coeﬃcient estimates lasso solution measure ﬂexibility lasso figure highlights three important points regularization shrinkage plays key role high dimensional problems appropriate tuning parameter selection crucial good predictive performance test error tends increase dimensionality problem number features predictors increases unless additional features truly associated response third point fact key principle analysis high dimensional data known curse dimensionality one might curse mensionality think number features used model increases quality ﬁtted model increase well however comparing left hand right hand panels figure see necessarily case example test set mse almost doubles increases general adding additional signal features truly associated response improve ﬁtted model sense leading reduction test set error however adding noise features truly associated response lead deterioration ﬁtted model consequently increased test set error noise features increase dimensionality considerations high dimensions problem exacerbating risk overﬁtting since noise features may assigned nonzero coeﬃcients due chance associations response training set without potential upside terms improved test set error thus see new technologies allow collection measurements thousands millions features double edged sword lead improved predictive models features fact relevant problem hand lead worse results features relevant even relevant variance incurred ﬁtting coeﬃcients may outweigh reduction bias bring 
[linear, model, selection, regularization, considerations, high, dimensions, interpreting, results, high, dimensions] perform lasso ridge regression regression proce dures high dimensional setting must quite cautious way report results obtained chapter learned multi collinearity concept variables regression might corre lated high dimensional setting multicollinearity problem extreme variable model written linear combination variables model essentially means never know exactly variables truly predictive outcome never identify best coeﬃcients use regression hope assign large regression coeﬃcients variables correlated variables truly predictive outcome instance suppose trying predict blood pressure basis half million snps forward stepwise selection indicates snps lead good predictive model training data would incorrect conclude snps predict blood pressure eﬀectively snps included model likely many sets snps would predict blood pressure well selected model obtain independent data set perform forward stepwise selection data set would likely obtain model containing diﬀerent perhaps even non overlapping set snps detract value model obtained instance model might turn eﬀective predicting blood pressure independent set patients might clinically useful physicians must careful overstate results obtained make clear identiﬁed simply one many possible models predicting blood pressure must validated independent data sets also important particularly careful reporting errors measures model high dimensional setting seen easy obtain useless model zero residu als therefore one never use sum squared errors values linear model selection regularization statistics traditional measures model training data evidence good model high dimensional setting instance saw figure one easily obtain model reporting fact might mislead others thinking sta tistically valid useful model obtained whereas fact provides absolutely evidence compelling model important instead report results independent test set cross validation errors instance mse independent test set valid measure model mse training set certainly 
[linear, model, selection, regularization, lab, subset, selection, methods, best, subset, selection] apply best subset selection approach hitters data wish predict baseball player salary basis various statistics associated performance previous year first note salary variable missing players function used identify missing observa tions returns vector length input vector true elements missing false non missing elements sum function used count missing elements sum fix rbi dim sum hence see salary missing players omit function removes rows missing values variable dim sum regsubsets function part leaps library performs best sub regsubsets set selection identifying best model contains given number predictors best quantiﬁed using rss syntax summary command outputs best set variables model size lab subset selection methods hitters hitters rbi asterisk indicates given variable included corresponding model instance output indicates best two variable model contains hits crbi default regsubsets reports results best eight variable model nvmax option used order return many variables desired variable model data hitters nvmax reg summary function also returns rss adjusted bic examine try select best overall model reg rsq rss bic obj linear model selection regularization instance see statistic increases one variable included model almost variables included expected statistic increases monotonically variables included reg plotting rss adjusted bic models help decide model select note type option tells connect plotted points lines par reg rss reg rsq points command works like plot command except points puts points plot already created instead creating new plot max function used identify location maximum point vector plot red dot indicate model largest adjusted statistic max reg reg col red cex pch similar fashion plot bic statistics indicate models smallest statistic using min min reg min reg reg col red cex pch min reg reg bic reg col red cex pch regsubsets function built plot command used display selected variables best model given number predictors ranked according bic adjusted aic ﬁnd function type plot regsubsets full full full full bic lab subset selection methods top row plot contains black square variable selected according optimal model associated statistic instance see several models share bic close however model lowest bic six variable model contains atbat hits walks crbi divisionw putouts use coef function see coeﬃcient estimates associated model full 
[linear, model, selection, regularization, lab, subset, selection, methods, forward, backward, stepwise, selection] also use regsubsets function perform forward stepwise backward stepwise selection using argument method forward method backward fwd data hitters nvmax fwd bwd data hitters nvmax bwd instance see using forward stepwise selection best one variable model contains crbi best two variable model ditionally includes hits data best one variable six variable models identical best subset forward selection however best seven variable models identiﬁed forward stepwise lection backward stepwise selection best subset selection diﬀerent full fwd bwd linear model selection regularization 
[linear, model, selection, regularization, lab, subset, selection, methods, choosing, among, models, using, validation, set, approach, cross-validation] saw possible choose among set models diﬀerent sizes using bic adjusted consider using validation set cross validation approaches order approaches yield accurate estimates test error must use training observations perform aspects model ﬁtting including variable selection therefore determination model given size best must made using training observations point subtle important full data set used perform best subset selection step validation set errors cross validation errors obtain accurate estimates test error order use validation set approach begin splitting observations training set test set creating random vector train elements equal true corresponding observation training set false otherwise vector test true observation test set false otherwise note command create test causes true switched false vice versa also set random seed user obtain training set test set split set true rep apply regsubsets training set order perform best subset selection data hitters train notice subset hitters data frame directly call der access training subset data using expression hitters train compute validation set error best model model size ﬁrst make model matrix test data mat data hitters test model matrix function used many regression packages build model matrix ing matrix data run loop size extract coeﬃcients regfit best best model size multiply appropriate columns test model matrix form predictions compute test mse val rep best lab subset selection methods mat val ﬁnd best model one contains ten variables val min val best little tedious partly predict method regsubsets since using function capture steps write predict method object newdata mat form object mat function pretty much mimics complex part extracted formula used call regsubsets demonstrate use function cross validation finally perform best subset selection full data set select best ten variable model important make use full data set order obtain accurate coeﬃcient estimates note perform best subset selection full data set select best ten variable model rather simply using variables obtained training set best ten variable model full data set may diﬀer corresponding model training set data hitters nvmax best linear model selection regularization fact see best ten variable model full data set diﬀerent set variables best ten variable model training set try choose among models diﬀerent sizes using cross validation approach somewhat involved must perform best subset selection within training sets despite see clever subsetting syntax makes job quite easy first create vector allocates observation one folds create matrix store results set null write loop performs cross validation fold elements folds equal test set remainder training set make predictions model size using new predict method compute test errors appropriate subset store appropriate slot matrix errors fit data hitters folds fit given matrix element corresponds test mse cross validation fold best variable model use apply function average columns apply matrix order obtain vector element cross validation error variable model errors par errors see cross validation selects variable model perform best subset selection full data set order obtain variable model reg data hitters nvmax reg best lab ridge regression lasso 
[linear, model, selection, regularization, lab, ridge, regression, lasso] use glmnet package order perform ridge regression lasso main function package glmnet used glmnet ridge regression models lasso models function slightly diﬀerent syntax model ﬁtting functions encountered thus far book particular must pass matrix well vector use syntax perform ridge regression lasso order predict salary hitters data proceeding ensure missing values removed data described section hitters model matrix function particularly useful creating produce matrix corresponding predictors also automatically transforms qualitative variables dummy variables latter property important glmnet take numerical quantitative inputs 
[linear, model, selection, regularization, lab, ridge, regression, lasso, ridge, regression] glmnet function alpha argument determines type model alpha ridge regression model alpha lasso model ﬁrst ridge regression model seq mod default glmnet function performs ridge regression automati cally selected range values however chosen implement function grid values ranging sentially covering full range scenarios null model containing intercept least squares see also com pute model ﬁts particular value one original grid values note default glmnet function standardizes variables scale turn default setting use argument standardize false associated value vector ridge regression coeﬃcients stored matrix accessed coef case linear model selection regularization matrix rows one predictor plus intercept columns one value dim mod expect coeﬃcient estimates much smaller terms norm large value used compared small value used coeﬃcients along norm mod rbi sum mod contrast coeﬃcients along norm note much larger norm coeﬃcients associated smaller value mod rbi sum mod use predict function number purposes instance obtain ridge regression coeﬃcients new value say mod rbi lab ridge regression lasso split samples training set test set order estimate test error ridge regression lasso two common ways randomly split data set ﬁrst produce random vector true false elements select observations corresponding true training data second randomly choose subset numbers used indices training observations two approaches work equally well used former method section demonstrate latter approach ﬁrst set random seed results obtained repro ducible set next ridge regression model training set evaluate mse test set using note use predict function time get predictions test set replacing type coefficients newx argument mod train grid mod test pred test mse note instead simply model intercept would predicted test observation using mean training observations case could compute test set mse like could also get result ﬁtting ridge regression model large value note means mod test pred ﬁtting ridge regression model leads much lower test mse ﬁtting model intercept check whether beneﬁt performing ridge regression instead performing least squares regression recall least squares simply ridge regression order glmnet yield exact least squares coeﬃcients use argument exact calling predict function otherwise predict function interpolate grid values used ﬁtting linear model selection regularization mod test pred subset train mod general want unpenalized least squares model use function since function provides useful outputs standard errors values coeﬃcients general instead arbitrarily choosing would better use cross validation choose tuning parameter using built cross validation function glmnet default function glmnet performs ten fold cross validation though changed using argument nfolds note set random seed ﬁrst results reproducible since choice cross validation folds random set train min therefore see value results smallest cross validation error test mse associated value mod bestlam test pred represents improvement test mse got using finally reﬁt ridge regression model full data set using value chosen cross validation examine coeﬃcient estimates rbi glmnet model yielding approximate results use exact remains slight discrepancy third decimal place output glmnet output due numerical approximation part glmnet lab ridge regression lasso expected none coeﬃcients zero ridge regression perform variable selection 
[linear, model, selection, regularization, lab, ridge, regression, lasso, lasso] saw ridge regression wise choice outperform least squares well null model hitters data set ask whether lasso yield either accurate interpretable model ridge regression order lasso model use glmnet function however time use argument alpha change proceed ﬁtting ridge model mod train mod see coeﬃcient plot depending choice tuning parameter coeﬃcients exactly equal zero perform cross validation compute associated test error set train min mod bestlam test pred substantially lower test set mse null model least squares similar test mse ridge regression chosen cross validation however lasso substantial advantage ridge regression resulting coeﬃcient estimates sparse see coeﬃcient estimates exactly zero lasso model chosen cross validation contains seven variables rbi linear model selection regularization 
[linear, model, selection, regularization, lab, pcr, pls, regression, principal, components, regression] principal components regression pcr performed using pcr pcr function part pls library apply pcr hitters data order predict salary ensure missing values removed data described section pls set pcr fit pcr data hitters scale true syntax pcr function similar additional options setting scale true eﬀect standardizing predictor using prior generating principal components scale variable measured eﬀect setting validation causes pcr compute ten fold cross validation error possible value number principal components used resulting examined using summary pcr fit fit cross score provided possible number components ranging onwards printed output note pcr reports root mean squared error order obtain usual mse must square quantity instance root mean squared error corresponds mse one also plot cross validation scores using validationplot validation plot function using val type msep cause cross validation mse plotted pcr fit val lab pcr pls regression see smallest cross validation error occurs com ponents used barely fewer amounts simply performing least squares components used pcr dimension reduction occurs however plot also see cross validation error roughly one component included model suggests model uses small number components might suﬃce summary function also provides percentage variance explained predictors response using diﬀerent numbers compo nents concept discussed greater detail chapter brieﬂy think amount information predictors response captured using principal components example setting captures variance information predictors contrast using increases value use components would increase perform pcr training data evaluate test set performance set pcr fit pcr data hitters subset train scale true pcr fit val ﬁnd lowest cross validation error occurs component used compute test mse follows pcr pcr fit test pcr pred test set mse competitive results obtained using ridge gression lasso however result way pcr implemented ﬁnal model diﬃcult interpret perform kind variable selection even directly produce coeﬃcient estimates finally pcr full data set using number components identiﬁed cross validation pcr fit pcr scale true ncomp pcr fit fit linear model selection regularization 
[linear, model, selection, regularization, lab, pcr, pls, regression, partial, least, squares] implement partial least squares pls using plsr function also plsr pls library syntax like pcr function set pls fit data hitters subset train scale true pls fit fit cross pls fit val lowest cross validation error occurs partial least squares directions used evaluate corresponding test set mse pls pls fit test pls pred test mse comparable slightly higher test mse obtained using ridge regression lasso pcr finally perform pls using full data set using number components identiﬁed cross validation pls fit data hitters scale true ncomp pls fit fit notice percentage variance salary two component pls explains almost much explained using exercises ﬁnal seven component model pcr pcr attempts maximize amount variance explained predictors pls searches directions explain variance predic tors response 
[linear, model, selection, regularization, exercises, conceptual] perform best subset forward stepwise backward stepwise selection single data set approach obtain models containing predictors explain answers three models predictors smallest training rss three models predictors smallest test rss true false predictors variable model identiﬁed forward stepwise subset predictors variable model identiﬁed forward stepwise selection predictors variable model identiﬁed back ward stepwise subset predictors variable model identiﬁed backward stepwise selection iii predictors variable model identiﬁed back ward stepwise subset predictors variable model identiﬁed forward stepwise selection predictors variable model identiﬁed forward stepwise subset predictors variable model identiﬁed backward stepwise selection predictors variable model identiﬁed best subset subset predictors variable model identiﬁed best subset selection parts indicate correct justify answer lasso relative least squares ﬂexible hence give improved prediction curacy increase bias less decrease variance ﬂexible hence give improved prediction accu racy increase variance less decrease bias linear model selection regularization iii less ﬂexible hence give improved prediction accu racy increase bias less decrease variance less ﬂexible hence give improved prediction accu racy increase variance less decrease bias repeat ridge regression relative least squares repeat non linear methods relative least squares suppose estimate regression coeﬃcients linear regression model minimizing subject particular value parts indicate correct justify answer increase training rss increase initially eventually start decreasing inverted shape decrease initially eventually start increasing shape iii steadily increase steadily decrease remain constant repeat test rss repeat variance repeat squared bias repeat irreducible error suppose estimate regression coeﬃcients linear regression model minimizing particular value parts indicate correct justify answer exercises increase training rss increase initially eventually start decreasing inverted shape decrease initially eventually start increasing shape iii steadily increase steadily decrease remain constant repeat test rss repeat variance repeat squared bias repeat irreducible error well known ridge regression tends give similar coeﬃcient values correlated variables whereas lasso may give quite dif ferent coeﬃcient values correlated variables explore property simple setting suppose furthermore suppose estimate intercept least squares ridge regression lasso model zero write ridge regression optimization problem set ting argue setting ridge coeﬃcient estimates satisfy write lasso optimization problem setting argue setting lasso coeﬃcients unique words many possible solutions optimization problem describe solutions explore consider choice plot function plot conﬁrm consider choice plot function plot conﬁrm linear model selection regularization derive bayesian connection lasso ridge regression discussed section suppose inde pendent identically distributed distribution write likelihood data assume following prior independent identically distributed according double exponential distribution mean common scale parameter exp write posterior setting argue lasso estimate mode pos terior distribution assume following prior independent identically distributed according normal distribution mean zero variance write posterior setting argue ridge regression estimate mode mean posterior distribution 
[linear, model, selection, regularization, exercises, applied] exercise generate simulated data use data perform best subset selection use rnorm function generate predictor length well noise vector length generate response vector length according model constants choice use regsubsets function perform best subset selection order choose best model containing predictors best model obtained according bic adjusted show plots provide evidence answer report coeﬃcients best model tained note need use data frame function create single data set containing exercises repeat using forward stepwise selection also using back wards stepwise selection answer compare results lasso model simulated data using predictors use cross validation select optimal value create plots cross validation error function report resulting coeﬃcient estimates discuss results obtained generate response vector according model perform best subset selection lasso discuss results obtained exercise predict number applications received using variables college data set split data set training set test set fit linear model using least squares training set report test error obtained fit ridge regression model training set chosen cross validation report test error obtained fit lasso model training set chosen cross validation report test error obtained along num ber non zero coeﬃcient estimates fit pcr model training set chosen cross validation report test error obtained along value selected cross validation fit pls model training set chosen cross validation report test error obtained along value selected cross validation comment results obtained accurately pre dict number college applications received much diﬀerence among test errors resulting ﬁve proaches seen number features used model increases training error necessarily decrease test error may explore simulated data set generate data set features observa tions associated quantitative response vector generated according model elements exactly equal zero linear model selection regularization split data set training set containing observations test set containing observations perform best subset selection training set plot training set mse associated best model size plot test set mse associated best model size model size test set mse take minimum value comment results takes minimum value model containing intercept model containing features play around way generating data come scenario test set mse minimized intermediate model size model test set mse minimized compare true model used generate data comment coeﬃcient values create plot displaying range values coeﬃcient estimate best model containing coeﬃcients comment observe compare test mse plot try predict per capita crime rate boston data set try regression methods explored chapter best subset selection lasso ridge regression pcr present discuss results approaches consider propose model set models seem perform well data set justify answer make sure evaluating model performance using validation set error cross validation reasonable alternative opposed using training error chosen model involve features data set 
[moving, beyond, linearity] far book mostly focused linear models linear models relatively simple describe implement advantages approaches terms interpretation inference however stan dard linear regression signiﬁcant limitations terms predic tive power linearity assumption almost always approximation sometimes poor one chapter see improve upon least squares using ridge regression lasso principal com ponents regression techniques setting improvement obtained reducing complexity linear model hence variance estimates still using linear model improved far chapter relax linearity assumption still attempting maintain much interpretability possible examining simple extensions linear models like polyno mial regression step functions well sophisticated approaches splines local regression generalized additive models polynomial regression extends linear model adding extra pre dictors obtained raising original predictors power example cubic regression uses three variables predictors approach provides simple way provide non linear data step functions cut range variable distinct regions order produce qualitative variable eﬀect ﬁtting piecewise constant function james introduction statistical learning applications springer texts statistics doi springer science business media new york moving beyond linearity regression splines ﬂexible polynomials step functions fact extension two involve viding range distinct regions within region polynomial function data however polynomials constrained join smoothly region boundaries knots provided interval divided enough regions produce extremely ﬂexible smoothing splines similar regression splines arise slightly diﬀerent situation smoothing splines result minimizing residual sum squares criterion subject smoothness penalty local regression similar splines diﬀers important way regions allowed overlap indeed smooth way generalized additive models allow extend methods deal multiple predictors sections present number approaches modeling relationship response single predictor ﬂexible way section show approaches seamlessly inte grated order model response function several predictors 
[moving, beyond, linearity, polynomial, regression] historically standard way extend linear regression settings relationship predictors response non linear replace standard linear model polynomial function error term approach known polynomial regression polynomial regression fact saw example method section large enough degree polynomial regression allows produce extremely non linear curve notice coeﬃcients easily estimated using least squares linear regression standard linear model predictors generally speaking unusual use greater large values polynomial curve become overly ﬂexible take strange shapes especially true near boundary variable polynomial regression age degree polynomial age age figure wage data left solid blue curve degree polynomial wage thousands dollars function age least squares dotted curves indicate estimated conﬁdence interval right model binary event wagegt using logistic regression degree polynomial ﬁtted posterior probability wage exceeding shown blue along estimated conﬁdence interval left hand panel figure plot wage age wage data set contains income demographic information males reside central atlantic region united states see results ﬁtting degree polynomial using least squares solid blue curve even though linear regression model like individual coeﬃcients particular interest instead look entire ﬁtted function across grid values age order understand relationship age wage figure pair dotted curves accompanies standard error curves let see arise suppose computed particular value age variance var least squares returns variance estimates ﬁtted coeﬃcients well covariances pairs coeﬃcient estimates use compute estimated variance estimated pointwise standard error square root variance computation repeated covariance matrix var moving beyond linearity reference point plot ﬁtted curve well twice standard error either side ﬁtted curve plot twice standard error normally distributed error terms quantity corresponds approximate conﬁdence interval seems like wages figure two distinct populations appears high earners group earning per annum well low earners group treat wage binary variable splitting two groups logistic regression used predict binary response using polynomial functions age predictors words model exp exp result shown right hand panel figure gray marks top bottom panel indicate ages high earners low earners solid blue curve indicates ﬁtted probabilities high earner function age estimated conﬁdence interval shown well see conﬁdence intervals fairly wide especially right hand side although sample size data set substantial high earners results high variance estimated coeﬃcients consequently wide conﬁdence intervals 
[moving, beyond, linearity, step, functions] using polynomial functions features predictors linear model imposes global structure non linear function instead use step functions order avoid imposing global structure step function break range bins diﬀerent constant bin amounts converting continuous variable ordered categorical variable ordered categorical variable greater detail create cutpoints range construct new variables indicator function returns condition true indicator function returns otherwise example equals step functions age piecewise constant age age figure wage data left solid curve displays ﬁtted value least squares regression wage thousands dollars using step functions age dotted curves indicate estimated conﬁdence interval right model binary event wagegt using logistic regression using step functions age ﬁtted posterior probability wage exceeding shown along estimated conﬁdence interval equals otherwise sometimes called dummy variables notice value since must exactly one intervals use least squares linear model using predictors given value one non zero note predictors zero interpreted mean value comparison predicts response represents average increase response relative example ﬁtting step functions wage data figure shown left hand panel figure also logistic regression model exclude predictor redundant intercept similar fact need two dummy variables code qualitative variable three levels provided model contain intercept decision exclude instead arbitrary alternatively could include exclude intercept moving beyond linearity exp exp order predict probability individual high earner basis age right hand panel figure displays ﬁtted posterior probabilities obtained using approach unfortunately unless natural breakpoints predictors piecewise constant functions miss action example left hand panel figure ﬁrst bin clearly misses increasing trend wage age nevertheless step function approaches popular biostatistics epidemiology among disciplines example year age groups often used deﬁne bins 
[moving, beyond, linearity, basis, functions] polynomial piecewise constant regression models fact special cases basis function approach idea hand fam basis function ily functions transformations applied variable instead ﬁtting linear model model note basis functions ﬁxed known words choose functions ahead time polynomial regression basis functions piecewise constant functions think standard linear model predictors hence use least squares estimate unknown regression coeﬃcients importantly means inference tools linear models discussed chapter standard errors coeﬃcient estimates statistics model overall signiﬁcance available setting thus far considered use polynomial functions piece wise constant functions basis functions however many alternatives possible instance use wavelets fourier series construct basis functions next section investigate common choice basis function regression splines regression spline regression splines 
[moving, beyond, linearity, regression, splines] discuss ﬂexible class basis functions extends upon polynomial regression piecewise constant regression approaches seen 
[moving, beyond, linearity, regression, splines, piecewise, polynomials] instead ﬁtting high degree polynomial entire range piece wise polynomial regression involves ﬁtting separate low degree polynomials piecewise polynomial regression diﬀerent regions example piecewise cubic polynomial works ﬁtting cubic regression model form coeﬃcients diﬀer diﬀerent parts range points coeﬃcients change called knots knot example piecewise cubic knots standard cubic polynomial piecewise cubic polynomial single knot point takes form words two diﬀerent polynomial functions data one subset observations one subset observations ﬁrst polynomial function coeﬃcients second coeﬃcients polynomial functions using least squares applied simple functions original predictor using knots leads ﬂexible piecewise polynomial gen eral place diﬀerent knots throughout range end ﬁtting diﬀerent cubic polynomials note need use cubic polynomial example instead piecewise linear functions fact piecewise constant functions section piecewise polynomials degree top left panel figure shows piecewise cubic polynomial subset wage data single knot age immediately see problem function discontinuous looks ridiculous since polynomial four parameters using total eight degrees freedom ﬁtting piecewise polynomial model degrees freedom 
[moving, beyond, linearity, regression, splines, constraints, splines] top left panel figure looks wrong ﬁtted curve ﬂexible remedy problem piecewise polynomial moving beyond linearity age piecewise cubic age continuous piecewise cubic age cubic spline age linear spline figure various piecewise polynomials subset wage data knot age top left cubic polynomials unconstrained top right cubic polynomials constrained continuous age bottom left cubic polynomials constrained continuous continuous ﬁrst second derivatives bottom right linear spline shown constrained continuous constraint ﬁtted curve must continuous words cannot jump age top right plot figure shows resulting looks better top left plot shaped join looks unnatural lower left plot added two additional constraints ﬁrst second derivatives piecewise polynomials continuous derivative age words requiring piecewise polynomial continuous age also smooth constraint impose piecewise cubic polynomials eﬀectively frees one degree freedom reducing complexity resulting piecewise polynomial top left plot using eight degrees free dom bottom left plot imposed three constraints continuity continuity ﬁrst derivative continuity second derivative left ﬁve degrees freedom curve bottom left regression splines plot called cubic spline general cubic spline knots uses cubic spline total degrees freedom figure lower right plot linear spline continuous linear spline age general deﬁnition degree spline piecewise degree polynomial continuity derivatives degree knot therefore linear spline obtained ﬁtting line region predictor space deﬁned knots requiring continuity knot figure single knot age course could add knots impose continuity 
[moving, beyond, linearity, regression, splines, spline, basis, representation] regression splines saw previous section may seemed somewhat complex piecewise degree polynomial constraint possibly ﬁrst derivatives continuous turns use basis model represent regression spline cubic spline knots modeled appropriate choice basis functions model using least squares several ways represent polynomials also many equivalent ways represent cubic splines using diﬀerent choices basis functions direct way represent cubic spline using start basis cubic polynomial namely add one truncated power basis function per knot truncated power basis truncated power basis function deﬁned otherwise knot one show adding term form model cubic polynomial lead discontinuity third derivative function remain continuous continuous ﬁrst second derivatives knots words order cubic spline data set knots perform least squares regression intercept predictors form knots amounts estimating total regression coeﬃ cients reason ﬁtting cubic spline knots uses degrees freedom cubic splines popular human eyes cannot detect discontinuity knots moving beyond linearity age age natural cubic spline cubic spline figure cubic spline natural cubic spline three knots subset wage data unfortunately splines high variance outer range predictors takes either small large value figure shows wage data three knots see conﬁdence bands boundary region appear fairly wild natu ral spline regression spline additional boundary constraints natural spline function required linear boundary region smaller smallest knot larger largest knot addi tional constraint means natural splines generally produce stable estimates boundaries figure natural cubic spline also displayed red line note corresponding conﬁdence intervals narrower 
[moving, beyond, linearity, regression, splines, choosing, number, locations, knots] spline place knots regression spline ﬂexible regions contain lot knots regions polynomial coeﬃcients change rapidly hence one option place knots places feel function might vary rapidly place fewer knots seems stable option work well practice common place knots uniform fashion one way specify desired degrees freedom software automatically place corresponding number knots uniform quantiles data figure shows example wage data figure natural cubic spline three knots except time knot locations chosen automatically percentiles regression splines age age natural cubic spline age age figure natural cubic spline function four degrees freedom wage data left spline wage thousands dollars function age right logistic regression used model binary event wagegt function age ﬁtted posterior probability wage exceeding shown age speciﬁed requesting four degrees freedom gument four degrees freedom leads three interior knots somewhat technical many knots use equivalently many degrees freedom spline contain one option try diﬀerent num bers knots see produces best looking curve somewhat objective approach use cross validation discussed chap ters method remove portion data say spline certain number knots remaining data use spline make predictions held portion repeat process multiple times observation left compute overall cross validated rss procedure peated diﬀerent numbers knots value giving smallest rss chosen actually ﬁve knots including two boundary knots cubic spline ﬁve knots would nine degrees freedom natural cubic splines two additional natural constraints boundary enforce linearity resulting degrees freedom since includes constant absorbed intercept count four degrees freedom moving beyond linearity degrees freedom natural spline mean squared error degrees freedom cubic spline mean squared error figure ten fold cross validated mean squared errors selecting degrees freedom ﬁtting splines wage data response wage predictor age left natural cubic spline right cubic spline figure shows ten fold cross validated mean squared errors splines various degrees freedom wage data left hand panel corresponds natural spline right hand panel cubic spline two methods produce almost identical results clear evidence one degree linear regression adequate curves ﬂatten quickly seems three degrees freedom natural spline four degrees freedom cubic spline quite adequate section additive spline models simultaneously several variables time could potentially require selection degrees freedom variable cases like typically adopt pragmatic approach set degrees freedom ﬁxed number say four terms 
[moving, beyond, linearity, regression, splines, comparison, polynomial, regression] regression splines often give superior results polynomial regression unlike polynomials must use high degree exponent highest monomial term produce ﬂexible ﬁts splines intro duce ﬂexibility increasing number knots keeping degree ﬁxed generally approach produces stable estimates splines also allow place knots hence ﬂexibility regions function seems changing rapidly fewer knots appears stable figure compares natural cubic spline degrees freedom degree polynomial wage data set extra ﬂexibil ity polynomial produces undesirable results boundaries natural cubic spline still provides reasonable data smoothing splines age age natural cubic spline polynomial figure wage data set natural cubic spline degrees freedom compared degree polynomial polynomials show wild behavior especially near tails 
[moving, beyond, linearity, smoothing, splines, overview, smoothing, splines] last section discussed regression splines create spec ifying set knots producing sequence basis functions using least squares estimate spline coeﬃcients introduce somewhat diﬀerent approach also produces spline ﬁtting smooth curve set data really want ﬁnd function say ﬁts observed data well want rss small however problem approach put constraints always make rss zero simply choosing interpolates function would woefully overﬁt data would far ﬂexible really want function makes rss small also smooth might ensure smooth number ways natural approach ﬁnd function minimizes nonnegative tuning parameter function minimizes known smoothing spline smoothing spline mean equation takes loss penalty mulation encounter context ridge regression lasso chapter term loss function encour loss function ages data well term penalty term moving beyond linearity penalizes variability notation indicates second derivative function ﬁrst derivative measures slope function second derivative corresponds amount slope changing hence broadly speaking second derivative function measure roughness large absolute value wiggly near close zero otherwise second derivative straight line zero note line perfectly smooth notation integral think summation range words simply measure total change function entire range smooth close constant take small value conversely jumpy variable vary signiﬁcantly take large value therefore courages smooth larger value smoother penalty term eﬀect function jumpy exactly interpolate training observations perfectly smooth straight line passes closely possible training points fact case linear least squares line since loss function amounts minimizing residual sum squares intermediate value approximate training observations somewhat smooth see controls bias variance trade smoothing spline function minimizes shown spe cial properties piecewise cubic polynomial knots unique values continuous ﬁrst second derivatives knot furthermore linear region outside extreme knots words function minimizes natural cubic spline knots however natural cubic spline one would get one applied basis function approach scribed section knots rather shrunken version natural cubic spline value tuning rameter controls level shrinkage 
[moving, beyond, linearity, smoothing, splines] seen smoothing spline simply natural cubic spline knots every unique value might seem smoothing spline far many degrees freedom since knot data point allows great deal ﬂexibility tuning parameter controls roughness smoothing spline hence eﬀective degrees freedom possible show increases eﬀective eﬀective degrees freedom degrees freedom write decrease context smoothing splines discuss eﬀective degrees freedom instead degrees freedom usually degrees freedom refer smoothing splines number free parameters number coeﬃcients polynomial cubic spline although smoothing spline parameters hence nominal degrees freedom parameters heavily constrained shrunk hence measure ﬂexibility smoothing spline higher ﬂexible lower bias higher variance smoothing spline deﬁnition eﬀective degrees freedom somewhat technical write solution particular choice vector containing ﬁtted values smoothing spline training points equation indicates vector ﬁtted values applying smoothing spline data written matrix formula times response vector eﬀective degrees freedom deﬁned sum diagonal elements matrix ﬁtting smoothing spline need select number location knots knot training observation instead another problem need choose value come surprise one possible solution problem cross validation words ﬁnd value makes cross validated rss small possible turns leave one cross validation error loocv computed eﬃciently smoothing splines essentially cost computing single using following formula rss notation indicates ﬁtted value smoothing spline evaluated uses training observations except observation contrast indicates smoothing spline function training observations evaluated remarkable formula says compute leave one ﬁts using original data similar formula page chapter least squares linear regression using quickly perform loocv regression splines discussed earlier chapter well least squares regression using arbitrary basis functions exact formulas computing technical however eﬃcient algorithms available computing quantities moving beyond linearity age age smoothing spline degrees freedom degrees freedom loocv figure smoothing spline ﬁts wage data red curve results specifying eﬀective degrees freedom blue curve found automatically leave one cross validation resulted eﬀective degrees freedom figure shows results ﬁtting smoothing spline wage data red curve indicates obtained pre specifying would like smoothing spline eﬀective degrees freedom blue curve smoothing spline obtained chosen using loocv case value chosen results eﬀective degrees freedom computed using data little discernible diﬀerence two smoothing splines beyond fact one degrees freedom seems slightly wigglier since little diﬀerence two ﬁts smoothing spline degrees freedom preferable since general simpler models better unless data provides evidence support complex model 
[moving, beyond, linearity, local, regression] local regression diﬀerent approach ﬁtting ﬂexible non linear func local regression tions involves computing target point using nearby training observations figure illustrates idea simu lated data one target point near another near boundary ﬁgure blue line represents function data generated light orange line corresponds local regression estimate local regression described algorithm note step algorithm weights diﬀer value words order obtain local regression new point need new weighted least squares regression model local regression ooo ooo ooo local regression figure local regression illustrated simulated data blue curve represents data generated light orange curve corresponds local regression estimate orange colored points local target point represented orange vertical line yellow bell shape superimposed plot indicates weights assigned point decreasing zero distance target point obtained ﬁtting weighted linear regression orange line segment using ﬁtted value orange solid dot estimate minimizing new set weights local regression sometimes referred memory based procedure like nearest neighbors need training data time wish compute prediction avoid getting technical details local regression books written topic order perform local regression number choices made deﬁne weighting function whether linear constant quadratic regression step equation corresponds linear regression choices make diﬀerence important choice span deﬁned step span plays role like tuning parameter smoothing splines controls ﬂexibility non linear smaller value local wiggly alternatively large value lead global data using training observations use cross validation choose specify directly figure displays local linear regression ﬁts wage data using two values expected obtained using smoother obtained using idea local regression generalized many diﬀerent ways setting multiple features one useful general ization involves ﬁtting multiple linear regression model global variables local another time varying coeﬃcient moving beyond linearity algorithm local regression gather fraction training points whose closest assign weight point neighborhood point furthest weight zero closest highest weight nearest neighbors get weight zero fit weighted least squares regression using aforementioned weights ﬁnding minimize ﬁtted value given models useful way adapting model recently gathered varying coeﬃcient model data local regression also generalizes naturally want models local pair variables rather one simply use two dimensional neighborhoods bivariate linear regression models using observations near target point two dimensional space theoretically approach imple mented higher dimensions using linear regressions dimensional neighborhoods however local regression perform poorly much larger generally training observations close nearest neighbors regression discussed chap ter suﬀers similar problem high dimensions 
[moving, beyond, linearity, generalized, additive, models] sections present number approaches ﬂexibly predict ing response basis single predictor approaches seen extensions simple linear regression explore prob lem ﬂexibly predicting basis several predictors amounts extension multiple linear regression generalized additive models gams provide general framework generalized additive model extending standard linear model allowing non linear functions variables maintaining additivity like linear models gams additivity applied quantitative qualitative responses ﬁrst examine gams quantitative response section qualitative response section generalized additive models age age local linear regression span degrees freedom span degrees freedom figure local linear ﬁts wage data span speciﬁes fraction data used compute target point 
[moving, beyond, linearity, generalized, additive, models, gams, regression, problems] natural way extend multiple linear regression model order allow non linear relationships feature response replace linear component smooth non linear function would write model example gam called additive model calculate separate add together contributions sections discuss many methods ﬁtting functions single variable beauty gams use methods building blocks ﬁtting additive model fact methods seen far chapter done fairly trivially take example natural splines consider task ﬁtting model wage year age education moving beyond linearity coll coll coll age education age year education figure wage data plots relationship feature response wage ﬁtted model plot displays ﬁtted function pointwise standard errors ﬁrst two functions natural splines year age four ﬁve degrees freedom respectively third function step function qualitative variable education wage data year age quantitative variables education qualitative variable ﬁve levels coll coll coll referring amount high school college education individual completed ﬁrst two functions using natural splines third function using separate constant level via usual dummy variable approach section figure shows results ﬁtting model using least squares easy since discussed section natural splines constructed using appropriately chosen set basis functions hence entire model big regression onto spline basis variables dummy variables packed one big regression matrix figure easily interpreted left hand panel indicates holding age education ﬁxed wage tends increase slightly year may due inﬂation center panel indicates holding education year ﬁxed wage tends highest intermediate val ues age lowest young old right hand panel indicates holding year age ﬁxed wage tends increase education educated person higher salary average ﬁndings intuitive figure shows similar triple plots time smoothing splines four ﬁve degrees freedom respectively fit ting gam smoothing spline quite simple ﬁtting gam natural spline since case smoothing splines least squares cannot used however standard software gam function used gams using smoothing splines via approach known backﬁtting method ﬁts model involving multiple predictors backﬁtting generalized additive models coll coll coll age education age year education figure details figure smoothing splines four ﬁve degrees freedom respectively repeatedly updating predictor turn holding others ﬁxed beauty approach time update function simply apply ﬁtting method variable partial residual ﬁtted functions figures look rather similar situations diﬀerences gams obtained using smoothing splines versus natural splines small use splines building blocks gams well use local regression polynomial regression combination approaches seen earlier chapter order create gam gams investigated detail lab end chapter pros cons gams move let summarize advantages limitations gam gams allow non linear automatically model non linear relationships standard linear gression miss means need manually try many diﬀerent transformations variable individually non linear ﬁts potentially make accurate predictions response model additive still examine eﬀect individually holding variables ﬁxed hence interested inference gams provide useful representation partial residual example form know treating residual response non linear regression moving beyond linearity smoothness function variable sum marized via degrees freedom main limitation gams model restricted additive many variables important interactions missed however linear regression manually add interaction terms gam model including additional predictors form addition add low dimensional interaction functions form model terms using two dimensional smoothers local regression two dimensional splines covered fully general models look even ﬂexible approaches random forests boosting described chapter gams provide useful compromise linear fully nonparametric models 
[moving, beyond, linearity, generalized, additive, models, gams, classiﬁcation, problems] gams also used situations qualitative simplicity assume takes values zero one let conditional probability given predictors response equals one recall logistic regression model log logit log odds versus represents linear function predictors natural way extend allow non linear relationships use model log equation logistic regression gam pros cons discussed previous section quantitative responses gam wage data order predict probability individual income exceeds per year gam takes form log year age education wage year age education lab non linear modeling coll coll coll age education age year education figure wage data logistic regression gam given binary response wagegt plot displays ﬁtted function pointwise standard errors ﬁrst function linear year second function smoothing spline ﬁve degrees freedom age third step function education wide standard errors ﬁrst level education using smoothing spline ﬁve degrees freedom step function creating dummy variables levels education resulting shown figure last panel looks suspicious wide conﬁdence intervals level fact ones category individuals less high school education make per year hence reﬁt gam excluding individuals less high school education resulting model shown figure figures three panels vertical scale allows visually assess relative contributions variables observe age education much larger eﬀect year probability high earner 
[moving, beyond, linearity, lab, non-linear, modeling] lab analyze wage data considered examples chapter order illustrate fact many complex non linear ﬁtting procedures discussed easily implemented begin loading islr library contains data moving beyond linearity coll coll coll age education age yea education figure model figure time excluding observations education see increased education tends associated higher salaries 
[moving, beyond, linearity, lab, non-linear, modeling, polynomial, regression, step, functions] examine figure produced ﬁrst model using following command fit poly age data wage fit std age age age age syntax ﬁts linear model using function order predict wage using fourth degree polynomial age poly age poly com mand allows avoid write long formula powers age function returns matrix whose columns basis thogonal polynomials essentially means column linear orthogonal polynomial combination variables age age age age however also use poly obtain age age age age directly prefer using raw true argument poly function later see aﬀect model meaningful way though choice basis clearly aﬀects coeﬃcient estimates aﬀect ﬁtted values obtained poly age raw data wage std age raw age raw lab non linear modeling age raw age raw several equivalent ways ﬁtting model show case ﬂexibility formula language example age age age age data wage age age age age simply creates polynomial basis functions taking care protect terms like age via wrapper function symbol wrapper special meaning formulas cbind age age age age data wage compactly using cbind function building matrix collection vectors function call cbind inside formula also serves wrapper create grid values age want predictions call generic predict function specifying want standard errors well age age seq fit age age fit fit finally plot data add degree polynomial par mar oma age wage agelims cex col degree age grid lwd col age grid bands lwd col lty mar oma arguments par allow control margins plot title function creates ﬁgure title spans title subplots mentioned earlier whether orthogonal set basis func tions produced poly function aﬀect model obtained meaningful way mean ﬁtted values obtained either case identical fit age age max abs performing polynomial regression must decide degree polynomial use one way using hypothesis tests models ranging linear degree polynomial seek determine simplest model suﬃcient explain relationship moving beyond linearity wage age use anova function performs anova analysis variance anova using test order test null analysis variance hypothesis model suﬃcient explain data alternative hypothesis complex model required order use anova function must nested models predictors must subset predictors case ﬁve diﬀerent models sequentially compare simpler model complex model fit age data wage fit poly age data wage fit poly age data wage fit poly age data wage fit poly age data wage fit fit fit fit fit age poly age poly age poly age poly age res rss sum value comparing linear model quadratic model essentially zero indicating linear suﬃcient sim ilarly value comparing quadratic model cubic model low quadratic also insuﬃcient value comparing cubic degree polynomials model model proximately degree polynomial model seems unnecessary value hence either cubic quartic polynomial appear provide reasonable data lower higher order models justiﬁed case instead using anova function could obtained values succinctly exploiting fact poly creates orthogonal polynomials fit std age age age lab non linear modeling age age notice values fact square statistics equal statistics anova function example however anova method works whether used orthogonal polynomials also works terms model well example use anova compare three models fit education age data wage fit education poly age data wage fit education poly age data wage fit fit fit alternative using hypothesis tests anova could choose polynomial degree using cross validation discussed chapter next consider task predicting whether individual earns per year proceed much except ﬁrst create appropriate response vector apply glm function using family binomial order polynomial logistic regression model fit glm wage poly age data wage family binomial note use wrapper create binary response variable expression wagegt evaluates logical variable containing true false glm coerces binary setting true false make predictions using predict function fit age age however calculating conﬁdence intervals slightly involved linear regression case default prediction type glm model type link use means get predictions logit model form log predictions given form standard errors given also form order obtain conﬁdence intervals use transformation exp exp moving beyond linearity exp exp fit fit exp exp note could directly computed probabilities selecting type response option predict function fit age age however corresponding conﬁdence intervals would sen sible would end negative probabilities finally right hand plot figure made follows age wage agelims age wage cex pch col age grid pfit lwd col age grid bands lwd col lty drawn age values corresponding observations wage values gray marks top plot wage values shown gray marks bottom plot used jitter function jitter age values bit observations jitter age value cover often called rug plot rug plot order step function discussed section use cut function cut cut age fit cut age data wage fit std cut age cut age cut age cut automatically picked cutpoints years age could also speciﬁed cutpoints directly using breaks option function cut returns ordered categorical variable function creates set dummy variables use gression agelt category left intercept coeﬃcient interpreted average salary years age coeﬃcients interpreted average addi tional salary age groups produce predictions plots case polynomial lab non linear modeling 
[moving, beyond, linearity, lab, non-linear, modeling, splines] order regression splines use splines library section saw regression splines constructing appropriate matrix basis functions function generates entire matrix basis functions splines speciﬁed set knots default cubic splines produced fitting wage age using regression spline simple fit age knots data wage fit age age age wage col age grid lwd age grid pred lty age grid pred lty prespeciﬁed knots ages produces spline six basis functions recall cubic spline three knots seven degrees freedom degrees freedom used intercept plus six basis functions could also use option produce spline knots uniform quantiles data dim age dim age age case chooses knots ages correspond percentiles age function also degree argument splines degree rather default degree yields cubic spline order instead natural spline use function natural spline four degrees freedom age data wage fit age age age grid col red lwd function could instead specify knots directly using knots option order smoothing spline use smooth spline function smooth spline figure produced following code age wage agelims cex col fit age wage age wage fit col red lwd moving beyond linearity fit col lwd col red lty lwd cex notice ﬁrst call smooth spline speciﬁed function determines value leads degrees freedom second call smooth spline select smoothness level cross validation results value yields degrees freedom order perform local regression use loess function loess age wage agelims cex col fit age span data wage age span data wage age grid fit age age col red lwd age grid fit age age col lwd col red lty lwd cex performed local linear regression using spans neighborhood consists observations larger span smoother locfit library also used ﬁtting local regression models 
[moving, beyond, linearity, lab, non-linear, modeling, gams] gam predict wage using natural spline functions year age treating education qualitative predictor since big linear regression model using appropriate choice basis functions simply using function year age education data wage model using smoothing splines rather natural splines order general sorts gams using smoothing splines components cannot expressed terms basis functions using least squares regression need use gam library function part gam library used indicate would like use smoothing spline specify function year degrees freedom function age degrees freedom since education qualitative leave converted four dummy variables use gam function gam order gam using components terms simultaneously taking account explain response gam gam gam year age education data wage lab non linear modeling order produce figure simply call plot function par gam true col generic plot function recognizes gam object class gam invokes appropriate plot gam method conveniently even though plot gam gam class gam rather class still use plot gam figure produced using following expression gam gam true col red notice use plot gam rather generic plot function plots function year looks rather linear perform series anova tests order determine three models best gam excludes year gam uses linear function year gam uses spline function year gam gam age education data wage gam gam year age education data wage gam gam gam age education year age education year age education dev ﬁnd compelling evidence gam linear func tion year better gam include year value however evidence non linear func tion year needed value words based results anova preferred summary function produces summary gam gam gam year age education min max moving beyond linearity aic year age values year age correspond null hypothesis linear relationship versus alternative non linear relationship large value year reinforces conclusion anova test lin ear function adequate term however clear evidence non linear term required age make predictions gam objects like objects using predict method class gam make predictions training set gam also use local regression ﬁts building blocks gam using function gam gam year age span education gam gam true col used local regression age term span also use function create interactions calling gam function example gam gam year age span education ﬁts two term model ﬁrst term interaction year age local regression surface plot resulting two dimensional surface ﬁrst install akima package gam order logistic regression gam use func tion constructing binary response variable set family binomial gam gam wage year age education par gam col exercises easy see high earners category wage hence logistic regression gam using category provides sensible results gam gam wage year age education family wage gam col 
[moving, beyond, linearity, exercises, conceptual] mentioned chapter cubic regression spline one knot obtained using basis form equals otherwise show function form indeed cubic regression spline regardless values find cubic polynomial express terms find cubic polynomial express terms established piecewise polynomial show continuous show continuous moving beyond linearity show continuous therefore indeed cubic spline hint parts problem require knowledge single variable calculus reminder given cubic polynomial ﬁrst derivative takes form second derivative takes form suppose curve computed smoothly set points using following formula arg min represents derivative provide example sketches following scenarios suppose curve basis functions note equals otherwise linear regression model obtain coeﬃcient estimates sketch estimated curve note intercepts slopes relevant information suppose curve basis functions linear regression model obtain coeﬃcient estimates sketch estimated curve note intercepts slopes relevant information exercises consider two curves deﬁned arg min arg min represents derivative smaller training rss smaller test rss smaller training test rss 
[moving, beyond, linearity, exercises, applied] exercise analyze wage data set considered throughout chapter perform polynomial regression predict wage using age use cross validation select optimal degree polyno mial degree chosen compare results hypothesis testing using anova make plot resulting polynomial data fit step function predict wage using age perform cross validation choose optimal number cuts make plot obtained wage data set contains number features explored chapter marital status maritl job class jobclass others explore relationships predictors wage use non linear ﬁtting techniques order ﬂexible models data create plots results obtained write summary ﬁndings fit non linear models investigated chapter auto data set evidence non linear relationships data set create informative plots justify answer question uses variables dis weighted mean distances ﬁve boston employment centers nox nitrogen oxides concen tration parts per million boston data treat dis predictor nox response use poly function cubic polynomial regression predict nox using dis report regression output plot resulting data polynomial ﬁts moving beyond linearity plot polynomial ﬁts range diﬀerent polynomial degrees say report associated residual sum squares perform cross validation another approach select opti mal degree polynomial explain results use function regression spline predict nox using dis report output using four degrees freedom choose knots plot resulting regression spline range degrees freedom plot resulting ﬁts report resulting rss describe results obtained perform cross validation another approach order select best degrees freedom regression spline data describe results question relates college data set split data training set test set using state tuition response variables predictors perform forward stepwise selection training set order identify satisfactory model uses subset predictors fit gam training data using state tuition response features selected previous step predictors plot results explain ﬁndings evaluate model obtained test set explain results obtained variables evidence non linear relationship response section mentioned gams generally using backﬁtting approach idea behind backﬁtting actually quite simple explore backﬁtting context multiple linear regression suppose would like perform multiple linear regression software instead software perform simple linear regression therefore take following iterative approach repeatedly hold one coeﬃcient esti mate ﬁxed current value update coeﬃcient estimate using simple linear regression process continued til convergence coeﬃcient estimates stop changing try toy example exercises generate response two predictors initialize take value choice matter value choose keeping ﬁxed model follows coef keeping ﬁxed model follows coef write loop repeat times report estimates iteration loop create plot values displayed shown diﬀerent color compare answer results simply performing multiple linear regression predict using use abline function overlay multiple linear regression coeﬃcient estimates plot obtained data set many backﬁtting iterations required order obtain good approximation multiple gression coeﬃcient estimates problem continuation previous exercise toy example show one approximate multiple linear regression coeﬃcient estimates repeatedly performing simple linear regression backﬁtting procedure many backﬁtting iterations required order obtain good approximation multiple regression coeﬃcient estimates create plot justify answer 
[tree-based, methods] chapter describe tree based methods regression classiﬁcation involve stratifying segmenting predictor space number simple regions order make prediction given observation typically use mean mode training observa tions region belongs since set splitting rules used segment predictor space summarized tree types approaches known decision tree methods decision tree tree based methods simple useful interpretation however typically competitive best supervised learning proaches seen chapters terms prediction accuracy hence chapter also introduce bagging random forests boosting approaches involves producing multiple trees combined yield single consensus prediction see combining large number trees often result dramatic improvements prediction accuracy expense loss inter pretation 
[tree-based, methods, basics, decision, trees] decision trees applied regression classiﬁcation problems ﬁrst consider regression problems move classiﬁcation james introduction statistical learning applications springer texts statistics doi springer science business media new york tree based methods years hits figure hitters data regression tree predicting log salary baseball player based number years played major leagues number hits made previous year given internal node label form indicates left hand branch emanating split right hand branch corresponds instance split top tree results two large branches left hand branch corresponds yearslt right hand branch corresponds yearsgt tree two internal nodes three terminal nodes leaves number leaf mean response observations fall 
[tree-based, methods, basics, decision, trees, regression, trees] order motivate regression trees begin simple example regression tree predicting baseball players salaries using regression trees use hitters data set predict baseball player salary based years number years played major leagues hits number hits made previous year ﬁrst remove observations missing salary values log transform salary distribution typical bell shape recall salary measured thousands dollars figure shows regression tree data consists series splitting rules starting top tree top split assigns observations yearslt left branch predicted salary years hits integers data tree function labels splits midpoint two adjacent values basics decision trees years hits figure three region partition hitters data set regression tree illustrated figure players given mean response value players data set yearslt players mean log salary make prediction thousands dollars players players yearsgt assigned right branch group subdivided hits overall tree stratiﬁes segments players three regions predictor space players played four fewer years players played ﬁve years made fewer hits last year players played ﬁve years made least hits last year three regions written yearslt yearsgt hitslt yearsgt hitsgt figure illustrates regions function years hits predicted salaries three groups respectively keeping tree analogy regions known terminal nodes leaves tree case figure decision terminal node leaf trees typically drawn upside sense leaves bottom tree points along tree predictor space split referred internal nodes figure two internal internal node nodes indicated text yearslt hitslt refer segments trees connect nodes branches branch might interpret regression tree displayed figure follows years important factor determining salary players less experience earn lower salaries experienced players given player less experienced number hits made previous year seems play little role salary among players tree based methods major leagues ﬁve years number hits made previous year aﬀect salary players made hits last year tend higher salaries regression tree shown figure likely simpliﬁcation true relationship hits years salary however advantages types regression models seen chapters easier interpret nice graphical representation prediction via stratiﬁcation feature space discuss process building regression tree roughly speaking two steps divide predictor space set possible values distinct non overlapping regions every observation falls region make prediction simply mean response values training observations instance suppose step obtain two regions response mean training observations ﬁrst region response mean training observations second region given observation predict value predict value elaborate step construct regions theory regions could shape however choose divide predictor space high dimensional rectangles boxes simplicity ease interpretation resulting predic tive model goal ﬁnd boxes minimize rss given mean response training observations within box unfortunately computationally infeasible consider every possible partition feature space boxes reason take top greedy approach known recursive binary splitting recursive binary splitting approach top begins top tree point observations belong single region successively splits predictor space split indicated via two new branches tree greedy step tree building process best split made particular step rather looking ahead picking split lead better tree future step basics decision trees order perform recursive binary splitting ﬁrst select pre dictor cutpoint splitting predictor space regions leads greatest possible reduction rss notation means region predictor space takes value less consider predictors possible values cutpoint predictors choose predictor cutpoint resulting tree lowest rss greater detail deﬁne pair half planes seek value minimize equation mean response training observations mean response training observations finding values minimize done quite quickly especially number features large next repeat process looking best predictor best cutpoint order split data minimize rss within resulting regions however time instead splitting entire predictor space split one two previously identiﬁed regions three regions look split one three regions minimize rss process continues stopping criterion reached instance may continue region contains ﬁve observations regions created predict response given test observation using mean training observations region test observation belongs ﬁve region example approach shown figure tree pruning process described may produce good predictions training set likely overﬁt data leading poor test set performance resulting tree might complex smaller tree fewer splits fewer regions might lead lower variance better interpretation cost little bias one possible alternative process described build tree long decrease rss due split exceeds high threshold strategy result smaller trees short sighted since seemingly worthless split early tree might followed good split split leads large reduction rss later tree based methods figure top left partition two dimensional feature space could result recursive binary splitting top right output recursive binary splitting two dimensional example bottom left tree corresponding partition top right panel bottom right perspective plot prediction surface corresponding tree therefore better strategy grow large tree prune back order obtain subtree determine best prune subtree way prune tree intuitively goal select subtree leads lowest test error rate given subtree estimate test error using cross validation validation set approach however estimating cross validation error every possible subtree would cumbersome since extremely large number possible subtrees instead need way select small set subtrees consideration cost complexity pruning also known weakest link pruning gives cost complexity pruning weakest link pruning way rather considering every possible subtree consider sequence trees indexed nonnegative tuning parameter basics decision trees algorithm building regression tree use recursive binary splitting grow large tree training data stopping terminal node fewer minimum number observations apply cost complexity pruning large tree order obtain sequence best subtrees function use fold cross validation choose divide training observations folds repeat steps fold training data evaluate mean squared prediction error data left fold function average results value pick minimize average error return subtree step corresponds chosen value value corresponds subtree small possible indicates number terminal nodes tree rectangle subset predictor space cor responding terminal node predicted response associated mean training observations tuning parameter controls trade subtree com plexity training data subtree simply equal measures training error however increases price pay tree many terminal nodes quantity tend minimized smaller subtree equation reminiscent lasso chapter similar formulation used order control complexity linear model turns increase zero branches get pruned tree nested predictable fashion obtaining whole sequence subtrees function easy select value using validation set using cross validation return full data set obtain subtree corresponding process summarized algorithm tree based methods years rbi putouts years years hits walks runs walks rbi years figure regression tree analysis hitters data unpruned tree results top greedy splitting training data shown figures display results ﬁtting pruning regression tree hitters data using nine features first randomly divided data set half yielding observations training set observations test set built large regression tree training data varied order create subtrees diﬀerent numbers terminal nodes finally performed six fold cross validation order estimate cross validated mse trees function chose perform six fold cross validation exact multiple six unpruned regression tree shown figure green curve figure shows error function number leaves orange curve indicates test error also shown standard error bars around estimated errors reference training error curve shown black error reasonable approximation test error error takes although error computed function convenient display result function number leaves based relationship original tree grown training data basics decision trees tree size mean squared error training cross validation test figure regression tree analysis hitters data training cross validation test mse shown function number termi nal nodes pruned tree standard error bands displayed minimum cross validation error occurs tree size three minimum three node tree test error also dips three node tree though takes lowest value ten node tree pruned tree containing three terminal nodes shown figure 
[tree-based, methods, basics, decision, trees, classiﬁcation, trees] classiﬁcation tree similar regression tree except classiﬁcation tree used predict qualitative response rather quantitative one call regression tree predicted response observation given mean response training observations belong terminal node contrast classiﬁcation tree predict observation belongs commonly occurring class training observations region belongs interpreting results classiﬁcation tree often interested class prediction corresponding particular terminal node region also class proportions among training observations fall region task growing classiﬁcation tree quite similar task growing regression tree regression setting use recursive binary splitting grow classiﬁcation tree however classiﬁcation setting rss cannot used criterion making binary splits natural alternative rss classiﬁcation error rate since plan classiﬁcation error rate assign observation given region commonly occurring class training observations region classiﬁcation error rate simply fraction training observations region belong common class tree based methods max represents proportion training observations region class however turns classiﬁcation error suﬃciently sensitive tree growing practice two measures preferable gini index deﬁned gini index measure total variance across classes hard see gini index takes small value close zero one reason gini index referred measure node purity small value indicates node contains predominantly observations single class entropy typically used evaluate quality particular split since two approaches sensitive node purity classiﬁcation error rate three approaches might used pruning tree classiﬁcation error rate preferable prediction accuracy ﬁnal pruned tree goal figure shows example heart data set data con tain binary outcome patients presented chest pain outcome value yes indicates presence heart disease based angiographic test means heart disease predic tors including age sex chol cholesterol measurement heart lung function measurements cross validation results tree six terminal nodes discussion thus far assumed predictor vari ables take continuous values however decision trees constructed even presence qualitative predictor variables instance heart data predictors sex thal thallium stress test chestpain qualitative therefore split one variables amounts assigning qualitative values one branch alternative gini index entropy given entropy log since follows log one show entropy take value near zero near zero near one therefore like gini index entropy take small value node pure fact turns gini index entropy quite similar numerically building classiﬁcation tree either gini index basics decision trees thal maxhr restbp chol maxhr maxhr chestpain chol sex slope age thal chestpain oldpeak restecg yes yes yes yes yes yes yes yes tree size error training cross validation test thal maxhr chestpain yes yes yes figure heart data top unpruned tree bottom left cross validation error training test error diﬀerent sizes pruned tree bottom right pruned tree corresponding minimal cross validation error assigning remaining branch figure ternal nodes correspond splitting qualitative variables instance top internal node corresponds splitting thal text thal indicates left hand branch coming node consists observations ﬁrst value thal variable normal right hand node consists remaining observations ﬁxed reversible defects text chestpain two splits tree left indicates left hand branch coming node consists observations second third values chestpain variable possible values typical angina atypical angina non anginal pain asymptomatic tree based methods figure surprising characteristic splits yield two terminal nodes predicted value instance consider split restecglt near bottom right unpruned tree regardless value restecg response value yes predicted servations split performed split performed leads increased node purity observations corresponding right hand leaf response value yes whereas corresponding left hand leaf response value yes node purity important suppose test obser vation belongs region given right hand leaf pretty certain response value yes contrast test observation belongs region given left hand leaf sponse value probably yes much less certain even though split restecglt reduce classiﬁcation error improves gini index entropy sensitive node purity 
[tree-based, methods, basics, decision, trees, trees, versus, linear, models] regression classiﬁcation trees diﬀerent ﬂavor classical approaches regression classiﬁcation presented chapters particular linear regression assumes model form whereas regression trees assume model form represent partition feature space figure model better depends problem hand relationship features response well approximated linear model approach linear regression likely work well outperform method regression tree exploit linear structure instead highly non linear complex relationship features response indicated model decision trees may outperform classical approaches illustrative example displayed figure rela tive performances tree based classical approaches assessed estimating test error using either cross validation validation set approach chapter course considerations beyond simply test error may come play selecting statistical learning method instance certain set tings prediction using tree may preferred sake interpretabil ity visualization basics decision trees figure top row two dimensional classiﬁcation example true decision boundary linear indicated shaded regions classical approach assumes linear boundary left outperform cision tree performs splits parallel axes right bottom row true decision boundary non linear linear model unable capture true decision boundary left whereas decision tree successful right 
[tree-based, methods, basics, decision, trees, advantages, disadvantages, trees] decision trees regression classiﬁcation number advantages classical approaches seen chapters trees easy explain people fact even easier explain linear regression people believe decision trees closely mirror human decision making regression classiﬁcation approaches seen previous chapters trees displayed graphically easily interpreted even non expert especially small trees easily handle qualitative predictors without need create dummy variables tree based methods unfortunately trees generally level predictive accuracy regression classiﬁcation approaches seen book additionally trees non robust words small change data cause large change ﬁnal estimated tree however aggregating many decision trees using methods like bagging random forests boosting predictive performance trees substantially improved introduce concepts next section 
[tree-based, methods, bagging, random, forests, boosting] bagging random forests boosting use trees building blocks construct powerful prediction models 
[tree-based, methods, bagging, random, forests, boosting, bagging] bootstrap introduced chapter extremely powerful idea used many situations hard even impossible directly compute standard deviation quantity interest see bootstrap used completely diﬀerent context order improve statistical learning methods decision trees decision trees discussed section suﬀer high variance means split training data two parts random decision tree halves results get could quite diﬀerent contrast procedure low variance yield similar results applied repeatedly distinct data sets linear regression tends low variance ratio moderately large bootstrap aggregation bagging general purpose procedure reducing bagging variance statistical learning method introduce particularly useful frequently used context decision trees recall given set independent observations variance variance mean observations given words averaging set observations reduces variance hence natural way reduce variance hence increase predic tion accuracy statistical learning method take many training sets population build separate prediction model using training set average resulting predictions words could cal culate using separate training sets average order obtain single low variance statistical learning model bagging random forests boosting given avg course practical generally access multiple training sets instead bootstrap taking repeated samples single training data set approach generate diﬀerent bootstrapped training data sets train method bootstrapped training set order get ﬁnally average predictions obtain bag called bagging bagging improve predictions many regression methods particularly useful decision trees apply bagging regression trees simply construct regression trees using bootstrapped training sets average resulting predictions trees grown deep pruned hence individual tree high variance low bias averaging trees reduces variance bagging demonstrated give impressive improvements accuracy combining together hundreds even thousands trees single procedure thus far described bagging procedure regression context predict quantitative outcome bagging extended classiﬁcation problem qualitative situation possible approaches simplest follows given test observation record class predicted trees take majority vote overall prediction commonly occurring majority vote class among predictions figure shows results bagging trees heart data test error rate shown function number trees constructed using bootstrapped training data sets see bagging test error rate slightly lower case test error rate obtained single tree number trees critical parameter bagging using large value lead overﬁtting practice use value suﬃciently large error settled using suﬃcient achieve good performance example bag error estimation turns straightforward way estimate test error bagged model without need perform cross validation validation set approach recall key bagging trees repeatedly bootstrapped subsets observations one show tree based methods number trees error test bagging test randomforest oob bagging oob randomforest figure bagging random forest results heart data test error black orange shown function number bootstrapped training sets used random forests applied dashed line indicates test error resulting single classiﬁcation tree green blue traces show oob error case considerably lower average bagged tree makes use around two thirds observations remaining one third observations used given bagged tree referred bag oob observations bag predict response observation using trees observation oob yield around predictions observation order obtain single prediction observation average predicted responses regression goal take majority vote classiﬁcation goal leads single oob prediction observation oob prediction obtained way observations overall oob mse regression problem classiﬁcation error classiﬁcation problem computed resulting oob error valid estimate test error bagged model since response observation predicted using trees using observation figure displays oob error heart data shown suﬃciently large oob error virtually equivalent leave one cross validation error oob approach estimating relates exercise chapter bagging random forests boosting test error particularly convenient performing bagging large data sets cross validation would computationally onerous variable importance measures discussed bagging typically results improved accuracy prediction using single tree unfortunately however diﬃcult interpret resulting model recall one advantages decision trees attractive easily interpreted diagram results one displayed figure however bag large number trees longer possible represent resulting statistical learning procedure using single tree longer clear variables important procedure thus bagging improves prediction accuracy expense interpretability although collection bagged trees much diﬃcult interpret single tree one obtain overall summary importance predictor using rss bagging regression trees gini index bagging classiﬁcation trees case bagging regression trees record total amount rss decreased due splits given predictor averaged trees large value indicates important predictor similarly context bagging classiﬁcation trees add total amount gini index decreased splits given predictor averaged trees graphical representation variable importances heart data variable importance shown figure see mean decrease gini index vari able relative largest variables largest mean decrease gini index thal chestpain 
[tree-based, methods, bagging, random, forests, boosting, random, forests] random forests provide improvement bagged trees way random forest small tweak decorrelates trees bagging build number decision trees bootstrapped training samples building decision trees time split tree considered random sample predictors chosen split candidates full set predictors split allowed use one predictors fresh sample predictors taken split typically choose number predictors considered split approximately equal square root total number predictors heart data words building random forest split tree algorithm even allowed consider majority available predictors may sound crazy clever rationale suppose one strong predictor data set along num ber moderately strong predictors collection bagged tree based methods thal chestpain oldpeak maxhr restbp age chol slope sex exang restecg fbs variable importance figure variable importance plot heart data variable impor tance computed using mean decrease gini index expressed relative maximum trees trees use strong predictor top split consequently bagged trees look quite similar hence predictions bagged trees highly correlated fortunately averaging many highly correlated quantities lead large reduction variance averaging many uncorrelated quanti ties particular means bagging lead substantial reduction variance single tree setting random forests overcome problem forcing split consider subset predictors therefore average splits even consider strong predictor predictors chance think process decorrelating trees thereby making average resulting trees less variable hence reliable main diﬀerence bagging random forests choice predictor subset size instance random forest built using amounts simply bagging heart data random forests using leads reduction test error oob error bagging figure using small value building random forest typically helpful large number correlated predictors applied random forests high dimensional biological data set consisting pression measurements genes measured tissue samples patients around genes humans individual genes bagging random forests boosting diﬀerent levels activity expression particular cells tissues biological conditions data set patient samples qualitative label diﬀerent levels either normal diﬀerent types cancer goal use random forests predict cancer type based genes largest variance training set randomly divided observations training test set applied random forests training set three diﬀerent values number splitting variables results shown figure error rate single tree null rate see using trees suﬃcient give good performance choice gave small improvement test error bagging example bagging random forests overﬁt increase practice use value suﬃciently large error rate settled 
[tree-based, methods, bagging, random, forests, boosting, boosting] discuss boosting yet another approach improving predic boosting tions resulting decision tree like bagging boosting general approach applied many statistical learning methods gression classiﬁcation restrict discussion boosting context decision trees recall bagging involves creating multiple copies original train ing data set using bootstrap ﬁtting separate decision tree copy combining trees order create single predic tive model notably tree built bootstrap data set independent trees boosting works similar way except trees grown sequentially tree grown using information previously grown trees boosting involve bootstrap sampling instead tree modiﬁed version original data set consider ﬁrst regression setting like bagging boosting involves com bining large number decision trees boosting described algorithm idea behind procedure unlike ﬁtting single large deci sion tree data amounts ﬁtting data hard potentially overﬁtting boosting approach instead learns slowly given current model decision tree residuals model tree using current residuals rather outcome sponse add new decision tree ﬁtted function order update residuals trees rather small terminal nodes determined parameter algorithm null rate results simply classifying observation dominant class overall case normal class tree based methods number trees classification error figure results random forests class gene expression data set predictors test error displayed function number trees colored line corresponds diﬀerent value number predictors available splitting interior tree node random forests lead slight improvement bagging single classiﬁcation tree error rate ﬁtting small trees residuals slowly improve areas perform well shrinkage parameter slows process even allowing diﬀerent shaped trees attack resid uals general statistical learning approaches learn slowly tend perform well note boosting unlike bagging construction tree depends strongly trees already grown described process boosting regression trees boosting classiﬁcation trees proceeds similar slightly complex way details omitted boosting three tuning parameters number trees unlike bagging random forests boosting overﬁt large although overﬁtting tends occur slowly use cross validation select shrinkage parameter small positive number controls rate boosting learns typical values right choice depend problem small require using large value order achieve good performance number splits tree controls complexity boosted ensemble often works well case tree stump consisting single split case boosted stump ensemble ﬁtting additive model since term involves single variable generally interaction depth controls interaction depth lab decision trees algorithm boosting regression trees set training set repeat fit tree splits terminal nodes training data update adding shrunken version new tree update residuals output boosted model interaction order boosted model since splits involve variables figure applied boosting class cancer gene expression data set order develop classiﬁer distinguish normal class cancer classes display test error function total number trees interaction depth see simple stumps interaction depth one perform well enough included model outperforms depth two model perform random forest highlights one diﬀerence boosting random forests boosting growth particular tree takes account trees already grown smaller trees typically suﬃcient using smaller trees aid interpretability well instance using stumps leads additive model 
[tree-based, methods, lab, decision, trees, fitting, classiﬁcation, trees] tree library used construct classiﬁcation regression trees tree based methods number trees classification error boosting depth boosting depth randomforest figure results performing boosting random forests class gene expression data set order predict cancer versus normal test error displayed function number trees two boosted models depth trees slightly outperform depth trees perform random forest although standard errors around making none diﬀerences signiﬁcant test error rate single tree ﬁrst use classiﬁcation trees analyze carseats data set data sales continuous variable begin recoding binary variable use ifelse function create variable called ifelse high takes value yes sales variable exceeds takes value otherwise sales yes finally use data frame function merge high rest carseats data use tree function classiﬁcation tree order predict tree high using variables sales syntax tree function quite similar function sales carseats summary function lists variables used internal nodes tree number terminal nodes training error rate sales data carseats lab decision trees age see training error rate classiﬁcation trees viance reported output summary given log number observations terminal node belong class small deviance indicates tree provides good training data residual mean deviance reported simply deviance divided case one attractive properties trees graphically displayed use plot function display tree struc ture text function display node labels argument pretty instructs include category names qualitative pre dictors rather simply displaying letter category important indicator sales appears shelving location since ﬁrst branch diﬀerentiates good locations bad medium locations type name tree object prints output corresponding branch tree displays split criterion pricelt number observations branch deviance overall prediction branch yes fraction observations branch take values yes branches lead terminal nodes indicated using asterisks split yval bad yes order properly evaluate performance classiﬁcation tree data must estimate test error rather simply computing training error split observations training set test set build tree using training set evaluate performance test data predict function used purpose case classiﬁcation tree argument type class instructs return actual class prediction approach leads correct predictions around locations test data set tree based methods set train sales carseats subset train test pred yes yes next consider whether pruning tree might lead improved results function tree performs cross validation order tree determine optimal level tree complexity cost complexity pruning used order select sequence trees consideration use argument fun prune misclass order indicate want classiﬁcation error rate guide cross validation pruning process rather default tree function deviance tree function reports number terminal nodes tree con sidered size well corresponding error rate value cost complexity parameter used corresponds set fun dev inf note despite name dev corresponds cross validation error rate instance tree terminal nodes results lowest cross validation error rate cross validation errors plot error rate function size par lab decision trees apply prune misclass function order prune tree prune misclass obtain nine node tree well pruned tree perform test data set apply predict function test pred yes yes test observations correctly classiﬁed pruning process produced interpretable tree also improved classiﬁcation accuracy increase value best obtain larger pruned tree lower classiﬁcation accuracy test pred yes yes 
[tree-based, methods, lab, decision, trees, fitting, regression, trees] regression tree boston data set first create training set tree training data set boston subset train data boston subset train tree based methods dis min max notice output summary indicates three vari ables used constructing tree context regression tree deviance simply sum squared errors tree plot tree boston variable lstat measures percentage individuals lower socioeconomic status tree indicates lower values lstat cor respond expensive houses tree predicts median house price larger homes suburbs residents high socioe conomic status rmgt lstatlt use tree function see whether pruning tree improve performance case complex tree selected cross validation ever wish prune tree could follows using prune tree function prune tree boston boston keeping cross validation results use unpruned tree make predictions test set boston train train yhat yhat words test set mse associated regression tree square root mse therefore around indicating model leads test predictions within around true median home value suburb 
[tree-based, methods, lab, decision, trees, bagging, random, forests] apply bagging random forests boston data using randomforest package exact results obtained section may lab decision trees depend version version randomforest package installed computer recall bagging simply special case random forest therefore randomforest function random forest used perform random forests bagging perform bagging follows set bag data boston subset train bag data boston mtry true var argument mtry indicates predictors considered split tree words bagging done well bagged model perform test set bag bag boston train bag bag test set mse associated bagged regression tree almost half obtained using optimally pruned single tree could change number trees grown randomforest using ntree argument bag data boston subset train bag bag boston train bag growing random forest proceeds exactly way except use smaller value mtry argument default randomforest uses variables building random forest regression trees variables building random forest classiﬁcation trees use mtry set data boston subset train boston train tree based methods test set mse indicates random forests yielded improvement bagging case using importance function view importance importance variable nox age dis rad tax two measures variable importance reported former based upon mean decrease accuracy predictions bag samples given variable excluded model latter measure total decrease node impurity results splits variable averaged trees plotted figure case regression trees node impurity measured training rss classiﬁcation trees deviance plots importance measures produced using varimpplot function varimpplot results indicate across trees considered random forest wealth level community lstat house size far two important variables 
[tree-based, methods, lab, decision, trees, boosting] use gbm package within gbm function boosted gbm regression trees boston data set run gbm option distribution gaussian since regression problem nary classiﬁcation problem would use distribution bernoulli argument trees indicates want trees option interaction depth limits depth tree gbm set gbm data boston train distribution summary function produces relative inﬂuence plot also outputs relative inﬂuence statistics lab decision trees var rel inf dis nox age tax rad see lstat far important variables also produce partial dependence plots two variables plots partial dependence plot illustrate marginal eﬀect selected variables response integrating variables case might expect median house prices increasing decreasing lstat par boston boston use boosted model predict medv test set boston train boost test mse obtained similar test mse random forests superior bagging want perform boosting diﬀerent value shrinkage parameter default value easily modiﬁed take gbm data boston train distribution boston train boost case using leads slightly lower test mse tree based methods 
[tree-based, methods, exercises, conceptual] draw example invention partition two dimensional feature space could result recursive binary splitting example contain least six regions draw decision tree corresponding partition sure label pects ﬁgures including regions cutpoints forth hint result look something like figures mentioned section boosting using depth one trees stumps leads additive model model form explain case begin algorithm simple classiﬁcation setting two classes create single plot displays quantities function axis display ranging axis display value gini index classiﬁcation error entropy hint setting two classes could make plot hand much easier make question relates plots figure sketch tree corresponding partition predictor space illustrated left hand panel figure num bers inside boxes indicate mean within region create diagram similar left hand panel figure using tree illustrated right hand panel ﬁgure divide predictor space correct regions indicate mean region suppose produce ten bootstrapped samples data set containing red green classes apply classiﬁcation tree bootstrapped sample speciﬁc value produce estimates class red consider gini index classiﬁcation error entropy exercises figure left partition predictor space corresponding exer cise right tree corresponding exercise two common ways combine results together single class prediction one majority vote approach discussed chapter second approach classify based average probability example ﬁnal classiﬁcation two approaches provide detailed explanation algorithm used regression tree 
[tree-based, methods, exercises, applied] lab applied random forests boston data using mtry using ntree ntree create plot displaying test error resulting random forests data set com prehensive range values mtry ntree model plot figure describe results obtained lab classiﬁcation tree applied carseats data set ter converting sales qualitative response variable seek predict sales using regression trees related approaches treating response quantitative variable split data set training set test set fit regression tree training set plot tree inter pret results test mse obtain use cross validation order determine optimal level tree complexity pruning tree improve test mse use bagging approach order analyze data test mse obtain use importance function termine variables important tree based methods use random forests analyze data test mse obtain use importance function determine vari ables important describe eﬀect number variables considered split error rate obtained problem involves data set part islr package create training set containing random sample obser vations test set containing remaining observations fit tree training data purchase response variables predictors use summary function produce summary statistics tree describe results obtained training error rate many terminal nodes tree type name tree object order get detailed text output pick one terminal nodes interpret information displayed create plot tree interpret results predict response test data produce confusion matrix comparing test labels predicted test labels test error rate apply tree function training set order determine optimal tree size produce plot tree size axis cross validated classiﬁcation error rate axis tree size corresponds lowest cross validated classi ﬁcation error rate produce pruned tree corresponding optimal tree size obtained using cross validation cross validation lead selection pruned tree create pruned tree ﬁve terminal nodes compare training error rates pruned pruned trees higher compare test error rates pruned unpruned trees higher use boosting predict salary hitters data set remove observations salary information unknown log transform salaries exercises create training set consisting ﬁrst observations test set consisting remaining observations perform boosting training set trees range values shrinkage parameter produce plot diﬀerent shrinkage values axis corresponding training set mse axis produce plot diﬀerent shrinkage values axis corresponding test set mse axis compare test mse boosting test mse results applying two regression approaches seen chapters variables appear important predictors boosted model apply bagging training set test set mse approach question uses caravan data set create training set consisting ﬁrst observations test set consisting remaining observations fit boosting model training set purchase response variables predictors use trees shrinkage value predictors appear important use boosting model predict response test data predict person make purchase estimated prob ability purchase greater form confusion trix fraction people predicted make purchase fact make one compare results obtained applying knn logistic regression data set apply boosting bagging random forests data set choice sure models training set evaluate performance test set accurate results compared simple methods like linear logistic regression approaches yields best performance 
[support, vector, machines] chapter discuss support vector machine svm approach classiﬁcation developed computer science community grown popularity since svms shown perform well variety settings often considered one best box classiﬁers support vector machine generalization simple intu itive classiﬁer called maximal margin classiﬁer introduce section though elegant simple see classiﬁer unfortunately cannot applied data sets since requires classes separable linear boundary section introduce support vector classiﬁer extension maximal margin classiﬁer applied broader range cases section introduces support vector machine extension support vec tor classiﬁer order accommodate non linear class boundaries support vector machines intended binary classiﬁcation setting two classes section discuss extensions support vector machines case two classes section discuss close connections support vector machines statistical methods logistic regression people often loosely refer maximal margin classiﬁer support vector classiﬁer support vector machine support vector machines avoid confusion carefully distinguish three notions chapter james introduction statistical learning applications springer texts statistics doi springer science business media new york support vector machines 
[support, vector, machines, maximal, margin, classifier] section deﬁne hyperplane introduce concept optimal separating hyperplane 
[support, vector, machines, maximal, margin, classifier, hyperplane] dimensional space hyperplane ﬂat aﬃne subspace hyperplane dimension instance two dimensions hyperplane ﬂat one dimensional subspace words line three dimensions hyperplane ﬂat two dimensional subspace plane dimensions hard visualize hyperplane notion dimensional ﬂat subspace still applies mathematical deﬁnition hyperplane quite simple two mensions hyperplane deﬁned equation parameters say deﬁnes hyper plane mean holds point hyperplane note simply equation line since indeed two dimensions hyperplane line equation easily extended dimensional setting deﬁnes dimensional hyperplane sense point dimensional space vector length satisﬁes lies hyperplane suppose satisfy rather tells lies one side hyperplane hand lies side hyperplane think hyperplane dividing dimensional space two halves one easily determine side hyperplane point lies simply calculating sign left hand side hyperplane two dimensional space shown figure word aﬃne indicates subspace need pass origin maximal margin classiﬁer figure hyperplane shown blue region set points purple region set points 
[support, vector, machines, maximal, margin, classifier, classiﬁcation, using, separating, hyperplane] suppose data matrix consists training observations dimensional space observations fall two classes represents one class class also test observation vector observed features goal develop classiﬁer based training data correctly classify test observation using feature measurements seen number approaches task linear discriminant analysis logistic regression chapter classiﬁcation trees bagging boosting chapter see new approach based upon concept separating hyperplane separating hyperplane suppose possible construct hyperplane separates training observations perfectly according class labels examples three separating hyperplanes shown left hand panel figure label observations blue class support vector machines figure left two classes observations shown blue purple measurements two variables three separating hyperplanes many possible shown black right separating perplane shown black blue purple grid indicates decision rule made classiﬁer based separating hyperplane test observation falls blue portion grid assigned blue class test observation falls purple portion grid assigned purple class purple class separating hyperplane property equivalently separating hyperplane property separating hyperplane exists use construct natural classiﬁer test observation assigned class depending side hyperplane located right hand panel figure shows example classiﬁer classify test observation based sign positive assign test observation class negative assign class also make use magnitude far zero means lies far hyperplane conﬁdent class assignment maximal margin classiﬁer hand close zero located near hyperplane less certain class assignment surprisingly see figure classiﬁer based separating hyperplane leads linear decision boundary 
[support, vector, machines, maximal, margin, classifier, maximal, margin, classiﬁer] general data perfectly separated using hyperplane fact exist inﬁnite number hyperplanes given separating hyperplane usually shifted tiny bit rotated without coming contact observations three possible separating hyperplanes shown left hand panel figure order construct classiﬁer based upon separating hyperplane must reasonable way decide inﬁnite possible separating hyperplanes use natural choice maximal margin hyperplane also known maximal margin hyperplane optimal separating hyperplane separating hyperplane optimal separating hyperplane farthest training observations compute perpendicular distance training observation given separat ing hyperplane smallest distance minimal distance observations hyperplane known margin maximal margin margin hyperplane separating hyperplane margin largest hyperplane farthest minimum dis tance training observations classify test observation based side maximal margin hyperplane lies known maximal margin classiﬁer hope classiﬁer large maximal margin classiﬁer margin training data also large margin test data hence classify test observations correctly although maxi mal margin classiﬁer often successful also lead overﬁtting large coeﬃcients maximal margin hyperplane maximal margin classiﬁer classiﬁes test observation based sign figure shows maximal margin hyperplane data set figure comparing right hand panel figure figure see maximal margin hyperplane shown figure deed result greater minimal distance observations separating hyperplane larger margin sense maximal margin hyperplane represents mid line widest slab insert two classes examining figure see three training observations equidis tant maximal margin hyperplane lie along dashed lines indicating width margin three observations known support vector machines figure two classes observations shown blue pur ple maximal margin hyperplane shown solid line margin distance solid line either dashed lines two blue points purple point lie dashed lines support vectors dis tance points hyperplane indicated arrows purple blue grid indicates decision rule made classiﬁer based separating hyperplane support vectors since vectors dimensional space figure support vector support maximal margin hyperplane sense points moved slightly maximal margin hyper plane would move well interestingly maximal margin hyperplane depends directly support vectors observations movement observations would aﬀect separating hyperplane provided observation movement cause cross boundary set margin fact maximal margin hyperplane depends directly small subset observations important property arise later chapter discuss support vector classiﬁer support vector machines 
[support, vector, machines, maximal, margin, classifier, construction, maximal, margin, classiﬁer] consider task constructing maximal margin hyperplane based set training observations associated class labels brieﬂy maximal margin hyperplane solution optimization problem maximal margin classiﬁer subject optimization problem actually simpler looks first constraint guarantees observation correct side hyper plane provided positive actually observation correct side hyperplane would simply need constraint fact requires observation correct side hyperplane cushion provided positive second note really constraint hyperplane since deﬁnes hyperplane however adds meaning one show constraint perpendicular distance observation hyperplane given therefore constraints ensure observation correct side hyperplane least distance hyperplane hence represents margin hyperplane optimization problem chooses maximize exactly deﬁnition maximal margin hyperplane problem solved eﬃciently details optimization outside scope book 
[support, vector, machines, maximal, margin, classifier, non-separable, case] maximal margin classiﬁer natural way perform classiﬁcation separating hyperplane exists however hinted many cases separating hyperplane exists maximal margin classiﬁer case optimization problem solution example shown figure case cannot exactly separate two classes however see next section extend concept separating hyperplane order develop hyperplane almost separates classes using called soft margin generalization maximal margin classiﬁer non separable case known support vector classiﬁer maximize support vector machines figure two classes observations shown blue pur ple case two classes separable hyperplane maximal margin classiﬁer cannot used 
[support, vector, machines, support, vector, classifiers, overview, support, vector, classiﬁer] figure see observations belong two classes necessarily separable hyperplane fact even separating hyper plane exist instances classiﬁer based separating hyperplane might desirable classiﬁer based separating hyperplane necessarily perfectly classify training observations lead sensitivity individual observations ample shown figure addition single observation right hand panel figure leads dramatic change maxi mal margin hyperplane resulting maximal margin hyperplane satisfactory one thing tiny margin problematic discussed previously distance observation hyperplane seen measure conﬁdence obser vation correctly classiﬁed moreover fact maximal mar gin hyperplane extremely sensitive change single observation suggests may overﬁt training data case might willing consider classiﬁer based perplane perfectly separate two classes interest support vector classiﬁers figure left two classes observations shown blue purple along maximal margin hyperplane right additional blue observation added leading dramatic shift maximal margin hyperplane shown solid line dashed line indicates maximal margin hyperplane obtained absence additional point greater robustness individual observations better classiﬁcation training observations could worthwhile misclassify training observations order better job classifying remaining observations support vector classiﬁer sometimes called soft margin classiﬁer support vector classiﬁer soft margin classiﬁer exactly rather seeking largest possible margin every observation correct side hyperplane also correct side margin instead allow observations incorrect side margin even incorrect side hyperplane margin soft violated training observations example shown left hand panel figure observations correct side margin however small subset observations wrong side margin observation wrong side margin also wrong side hyperplane fact separating hyperplane situation inevitable observations wrong side hyperplane correspond training observations misclassiﬁed support vector classiﬁer right hand panel figure illustrates scenario 
[support, vector, machines, support, vector, classifiers, details, support, vector, classiﬁer] support vector classiﬁer classiﬁes test observation depending side hyperplane lies hyperplane chosen correctly support vector machines figure left support vector classiﬁer small data set hyperplane shown solid line margins shown dashed lines purple observations observations correct side margin observation margin observation wrong side margin blue observations observations correct side margin observation margin observation wrong side margin observations wrong side hyperplane right left panel two additional points two observations wrong side hyperplane wrong side margin separate training observations two classes may misclassify observations solution optimization problem subject nonnegative tuning parameter width margin seek make quantity large possible slack variables allow individual observations slack variable wrong side margin hyperplane explain greater detail momentarily solved classify test observation simply determining side hyperplane lies classify test observation based sign problem seems complex insight behavior made series simple observations presented first slack variable tells observation located relative hyperplane relative margin maximize support vector classiﬁers observation correct side margin saw section observation wrong side margin say observation violated margin wrong side hyperplane consider role tuning parameter bounds sum determines number severity vio lations margin hyperplane tolerate think budget amount margin violated observations budget violations margin must case case simply amounts maximal margin hyperplane optimiza tion problem course maximal margin hyperplane exists two classes separable observa tions wrong side hyperplane observation wrong side hyperplane requires budget increases become tolerant violations margin margin widen conversely decreases become less tolerant violations margin margin narrows example shown figure practice treated tuning parameter generally chosen via cross validation tuning parameters seen book controls bias variance trade statistical learn ing technique small seek narrow margins rarely violated amounts classiﬁer highly data may low bias high variance hand larger margin wider allow violations amounts ﬁtting data less hard obtaining classiﬁer potentially biased may lower variance optimization problem interesting property turns observations either lie margin violate margin aﬀect hyperplane hence classiﬁer tained words observation lies strictly correct side margin aﬀect support vector classiﬁer changing position observation would change classiﬁer provided position remains correct side margin observations lie directly margin wrong side margin class known support vectors observations aﬀect support vector classiﬁer fact support vectors aﬀect classiﬁer line previous assertion controls bias variance trade support vector classiﬁer tuning parameter large margin wide many observations violate margin many support vectors case many observations involved determining hyperplane top left panel figure illustrates setting classiﬁer low variance since many observations support vectors support vector machines figure support vector classiﬁer using four diﬀerent values tuning parameter largest value used top left panel smaller values used top right bottom left bottom right panels large high tolerance observations wrong side margin margin large decreases tolerance observations wrong side margin decreases margin narrows potentially high bias contrast small fewer support vectors hence resulting classiﬁer low bias high variance bottom right panel figure illustrates setting eight support vectors fact support vector classiﬁer decision rule based potentially small subset training observations support vec tors means quite robust behavior observations far away hyperplane property distinct classiﬁcation methods seen preceding chapters linear discriminant analysis recall lda classiﬁcation rule support vector machines figure left observations fall two classes non linear boundary right support vector classiﬁer seeks linear bound ary consequently performs poorly depends mean observations within class well within class covariance matrix computed using observations contrast logistic regression unlike lda low sensitivity servations far decision boundary fact see section support vector classiﬁer logistic regression closely related 
[support, vector, machines, support, vector, machines] ﬁrst discuss general mechanism converting linear classiﬁer one produces non linear decision boundaries introduce support vector machine automatic way 
[support, vector, machines, support, vector, machines, classiﬁcation, non-linear, decision, boundaries] support vector classiﬁer natural approach classiﬁcation two class setting boundary two classes linear ever practice sometimes faced non linear class boundaries instance consider data left hand panel figure clear support vector classiﬁer linear classiﬁer perform poorly indeed support vector classiﬁer shown right hand panel figure useless chapter faced analogous situation see performance linear regression suﬀer non linear relationship predictors outcome case consider enlarging feature space using functions predictors support vector machines quadratic cubic terms order address non linearity case support vector classiﬁer could address prob lem possibly non linear boundaries classes similar way enlarging feature space using quadratic cubic even higher order polynomial functions predictors instance rather ﬁtting support vector classiﬁer using features could instead support vector classiﬁer using features would become subject lead non linear decision boundary enlarged feature space decision boundary results fact lin ear original feature space decision boundary form quadratic polynomial solutions gener ally non linear one might additionally want enlarge feature space higher order polynomial terms interaction terms form alternatively functions predictors could considered rather polynomials hard see many possible ways enlarge feature space unless careful could end huge number features compu tations would become unmanageable support vector machine present next allows enlarge feature space used support vector classiﬁer way leads eﬃcient computations 
[support, vector, machines, support, vector, machines, support, vector, machine] support vector machine svm extension support vector support vector machine classiﬁer results enlarging feature space speciﬁc way using kernels discuss extension details kernel somewhat complex beyond scope book however main idea described section may want enlarge feature space maximize support vector machines order accommodate non linear boundary classes kernel approach describe simply eﬃcient computational approach enacting idea discussed exactly support vector classiﬁer com puted details become somewhat technical however turns solution support vector classiﬁer problem involves inner products observations opposed observations inner product two vectors deﬁned thus inner product two observations given shown linear support vector classiﬁer represented parameters one per training observation estimate parameters need inner products pairs training observations notation means gives number pairs among set items notice order evaluate function need compute inner product new point training points however turns nonzero support vectors solution training observation support vector equals zero collection indices support points rewrite solution function form typically involves far fewer terms summarize representing linear classiﬁer computing coeﬃcients need inner products suppose every time inner product appears representation calculation solution support expanding inner products easy see linear function coordinates also establishes correspondence original parameters support vector machines vector classiﬁer replace generalization inner product form function refer kernel kernel kernel function quantiﬁes similarity two observations instance could simply take would give back support vector classiﬁer equation known linear kernel support vector classiﬁer linear features linear kernel essentially quantiﬁes similarity pair observations using pearson standard correlation one could instead choose another form instance one could replace every instance quantity known polynomial kernel degree positive polynomial kernel integer using kernel instead standard linear kernel support vector classiﬁer algorithm leads much ﬂexible decision boundary essentially amounts ﬁtting support vector classiﬁer higher dimensional space involving polynomials degree rather original feature space support vector classiﬁer combined non linear kernel resulting classiﬁer known support vector machine note case non linear function form left hand panel figure shows example svm polynomial kernel applied non linear data figure substantial improvement linear support vector classiﬁer svm reduces support vector classiﬁer seen earlier chapter polynomial kernel shown one example possible non linear kernel alternatives abound another popular choice radial kernel takes form radial kernel exp support vector machines figure left svm polynomial kernel degree applied non linear data figure resulting far appropriate decision rule right svm radial kernel applied example either kernel capable capturing decision boundary positive constant right hand panel figure shows example svm radial kernel non linear data also good job separating two classes radial kernel actually work given test obser vation far training observation terms euclidean distance large exp tiny means play virtually role recall predicted class label test observation based sign words training observations far play essentially role predicted class label means radial kernel local behavior sense nearby training observations eﬀect class label test observation advantage using kernel rather simply enlarging feature space using functions original features one advantage computational amounts fact using kernels one need compute distinct pairs done without explicitly working enlarged feature space portant many applications svms enlarged feature space large computations intractable kernels radial kernel feature space implicit inﬁnite dimensional could never computations anyway support vector machines false positive rate rue positiv ate support vector classifier lda false positive rate rue positiv ate support vector classifier svm svm svm figure roc curves heart data training set left support vector classiﬁer lda compared right support vector classiﬁer compared svm using radial basis kernel 
[support, vector, machines, support, vector, machines, application, heart, disease, data] chapter apply decision trees related methods heart data aim use predictors age sex chol order predict whether individual heart disease investigate svm compares lda data removing missing observations data consist subjects randomly split training test observations ﬁrst lda support vector classiﬁer training data note support vector classiﬁer equivalent svm using poly nomial kernel degree left hand panel figure displays roc curves described section training set predictions lda support vector classiﬁer classiﬁers compute scores form observation given cutoﬀ classify observations heart disease heart disease categories depending whether roc curve obtained forming predictions computing false positive true positive rates range values opti mal classiﬁer hug top left corner roc plot instance lda support vector classiﬁer perform well though suggestion support vector classiﬁer may slightly superior right hand panel figure displays roc curves svms using radial kernel various values increases becomes non linear roc curves improve using appears give almost perfect roc curve however curves represent training error rates misleading terms performance new test data figure displays roc curves computed test observa svms two classes false positive rate rue positiv support vector classifier lda false positive rate rue positiv support vector classifier svm svm svm figure roc curves test set heart data left support vector classiﬁer lda compared right support vector classiﬁer compared svm using radial basis kernel tions observe diﬀerences training roc curves left hand panel figure support vector classiﬁer appears small advantage lda although diﬀerences statisti cally signiﬁcant right hand panel svm using showed best results training data produces worst estimates test data evidence ﬂexible method often produce lower training error rates neces sarily lead improved performance test data svms perform comparably support vector classiﬁer three outperform svm 
[support, vector, machines, svms, two, classes] far discussion limited case binary classiﬁcation classiﬁcation two class setting extend svms general case arbitrary number classes turns concept separating hyperplanes upon svms based lend naturally two classes though number proposals extending svms class case made two popular one versus one one versus approaches brieﬂy discuss two approaches 
[support, vector, machines, svms, two, classes, one-versus-one, classiﬁcation] suppose would like perform classiﬁcation using svms classes one versus one pairs approach constructs one versus one support vector machines svms compares pair classes example one svm might compare class coded class coded classify test observation using classiﬁers tally number times test observation assigned classes ﬁnal classiﬁcation performed assigning test observation class frequently assigned pairwise classiﬁcations 
[support, vector, machines, svms, two, classes, one-versus-all, classiﬁcation] one versus approach alternative procedure applying svms one versus case classes svms time comparing one classes remaining classes let denote parameters result ﬁtting svm comparing class coded others coded let denote test observation assign observation class largest amounts high level conﬁdence test observation belongs class rather classes 
[support, vector, machines, relationship, logistic, regression] svms ﬁrst introduced mid made quite splash statistical machine learning communities due part good performance good marketing also fact underlying approach seemed novel mysterious idea ﬁnding hyperplane separates data well possible lowing violations separation seemed distinctly diﬀerent classical approaches classiﬁcation logistic regression lin ear discriminant analysis moreover idea using kernel expand feature space order accommodate non linear class boundaries peared unique valuable characteristic however since time deep connections svms classical statistical methods emerged turns one rewrite criterion ﬁtting support vector classiﬁer minimize max relationship logistic regression nonnegative tuning parameter large small violations margin tolerated low variance high bias classiﬁer result small violations margin occur amounts high variance low bias classiﬁer thus small value amounts small value note term ridge penalty term section plays similar role controlling bias variance trade support vector classiﬁer takes loss penalty form seen repeatedly throughout book minimize loss function quantifying extent model parametrized ﬁts data penalty function parameter vector whose eﬀect controlled nonneg ative tuning parameter instance ridge regression lasso take form ridge regression lasso case loss function instead takes form max known hinge loss depicted figure however hinge loss turns hinge loss function closely related loss function used logistic regression also shown figure interesting characteristic support vector classiﬁer support vectors play role classiﬁer obtained observations correct side margin aﬀect due fact loss function shown figure exactly zero observations correspond observations correct side margin contrast loss function logistic regression shown figure exactly zero anywhere small observations far decision boundary due similarities loss functions logistic regression support vector classiﬁer often give similar results classes well separated svms tend behave better logistic regression overlapping regimes logistic regression often preferred hinge loss penalty representation margin corresponds value one width margin determined support vector machines loss svm loss logistic regression loss figure svm logistic regression loss functions compared function greater svm loss zero since corresponds observation correct side margin overall two loss functions quite similar behavior support vector classiﬁer svm ﬁrst introduced thought tuning parameter unimportant nui sance parameter could set default value like however loss penalty formulation support vector classiﬁer indicates case choice tuning parameter important determines extent model underﬁts ﬁts data illustrated example figure established support vector classiﬁer closely related logistic regression preexisting statistical methods svm unique use kernels enlarge feature space accommodate non linear class boundaries answer question could well perform logistic regression many classiﬁcation methods seen book using non linear kernels closely related non linear approaches seen chapter however torical reasons use non linear kernels much widespread context svms context logistic regression methods though addressed fact extension svm regression quantitative rather qualita tive response called support vector regression chapter saw support vector regression least squares regression seeks coeﬃcients sum squared residuals small possible recall chapter residuals deﬁned support vector regression instead seeks coeﬃcients minimize diﬀerent type loss residuals larger absolute value positive constant lab support vector machines contribute loss function extension margin used support vector classiﬁers regression setting 
[support, vector, machines, lab, support, vector, machines] use library demonstrate support vector classiﬁer svm another option liblinear library useful large linear problems 
[support, vector, machines, lab, support, vector, machines, support, vector, classiﬁer] library contains implementations number statistical learning methods particular svm function used svm support vector classiﬁer argument kernel linear used function uses slightly diﬀerent formulation support vector classiﬁer cost argument allows specify cost violation margin cost argument small mar gins wide many support vectors margin violate margin cost argument large margins narrow support vectors margin violating margin use svm function support vector classiﬁer given value cost parameter demonstrate use function two dimensional example plot resulting decision boundary begin generating observations belong two classes checking whether classes linearly separable set rep rep col next support vector classiﬁer note order svm function perform classiﬁcation opposed svm based regression must encode response factor variable create data frame response coded factor dat svm data dat kernel linear cost support vector machines argument scale false tells svm function scale feature mean zero standard deviation one depending application one might prefer use scale true plot support vector classiﬁer obtained svmfit dat note two arguments plot svm function output call svm well data used call svm region feature space assigned class shown light blue region assigned class shown purple decision boundary two classes linear used argument kernel linear though due way plotting function implemented library decision boundary looks obtain basic information support vector classiﬁer using summary command svm data dat kernel linear cost svm svm tells instance linear kernel used cost seven support vectors four one class three instead used smaller value cost parameter svm data dat kernel linear cost svmfit dat usual plot function support vectors plotted crosses remaining observations plotted circles see seven support vectors determine identities follows somewhat jagged plot note second feature plotted axis ﬁrst feature plotted axis contrast behavior lab support vector machines smaller value cost parameter used obtain larger number support vectors margin wider unfor tunately svm function explicitly output coeﬃcients linear decision boundary obtained support vector classiﬁer output width margin library includes built function tune perform cross tune validation default tune performs ten fold cross validation set models interest order use function pass relevant information set models consideration following command indicates want compare svms linear kernel using range values cost parameter set svm data dat kernel linear easily access cross validation errors models using summary command svm see cost results lowest cross validation error rate tune function stores best model obtained accessed follows predict function used predict class label set test observations given value cost parameter begin generating test data set rep xtest predict class labels test observations use best model obtained cross validation order make predictions support vector machines bestmod ypred thus value cost test observations correctly classiﬁed instead used cost svm data dat kernel linear cost svmfit ypred case one additional observation misclassiﬁed consider situation two classes linearly separable ﬁnd separating hyperplane using svm function ﬁrst separate two classes simulated data linearly separable col pch observations barely linearly separable support vector classiﬁer plot resulting hyperplane using large value cost observations misclassiﬁed dat svm data dat kernel linear cost svm data dat kernel linear cost svm svm svmfit dat training errors made three support vectors used however see ﬁgure margin narrow observations support vectors indicated circles lab support vector machines close decision boundary seems likely model perform poorly test data try smaller value cost svm data dat kernel linear cost svmfit dat using cost misclassify training observation also obtain much wider margin make use seven support vectors seems likely model perform better test data model cost 
[support, vector, machines, lab, support, vector, machines, support, vector, machine] order svm using non linear kernel use svm function however use diﬀerent value parameter kernel svm polynomial kernel use kernel polynomial svm radial kernel use kernel radial former case also use degree argument specify degree polynomial kernel latter case use gamma specify value radial basis kernel ﬁrst generate data non linear class boundary follows set rep rep dat plotting data makes clear class boundary indeed non linear col data randomly split training testing groups training data using svm function radial kernel svm data dat train kernel radial svmfit dat train plot shows resulting svm decidedly non linear boundary summary function used obtain information svm svm data dat kernel radial svm support vector machines svm see ﬁgure fair number training errors svm increase value cost reduce number training errors however comes price irregular decision boundary seems risk overﬁtting data svm data dat train kernel radial gamma svmfit dat train perform cross validation using tune select best choice cost svm radial kernel set svm data dat train kernel radial svm therefore best choice parameters involves cost gamma view test set predictions model applying predict function data notice subset dataframe dat using train index set dat train model dat train test observations misclassiﬁed svm lab support vector machines 
[support, vector, machines, lab, support, vector, machines, roc, curves] rocr package used produce roc curves figures ﬁrst write short function plot roc curve given vector containing numerical score observation pred vector containing class label observation truth pred truth pred predob tpr fpr perf svms support vector classiﬁers output class labels observa tion however also possible obtain ﬁtted values observation numerical scores used obtain class labels instance case support vector classiﬁer ﬁtted value observation takes form svm non linear kernel equation yields ﬁtted value given essence sign ﬁtted value determines side decision boundary observation lies therefore relationship ﬁtted value class prediction given observation simple ﬁtted value exceeds zero observation assigned one class less zero assigned order obtain ﬁtted values given svm model use decision values true ﬁtting svm predict function output ﬁtted values opt svm data dat train kernel radial opt dat train produce roc plot par fitted dat train svm appears producing accurate predictions increasing produce ﬂexible generate improvements accuracy svm data dat train kernel radial flex dat train fitted dat train add col red however roc curves training data really interested level prediction accuracy test data compute roc curves test data model appears provide accurate results support vector machines opt dat train fitted dat train flex dat train fitted dat train add col red 
[support, vector, machines, lab, support, vector, machines, svm, multiple, classes] response factor containing two levels svm function perform multi class classiﬁcation using one versus one proach explore setting generating third class obser vations set rep dat par col svm data svm data dat kernel radial cost gamma svmfit dat library also used perform support vector regression response vector passed svm numerical rather factor 
[support, vector, machines, lab, support, vector, machines, application, gene, expression, data] examine khan data set consists number tissue samples corresponding four distinct types small round blue cell mors tissue sample gene expression measurements available data set consists training data xtrain ytrain testing data xtest ytest examine dimension data dim dim lab support vector machines data set consists expression measurements genes training test sets consist observations respectively use support vector approach predict cancer subtype using gene expression measurements data set large number features relative number observations suggests use linear kernel additional ﬂexibility result using polynomial radial kernel unnecessary dat svm data dat kernel linear cost svm data dat kernel linear svm svm see training errors fact surprising large number variables relative number observations implies easy ﬁnd hyperplanes fully separate classes interested support vector classiﬁer performance training observations rather performance test observations dat dat dat support vector machines see using cost yields two test set errors data 
[support, vector, machines, exercises, conceptual] problem involves hyperplanes two dimensions sketch hyperplane indicate set points well set points plot sketch hyperplane indicate set points well set points seen dimensions linear decision boundary takes form investigate non linear decision boundary sketch curve sketch indicate set points well set points suppose classiﬁer assigns observation blue class red class otherwise class observation classiﬁed argue decision boundary linear terms linear terms explore maximal margin classiﬁer toy data set given observations dimensions observation associated class label exercises obs red red red red blue blue blue sketch observations sketch optimal separating hyperplane provide equa tion hyperplane form describe classiﬁcation rule maximal margin classiﬁer something along lines classify red classify blue otherwise provide values sketch indicate margin maximal margin hyperplane indicate support vectors maximal margin classiﬁer argue slight movement seventh observation would aﬀect maximal margin hyperplane sketch hyperplane optimal separating hyper plane provide equation hyperplane draw additional observation plot two classes longer separable hyperplane 
[support, vector, machines, exercises, applied] generate simulated two class data set observations two features visible non linear separation tween two classes show setting support vector machine polynomial kernel degree greater radial kernel outperform support vector classiﬁer train ing data technique performs best test data make plots report training test error rates order back assertions seen svm non linear kernel order perform classiﬁcation using non linear decision boundary see also obtain non linear decision boundary performing logistic regression using non linear transformations features support vector machines generate data set obser vations belong two classes quadratic decision boundary instance follows plot observations colored according class labels plot display axis axis fit logistic regression model data using predictors apply model training data order obtain pre dicted class label training observation plot servations colored according predicted class labels decision boundary linear logistic regression model data using non linear functions predictors log forth apply model training data order obtain pre dicted class label training observation plot servations colored according predicted class labels decision boundary obviously non linear repeat come example predicted class labels obviously non linear fit support vector classiﬁer data predictors obtain class prediction training observa tion plot observations colored according predicted class labels fit svm using non linear kernel data obtain class prediction training observation plot observations colored according predicted class labels comment results end section claimed case data barely linearly separable support vector classiﬁer small value cost misclassiﬁes couple training observations may perform better test data one huge value cost misclassify training observations investigate claim generate two class data way classes barely linearly separable exercises compute cross validation error rates support vector classiﬁers range cost values many training rors misclassiﬁed value cost considered relate cross validation errors obtained generate appropriate test data set compute test errors corresponding values cost considered value cost leads fewest test errors compare values cost yield fewest training errors fewest cross validation errors discuss results problem use support vector approaches order predict whether given car gets high low gas mileage based auto data set create binary variable takes cars gas mileage median cars gas mileage median fit support vector classiﬁer data various values cost order predict whether car gets high low gas mileage report cross validation errors associated dif ferent values parameter comment results repeat time using svms radial polyno mial basis kernels diﬀerent values gamma degree cost comment results make plots back assertions hint lab used plot function svm objects cases use plot function create plots displaying pairs variables time essentially instead typing svmfit dat svmfit contains ﬁtted model dat data frame containing data type svmfit dat order plot ﬁrst fourth variables however must replace correct variable names ﬁnd type plot svm problem involves data set part islr package support vector machines create training set containing random sample observations test set containing remaining observations fit support vector classiﬁer training data using cost purchase response variables predictors use summary function produce summary statistics describe results obtained training test error rates use tune function select optimal cost consider val ues range compute training test error rates using new value cost repeat parts using support vector machine radial kernel use default value gamma repeat parts using support vector machine polynomial kernel set degree overall approach seems give best results data 
[unsupervised, learning] book concerns supervised learning methods regression classiﬁcation supervised learning setting typically access set features measured obser vations response also measured observations goal predict using chapter instead focus unsupervised learning set sta tistical tools intended setting set fea tures measured observations interested prediction associated response variable rather goal discover interesting things measurements informative way visualize data discover subgroups among variables among observations unsupervised learning refers diverse set techniques answering questions chapter focus two particu lar types unsupervised learning principal components analysis tool used data visualization data pre processing supervised tech niques applied clustering broad class methods discovering unknown subgroups data 
[unsupervised, learning, challenge, unsupervised, learning] supervised learning well understood area fact read preceding chapters book good james introduction statistical learning applications springer texts statistics doi springer science business media new york unsupervised learning grasp supervised learning instance asked predict binary outcome data set well developed set tools disposal logistic regression linear discriminant analysis classiﬁcation trees support vector machines well clear understanding assess quality results obtained using cross validation validation independent test set forth contrast unsupervised learning often much challenging exercise tends subjective simple goal analysis prediction response unsupervised learning often performed part exploratory data analysis furthermore exploratory data analysis hard assess results obtained unsupervised learning methods since universally accepted mechanism performing cross validation validating results independent data set reason diﬀerence simple predictive model using supervised learning technique possible check work seeing well model predicts response observations used ﬁtting model however unsupervised learning way check work know true answer problem unsupervised techniques unsupervised learning growing importance number ﬁelds cancer researcher might assay gene expression levels patients breast cancer might look subgroups among breast cancer samples among genes order obtain better understanding disease online shopping site might try identify groups shoppers similar browsing purchase histo ries well items particular interest shoppers within group individual shopper preferentially shown items particularly likely interested based purchase histories similar shoppers search engine might choose search results display particular individual based click histories individuals similar search patterns statistical learning tasks many performed via unsupervised learning techniques 
[unsupervised, learning, principal, components, analysis] principal components discussed section context principal components regression faced large set corre lated variables principal components allow summarize set smaller number representative variables collectively explain variability original set principal component directions presented section directions feature space along original data highly variable directions also deﬁne lines subspaces close possible data cloud perform principal components analysis principal components regression simply use principal components predictors regression model place original larger set vari ables principal component analysis pca refers process prin principal component analysis cipal components computed subsequent use compo nents understanding data pca unsupervised approach since involves set features associated response apart producing derived variables use supervised learning problems pca also serves tool data visualization visualization observations visualization variables discuss pca greater detail focusing use pca tool unsupervised data exploration keeping topic chapter 
[unsupervised, learning, principal, components, analysis, principal, components] suppose wish visualize observations measurements set features part exploratory data analysis could examining two dimensional scatterplots data contains observations measurements two features however scatterplots example plots large certainly possible look moreover likely none informative since contain small fraction total information present data set clearly better method required visualize observations large particular would like ﬁnd low dimensional representation data captures much information possible instance obtain two dimensional representation data captures information plot observations low dimensional space pca provides tool ﬁnds low dimensional represen tation data set contains much possible variation idea observations lives dimensional space dimensions equally interesting pca seeks small number dimensions interesting possible concept teresting measured amount observations vary along dimension dimensions found pca linear combination features explain manner dimensions principal components found ﬁrst principal component set features normalized linear combination features largest variance normalized mean refer elements loadings ﬁrst principal loading unsupervised learning component together loadings make principal component load ing vector constrain loadings sum squares equal one since otherwise setting elements arbitrarily large absolute value could result arbitrarily large variance given data set compute ﬁrst principal com ponent since interested variance assume variables centered mean zero col umn means zero look linear combination sample feature values form largest sample variance subject constraint words ﬁrst principal component loading vector solves timization problem maximize subject since average zero well hence objective maximizing sample variance values refer scores ﬁrst princi score pal component problem solved via eigen decomposition standard technique linear algebra details outside scope book nice geometric interpretation ﬁrst principal component loading vector elements deﬁnes direction feature space along data vary project data points onto direction projected values princi pal component scores instance figure page displays ﬁrst principal component loading vector green solid line advertising data set data two features observations well ﬁrst principal component loading vector easily displayed seen data set ﬁrst principal component features deter mined ﬁnd second principal component second prin cipal component linear combination maximal variance linear combinations uncorrelated second principal component scores take form principal components analysis murder assault urbanpop rape table principal component loading vectors usarrests data also displayed figure second principal component loading vector elements turns constraining uncorrelated equivalent constraining direction orthogonal perpen dicular direction example figure observations lie two dimensional space since found one possibility shown blue dashed line section know larger data set variables multiple distinct principal components deﬁned similar manner ﬁnd solve problem similar replacing additional constraint orthogonal computed principal components plot order produce low dimensional views data instance plot score vector forth geometrically amounts projecting original data onto subspace spanned plotting projected points illustrate use pca usarrests data set states united states data set contains number arrests per residents three crimes assault murder rape also record urbanpop percent population state living urban areas principal component score vectors length principal component loading vectors length pca performed standardizing variable mean zero standard deviation one figure plots ﬁrst two principal components data ﬁgure represents principal component scores loading vectors single biplot display loadings also given biplot table figure see ﬁrst loading vector places approximately equal weight assault murder rape much less weight technical note principal component directions ordered sequence eigenvectors matrix variances compo nents eigenvalues min principal components unsupervised learning first principal component second incipal component alabama alaska arizona arkansas california colorado connecticut delaware florida georgia hawaii idaho illinois indiana iowa kansas kentucky louisiana maine maryland massachusetts michigan minnesota mississippi missouri montana nebraska nevada new hampshire new jersey new mexico new york north carolina ohio oklahoma oregon pennsylvania rhode island south carolina south dakota tennessee texas utah vermont virginia washington west virginia wisconsin wyoming rth dakota murder assault urbanpop rape figure ﬁrst two principal components usarrests data blue state names represent scores ﬁrst two principal components orange arrows indicate ﬁrst two principal component loading vectors axes top right example loading rape ﬁrst com ponent loading second principal component word rape centered point ﬁgure known biplot cause displays principal component scores principal component loadings urbanpop hence component roughly corresponds measure overall rates serious crimes second loading vector places weight urbanpop much less weight three features hence component roughly corresponds level urbanization state overall see crime related variables murder assault rape located close urbanpop variable far three indicates crime related variables corre lated states high murder rates tend high assault rape rates urbanpop variable less correlated three principal components analysis examine diﬀerences states via two principal com ponent score vectors shown figure discussion loading vectors suggests states large positive scores ﬁrst compo nent california nevada florida high crime rates states like north dakota negative scores ﬁrst component low crime rates california also high score second component indicating high level urbanization opposite true states like mississippi states close zero components indiana approximately average levels crime urbanization 
[unsupervised, learning, principal, components, analysis, another, interpretation, principal, components] ﬁrst two principal component loading vectors simulated three dimensional data set shown left hand panel figure two loading vectors span plane along observations highest variance previous section describe principal component loading vec tors directions feature space along data vary principal component scores projections along directions however alternative interpretation principal components also useful principal components provide low dimensional linear surfaces closest observations expand upon interpretation ﬁrst principal component loading vector special property line dimensional space closest observations using average squared euclidean distance measure closeness interpretation seen left hand panel figure dashed lines indicate distance observation ﬁrst principal component loading vector appeal interpretation clear seek single dimension data lies close possible data points since line likely provide good summary data notion principal components dimensions clos est observations extends beyond ﬁrst principal com ponent instance ﬁrst two principal components data set span plane closest observations terms average squared euclidean distance example shown left hand panel figure ﬁrst three principal components data set span three dimensional hyperplane closest observations forth using interpretation together ﬁrst principal component score vectors ﬁrst principal component loading vectors provide best dimensional approximation terms euclidean distance observation representation written unsupervised learning first principal component second principal component figure ninety observations simulated three dimensions left ﬁrst two principal component directions span plane best ﬁts data minimizes sum squared distances point plane right ﬁrst two principal component score vectors give coordinates projection observations onto plane variance plane maximized assuming original data matrix column centered words together principal component score vectors principal com ponent loading vectors give good approximation data suﬃciently large min representation exact 
[unsupervised, learning, principal, components, analysis, pca] scaling variables already mentioned pca performed variables centered mean zero furthermore results obtained perform pca also depend whether variables individually scaled multiplied diﬀerent constant contrast supervised unsupervised learning techniques linear regression scaling variables eﬀect linear regression multiplying variable factor simply lead multiplication corresponding coeﬃcient estimate factor thus substantive eﬀect model obtained instance figure obtained scaling variables standard deviation one reproduced left hand plot figure matter scaled variables data principal components analysis first principal component second incipal component murder assault urbanpop rape scaled first principal component second incipal component murder assau urbanpop rape unscaled figure two principal component biplots usarrests data left figure variables scaled unit standard deviations right principal components using unscaled data assault far largest loading ﬁrst principal component highest variance among four variables general scaling variables standard deviation one recommended variables measured diﬀerent units murder rape assault reported number occurrences per people urbanpop percentage state population lives urban area four variables variance respec tively consequently perform pca unscaled variables ﬁrst principal component loading vector large loading assault since variable far highest variance right hand plot figure displays ﬁrst two principal components usarrests data set without scaling variables standard devia tion one predicted ﬁrst principal component loading vector places almost weight assault second principal component loading vector places almost weight urpanpop comparing left hand plot see scaling indeed substantial eﬀect results obtained however result simply consequence scales variables measured instance assault measured units number occurrences per people rather number currences per people would amount dividing elements variable variance variable would tiny ﬁrst principal component loading vector would small value variable undesirable principal components obtained depend arbitrary choice scaling typically scale variable standard deviation one perform pca unsupervised learning certain settings however variables may measured units case might wish scale variables stan dard deviation one performing pca instance suppose variables given data set correspond expression levels genes since expression measured units gene might choose scale genes standard deviation one uniqueness principal components principal component loading vector unique sign ﬂip means two diﬀerent software packages yield principal component loading vectors although signs loading vectors may diﬀer signs may diﬀer principal component loading vector speciﬁes direction dimensional space ﬂipping sign eﬀect direction change consider figure principal component loading vector line extends either direction ﬂipping sign would eﬀect similarly score vectors unique sign ﬂip since variance variance worth noting use approximate multiply hence sign ﬂipped loading score vectors ﬁnal product two quantities unchanged proportion variance explained figure performed pca three dimensional data set left hand panel projected data onto ﬁrst two principal component loading vectors order obtain two dimensional view data principal component score vectors right hand panel see two dimensional representation three dimensional data success fully capture major pattern data orange green cyan observations near three dimensional space remain nearby two dimensional representation similarly seen usarrests data set summarize observations variables using ﬁrst two principal component score vectors ﬁrst two principal component loading vectors ask natural question much information given data set lost projecting observations onto ﬁrst principal components much variance data contained ﬁrst principal components generally interested knowing proportion variance explained pve proportion variance explained principal component total variance present data set assuming variables centered mean zero deﬁned var principal components analysis principal component prop ance explained principal component cumulative prop variance explained figure left scree plot depicting proportion variance explained four principal components usarrests data right mulative proportion variance explained four principal components usarrests data variance explained principal component therefore pve principal component given pve principal component positive quantity order compute cumulative pve ﬁrst principal components simply sum ﬁrst pves total min principal components pves sum one usarrests data ﬁrst principal component explains variance data next principal component explains variance together ﬁrst two principal components explain almost variance data last two principal components explain variance means figure provides pretty accurate summary data using two dimensions pve principal component well cumulative pve shown figure left hand panel known scree plot scree plot discussed next deciding many principal components use general data matrix min distinct principal components however usually interested rather unsupervised learning would like use ﬁrst principal components order visualize interpret data fact would like use smallest number principal components required get good understanding data many principal components needed unfortunately single simple answer question typically decide number principal components required visualize data examining scree plot one shown left hand panel figure choose smallest number principal components required order explain sizable amount variation data done eyeballing scree plot looking point proportion variance explained subsequent principal component drops often referred elbow scree plot instance inspection figure one might conclude fair amount variance explained ﬁrst two principal components elbow second component third principal component explains less ten percent variance data fourth principal component explains less half essentially worthless however type visual analysis inherently hoc unfortunately well accepted objective way decide many principal com ponents enough fact question many principal compo nents enough inherently ill deﬁned depend speciﬁc area application speciﬁc data set practice tend look ﬁrst principal components order ﬁnd interesting patterns data interesting patterns found ﬁrst principal components principal components unlikely inter est conversely ﬁrst principal components interesting typically continue look subsequent principal components interesting patterns found admittedly subjective proach reﬂective fact pca generally used tool exploratory data analysis hand compute principal components use supervised analysis principal components regression presented section simple objective way determine many principal components use treat number principal component score vectors used regression tuning parameter selected via cross validation related approach comparative simplicity selecting number principal components supervised analysis one manifestation fact supervised analyses tend clearly deﬁned objectively evaluated unsupervised analyses clustering methods 
[unsupervised, learning, principal, components, analysis, uses, principal, components] saw section perform regression using principal component score vectors features fact many statistical techniques regression classiﬁcation clustering easily adapted use matrix whose columns ﬁrst principal com ponent score vectors rather using full data matrix lead less noisy results since often case signal opposed noise data set concentrated ﬁrst principal components 
[unsupervised, learning, clustering, methods] clustering refers broad set techniques ﬁnding subgroups clustering clusters data set cluster observations data set seek partition distinct groups observations within group quite similar observations diﬀerent groups quite diﬀerent course make concrete must deﬁne means two observations similar diﬀerent indeed often domain speciﬁc consideration must made based knowledge data studied instance suppose set observations features observations could correspond tissue samples patients breast cancer features could correspond measurements collected tissue sample could clinical measurements tumor stage grade could gene expression measurements may reason believe heterogeneity among tissue samples instance perhaps diﬀerent known subtypes breast cancer clustering could used ﬁnd subgroups unsupervised problem trying dis cover structure case distinct clusters basis data set goal supervised problems hand try predict outcome vector survival time response drug treatment clustering pca seek simplify data via small number summaries mechanisms diﬀerent pca looks ﬁnd low dimensional representation observa tions explain good fraction variance clustering looks ﬁnd homogeneous subgroups among observa tions another application clustering arises marketing may cess large number measurements median household income occupation distance nearest urban area forth large unsupervised learning number people goal perform market segmentation identify ing subgroups people might receptive particular form advertising likely purchase particular product task performing market segmentation amounts clustering people data set since clustering popular many ﬁelds exist great number clustering methods section focus perhaps two best known clustering approaches means clustering hierarchical clustering means clustering hierarchical clustering means clustering seek partition observations pre speciﬁed number clusters hand hierarchical clustering know advance many clusters want fact end tree like visual representation observations called dendrogram dendrogram allows view clusterings obtained possible number clusters advantages disadvantages clustering approaches highlight chapter general cluster observations basis features order identify subgroups among observations cluster fea tures basis observations order discover subgroups among features follows simplicity discuss clustering obser vations basis features though converse performed simply transposing data matrix 
[unsupervised, learning, clustering, methods, k-means, clustering] means clustering simple elegant approach partitioning data set distinct non overlapping clusters perform means clustering must ﬁrst specify desired number clusters means algorithm assign observation exactly one clusters figure shows results obtained performing means clustering simulated example consisting observations two dimensions using three diﬀerent values means clustering procedure results simple intuitive mathematical problem begin deﬁning notation let denote sets containing indices observations cluster sets satisfy two properties words observation belongs least one clusters words clusters non overlapping observation belongs one cluster instance observation cluster idea behind means clustering good clustering one within cluster variation small possible within cluster variation clustering methods figure simulated data set observations two dimensional space panels show results applying means clustering diﬀerent val ues number clusters color observation indicates clus ter assigned using means clustering algorithm note ordering clusters cluster coloring arbitrary cluster labels used clustering instead outputs clustering procedure cluster measure amount observations within cluster diﬀer hence want solve problem minimize words formula says want partition observations clusters total within cluster variation summed clusters small possible solving seems like reasonable idea order make actionable need deﬁne within cluster variation many possible ways deﬁne concept far common choice involves squared euclidean distance deﬁne denotes number observations cluster words within cluster variation cluster sum pairwise squared euclidean distances observations cluster divided total number observations cluster combining gives optimization problem deﬁnes means clustering minimize unsupervised learning would like ﬁnd algorithm solve method partition observations clusters objective minimized fact diﬃcult problem solve precisely since almost ways partition observations clusters huge number unless tiny fortunately simple algorithm shown provide local optimum pretty good solution means optimization problem approach laid algorithm algorithm means clustering randomly assign number observations serve initial cluster assignments observations iterate cluster assignments stop changing clusters compute cluster centroid cluster centroid vector feature means observations cluster assign observation cluster whose centroid closest closest deﬁned using euclidean distance algorithm guaranteed decrease value objective step understand following identity illu minating mean feature cluster step cluster means feature constants minimize sum squared deviations step reallocating observations improve means algorithm run clustering obtained continually improve result longer changes objective never increase result longer changes local optimum reached figure shows progression algorithm toy example figure means clustering derives name fact step cluster centroids computed mean observations assigned cluster means algorithm ﬁnds local rather global opti mum results obtained depend initial random cluster signment observation step algorithm reason important run algorithm multiple times diﬀerent random clustering methods data step iteration step iteration step iteration step final results figure progress means algorithm example fig ure top left observations shown top center step algorithm observation randomly assigned cluster top right step cluster centroids computed shown large col ored disks initially centroids almost completely overlapping initial cluster assignments chosen random bottom left step observation assigned nearest centroid bottom center step performed leading new cluster centroids bottom right results obtained ten iterations initial conﬁgurations one selects best solution objective smallest figure shows local optima tained running means clustering six times using six diﬀerent initial cluster assignments using toy data figure case best clustering one objective value seen perform means clustering must decide many clusters expect data problem selecting far simple issue along practical considerations arise performing means clustering addressed section unsupervised learning figure means clustering performed six times data fig ure time diﬀerent random assignment servations step means algorithm plot value objective three diﬀerent local optima obtained one resulted smaller value objective provides better separation clusters labeled red achieved best solution objective value 
[unsupervised, learning, clustering, methods, hierarchical, clustering] one potential disadvantage means clustering requires pre specify number clusters hierarchical clustering alter native approach require commit particular choice hierarchical clustering added advantage means clustering results attractive tree based representation observations called dendrogram section describe bottom agglomerative clustering bottom agglomerative common type hierarchical clustering refers fact dendrogram generally depicted upside tree see clustering methods figure forty ﬁve observations generated two dimensional space reality three distinct classes shown separate colors however treat class labels unknown seek cluster observations order discover classes data figure built starting leaves combining clusters trunk begin discussion interpret dendrogram discuss hierarchical clustering actually performed dendrogram built interpreting dendrogram begin simulated data set shown figure consisting observations two dimensional space data generated three class model true class labels observation shown distinct colors however suppose data observed without class labels wanted perform hierarchical clustering data hierarchical clustering complete linkage discussed later yields result shown left hand panel figure interpret dendrogram left hand panel figure leaf dendrogram rep resents one observations figure however move tree leaves begin fuse branches correspond observations similar move higher tree branches fuse either leaves branches earlier lower tree fusions occur similar groups observa tions hand observations fuse later near top tree quite diﬀerent fact statement made precise two observations look point tree branches containing two observations ﬁrst fused height fusion measured vertical axis indicates unsupervised learning figure left dendrogram obtained hierarchically clustering data figure complete linkage euclidean distance center den drogram left hand panel cut height nine indicated dashed line cut results two distinct clusters shown diﬀerent colors right dendrogram left hand panel cut height ﬁve cut results three distinct clusters shown diﬀerent colors note colors used clustering simply used display purposes ﬁgure diﬀerent two observations thus observations fuse bottom tree quite similar whereas observations fuse close top tree tend quite diﬀerent highlights important point interpreting dendrograms often misunderstood consider left hand panel figure shows simple dendrogram obtained hierarchically clustering nine observations one see observations quite similar since fuse lowest point dendrogram obser vations also quite similar however tempting incorrect conclude ﬁgure observations quite similar basis located near dendrogram fact based information contained dendrogram observation similar observation observations seen right hand panel figure raw data displayed put mathe matically possible reorderings dendrogram number leaves points fusions occur positions two fused branches could swapped without aﬀecting meaning dendrogram therefore cannot draw conclusions similarity two observations based proximity along horizontal axis rather draw conclusions similarity two observations based location vertical axis branches containing two observations ﬁrst fused clustering methods figure illustration properly interpret dendrogram nine observations two dimensional space left dendrogram generated using euclidean distance complete linkage observations quite similar observations however observation similar observation observations even though obser vations close together terms horizontal distance observations fuse observation height approx imately right raw data used generate dendrogram used conﬁrm indeed observation similar observation observations understand interpret left hand panel fig ure move issue identifying clusters basis dendrogram order make horizontal cut across dendrogram shown center right hand panels figure distinct sets observations beneath cut interpreted clus ters center panel figure cutting dendrogram height nine results two clusters shown distinct colors right hand panel cutting dendrogram height ﬁve results three clusters cuts made one descends dendrogram order tain number clusters corresponding cut corresponding cut height observation cluster words height cut dendrogram serves role means clustering controls number clusters obtained figure therefore highlights attractive aspect hierarchical clustering one single dendrogram used obtain number clusters practice people often look dendrogram select eye sensible number clusters based heights fusion number clusters desired case figure one might choose select either two three clusters however often choice cut dendrogram clear unsupervised learning term hierarchical refers fact clusters obtained cutting dendrogram given height necessarily nested within clusters obtained cutting dendrogram greater height however arbitrary data set assumption hierarchical structure might unrealistic instance suppose observations correspond group people split males females evenly split among americans japanese french imagine scenario best division two groups might split people gender best division three groups might split nationality case true clusters nested sense best division three groups result taking best division two groups splitting one groups consequently situation could well represented hierarchical clustering due situations one hierarchical clustering sometimes yield worse less accurate results means clustering given number clusters hierarchical clustering algorithm hierarchical clustering dendrogram obtained via extremely simple algorithm begin deﬁning sort dissimilarity measure pair observations often euclidean distance used discuss choice dissimilarity measure later chapter algo rithm proceeds iteratively starting bottom dendrogram observations treated cluster two clusters similar fused clusters next two clusters similar fused clusters algorithm proceeds fashion observations belong one single cluster dendrogram complete figure depicts ﬁrst steps algorithm data figure summarize hierarchical clustering algorithm given algorithm algorithm seems simple enough one issue dressed consider bottom right panel figure determine cluster fused cluster concept dissimilarity pairs observations deﬁne dissimilarity two clusters one clusters contains multiple observations concept dissimilarity pair observations needs extended pair groups observations extension achieved developing notion linkage deﬁnes dissimilarity two groups observa linkage tions four common types linkage complete average single centroid brieﬂy described table average complete single linkage popular among statisticians average complete clustering methods algorithm hierarchical clustering begin observations measure euclidean dis tance pairwise dissimilarities treat observation cluster examine pairwise inter cluster dissimilarities among clusters identify pair clusters least dissimilar similar fuse two clusters dissimilarity two clusters indicates height dendro gram fusion placed compute new pairwise inter cluster dissimilarities among remaining clusters linkage description complete maximal intercluster dissimilarity compute pairwise dis similarities observations cluster observations cluster record largest dissimilarities single minimal intercluster dissimilarity compute pairwise dis similarities observations cluster observations cluster record smallest dissimilarities single linkage result extended trailing clusters single observations fused one time average mean intercluster dissimilarity compute pairwise dis similarities observations cluster observations cluster record average dissimilarities centroid dissimilarity centroid cluster mean vector length centroid cluster centroid linkage result undesirable inversions table summary four commonly used types linkage hierarchical clustering linkage generally preferred single linkage tend yield balanced dendrograms centroid linkage often used genomics suﬀers major drawback inversion occur whereby inversion two clusters fused height either individual clusters dendrogram lead diﬃculties visualization well terpretation dendrogram dissimilarities computed step hierarchical clustering algorithm depend type linkage used well choice dissimilarity measure hence resulting unsupervised learning figure illustration ﬁrst steps hierarchical clustering algorithm using data figure complete linkage euclidean distance top left initially nine distinct clusters top right two clusters closest together fused single cluster bottom left two clusters closest together fused single cluster bottom right two clus ters closest together using complete linkage cluster fused single cluster dendrogram typically depends quite strongly type linkage used shown figure choice dissimilarity measure thus far examples chapter used euclidean distance dissimilarity measure sometimes dissimilarity measures might preferred example correlation based distance considers two obser vations similar features highly correlated even though observed values may far apart terms euclidean distance clustering methods average linkage complete linkage single linkage figure average complete single linkage applied example data set average complete linkage tend yield balanced clusters unusual use correlation normally computed vari ables computed observation proﬁles pair observations figure illustrates diﬀerence euclidean correlation based distance correlation based distance focuses shapes observation proﬁles rather magnitudes choice dissimilarity measure important strong eﬀect resulting dendrogram general careful attention paid type data clustered scientiﬁc question hand considerations determine type dissimilarity measure used hierarchical clustering instance consider online retailer interested clustering shoppers based past shopping histories goal identify subgroups similar shoppers shoppers within subgroup shown items advertisements particularly likely interest sup pose data takes form matrix rows shoppers columns items available purchase elements data matrix indicate number times given shopper purchased given item shopper never purchased item shopper purchased etc type dissimilarity measure used cluster shoppers euclidean distance used shoppers bought items overall infrequent users online shopping site clustered together may desir able hand correlation based distance used shoppers similar preferences shoppers bought items unsupervised learning variable index observation observation observation figure three observations measurements variables shown observations similar values variable small euclidean distance weakly correlated large correlation based distance hand observations quite diﬀerent values variable large euclidean distance highly correlated small correlation based distance never items clustered together even shoppers preferences higher volume shoppers others therefore application correlation based distance may better choice addition carefully selecting dissimilarity measure used one must also consider whether variables scaled stan dard deviation one dissimilarity observations computed illustrate point continue online shopping example described items may purchased frequently others instance shopper might buy ten pairs socks year computer rarely high frequency purchases like socks therefore tend much larger eﬀect inter shopper dissimilarities hence clustering ultimately obtained rare purchases like computers may desirable variables scaled standard viation one inter observation dissimilarities computed variable eﬀect given equal importance hierarchical clustering performed might also want scale variables standard deviation one measured diﬀerent scales otherwise choice units centimeters versus kilometers particular vari able greatly aﬀect dissimilarity measure obtained come surprise whether good decision scale variables computing dissimilarity measure depends application hand example shown figure note issue whether scale variables performing clustering applies means clustering well clustering methods socks computers socks computers socks computers figure eclectic online retailer sells two items socks computers left number pairs socks computers purchased eight online shop pers displayed shopper shown diﬀerent color inter observation dissimilarities computed using euclidean distance raw variables number socks purchased individual drive dissimilarities tained number computers purchased little eﬀect might undesirable since computers expensive socks online retailer may interested encouraging shoppers buy computers socks large diﬀerence number socks purchased two shoppers may less informative shoppers overall shopping preferences small diﬀerence number computers purchased center data shown scaling variable standard deviation number computers purchased much greater eﬀect inter observation dissimilarities obtained right data displayed axis represents number dollars spent online shopper socks computers since computers much expensive socks computer purchase history drive inter observation dissimilarities obtained 
[unsupervised, learning, clustering, methods, practical, issues, clustering] clustering useful tool data analysis unsupervised setting however number issues arise performing clustering describe issues small decisions big consequences order perform clustering decisions must made observations features ﬁrst standardized way instance maybe variables centered mean zero scaled standard deviation one unsupervised learning case hierarchical clustering dissimilarity measure used type linkage used cut dendrogram order obtain clusters case means clustering many clusters look data decisions strong impact results obtained practice try several diﬀerent choices look one useful interpretable solution methods single right answer solution exposes interesting aspects data considered validating clusters obtained time clustering performed data set ﬁnd clusters really want know whether clusters found represent true subgroups data whether simply result clustering noise instance obtain independent set observa tions would observations also display set clusters hard question answer exist number techniques assigning value cluster order assess whether evidence cluster one would expect due chance however consensus single best approach details found hastie considerations clustering means hierarchical clustering assign observation cluster however sometimes might appropriate instance suppose observations truly belong small number unknown subgroups small subset observations quite diﬀerent observations since means hierarchical clustering force every observation cluster clusters found may heavily distorted due presence outliers belong cluster mixture models attractive approach accommodating presence outliers amount soft version means clustering described hastie addition clustering methods generally robust pertur bations data instance suppose cluster observations cluster observations removing subset observations random one would hope two sets clusters tained would quite similar often case lab principal components analysis tempered approach interpreting results clustering described issues associated clustering however clustering useful valid statistical tool used properly mentioned small decisions clustering performed data standardized type linkage used large eﬀect results therefore recommend performing clustering diﬀerent choices parameters looking full set results order see patterns consistently emerge since clustering non robust recommend clustering subsets data order get sense robustness clusters obtained importantly must careful results clustering analysis reported results taken absolute truth data set rather constitute starting point development scientiﬁc hypothesis study preferably independent data set 
[unsupervised, learning, lab, principal, components, analysis] lab perform pca usarrests data set part base package rows data set contain states alphabetical order row columns data set contain four variables ﬁrst brieﬂy examine data notice variables vastly diﬀerent means note apply function allows apply function case mean function row column data set second input denotes whether wish compute mean rows columns see average three times many rapes murders eight times many assaults rapes also examine variances four variables using apply function var unsupervised learning surprisingly variables also vastly diﬀerent variances urbanpop variable measures percentage population state living urban area comparable number num ber rapes state per individuals failed scale variables performing pca principal components observed would driven assault variable since far largest mean variance thus important standardize variables mean zero standard deviation one performing pca perform principal components analysis using prcomp func prcomp tion one several functions perform pca default prcomp function centers variables mean zero using option scale true scale variables standard deviation one output prcomp contains number useful quan tities center scale components correspond means standard deviations variables used scaling prior implementing pca rotation matrix provides principal component loadings col umn rotation contains corresponding principal component loading vector see four distinct principal components expected general min informative principal components data set observations variables function names rotation matrix matrix multiply matrix rotation gives coordinates data rotated coordinate system coordinates principal component scores lab principal components analysis using prcomp function need explicitly multiply data principal component loading vectors order obtain principal component score vectors rather matrix columns principal component score vectors column principal component score vector dim plot ﬁrst two principal components follows scale argument biplot ensures arrows scaled biplot represent loadings values scale give slightly diﬀerent biplots diﬀerent interpretations notice ﬁgure mirror image figure recall principal components unique sign change reproduce figure making small changes prcomp function also outputs standard deviation prin cipal component instance usarrests data set access standard deviations follows variance explained principal component obtained squar ing var var compute proportion variance explained principal compo nent simply divide variance explained principal component total variance explained four principal components pve var sum var pve see ﬁrst principal component explains variance data next principal component explains variance forth plot pve explained component well cumulative pve follows pve pve unsupervised learning result shown figure note function cumsum com cumsum putes cumulative sum elements numeric vector instance 
[unsupervised, learning, lab, clustering, k-means, clustering] function kmeans performs means clustering begin kmeans simple simulated example truly two clusters data ﬁrst observations mean shift relative next observations set perform means clustering cluster assignments observations contained cluster means clustering perfectly separated observations two clus ters even though supply group information kmeans plot data observation colored according cluster assignment col pch cex observations easily plotted two dimensional two variables could instead perform pca plot ﬁrst two principal components score vectors example knew really two clusters generated data however real data general know true number clusters could instead performed means clustering example set lab clustering sum tot col pch cex means clustering splits two clusters run kmeans function multiple initial cluster assign ments use nstart argument value nstart greater one used means clustering performed using multiple random assignments step algorithm kmeans function report best results compare using nstart nstart set note tot withinss total within cluster sum squares seek minimize performing means clustering equation individual within cluster sum squares contained vector withinss strongly recommend always running means clustering large value nstart since otherwise undesirable local optimum may obtained performing means clustering addition using multiple ini tial cluster assignments also important set random seed using set seed function way initial cluster assignments step replicated means output fully reproducible unsupervised learning 
[unsupervised, learning, lab, clustering, hierarchical, clustering] hclust function implements hierarchical clustering fol hclust lowing example use data section plot hierarchical clustering dendrogram using complete single average linkage cluster ing euclidean distance dissimilarity measure begin clustering observations using complete linkage dist function used dist compute inter observation euclidean distance matrix could easily perform hierarchical clustering average single linkage instead plot dendrograms obtained using usual plot function numbers bottom plot identify observation par sub cex average sub cex single sub cex determine cluster labels observation associated given cut dendrogram use cutree function cutree average single data complete average linkage generally separate observa tions correct groups however single linkage identiﬁes one point belonging cluster sensible answer obtained four clusters selected although still two singletons single scale variables performing hierarchical clustering observations use scale function scale xsc xsc lab nci data example correlation based distance computed using dist func dist tion converts arbitrary square symmetric matrix form hclust function recognizes distance matrix however makes sense data least three features since absolute corre lation two observations measurements two features always hence cluster three dimensional data set cor sub 
[unsupervised, learning, lab, nci, data, example] unsupervised techniques often used analysis genomic data particular pca hierarchical clustering popular tools illus trate techniques nci cancer cell line microarray data consists gene expression measurements cancer cell lines nci nci cell line labeled cancer type make use cancer types performing pca clustering unsupervised techniques performing pca clustering check see extent cancer types agree results unsupervised techniques data rows columns dim nci begin examining cancer types cell lines nci cns cns cns nci nci cns mcfa mcfd unsupervised learning 
[unsupervised, learning, lab, nci, data, example, pca, nci, data] ﬁrst perform pca data scaling variables genes standard deviation one although one could reasonably argue better scale genes nci data plot ﬁrst principal component score vectors order visualize data observations cell lines corresponding given cancer type plotted color see extent observations within cancer type similar ﬁrst create simple function assigns distinct color element numeric vector function used assign color cell lines based cancer type corresponds vec vec vec note rainbow function takes argument positive integer rainbow returns vector containing number distinct colors plot principal component score vectors par col nci pch col nci pch resulting plots shown figure whole cell lines corresponding single cancer type tend similar values ﬁrst principal component score vectors indicates cell lines cancer type tend pretty similar gene expression levels obtain summary proportion variance explained pve ﬁrst principal components using summary method prcomp object truncated printout using plot function also plot variance explained ﬁrst principal components note height bar bar plot given squaring corresponding element sdev however informative lab nci data example figure projections nci cancer cell lines onto ﬁrst three principal components words scores ﬁrst three principal com ponents whole observations belonging single cancer type tend lie near low dimensional space would possible visualize data without using dimension reduction method pca since based full data set possible scatterplots none would particularly informative plot pve principal component scree plot mulative pve principal component done little work pve sum par pve pve col pve pve col note elements pve also computed directly sum mary summary importance elements cumsum pve given summary importance resulting plots shown figure see together ﬁrst seven principal components explain around variance data huge amount variance however looking scree plot see ﬁrst seven principal components explain substantial amount variance marked decrease variance explained principal components elbow plot approx imately seventh principal component suggests may little beneﬁt examining seven principal components though even examining seven principal components may diﬃcult unsupervised learning principal component pve principal component cum ulativ pve figure pve principal components nci cancer cell line microarray data set left pve principal component shown right cumulative pve principal components shown together principal components explain variance 
[unsupervised, learning, lab, nci, data, example, clustering, observations, nci, data] proceed hierarchically cluster cell lines nci data goal ﬁnding whether observations cluster distinct types cancer begin standardize variables mean zero standard deviation one mentioned earlier step optional performed want gene scale nci perform hierarchical clustering observations using complete single average linkage euclidean distance used dissimilarity measure par nci labs sub dist nci labs sub dist nci labs sub results shown figure see choice linkage certainly aﬀect results obtained typically single linkage tend yield trailing clusters large clusters onto individual observa tions attach one one hand complete average linkage tend yield balanced attractive clusters reason complete average linkage generally preferred single linkage clearly cell lines within single cancer type tend cluster together although lab nci data example breast breast cns cns renal breast nsclc renal melanoma arian arian nsclc arian colon colon arian tat nsclc nsclc nsclc ost nsclc melanoma renal renal renal arian unkno arian nsclc cns cns cns nsclc renal renal renal renal nsclc melanoma melanoma melanoma melanoma melanoma melanoma breast breast colon colon colon colon colon breast mcfa repro breast mcfd repro leukemia leukemia leukemia leukemia repro repro leukemia leukemia complete linkage leukemia leukemia leukemia leukemia leukemia leukemia repro repro renal nsclc breast nsclc breast mcfa repro breast mcfd repro colon colon colon renal melanoma melanoma breast breast melanoma melanoma melanoma melanoma melanoma arian arian nsclc arian unkno arian nsclc melanoma cns cns cns renal renal renal renal renal renal renal ost nsclc nsclc nsclc nsclc arian tat nsclc colon colon arian colon colon cns cns breast breast average linkage leukemia renal breast leukemia leukemia cns leukemia leukemia repro repro nsclc leukemia nsclc cns breast nsclc colon breast melanoma renal melanoma breast breast melanoma melanoma melanoma melanoma melanoma breast colon mcfa repro breast mcfd repro unkno nsclc nsclc ost melanoma colon nsclc renal colon ost colon arian colon colon nsclc nsclc renal nsclc renal renal renal renal renal cns cns cns single linkage figure nci cancer cell line microarray data clustered erage complete single linkage using euclidean distance dissim ilarity measure complete average linkage tend yield evenly sized clusters whereas single linkage tends yield extended clusters single leaves fused one one unsupervised learning clustering perfect use complete linkage hierarchical cluster ing analysis follows cut dendrogram height yield particular number clusters say four nci clear patterns leukemia cell lines fall cluster breast cancer cell lines spread three diﬀerent clusters plot cut dendrogram produces four clusters par nci col red abline function draws straight line top existing plot argument plots horizontal line height den drogram height results four distinct clusters easy verify resulting clusters ones obtained using cutree printing output hclust gives useful brief summary object dat claimed earlier section means clustering hier archical clustering dendrogram cut obtain number clusters yield diﬀerent results nci hierarchical clustering results compare get perform means clustering set data see four clusters obtained using hierarchical clustering means clustering somewhat diﬀerent cluster means clustering identical cluster hierarchical clustering however clusters exercises diﬀer instance cluster means clustering contains portion observations assigned cluster hierarchical clustering well observations assigned cluster hierarchical clustering rather performing hierarchical clustering entire data matrix simply perform hierarchical clustering ﬁrst principal component score vectors follows nci labs nci surprisingly results diﬀerent ones obtained performed hierarchical clustering full data set sometimes performing clustering ﬁrst principal component score vectors give better results performing clustering full data situation might view principal component step one denois ing data could also perform means clustering ﬁrst principal component score vectors rather full data set 
[unsupervised, learning, exercises, conceptual] problem involves means clustering algorithm prove basis identity argue means clustering algorithm algorithm iteration suppose four observations compute dissimilarity matrix given instance dissimilarity ﬁrst second obser vations dissimilarity second fourth observations basis dissimilarity matrix sketch dendrogram results hierarchically clustering four observa tions using complete linkage sure indicate plot height fusion occurs well observations corresponding leaf dendrogram unsupervised learning repeat time using single linkage clustering suppose cut dendogram obtained two clusters result observations cluster suppose cut dendogram obtained two clusters result observations cluster mentioned chapter fusion den drogram position two clusters fused swapped without changing meaning dendrogram draw dendrogram equivalent dendrogram two leaves repositioned meaning dendrogram problem perform means clustering manually small example observations features observations follows obs plot observations randomly assign cluster label observation use sample command report cluster labels observation compute centroid cluster assign observation centroid closest terms euclidean distance report cluster labels observation repeat answers obtained stop changing plot color observations according cluster labels obtained suppose particular data set perform hierarchical clus tering using single linkage using complete linkage obtain two dendrograms certain point single linkage dendrogram clus ters fuse complete linkage dendro gram clusters also fuse certain point fusion occur higher tree fuse height enough information tell exercises certain point single linkage dendrogram clusters fuse complete linkage dendrogram clus ters also fuse certain point fusion occur higher tree fuse height enough information tell words describe results would expect performed means clustering eight shoppers figure basis sock computer purchases give three answers one variable scalings displayed explain researcher collects expression measurements genes tissue samples data written matrix call row represents gene col umn tissue sample tissue sample processed diﬀerent day columns ordered samples processed earliest left samples processed later right tissue samples belong two groups con trol treatment samples processed random order across days researcher wishes deter mine whether gene expression measurements diﬀer treatment control groups pre analysis comparing versus researcher per forms principal component analysis data ﬁnds ﬁrst principal component vector length strong linear trend left right explains variation searcher remembers patient sample run one two machines machine used often earlier times used often later researcher record sample run machine explain means ﬁrst principal component plains variation researcher decides replace element score loading ﬁrst principal component perform two sample test gene new data set order determine whether expression diﬀers two conditions critique idea suggest better approach principal component analysis performed design run small simulation experiment demonstrate superiority idea unsupervised learning 
[unsupervised, learning, exercises, applied] chapter mentioned use correlation based distance euclidean distance dissimilarity measures hierarchical clus tering turns two measures almost equivalent observation centered mean zero standard deviation one let denote correlation observations quantity proportional squared euclidean distance observations usarrests data show proportionality holds hint euclidean distance calculated using dist func tion correlations calculated using cor function section formula calculating pve given equa tion also saw pve obtained using sdev output prcomp function usarrests data calculate pve two ways using sdev output prcomp function done section applying equation directly use prcomp function compute principal component loadings use loadings equation obtain pve two approaches give results hint obtain results data used cases instance performed prcomp using centered scaled variables must center scale variables applying equation consider usarrests data perform hierarchical clus tering states using hierarchical clustering complete linkage euclidean distance cluster states cut dendrogram height results three distinct clusters states belong clusters hierarchically cluster states using complete linkage clidean distance scaling variables standard viation one eﬀect scaling variables hierarchical clustering obtained opinion variables scaled inter observation dissimilarities computed provide justiﬁcation answer exercises problem generate simulated data perform pca means clustering data generate simulated data set observations three classes observations total variables hint number functions use generate data one example rnorm function runif another option sure add mean shift observations class three distinct classes perform pca observations plot ﬁrst two prin cipal component score vectors use diﬀerent color indicate observations three classes three classes appear separated plot continue part return part modify simulation greater separation three classes continue part three classes show least separation ﬁrst two principal component score vectors perform means clustering observations well clusters obtained means cluster ing compare true class labels hint use table function compare true class labels class labels obtained clustering careful interpret results means clustering arbitrarily number clusters cannot simply check whether true class labels clustering labels perform means clustering describe results perform means clustering describe results perform means clustering ﬁrst two principal component score vectors rather raw data perform means clustering matrix ﬁrst column ﬁrst principal component score vector second column second principal component score vector comment results using scale function perform means clustering data scaling variable standard deviation one results compare obtained explain book website gene expres sion data set chex csv consists tissue samples measurements genes ﬁrst samples healthy patients second diseased group unsupervised learning load data using read csv need select header apply hierarchical clustering samples using correlation based distance plot dendrogram genes separate samples two groups results depend type linkage used collaborator wants know genes diﬀer across two groups suggest way answer question apply 
