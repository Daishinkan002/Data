[introduction, information, theory] fundamental problem communication reproducing one point either exactly approximately message selected another point claude shannon first half book study measure information content learn compress data learn communicate perfectly imperfect communication channels start getting feeling last problem 
[introduction, information, theory, noisy, communication, channel?] examples noisy communication channels analogue telephone line two modems communicate digital modem phone line modem information radio communication link galileo jupiter orbiting space galileo radio waves earth craft earth parent cell daughter cell daughter cell reproducing cells daughter cells dna contains information parent cells computer memory disk drive computer memory disk drive last example shows communication involve informa tion going one place another write file disk drive read location later time channels noisy telephone line suffers cross talk lines hardware line distorts adds noise transmitted signal deep space network listens galileo puny transmitter receives background radiation terrestrial cosmic sources dna subject mutations damage disk drive writes binary digit one zero also known bit aligning patch magnetic material one two orientations may later fail read stored binary digit patch material might spontaneously flip magnetization glitch background noise might cause reading circuit report wrong value binary digit writing head might induce magnetization first place interference neighbouring bits cases transmit data string bits channel probability received message identical copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links introduction information theory transmitted message would prefer communication channel probability zero close zero practical purposes indistinguishable zero let consider noisy disk drive transmits bit correctly probability incorrectly probability model communi cation channel known binary symmetric channel figure figure binary symmetric channel transmitted symbol received symbol noise level probability bit flipped figure binary data sequence length transmitted binary symmetric channel noise level dilbert image copyright syndicate inc used permission example let imagine ten per cent bits flipped figure useful disk drive would flip bits entire lifetime expect read write gigabyte per day ten years require bit error probability order smaller two approaches goal 
[introduction, information, theory, physical, solution] physical solution improve physical characteristics commu nication channel reduce error probability could improve disk drive using reliable components circuitry evacuating air disk enclosure eliminate turbu lence perturbs reading head track using larger magnetic patch represent bit using higher power signals cooling circuitry order reduce thermal noise physical modifications typically increase cost communication channel 
[introduction, information, theory, ‘system’, solution] information theory coding theory offer alternative much citing approach accept given noisy channel add communi cation systems detect correct errors introduced channel shown figure add encoder channel decoder encoder encodes source message transmit ted message adding redundancy original message way channel adds noise transmitted message yielding received message decoder uses known redundancy introduced encoding system infer original signal added noise copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links error correcting codes binary symmetric channel noisy channel encoder decoder source figure system solution achieving reliable communication noisy channel encoding system introduces systematic redundancy transmitted vector decoding system uses known redundancy deduce received vector original source vector noise introduced channel whereas physical solutions give incremental channel improvements ever increasing cost system solutions turn noisy channels reliable communication channels cost computational requirement encoder decoder information theory concerned theoretical limitations tentials systems best error correcting performance could achieve coding theory concerned creation practical encoding decoding systems 
[introduction, information, theory, error-correcting, codes, binary, symmetric, channel] consider examples encoding decoding systems simplest way add useful redundancy transmission make rules game clear want able detect correct errors transmission option get one chance encode transmit decode 
[introduction, information, theory, repetition, codes] straightforward idea repeat every bit message prearranged number times example three times shown table call repetition code source transmitted sequence sequence table repetition code imagine transmit source message binary symmetric channel noise level using repetition code describe channel adding sparse noise vector transmitted vector adding modulo arithmetic binary algebra possible noise vector received vector shown figure figure example transmission using decode received vector optimal algorithm looks received bits three time takes majority vote algorithm copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links introduction information theory received sequence likelihood ratio decoded sequence algorithm majority vote decoding algorithm also shown likelihood ratios assuming channel binary symmetric channel risk explaining obvious let prove result optimal decoding decision optimal sense smallest probability wrong find value probable given consider decoding single bit encoded gave rise three received bits bayes theorem posterior probability spell posterior probability two alternatives thus posterior probability determined two factors prior probability data dependent term called likelihood normalizing constant computed finding optimal decoding decision guess otherwise find must make assumption prior probabilities two hypotheses must make assumption probability given assume prior prob abilities equal maximizing posterior probability equivalent maximizing likelihood assume channel binary symmetric channel noise level likelihood number transmitted bits block considering thus likelihood ratio two hypotheses factor equals ratio greater since winning hypothesis one votes vote counting factor likelihood ratio copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links error correcting codes binary symmetric channel thus majority vote decoder shown algorithm optimal decoder assume channel binary symmetric channel two possible source messages equal prior probability apply majority vote decoder received vector figure first three received bits decode triplet second triplet figure two one decode triplet case corrects error errors corrected however unlucky two errors fall single block fifth triplet figure decoding rule gets wrong answer shown figure corrected errors undetected errors figure decoding received vector figure exercise show error probability reduced use exercise rating indicates difficulty exercises easiest exercises accompanied marginal rat especially recommended solution partial solution provided page indicated difficulty rating example exercise solution page computing error probability code binary symmetric channel noise level error probability dominated probability two bits block three flipped scales case binary symmetric channel code probability error decoding per bit figure shows result transmitting binary image binary symmetric channel using repetition code encoder channel decoder figure transmitting source bits binary symmetric channel using repetition code majority vote decoding algorithm probability decoded bit error fallen rate fallen copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links introduction information theory rate useful codes rate useful codes figure error probability versus rate repetition codes binary symmetric channel right hand figure shows logarithmic scale would like rate large small repetition code therefore reduced probability error desired yet lost something rate information transfer fallen factor three use repetition code communicate data telephone line reduce error frequency also reduce communication rate pay three times much phone call similarly would need three original noisy gigabyte disk drives order create one gigabyte disk drive push error probability lower values required sell able disk drive could achieve lower error probabilities using repetition codes repetitions exercise show probability error repe tition code repetitions odd assuming terms sum biggest much bigger second biggest term use stirling approximation approximate largest term find approximately probability error repetition code repetitions assuming find many repetitions required get probability error answer build single gigabyte disk drive required reliability noisy gigabyte drives would need sixty noisy disk drives tradeoff error probability rate repetition codes shown figure 
[introduction, information, theory, block, codes, hamming, code] would like communicate tiny probability error substan tial rate improve repetition codes add redundancy blocks data instead encoding one bit time study simple block code copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links error correcting codes binary symmetric channel block code rule converting sequence source bits length say transmitted sequence length bits add redundancy make greater linear block code extra bits linear functions original bits extra bits called parity check bits example linear block code hamming code transmits bits every source bits figure pictorial representation encoding hamming code encoding operation code shown pictorially figure arrange seven transmitted bits three intersecting circles first four transmitted bits set equal four source bits parity check bits set parity within circle even first parity check bit parity first three source bits sum bits even sum odd second parity last three third parity bit parity source bits one three four example figure shows transmitted codeword case table shows codewords generated sixteen settings four source bits codewords special property pair differ least three bits table sixteen codewords hamming code pair codewords differ least three bits hamming code linear code written compactly terms matrices follows transmitted codeword obtained source sequence linear operation generator matrix code                   encoding operation uses modulo arithmetic etc encoding operation assumed column vectors instead row vectors equation replaced copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links introduction information theory       find easier relate right multiplication left multiplica tion many coding theory texts use left multiplying conventions however rows generator matrix viewed defining four basis vectors lying seven dimensional binary space sixteen codewords obtained making possible linear combinations vectors decoding hamming code invent complex encoder task decoding received vector becomes less straightforward remember bits may flipped including parity bits assume channel binary symmetric channel source vectors equiprobable optimal decoder identifies source vector whose encoding differs received vector fewest bits refer likelihood function see could solve decoding problem measuring far sixteen codewords table picking closest efficient way finding probable source vector syndrome decoding hamming code hamming code pictorial solution decoding problem based encoding picture figure first example let assume transmission noise flips second bit received vector write received vector three circles shown figure look three circles see whether parity even circles whose parity even shown dashed lines figure decoding task find smallest set flipped bits account violations parity rules pattern violations parity checks called syndrome written binary vector example figure syndrome first two circles unhappy parity third circle happy parity solve decoding task ask question find unique bit lies inside unhappy circles outside happy circles flipping bit would account observed syndrome case shown figure bit lies inside two unhappy circles outside happy circle single bit property single bit capable explaining syndrome let work couple examples figure shows happens one parity bits flipped noise one checks violated lies inside unhappy circle outside two happy circles identified single bit capable explaining syndrome central bit received flipped figure shows three checks violated lies inside three circles identified suspect bit copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links error correcting codes binary symmetric channel figure pictorial representation decoding hamming code received vector written diagram shown received vector shown assuming transmitted vector figure bits labelled flipped violated parity checks highlighted dashed circles one seven bits probable suspect account syndrome pattern violated satisfied parity checks examples probable suspect one bit flipped example two bits flipped probable suspect marked circle shows output decoding algorithm syndrome unflip bit none algorithm actions taken optimal decoder hamming code assuming binary symmetric channel small noise level syndrome vector lists whether parity check violated satisfied going checks order bits try flipping one seven bits find different syndrome obtained case seven non zero syndromes one bit one syndrome zero syndrome channel binary symmetric channel small noise level optimal decoder unflips one bit depending syndrome shown algorithm syndrome could caused noise patterns noise pattern syndrome must less probable involves larger number noise events happens noise actually flips one bit figure shows situation two bits received flipped syn drome makes suspect single bit optimal decoding gorithm flips bit giving decoded pattern three errors shown figure use optimal decoding algorithm two bit error pattern lead decoded seven bit vector contains three errors 
[introduction, information, theory, general, view, decoding, linear, codes, syndrome, decoding] also describe decoding problem linear code terms matrices first four received bits purport four source bits received bits purport parities source bits defined generator matrix evaluate three parity check bits received bits see whether match three received bits differences modulo two triplets called syndrome received vector syndrome zero three parity checks happy received vector codeword probable decoding copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links introduction information theory encoder parity bits channel decoder figure transmitting source bits binary symmetric channel using hamming code probability decoded bit error given reading first four bits syndrome non zero noise sequence block non zero syndrome pointer probable error pattern computation syndrome vector linear operation define matrix matrix equation identity matrix syndrome vector parity check matrix given modulo arithmetic   codewords code satisfy   exercise prove evaluating matrix since received vector given syndrome decoding problem find probable noise vector satisfying equation decoding algorithm solves problem called maximum likelihood decoder discuss decoding problems like later chapters 
[introduction, information, theory, summary, hamming, code’s, properties] every possible received vector length bits either codeword one flip away codeword since three parity constraints might might violated distinct syndromes divided seven non zero syndromes one one bit error patterns zero syndrome corresponding zero noise case optimal decoder takes action syndrome zero otherwise uses mapping non zero syndromes onto one bit error patterns unflip suspect bit copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links error correcting codes binary symmetric channel decoding error four decoded bits match source bits probability block error probability one decoded bits one block fail match corresponding source bits probability bit error average probability decoded bit fails match corresponding source bit case hamming code decoding error occur whenever noise flipped one bit block seven probability block error thus probability two bits flipped block probability scales probability error repetition code notice hamming code communicates greater rate figure shows binary image transmitted binary symmetric channel using hamming code decoded bits error notice errors correlated often two three successive decoded bits flipped exercise exercise next three refer hamming code decode received strings exercise calculate probability block error hamming code function noise level show leading order goes show leading order probability bit error goes exercise find noise vectors give zero syndrome noise vectors leave parity checks unviolated many noise vectors exercise asserted block decoding error result ever two bits flipped single block show indeed principle might error patterns coding led corruption parity bits source bits incorrectly decoded 
[introduction, information, theory, summary, codes’, performances] figure shows performance repetition codes hamming code also shows performance family linear block codes gen eralizations hamming codes called bch codes figure shows using linear block codes achieve better performance repetition codes asymptotic situation still looks grim copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links introduction information theory rate useful codes bch bch rate useful codes bch bch figure error probability versus rate repetition codes hamming code bch codes blocklengths binary symmetric channel righthand figure shows logarithmic scale exercise design error correcting code decoding algorithm estimate probability error add figure worry find difficult make code better hamming code find difficult find good decoder code point exercise exercise hamming code correct one error might code correct two errors optional extra answer question depend whether code linear nonlinear exercise design error correcting code repetition code correct two errors block size 
[introduction, information, theory, performance, best, codes, achieve?] seems trade decoded bit error probability would like reduce rate would like keep large trade characterized points plane achievable question addressed claude shannon pioneering paper created field information theory solved fundamental problems time widespread belief boundary achievable nonachievable points plane curve passing origin order achieve vanishingly small error probability one would reduce rate correspondingly close zero pain gain however shannon proved remarkable result boundary tween achievable nonachievable points meets axis non zero value shown figure channel exist codes make possible communicate arbitrarily small probability error non zero rates first half book parts iii devoted understanding remarkable result called noisy channel coding theorem 
[introduction, information, theory, example] maximum rate communication possible arbitrarily small called capacity channel formula capacity copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links summary rate achievable achievable rate achievable achievable figure shannon noisy channel coding theorem solid curve shows shannon limit achievable values binary symmetric channel rates achievable arbitrarily small points show performance textbook codes figure equation defining shannon limit solid curve defined equation binary symmetric channel noise level log log channel discussing earlier noise level capacity let consider means terms noisy disk drives repetition code could communicate channel rate thus know build single gigabyte disk drive three noisy gigabyte disk drives also know make single gigabyte disk drive sixty noisy one gigabyte drives exercise shannon passes notices juggling disk drives codes says performance trying achieve need sixty disk drives get performance two disk drives since less want anything get two disk drives strictly statements might quite right since shall see shannon proved noisy channel coding theorem studying sequences block codes ever increasing blocklengths required blocklength might bigger gigabyte size disk drive case shannon might say well tiny disk drives two noisy terabyte drives could make single high quality terabyte drive 
[introduction, information, theory, hamming, code] including three parity check bits block bits possible detect correct single bit error block 
[introduction, information, theory, shannon’s, noisy-channel, coding, theorem] information communicated noisy channel non zero rate arbitrarily small error probability copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links introduction information theory information theory addresses limitations possibilities communication noisy channel coding theorem prove chapter asserts reliable communication rate beyond capacity impossible reliable communication rates capacity possible next chapters lay foundations result discussing measure information content intimately related topic data compression 
[introduction, information, theory, exercises] exercise consider repetition code one way viewing code concatenation first encode source stream encode resulting output could call code idea motivates alternative decoding algorithm decode bits three time using decoder decode decoded bits first decoder using decoder evaluate probability error decoder compare probability error optimal decoder concatenated encoder decoder advantages 
[introduction, information, theory, solutions] solution exercise error made two bits flipped block three error probability sum two terms probability three bits flipped probability exactly two bits flipped expressions obvious see example expressions probability dominated small term see exercise discussion problem solution exercise probability error repetition code dominated probability bits flipped goes odd notation denotes smallest integer greater equal term approximated using binary entropy function approximation introduces error order shown equation setting equal required value find log log answer little approximation used overestimated distinguish copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links solutions slightly careful answer short explicit computation goes follows taking approximation next order find approximation proved accurate version stirling proximation considering binomial distribution noting equation follows distinction important term since maximum probability error odd leading order equation written equation logarithms taken base long base throughout equation use base log log log may solved iteratively first iteration starting answer found stable blocklength solution exercise probability block error hamming code sum six terms probabilities errors occur one block leading order goes probability bit error hamming code smaller probability block error block error rarely corrupts bits decoded block leading order behaviour found considering outcome probable case noise vector weight two decoder erroneously flip third bit modified received vector length differs three bits transmitted vector means average seven bits probability randomly chosen bit flipped times block error probability leading order really care probability copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links introduction information theory source bit flipped parity bits source bits likely among three flipped bits seven bits equally likely corrupted noise vector weight two hamming code fact completely symmetric protection affords seven bits assuming binary symmetric channel symmetry proved showing role parity bit exchanged source bit resulting code still hamming code see probability one bit ends corrupted seven bits probability bit error source bits simply three sevenths probability block error 
[introduction, information, theory, symmetry, hamming, code] prove code protects bits equally start parity check matrix   symmetry among seven transmitted bits easiest see reorder seven bits using permutation rewrite thus   take two parity constraints satisfies add together get another parity constraint example row asserts even row asserts even sum two constraints even drop terms since even whatever thus derived parity constraint even wish add parity check matrix fourth row set vectors satisfying changed thus define       fourth row sum modulo two top two rows notice second third fourth rows cyclic shifts top row added fourth redundant constraint drop first constraint obtain new parity check matrix   still satisfies codewords looks like starting except columns shifted along one copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links solutions right rightmost column reappeared left cyclic permutation columns establishes symmetry among seven bits iterating procedure five times make total seven different matrices original code assigns bit different role may also construct super redundant seven row parity check matrix code                   matrix redundant sense space spanned rows three dimensional seven matrix also cyclic matrix every row cyclic permutation top row cyclic codes ordering bits linear code cyclic parity check matrix code called cyclic code codewords code also cyclic properties cyclic permutation codeword codeword example hamming code bits ordered consists seven cyclic shifts codewords codewords cyclic codes cornerstone algebraic approach error correcting codes use book however superceded sparse graph codes part solution exercise fifteen non zero noise vectors give zero syndrome precisely fifteen non zero codewords hamming code notice hamming code linear sum two codewords codeword 
[introduction, information, theory, graphs, corresponding, codes] solution exercise answering question prob ably find easier invent new codes find optimal decoders many ways design codes follows one possible train thought make linear block code similar hamming code bigger figure graph hamming code circles bit nodes squares parity check nodes many codes conveniently expressed terms graphs fig ure introduced pictorial representation hamming code replace figure big circles shows parity four particular bits even parity check node connected four bits obtain representation hamming code bipartite graph shown figure circles transmitted bits squares parity check nodes confused parity check bits three peripheral circles graph bipartite graph nodes fall two classes bits checks copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links introduction information theory edges nodes different classes graph code parity check matrix simply related parity check node corresponds row bit node corresponds column every edge corresponding pair nodes noticed connection linear codes graphs one way invent linear codes simply think bipartite graph example pretty bipartite graph obtained dodecahedron calling vertices dodecahedron parity check nodes putting transmitted bit edge dodecahedron construction defines parity figure graph defining dodecahedron code circles transmitted bits triangles parity checks one parity check redundant check matrix every column weight every row weight weight binary vector number contains code bits appears apparent parity check constraints actually independent constraints constraint redundant constraints satisfied automatically satisfied number source bits code code hard find decoding algorithm code estimate probability error finding lowest weight codewords flip bits surrounding one face original dodecahedron parity checks satisfied code codewords weight one face since lowest weight codewords weight say code distance hamming code distance could correct single bit flip errors code distance correct double bit flip errors triple bit flip errors cannot correct error probability code assuming binary symmetric channel dominated least low noise levels term order perhaps something like course obligation make codes whose graphs rep resented plane one best linear codes simple graphical descriptions graphs tangled illustrated tiny code figure figure graph rate low density parity check code gallager code blocklength parity check constraints white circle represents transmitted bit bit participates constraints represented squares edges nodes placed random see chapter furthermore reason sticking linear codes indeed nonlinear codes codes whose codewords cannot defined linear equa tion like good properties encoding decoding nonlinear code even trickier tasks solution exercise first let assume making linear code decoding syndrome decoding transmitted bits number possible error patterns weight two patterns every distinguishable error pattern must give rise distinct syndrome syndrome list bits maximum possible number syndromes code syndromes number possible error patterns weight two bigger number syndromes immediately rule possibility code error correcting copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links solutions counting argument works fine nonlinear codes decoder receives aim deduce case sender select transmission code size channel select noise vector set size two selections recovered received bit string one possible strings must case two error correcting code whether linear nonlinear solution exercise various strategies making codes correct multiple errors strongly recommend think one two approach uses linear code one collection parity checks helpful bear mind counting argument given previous exercise order anticipate many parity checks might need examples codes correct two errors dodeca hedron code page pentagonful code introduced simple ideas making codes correct multiple errors codes correct one error discussed section solution exercise probability error leading order whereas probability error dominated probability five flips decoding procedure therefore suboptimal since noise vec tors weight four cause make decoding error advantage however requiring smaller computational sources memorization three bits counting three rather counting nine simple code illustrates important concept concatenated codes widely used practice concatenation allows large codes implemented using simple encoding decoding hardware best known practical codes concatenated codes 
[probability, entropy, inference] chapter sibling chapter devote time notation white knight distinguished song name song name song called carroll sometimes need careful distinguish random variable value random variable proposition asserts random variable particular value particular chapter however use simple friendly notation possible risk upsetting pure minded readers example something true probability usually simply say true 
[probability, entropy, inference, probabilities, ensembles] ensemble triple outcome value random variable takes one set possible values probabilities name mnemonic alphabet one example ensemble letter randomly selected english document ensemble shown figure twenty seven possible letters space character figure probability distribution outcomes randomly selected letter english language document estimated frequently asked questions manual linux picture shows probabilities areas white squares abbreviations briefer notation sometimes used example may written probability subset subset example define vowels figure joint ensemble ensemble outcome ordered pair call joint probability commas optional writing ordered pairs joint ensemble two variables necessarily inde pendent copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links probabilities ensembles figure probability distribution possible bigrams english language document frequently asked questions manual linux marginal probability obtain marginal probability joint probability summation similarly using briefer notation marginal probability conditional probability undefined pronounce probability equals given equals example example joint ensemble ordered pair consisting two successive letters english document possible outcomes ordered pairs might expect probable estimate joint probability distribution two neighbouring characters shown graphically figure joint ensemble special property two marginal dis tributions identical equal monogram distribution shown figure joint ensemble obtain conditional distributions normalizing rows columns respectively figure probability probability distribution second letter given first letter see figure two probable values second letter given copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links probability entropy inference figure conditional probability distributions row shows conditional distribution second letter given first letter bigram column shows conditional distribution first letter given second letter first letter space common source document makes heavy use word faq probability probability distribution first letter given second letter see figure two probable values given rather writing joint probability directly often define ensemble terms collection conditional probabilities following rules probability theory useful denotes assumptions probabilities based product rule obtained definition conditional probability rule also known chain rule sum rule rewriting marginal probability definition bayes theorem obtained product rule independence two random variables independent sometimes written exercise random variables joint ensemble figure independent copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links meaning probability said often define ensemble terms collection condi tional probabilities following example illustrates idea example test nasty disease denote state health variable test result disease disease result test either positive negative test reliable cases people really disease positive result returned cases people disease negative result obtained final piece background information people age background disease test result positive probability disease solution write provided probabilities test reliability specifies conditional probability given disease prevalence tells marginal probability marginal conditional probability deduce joint probability probabilities interested example sum rule marginal probability probability getting positive result received positive result interested plausible disease man street might duped statement test reliable positive result implies chance disease incorrect correct solution inference problem found using bayes theorem spite positive result probability disease 
[probability, entropy, inference, meaning, probability] probabilities used two ways probabilities describe frequencies outcomes random experiments giving noncircular definitions terms frequency random challenge mean say frequency tossed coin copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links probability entropy inference box cox axioms set beliefs satisfy axioms mapped onto probabilities satisfying false true rules probability notation let degree belief proposition denoted negation written degree belief condi tional proposition assuming proposition true represented axiom degrees belief ordered greater greater greater consequence beliefs mapped onto real numbers axiom degree belief proposition negation related function axiom degree belief conjunction propositions related degree belief conditional proposition degree belief proposition function coming heads say frequency average fraction heads long sequences define average hard define average without using word synonymous probability attempt cut philosophical knot probabilities also used generally describe degrees lief propositions involve random variables example probability murderer mrs given evidence either jury job assess probable probability thomas jefferson child one slaves probability shakespeare plays written francis bacon pick modern day example probability particular signature particular cheque genuine man street happy use probabilities ways books probability restrict probabilities refer frequencies outcomes repeatable random experiments nevertheless degrees belief mapped onto probabilities sat isfy simple consistency rules known cox axioms cox figure thus probabilities used describe assumptions describe ferences given assumptions rules probability ensure two people make assumptions receive data draw identical conclusions general use probability quantify beliefs known bayesian viewpoint also known subjective interpretation probability since probabilities depend assumptions advocates bayesian approach data modelling pattern recognition view subjectivity defect since view cannot inference without making assumptions book time time taken granted bayesian approach makes sense reader warned yet globally held view field statistics dominated century non bayesian methods probabilities allowed describe random variables big difference two approaches copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links forward probabilities inverse probabilities bayesians also use probabilities describe inferences 
[probability, entropy, inference, forward, probabilities, inverse, probabilities] probability calculations often fall one two categories forward prob ability inverse probability example forward probability problem exercise urn contains balls black white fred draws ball random urn replaces times probability distribution number times black ball drawn expectation variance standard deviation give numerical answers cases forward probability problems involve generative model describes pro cess assumed give rise data task compute probability distribution expectation quantity depends data another example forward probability problem exercise urn contains balls black white define fraction fred draws times urn exactly exercise obtaining blacks computes quantity expectation case probability distribution probability hint compare quantities computed previous exercise like forward probability problems inverse probability problems involve generative model process instead computing probability distri bution quantity produced process compute conditional probability one unobserved variables process given observed variables invariably requires use bayes theorem example eleven urns labelled con taining ten balls urn contains black balls white balls fred selects urn random draws times replacement urn obtaining blacks whites fred friend bill looks draws blacks drawn probability urn fred using urn bill point view bill know value solution joint probability distribution random variables written joint probability obtain conditional distribution given copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links probability entropy inference figure joint probability bill fred urn problem draws marginal probability wrote probability given solved exercise highly recommended exercises define denominator marginal probability obtain using sum rule conditional probability given figure conditional probability given conditional distribution found normalizing column figure shown figure normalizing constant marginal probability posterior probability correct including end points respectively posterior probability given equal zero fred drawing urn would impossible black balls drawn posterior probability also zero white balls urn hypotheses non zero posterior probability 
[probability, entropy, inference, terminology, inverse, probability] inverse probability problems convenient give names proba bilities appearing bayes theorem equation call marginal probability prior probability called like lihood important note terms likelihood probability synonyms quantity function fixed defines probability fixed defines likelihood copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links forward probabilities inverse probabilities never say likelihood data always say likelihood parameters likelihood function probability distribution want mention data likelihood function associated may say likelihood parameters given data conditional probability called posterior probability given normalizing constant dependence value important simply wish evaluate relative probabilities alternative hypotheses however data modelling problems complexity quantity becomes important given various names known evidence marginal likelihood denotes unknown parameters denotes data denotes overall hypothesis space general equation written posterior likelihood prior evidence 
[probability, entropy, inference, inverse, probability, prediction] example continued assuming bill observed blacks draws let fred draw another ball urn probability next drawn ball black make use posterior probabilities figure solution sum rule ball black ball black since balls drawn replacement chosen urn proba bility ball black whatever ball black using values given figure obtain ball black comment notice difference prediction obtained using prob ability theory widespread practice statistics making predictions first selecting plausible hypothesis would urn urn making predictions assuming hypothesis true would give probability next ball black correct prediction one takes account uncertainty marginalizing possible values hypothesis marginalization leads slightly moderate less extreme predictions copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links probability entropy inference 
[probability, entropy, inference, inference, inverse, probability] consider following exercise character simple sci entific investigation example bill tosses bent coin times obtaining sequence heads tails assume coin probability coming heads know heads occurred tosses probability distribution example might might lot tossing might probability outcome head given heads tosses unlike example problem subjective element given restricted definition probability says probabilities frequencies random variables example different eleven urns example whereas urn random variable bias coin would normally called random variable fixed unknown parameter interested yet two examples seem essential similarity especially solve example make assumption bias coin might prior probability distribution denotes probability density rather probability distribution corresponds prior eleven urns problem example helpful problem definition specified real life make assumptions order assign priors assumptions subjective answers depend exactly said probabilities generative model assuming example balls drawn urn independently could correlations sequence fred ball drawing action perfectly random indeed could likelihood function use depends assumptions real data modelling problems priors subjective likelihoods using denote probability densities continuous vari ables well probabilities discrete variables probabilities logical propositions probability continuous variable lies values defined dimensionless density dimensional quantity dimensions inverse dimensions contrast discrete probabilities dimensionless surprised see probability densities greater normal nothing wrong long interval conditional joint probability densities defined way conditional joint probabilities exercise assuming uniform prior solve problem posed example sketch posterior distribution compute probability outcome head find beta integral useful copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links forward probabilities inverse probabilities may also find instructive look back example equation people sometimes confuse assigning prior distribution unknown rameter making initial guess value parameter prior simple statement like initially would guess prior probability density specifies prior degree belief lies interval may well case prior symmetric mean prior case predictive distribution first toss would indeed head head prediction subsequent tosses depend whole prior dis tribution mean data compression inverse probability consider following task example write computer program capable compressing binary files like one string shown contains intuitively compression works taking advantage predictability file case source file appears likely emit data compression program compresses file must implicitly explicitly addressing question probability next character file think problem similar character example one themes book data compression data modelling one addressed like urn example using inverse probability example solved chapter 
[probability, entropy, inference, likelihood, principle] please solve following two exercises example urn contains three balls one black two white urn figure urns example contains three balls two black one white one urns selected random one ball drawn ball black probability selected urn urn example urn contains five balls one black two white one green figure urns example one pink urn contains five hundred balls two hundred black one hundred white yellow cyan sienna green silver gold purple one fifth balls black two fifths black one urns selected random one ball drawn ball black probability urn urn copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links probability entropy inference notice solutions answer depend detailed contents urn details possible outcomes probabilities relevant matters probability outcome actually happened ball drawn black given different hypothe ses need know likelihood probability data happened varies hypothesis simple rule inference known likelihood principle likelihood principle given generative model data given parameters observed particular outcome inferences predictions depend function spite simplicity principle many classical statistical methods violate 
[probability, entropy, inference, definition, entropy, related, functions] shannon information content outcome defined log measured bits word bit also used denote variable whose value hope context always make clear two meanings intended next chapters establish shannon information content indeed natural measure information content event point shorten name quantity information content log table shannon information contents outcomes fourth column table shows shannon information content possible outcomes random character picked english document outcome shannon information content bits information content bits entropy ensemble defined average shannon formation content outcome log convention log since lim log like information content entropy measured bits convenient may also write vector another name entropy uncertainty example entropy randomly selected letter english docu ment bits assuming probability given table obtain number averaging log shown fourth col umn probability distribution shown third column copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links decomposability entropy note properties entropy function equality iff one iff means entropy maximized uniform log equality iff notation vertical bars two meanings set denotes number elements number absolute value redundancy measures fractional difference max imum possible value log redundancy log make use redundancy book assigned symbol joint entropy log entropy additive independent random variables iff definitions information content far apply discrete probability distributions finite sets definitions extended infinite sets though entropy may infinite case probability density continuous set addressed section important definitions exercises entropy come along section 
[probability, entropy, inference, decomposability, entropy] entropy function satisfies recursive property useful computing entropies convenience stretch notation write probability vector associated ensemble let illustrate property example first imagine random variable created first flipping fair coin determine whether flipping fair coin second time determine whether probability distribution entropy either compute brute force log log log use following decomposition value revealed gradually imagine first learning whether learning non zero value case revelation whether entails copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links probability entropy inference revealing binary variable whose probability distribution revelation entropy log log bit learn value second coin flip binary variable whose probability distribution whose entropy bit get experience second revelation half time however entropy written generalizing observation making entropy probability distribution written formula property looks regrettably ugly nev ertheless simple property one make use generalizing entropy property example source produces character alphabet probability numeral probability vowel probability one consonants numerals equiprobable goes vowels consonants estimate entropy solution log log log log log log log bits 
[probability, entropy, inference, gibbs’, inequality] leibler pronounced heist relative entropy kullback leibler divergence two probability distributions defined alphabet log relative entropy satisfies gibbs inequality equality note general relative entropy symmetric interchange distributions general although sometimes called distance strictly distance relative entropy important pattern recognition neural networks well information theory gibbs inequality probably important inequality book many inequalities proved using concept convexity copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links jensen inequality convex functions 
[probability, entropy, inference, jensen’s, inequality, convex, functions] words convex concave may pronounced convex smile concave frown terminology useful redundancy one may forget way convex concave harder confuse smile frown convex functions function convex every chord figure definition convexity function lies function shown figure function strictly convex equality holds similar definitions apply concave strictly concave functions strictly convex functions log log log log figure convex functions jensen inequality convex function random variable denotes expectation strictly convex random variable constant jensen inequality also rewritten concave function direction inequality reversed physical version jensen inequality runs follows centre gravity collection masses placed convex curve locations centre gravity masses lies curve fails convince feel free following exercise exercise prove jensen inequality example three squares average area average lengths sides said size largest three squares use jensen inequality solution let length side square let probability three lengths information function mapping lengths areas strictly convex function notice equality holds therefore constant three lengths must equal area largest square copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links probability entropy inference 
[probability, entropy, inference, convexity, concavity, also, relate, maximization] concave exists point maximum value point converse hold concave maximized necessarily true gradient equal zero example maximized derivative undefined log probability maximized boundary range gradient 
[probability, entropy, inference, sums, random, variables] exercise two ordinary dice faces labelled thrown probability distribution sum val ues probability distribution absolute difference values one hundred ordinary dice thrown roughly prob exercise intended help think central limit theorem says independent random variables means finite variances limit large sum distribution tends normal gaussian distribution mean variance ability distribution sum values sketch probability distribution estimate mean standard deviation two cubical dice labelled using numbers two dice thrown sum uniform probability distribution integers way one hundred dice could labelled inte gers probability distribution sum uniform 
[probability, entropy, inference, inference, problems] exercise show exp sketch function find relationship hyperbolic tangent function tanh useful fluent base logarithms also log function exercise let dependent random variables binary variable taking values use bayes theorem show log posterior probability ratio given log log log exercise let random variables conditionally independent given binary variable use bayes theorem show posterior probability ratio given copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exercises 
[probability, entropy, inference, life, high-dimensional, spaces] solution exercise volume hypersphere radius dimensions fact need know question need dependence fractional volume fractional volumes shells required cases notice matter small large enough essentially probability mass surface shell thickness copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links solutions solution exercise solution exercise general solution exercise solution exercise log solution exercise type question approached two ways either differentiating function maximized finding maximum proving global maximum strategy somewhat risky since possible maximum function boundary space place derivative zero alternatively carefully chosen inequality establish answer second method much neater proof differentiation recommended method since slightly easier differentiate log temporarily define measured using natural logarithms thus scaling factor log maximize subject constraint enforced lagrange multiplier maximum equal extremum indeed maximum established finding curvature negative definite copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links probability entropy inference proof using jensen inequality recommended method first reminder inequality convex function random variable strictly convex random variable constant probability secret proof using jensen inequality choose right func tion right random variable could define log log convex function think log mean would get would give inequality wrong direction instead define find know exercise log equality holds random variable constant means constant solution exercise log prove gibbs inequality using jensen inequality let log log equality constant second solution proof expectations respect probability distribution second solution method uses jensen inequality instead define log let log equality constant copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links solutions solution exercise solution exercise probability tails one head get first head xth toss first toss tail probability distribution future looks like made first toss thus recursive expression entropy rearranging solution exercise probability number tails expected number heads definition problem expected number tails may shown variety ways example since situation one tail thrown equivalent opening situation write recurrence relation probability distribution estimator given plotted figure probability simply probability corresponding value figure probability distribution estimator given solution exercise mean number rolls one six next six six assuming start counting rolls first two sixes probability next six occurs rth roll probability getting six rolls multiplied probability getting six probability distribution number rolls may called exponential distribution since normalizing constant mean number rolls clock next six six mean number rolls going back time recent six six copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links probability entropy inference mean number rolls six clock struck six clock struck sum answers less one eleven rather explaining difference let give another hint imagine buses poissonville arrive indepen dently random poisson process average one bus every six minutes imagine passengers turn bus stops uniform rate scooped bus without delay interval tween two buses remains constant buses follow gaps bigger six minutes become overcrowded passengers representative com plains two thirds passengers found overcrowded buses bus operator claims one third buses overcrowded claims true figure probability distribution number rolls one next falling solid line probability distribution dashed line number rolls next tot tot probability probability tot mean mean tot solution exercise binomial distribution method solution exercise sum rule method marginal probabilities eight values illustrated posterior probabilities represented probabilities error representative cases thus error error notice average probability error probability given particular bit wrong either average error probability using sum rule error error first two terms cases remaining outcomes share probability occurring identical error probability error solution exercise entropy bits per word 
[probability, entropy, inference, expectations, entropies] probably familiar idea computing expectation function maybe comfortable computing expectation cases function depends probability next amples address concern exercise let let exercise arbitrary ensemble exercise let let exercise let proba bility log exercise prove assertion log equal ity iff denotes number elements set hint use jensen inequality first attempt use jensen succeed remember jensen involves random variable function quite lot freedom choosing think whether chosen function convex concave exercise prove relative entropy equation satisfies gibbs inequality equality exercise prove entropy indeed decomposable described equations copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links probability entropy inference exercise random variable selected flipping bent coin bias determine whether outcome either flipping second bent coin bias third bent coin bias respectively write probability distribution use decomposability entropy find entropy notice compact expression obtained make use binary entropy function compared writing four term entropy explicitly find derivative respect hint log exercise unbiased coin flipped one head thrown entropy random variable num ber flips repeat calculation case biased coin probability coming heads hint solve problem directly using decomposability entropy 
[probability, entropy, inference, forward, probability] exercise urn contains white balls black balls two balls drawn one without replacement prove probability first ball white equal probability second white exercise circular coin diameter thrown onto square grid whose squares probability coin lie entirely within one square ans exercise buffon needle needle length thrown onto plane covered equally spaced parallel lines separation probability needle cross line ans generalization buffon noodle average random curve length expected intersect lines times exercise two points selected random straight line segment length probability triangle constructed three resulting segments exercise unbiased coin flipped one head thrown expected number tails expected number heads fred know coin unbiased estimates bias using numbers heads tails tossed compute sketch probability distribution forward probability problem sampling theory problem inference problem use bayes theorem exercise fred rolls unbiased six sided die per second ing occasions outcome six mean number rolls one six next six two rolls clock strikes one mean number rolls next six copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exercises think back clock struck mean number rolls going back time recent six mean number rolls six clock struck next six answer different answer explain another version exercise refers fred waiting bus bus stop poissonville buses arrive independently random poisson process average one bus every six minutes average wait bus fred arrives stop minutes time two buses one fred missed one catches minutes explain apparent para dox note contrast situation clockville buses spaced exactly minutes apart confirm mean wait bus stop minutes time missed bus next one minutes 
[probability, entropy, inference, conditional, probability] exercise meet fred fred tells two brothers alf bob probability fred older bob fred tells older alf probability fred older bob conditional probability given exercise inhabitants island tell truth one third time lie probability occasion one made statement ask another statement true says yes probability statement indeed true exercise compare two ways computing probability error repetition code assuming binary symmetric channel exercise confirm give answer binomial distribution method add probability three bits flipped probability exactly two bits flipped sum rule method using sum rule compute marginal prob ability takes eight possible values compute posterior probabil ity eight values fact symmetry two example cases need consid ered notice inferred bits better determined equation gives posterior probability input given received vector others posterior probability read case case error probability probability probable hypothesis correct error find average error probability using sum rule error error copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links probability entropy inference exercise frequency nth frequent word english roughly approximated remarkable law known zipf law applies word frequencies many languages zipf assume english generated picking words random according distribution entropy english per word calculation found prediction entropy printed english shannon bell syst tech inexplicably great man made numerical errors 
[probability, entropy, inference, solutions] solution exercise independent conditional distributions would identical functions regardless figure solution exercise define fraction number black balls binomial distribution mean variance distribution var results derived example standard deviation var expectation variance standard deviation expectation variance standard deviation solution exercise numerator quantity recognized denominator equal variance definition expectation numerator expectation random variable like measures deviation data expected value sometimes called chi squared case var numerator five possible values one smaller probability probability copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links solutions solution exercise wish prove given property proceed recursion working right hand side proof handle cases details left pedantic reader first line use definition convexity second line forth solution exercise outcomes probabilities value one die mean variance sum one hundred mean variance central limit theorem probability distribution roughly gaussian confined integers mean variance order obtain sum uniform distribution start random variables spiky distribution probability mass concentrated extremes unique solution one ordinary die one faces yes uniform distribution created several ways example think uniform distribution contradict central limit theorem labelling rth die numbers solution exercise gives exp hyperbolic tangent tanh copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links probability entropy inference exp tanh case log repeat steps replacing obtain solution exercise log log log solution exercise conditional independence given means gives separation posterior probability ratio series factors one data point times prior probability ratio 
[inference] scratched head time education provided couple approaches solving inference problems construct ing estimators unknown parameters fitting model data processed version data since mean unconstrained exponential distribution seemed reasonable examine sample mean see estimator could obtained evident estimator would appropriate cases truncation distribution right hand side significant little ingenuity introduction hoc bins promising estimators could constructed obvious estimator would work conditions could find satisfactory approach based fitting density histogram derived data stuck general solution problem others like always necessary confronted new inference problem grope dark appropriate estimators worry finding best estimator whatever means copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links first inference problem lambda lambda lambda figure probability density function lambda lambda lambda figure probability density function three different values plotted way round function known likelihood marks indicate three values used preceding figure steve wrote probability one data point given otherwise seemed obvious enough wrote bayes theorem exp suddenly straightforward distribution defining probability data given hypothesis turned head define probability hypothesis given data simple figure showed probability single data point familiar function different values figure curve innocent exponential normalized area plotting function function fixed value something remarkable happens peak emerges figure help understand two points view one function figure shows surface plot function figure probability density function figures vertical sections surface dataset consisting several points six points likelihood function product functions figure figure likelihood function case six point dataset function copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links inference steve summarized bayes theorem embodying fact know data arrive knew data told probabilities used quantify degrees belief nip possible confusion bud must emphasized hypothesis cor rectly describes situation stochastic variable fact bayesian uses probability distribution mean thinks world stochastically changing nature states described different hypotheses uses notation probabilities represent beliefs mutually exclusive micro hypotheses values one actually true probabilities denote degrees belief given assumptions seemed reasonable posterior probability distribution represents unique com plete solution problem need invent estimators need invent criteria comparing alternative estimators whereas orthodox statisticians offer twenty ways solving problem twenty different criteria deciding solutions best bayesian statistics offers one answer well posed problem difficulty understanding chapter recommend ensuring happy exercises noting similarity exercise 
[inference, first, inference, problem] undergraduate cambridge privileged receive pervisions steve gull sitting desk dishevelled office john college asked one ought answer old tripos question exercise unstable particles emitted source decay distance real number exponential probability dis tribution characteristic length decay events served occur window extending decays observed locations 
[inference, assumptions, inference] inference conditional assumptions example prior critics view priors difficulty subjective see could otherwise one perform inference without making assumptions believe great value bayesian methods force one make tacit assumptions explicit first assumptions made inferences objective unique reproducible complete agreement anyone informa tion makes assumptions example given assumptions listed data everyone agree posterior prob ability decay length second assumptions explicit easier criticize easier modify indeed quantify sensitivity inferences details assumptions example note likelihood curves figure case single data point likelihood function less strongly peaked case details prior become increasingly important sample mean gets closer middle window case likelihood function peak data merely rule small values give information relative probabilities large values case details prior small end things important large end prior important third sure various alternative assumptions appropriate problem treat question another inference task thus given data compare alternative assumptions using bayes theorem copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links bent coin denotes highest assumptions questioning fourth take account uncertainty regarding assump tions make subsequent predictions rather choosing one partic ular assumption working predictions quantity obtain predictions take account uncertainty using sum rule another contrast orthodox statistics conventional test default model test accepts model significance level use exclusively model make predictions steve thus persuaded probability theory reaches parts hoc methods cannot reach let look examples simple inference problems 
[inference, bent, coin] bent coin tossed times observe sequence heads tails denote symbols wish know bias coin predict probability next toss result head first encountered task example encounter chapter discuss adaptive data compression also original inference problem studied thomas bayes essay published exercise assume uniform prior distribution obtain posterior distribution multiplying likelihood critic might object prior come claim uniform prior way fundamental indeed give examples nonuniform priors later prior subjective assumption one themes book inference data compression without making assumptions give name assumptions introducing ternative set assumptions moment probability given tosses result sequence contains counts two outcomes example aaba first model assumes uniform prior distribution inferring unknown parameters given string length interested inferring might predicting whether next character copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links inference predictions always expressed probabilities predicting whether next character computing probability next character assuming true posterior probability given string length counts bayes theorem factor function known likeli hood function given equation prior given equation inference thus normalizing constant given beta integral exercise sketch posterior probability aba probable value value maximizes posterior probability density mean value distribution answer questions posterior probability bbb inferences predictions prediction next toss probability next toss obtained integrating effect taking account uncertainty making predictions sum rule probability given simply known laplace rule 
[inference, bent, coin, model, comparison] imagine scientist introduces another theory data asserts source really bent coin really perfectly formed die one face painted heads five painted tails thus parameter original model could take value according new hypothesis free parameter rather equal hypothesis termed suffix model indicates number free parameters compare two models light data wish infer probable relative copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links bent coin model comparison model comparison inference order perform model comparison write bayes theorem time different argument left hand side wish know probable given data bayes theorem similarly posterior probability normalizing constant cases total proba bility getting observed data models consideration probability given sum rule evaluate posterior probabilities hypotheses need assign values prior probabilities case might set need evaluate data dependent terms give names quantities quantity measure much data favour call evidence model already encountered quantity equation appeared normalizing constant first inference made inference given data model comparison works evidence model usually normalizing constant earlier bayesian inference evaluated normalizing constant model evi dence model simple model parameters infer defining thus posterior probability ratio model model values posterior probability ratio illustrated table first five lines illustrate outcomes favour one model favour outcome completely incompatible either model small amounts data six tosses say typically case one two models overwhelmingly probable data evidence given data set ratio differing mounts predict advance much data needed pretty sure theory true depends simpler model since adjustable parameters able lose biggest margin odds may hundreds one complex model never lose large margin data set actually unlikely given model copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links inference data table outcome model comparison models bent coin model states true true figure typical behaviour evidence favour bent coin tosses accumulate three different conditions columns horizontal axis number tosses vertical axis left right hand vertical axis shows values three rows show independent simulated experiments see also figure exercise show tosses taken place biggest value log evidence ratio log scales linearly probable log evidence favour grow log exercise putting sampling theory hat assuming yet measured compute plausible range log evidence ratio might lie function true value sketch function hint sketch log evidence function random variable work mean standard deviation 
[inference, typical, behaviour, evidence] figure shows log evidence ratio function number tosses number simulated experiments left hand experiments true right hand ones true value either discuss model comparison later chapter copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links example legal evidence 
[inference, example, legal, evidence] following example illustrates bayesian inference priors two people left traces blood scene crime suspect oliver tested found type blood blood groups two traces found type common type local population frequency type rare type frequency data type blood found scene give evidence favour proposition oliver one two people present crime careless lawyer might claim fact suspect blood type found scene positive evidence theory present denote proposition suspect one unknown person present alternative states two unknown people population present prior problem prior probability ratio propositions quantity important final verdict would based available information case task evaluate contribution made data likelihood ratio view jury task generally multiply together carefully evaluated likelihood ratios independent piece admissible evidence equally carefully reasoned prior proba bility view shared many statisticians learned british appeal judges recently disagreed actually overturned verdict trial jurors taught use bayes theorem handle complicated dna evidence probability data given probability one unknown person drawn population blood type since given already know one trace type prob ability data given probability two unknown people drawn population types equations denotes assumptions two people present left blood probability distribution blood groups unknown people explanation population frequencies dividing obtain likelihood ratio thus data fact provide weak evidence supposition oliver present result may found surprising let examine various points view first consider case another suspect alberto type intuitively data provide evidence favour theory copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links inference suspect present relative null hypothesis indeed likelihood ratio case let change situation slightly imagine people blood type rest type two blood types exist population data scene consider data influence beliefs oliver suspect type alberto suspect type intuitively still believe presence rare blood provides positive evidence alberto fact type blood detected scene favour hypothesis oliver present case would mean regardless suspect data make probable present everyone population would greater suspicion would absurd data may compatible suspect either blood type present provide evidence theories must also provide evidence theories another way thinking imagine instead two people blood stains ten entire local population one hundred ninety type suspects ten type suspects consider particular type suspect oliver without information blood test results come one chance scene since know suspects present get results blood tests find nine ten stains type one stains type make likely oliver one ninety chance since know one person present type maybe intuition aided finally writing formulae general case blood stains individuals type found type total individuals unknown people come large population fractions may blood types task evaluate likelihood ratio two hypotheses type suspect oliver unknown others left stains unknowns left stains probability data hypothesis probability getting individuals two types individuals drawn random population case hypothesis need distribution indi viduals likelihood ratio instructive result likelihood ratio contribution data question whether oliver present depends simply comparison frequency blood type observed data background frequency population dependence counts types found scene frequencies population copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exercises type stains average number expected hypothesis data give evidence favour presence oliver conversely fewer type stains expected number data reduce probability hypothesis special case data contribute evidence either way regardless fact data compatible hypothesis 
[inference, exercises] exercise three doors normal rules game show contestant told rules follows three doors labelled single prize hidden behind one get select one door initially chosen door opened instead gameshow host open one two doors way reveal prize example first choose door open one doors guaranteed choose one open prize revealed point given fresh choice door either stick first choice switch closed door doors opened receive whatever behind final choice door imagine contestant chooses door first gameshow host opens door revealing nothing behind door promised contestant stick door switch door make difference exercise three doors earthquake scenario imagine game happens gameshow host open one doors violent earthquake rattles building one three doors flies open happens door happens prize behind contestant initially chosen door repositioning toup host suggests since chose door initially door valid door open according rules game let door stay open let carry nothing happened contestant stick door switch door make difference assume prize placed randomly gameshow host know door flew open latch broken earthquake similar alternative scenario gameshow whose confused host gets rules prize opens one unchosen doors random opens door prize revealed contestant choose behind door door opti mal decision contestant depend contestant beliefs whether gameshow host confused exercise another example emphasis priors visit family whose three children local school copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links inference know anything sexes children walking clum sily round home stumble one three unlabelled bedroom doors know belong one three children find bedroom contains girlie stuff sufficient quantities convince child lives bedroom girl later sneak look letter addressed parents reads headmaster sending letter parents male children school inform following boyish mat ters two sources evidence establish least one three children girl least one children boy probabilities two girls one boy two boys one girl exercise mrs found stabbed family garden behaves strangely death considered suspect investigation police social records found beaten wife least nine previous occasions prosecution advances data evidence favour hypothesis guilty murder says highly paid lawyer statistically one thousand wife beaters actually goes murder wife wife beating strong evidence fact given wife beating evidence alone extremely unlikely would murderer wife chance therefore find innocent lawyer right imply history wife beating point murderer lawyer slimy trickster latter wrong argument received indignant letter lawyer preceding paragraph like add extra inference exercise point suggestion lawyer may slimy trickster imply believe lawyers slimy tricksters answer exercise bag contains one counter known either white black white counter put bag shaken counter drawn proves white chance drawing white counter notice state bag operations exactly identical state exercise move new house phone connected pretty sure phone number sure would like experiment pick phone dial obtain busy signal sure phone number much exercise game two coins tossed either coins comes heads prize claim prize must point one coins head say look coin head watch fred play game tosses two coins estimated million women abused year partners women victims homicide women slain husbands boyfriends sources http www umn edu mincava papers factoid htm http www gunfree inter net vpc womenfs htm copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links solutions points coin says look coin head probability coin head exercise statistical statement appeared guardian friday january spun edge times belgian one euro coin came heads times tails looks suspicious said barry blight statistics lecturer london school economics coin unbiased chance getting result extreme would less data give evidence coin biased rather fair hint see equation 
[inference, solutions] solution exercise let data assuming equal prior probabilities solution exercise probability data given pothesis aba bbb figure posterior probability bias bent coin given two different data sets solution exercise aba probable value value maximizes posterior probability density mean value see figure copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links inference bbb probable value value maximizes posterior probability density mean value see figure true true figure range plausible values log evidence favour function vertical axis left log right hand vertical axis shows values solid line shows log evidence random variable takes mean value dotted lines show approximately log evidence percentile see also figure solution exercise curves figure found finding mean standard deviation setting mean two standard deviations get plausible range computing three corresponding values log evidence ratio solution exercise let denote hypothesis prize behind door make following assumptions three hypotheses equiprobable priori datum receive choosing door one mean ing door opened respectively assume two possible outcomes following probabilities prize behind door host free choice case assume host selects random otherwise choice host forced probabilities using bayes theorem evaluate posterior probabilities hypotheses denominator normalizing constant posterior distribution contestant switch door order biggest chance getting prize many people find outcome surprising two ways make intuitive one play game thirty times friend keep track frequency switching gets prize alternatively perform thought experiment game played million doors rules contestant chooses one door game copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links solutions show host opens doors way reveal prize leaving contestant selected door one door closed contestant may stick switch imagine contestant confronted million doors doors opened door contestant initial guess think prize solution exercise door opened earthquake inference comes differently even though visually scene looks nature data probability data different possible data outcomes firstly number doors might opened could label eight possible outcomes secondly might prize visible earthquake opened one doors data consists value statement whether prize revealed hard say probabilities outcomes since depend beliefs reliability door latches properties earthquakes possible extract desired posterior probability without naming values matters relative values quantities value actually occurred likelihood principle met section value actually occurred prize visible first clear since datum prize visible incompatible assuming contestant selected door probability compare assuming earthquakes sensitive decisions game show contestants two quantities equal symmetry know likely door falls hinges however likely likely whether prize behind door door equal obtain two possible hypotheses equally likely assume host knows prize might acting deceptively answer might modified view host words part data confused well worth making sure understand two gameshow problems worry slipped second problem first time met general rule helps immensely confusing probability problem always write probability everything steve gull joint probability desired inference mechanically tained figure prize door door door none none none none ors ened rthquak figure probability everything second three door problem assuming earthquake occurred probability door alone opened earthquake solution exercise statistic quoted lawyer indicates probability randomly selected wife beater also murder wife probability husband murderer given wife murdered completely different quantity copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links inference deduce latter need make assumptions probability wife murdered someone else lives neigh bourhood frequent random murders probability large posterior probability husband absence idence may large peaceful regions may well likely person murdered found murdered one closest relatives let work illustrative numbers help statistics page let denote proposition woman mur dered proposition husband propo sition beat year preceding murder statement someone else denoted need define order compute pos terior probability statistics read two million women million beaten finally need value man murders wife likely first time laid finger expect pretty unlikely maybe larger bayes theorem one way make obvious sliminess lawyer construct arguments logical structure clearly wrong example lawyer could say mrs murdered murdered statistically one million wife beaters actually goes murder wife wife beating strong evidence fact given wife beating evidence alone extremely unlikely would murder wife way chance solution exercise two hypotheses number another number data dialed got busy signal probability given hypothesis number expect busy signal certainty hand true probability number dialled returns busy signal smaller since various outcomes also possible ringing tone number unobtainable signal example value probability depend probability random phone number similar phone number would valid phone number probability get busy signal dial valid phone number estimate size phone book cambridge valid phone numbers length six digits probability random six digit number valid therefore exclude numbers beginning random choice probability assume telephone numbers clustered misremembered number might likely valid randomly chosen number probability guessed number would valid assuming true might bigger copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links solutions anyway must somewhere carry forward uncertainty probability see much matters end probability get busy signal dial valid phone number equal fraction phones think use hook make tentative call fraction varies town town time day cambridge day would guess phones use maybe fewer probability product according estimates one thousand chance getting busy signal dial random number one hundred valid numbers strongly clustered one dial wee hours data affect beliefs phone number pos terior probability ratio likelihood ratio times prior probability ratio likelihood ratio posterior probability ratio swung factor favour prior probability posterior probability solution exercise compare models coin fair coin biased prior bias set uniform distribution use uniform prior seems reasonable figure probability distribution number heads given two hypotheses coin fair biased prior distribution bias uniform outcome heads gives weak evidence favour hypothesis coin fair since know coins american pennies severe biases spun edge situations would surprise mention coin fair pedant would say absurd even consider coin fair coin surely biased extent course would agree pedants kindly understand meaning coin fair within one part thousand likelihood ratio thus data give scarcely evidence either way fact give weak evidence two one favour objects believer bias silly uniform prior represent prior beliefs bias biased coins expecting small bias generous possible let see well could fare prior presciently set let allow prior form beta distribution original uniform prior reproduced setting tweaking likelihood ratio copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links inference increased little shown several values figure figure likelihood ratio various choices prior distribution hyperparameter even favourable choice yield likelihood ratio two one favour conclusion data suspicious construed giving two one evidence favour one two hypotheses wimpy likelihood ratios fault restrictive priors way producing suspicious conclusion prior best matched data terms likelihood prior sets probability one let call model likelihood ratio strongest evidence data possibly muster hypothesis bias six one noticing absurdly misleading answers sampling ory statistics produces value exercise solved let stick boot make tiny change data set increasing number heads tosses find value goes mystical value value sampling theory statistician would happily squeak probability getting result extreme heads smaller thus reject null hypothesis significance level correct answer shown several values figure values worth highlighting table first likelihood ratio uses standard uniform prior favour null hypothesis second favourable choice point view yield likelihood ratio favour figure likelihood ratio various choices prior distribution hyperparameter data heads trials warned value often interpreted implying odds stacked twenty one null hypothesis truth case evidence either slightly favours null hypothesis disfavours one depending choice prior values significance levels classical statistics treated extreme caution shun ends sermon 
[source, coding, theorem, measure, information, content, random, variable?] next chapters talking probability distributions random variables time get sloppy notation occasionally need precise notation notation established chapter ensemble triple outcome value random variable takes one set possible values probabilities measure information content outcome ensemble chapter examine assertions shannon information content log sensible measure information content outcome entropy ensemble log sensible measure ensemble average information content log figure shannon information content log binary entropy function log log function figure shows shannon information content outcome prob ability function less probable outcome greater shannon information content figure also shows binary entropy function log log entropy ensemble whose alphabet probability dis tribution copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links source coding theorem 
[source, coding, theorem, information, content, independent, random, variables] log anything information content function explore question detail shortly first notice nice property particular function log imagine learning value two independent random variables definition independence probability distribution separable product intuitively might want measure amount information gained property additivity independent random variables information gained learn equal sum information gained alone learned information gained alone learned shannon information content outcome log log log log indeed satisfy independent exercise show independent entropy outcome satisfies words entropy additive independent variables explore ideas examples section chapters prove shannon information content entropy related number bits needed describe outcome experiment 
[source, coding, theorem, weighing, problem, designing, informative, experiments] solved weighing problem exercise yet sure notice three uses balance reads either left heavier right heavier balanced number conceivable outcomes whereas number possible states world odd ball could twelve balls could heavy light principle problem might solvable three weighings two since know determine odd weight whether heavy light three weighings may read found strategy always gets three weighings encourage think exercise strategy optimal series weighings allows useful information gained quickly possible answer step optimal procedure three outcomes left heavier right heavier balance close possible equiprobable optimal solution shown figure suboptimal strategies weighing balls first step achieve outcomes equal probability two sets balls never balance possible outcomes left heavy right heavy binary outcome rules half possible hypotheses copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links measure information content random variable figure optimal solution weighing problem step two boxes left box shows hypotheses still possible right box shows balls involved next weighing hypotheses written denoting odd ball heavy weighings written listing names balls two pans separated line example first weighing balls put left hand side right triplet arrows upper arrow leads situation left side heavier middle arrow situation right side heavier lower arrow situation outcome balanced three points labelled correspond impossible outcomes weigh bbn weigh weigh weigh copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links source coding theorem strategy uses outcomes must sometimes take longer find right answer insight outcomes near possible equiprobable makes easier search optimal strategy first weighing must divide possible hypotheses three groups eight second weighing must chosen split hypotheses thus might conclude outcome random experiment guaranteed formative probability distribution outcomes uniform conclusion agrees property entropy proved solved exercise entropy ensemble biggest outcomes equal probability 
[source, coding, theorem, guessing, games] game twenty questions one player thinks object player attempts guess object asking questions yes answers example alive human aim identify object questions possible best strategy playing game simplicity imagine playing rather dull version twenty questions called sixty three example game sixty three smallest number yes questions needed identify integer intuitively best questions successively divide possibilities equal sized sets six questions suffice one reasonable strategy asks following questions mod mod mod mod mod notation mod pronounced modulo denotes remainder divided example mod mod answers questions translated yes give binary expansion example shannon information contents outcomes ample assume values equally likely answers questions independent shannon information content log bit total shannon information gained always six bits furthermore number learn questions six bit nary number questioning strategy defines way encoding random variable binary file far shannon information content makes sense measures length binary file encodes however yet studied ensembles outcomes unequal probabilities shannon information content make sense copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links measure information content random variable move question outcome total info figure game submarine submarine hit attempt 
[source, coding, theorem, game, submarine, many, bits, one, bit, convey?] game battleships player hides fleet ships sea represented square grid turn one player attempts hit ships firing one square opponent sea response selected square either miss hit hit destroyed boring version battleships called submarine player hides one submarine one square eight eight grid figure shows pictures game progress circle represents square fired show squares outcome miss submarine hit outcome shown symbol attempt shot made player defines ensemble two possible comes corresponding hit miss probabili ties depend state board beginning second shot first shot missed third shot first two shots missed shannon information gained outcome log lucky hit submarine first shot log bits might seem little strange one binary outcome convey six bits learnt hiding place could squares one lucky binary question indeed learnt six bits first shot misses shannon information gain outcome log bits make sense obvious let keep going second shot also misses shannon information content second outcome log bits miss thirty two times firing new square time total non information gained log log log bits copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links source coding theorem round number well learnt know submarine squares fired learning fact like playing game sixty three asking first question one thirty two numbers corresponding squares fired receiving answer answer rules half hypotheses gives one bit unsuccessful shots information gained bits unknown location narrowed one quarter original hypothesis space hit submarine shot squares left shannon information content outcome log bits total shannon information content outcomes log log log log bits know submarine total shannon information con tent gained bits result holds regardless hit submarine hit squares left choose equation total information gained log log log log log log bits learned examples far think submarine example makes quite convincing case claim shannon infor mation content sensible measure information content game sixty three shows shannon information content intimately connected size file encodes outcomes random experi ment thus suggesting possible connection data compression case convinced let look one example 
[source, coding, theorem, wenglish, language] wenglish language similar english wenglish sentences consist words drawn random wenglish dictionary contains words length characters word wenglish dictionary constructed random picking five letters probability distribution depicted figure aaail aaaiu aaald abati azpan aztdn odrcr zatnt zxast figure wenglish dictionary entries dictionary shown alphabetical order fig ure notice number words dictionary much smaller total number possible words length letters probability letter words dictionary begin letter contrast probability letter words begin letter words two start start let imagine reading wenglish document let discuss shannon information content characters acquire copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links data compression given text one word time shannon information content five character word log bits since wenglish uses words equal probability average information content per character therefore bits let look information content read document one character time say first letter word shannon information content log bits first letter shannon information content log bits information content thus highly variable first character total information content characters word however exactly bits letters follow initial lower average information content per character letters follow initial rare initial letter indeed conveys information word common initial letter similarly english rare characters occur start word xyl often identify whole word immediately whereas words start common characters pro require charac ters identify 
[source, coding, theorem, data, compression] preceding examples justify idea shannon information content outcome natural measure information content improbable comes convey information probable outcomes discuss information content source considering many bits needed describe outcome experiment show compress data particular source file bits per source symbol recover data reliably say average information content source bits per symbol 
[source, coding, theorem, example, compression, text, files] file composed sequence bytes byte composed bits use word bit meaning symbol two values confused unit information content decimal value typical text file composed ascii character set decimal values character set uses seven eight bits byte exercise much could size file reduced given ascii file would achieve reduction intuitively seems reasonable assert ascii file contains much information arbitrary file size since already know one every eight bits even look file simple ample redundancy sources data redundancy english text files use ascii characters non equal frequency certain pairs letters probable others entire words predicted given context semantic understanding text 
[source, coding, theorem, simple, data, compression, methods, define, measures, informa-, tion, content] one way measuring information content random variable simply count number possible outcomes number elements set denoted gave binary name outcome copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links source coding theorem length name would log bits happened power thus make following definition raw bit content log lower bound number binary questions always guaranteed identify outcome ensemble additive quantity raw bit content ordered pair possible outcomes satisfies measure information content include probabilistic element encoding rule corresponds compress source data simply maps outcome constant length binary string exercise could compressor maps outcome binary code decompressor maps back every possible outcome compressed binary code length shorter bits even though simple counting argument shows impossible make reversible compression program reduces size files ama teur compression enthusiasts frequently announce invented program indeed compress com pressed files putting compressor several times stranger yet patents granted modern day alchemists see comp compression frequently asked questions reading two ways compressor actually compress files lossy compressor compresses files maps files encoding assume user requires perfect recovery source file occurrence one confusable files leads failure though applications image compression lossy compression viewed satisfactory denote probability source string one confusable files lossy compressor probability failure made small lossy compressor may practically useful lossless compressor maps files different encodings shortens files necessarily makes others longer try design compressor probability file lengthened small probability shortened large chapter discuss simple lossy compressor subsequent chapters discuss lossless compression methods 
[source, coding, theorem, information, content, defined, terms, lossy, compression] whichever type compressor construct need somehow take account probabilities different outcomes imagine comparing information contents two text files one ascii characters http sunsite org public usenet news faqs comp compression copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links information content defined terms lossy compression used equal probability one characters used frequencies english text define measure information content distinguishes two files intuitively latter file contains less information per character predictable one simple way use knowledge symbols smaller probability imagine recoding observations smaller alphabet thus losing ability encode improbable symbols measuring raw bit content new alphabet example might take risk compressing english text guessing infrequent characters occur make reduced ascii code omits characters thereby reducing size alphabet seventeen larger risk willing take smaller final alphabet becomes introduce parameter describes risk taking using compression method probability name outcome example let raw bit content ensemble bits corresponding binary names notice willing run risk name get four names half many names needed every name table shows binary names could given different comes cases need bits encode outcome need bits table binary names outcomes two failure probabilities let formalize idea make compression strategy risk make smallest possible subset probability less equal value define new measure information content log size smallest subset ensembles several elements probability may several smallest subsets contain different elements matters sizes equal dwell ambiguity smallest sufficient subset smallest subset satisfying subset constructed ranking elements order decreasing probability adding successive elements starting probable elements total probability make data compression code assigning binary name element smallest sufficient subset compression scheme motivates following measure information content essential bit content log note special case caution confuse function displayed figure figure shows ensemble example function copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links source coding theorem log figure outcomes example ranked probability essential bit content labels graph show smallest sufficient set function note bits bits 
[source, coding, theorem, extended, ensembles] compression method useful compress blocks symbols source turn examples outcome string independent identically distributed random variables single ensemble denote ensemble remem ber entropy additive independent variables exercise example consider string flips bent coin probabilities prob able strings number evaluate must find smallest sufficient subset subset contain max max figures show graphs cases steps values changes cusps slope staircase changes points max changes exercise mathematical shapes curves cusps examples shown figures depends strongly value might seem fundamental useful definition information content consider happens number independent variables increases find remarkable result becomes almost independent close entropy one random variables figure illustrates asymptotic tendency binary ensemble example increases becomes increasingly flat function copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links information content defined terms lossy compression log figure sixteen outcomes ensemble ranked probability essential bit content upper schematic diagram indicates strings probabilities vertical lines lengths scale figure binary variables figure binary variables copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links source coding theorem log figure top strings samples bottom two least probable strings ensemble final column shows log probabilities random strings may compared entropy bits except tails close long allowed tiny probability error compression bits possible even allowed large probability error still compress bits source coding theorem theorem shannon source coding theorem let ensemble entropy bits given exists positive integer 
[source, coding, theorem, typicality] increasing help let examine long strings table shows fifteen samples probability string contains number strings contain number binomial distribution functions shown figure mean standard deviation copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links typicality figure anatomy typical set graphs show number strings containing probability single string contains probability log scale total probability strings contain number horizontal axis plot log also shows dotted line mean value log equals typical set includes strings log close value range marked shows set defined section left right log copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links source coding theorem notice gets bigger probability distribution becomes concentrated sense range possible values grows standard deviation grows likely fall small range values implies outcome also likely fall corresponding small subset outcomes call typical set 
[source, coding, theorem, definition, typical, set] let define typicality arbitrary ensemble alphabet definition typical string involve string probability long string symbols usually contain occurrences first symbol occurrences second etc hence probability string roughly typ information content typical string log log random variable log information content likely close value build definition typicality observation define typical elements elements prob ability close note typical set unlike smallest sufficient subset include probable elements show probable elements contribute negligible probability introduce parameter defines close probability element typical call set typical elements typical set log show whatever value choose typical set contains almost probability increases important result sometimes called asymptotic equipartition principle asymptotic equipartition principle ensemble independent identically distributed random variables sufficiently large outcome almost certain belong subset members probability close notice tiny fraction number possible outcomes term equipartition chosen describe idea members typical set roughly equal probability taken literally hence use quotes around asymptotic equipartition see page second meaning equipartition thermal physics idea degree freedom classical system equal average energy second meaning intended copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links proofs log figure schematic diagram showing strings ensemble ranked probability typical set asymptotic equipartition principle equivalent shannon source coding theorem verbal statement ran dom variables entropy compressed bits negligible risk information loss conversely compressed fewer bits vir tually certain information lost two theorems equivalent define compression algo rithm gives distinct name length bits typical set 
[source, coding, theorem, proofs] section may skipped found tough going 
[source, coding, theorem, law, large, numbers] proof source coding theorem uses law large numbers mean variance real random variable var technical note strictly assuming function sample finite discrete ensemble summations written means finite sum delta functions restriction guarantees mean variance exist necessarily case general chebyshev inequality let non negative real random variable let positive real number proof multiply term obtain add non negative missing terms obtain copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links source coding theorem chebyshev inequality let random variable let positive real number proof take apply previous proposition weak law large numbers take average independent random variables common mean common vari ance proof obtained showing interested close mean small matter large matter small required matter small desired probability always achieve taking large enough 
[source, coding, theorem, proof, theoremp)] apply law large numbers random variable log defined drawn ensemble random variable written average information contents log random variable mean variance var log term shannon information content nth outcome define typical set parameters thus log probability satisfies law large numbers thus proved asymptotic equipartition principle increases probability falls approaches result relate source coding must relate show given sufficiently big part set best subset compression size gives upper bound show small must calculating big could possibly free set convenient value smallest possible probability member total probability contained bigger size typical set bounded set set becomes witness fact log figure schematic illustration two parts theorem given show large enough lies line line copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links comments part imagine someone claims second part smallest sufficient subset smaller inequality would allow make use typical set show must mistaken remember free set value choose set task prove subset achieving cannot exist greater specify let consider probability falling rival smaller subset probability subset amp amp cco denotes complement maximum value first term found contains outcomes maximum probability maximum value second term set shows cannot satisfy definition sufficient subset thus subset size probability less definition thus large enough function essentially constant function illustrated figures 
[source, coding, theorem, comments] source coding theorem two parts results interesting first part tells even probability error extremely small number bits per symbol needed specify long symbol string vanishingly small error probability exceed bits need tiny tolerance error number bits required drops significantly happens yet tolerant compression errors part tells even close errors made time average number bits per symbol needed specify must still least bits two extremes tell regardless specific allowance error number bits per symbol needed specify bits less 
[source, coding, theorem, caveat, regarding, ‘asymptotic, equipartition’] put words asymptotic equipartition quotes important think elements typical set really roughly probability similar probability sense values log within decreased increase keep bound mass typical set constant must grow write terms constant copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links source coding theorem probable string typical set order times greater least probable string typical set decreases increases ratio grows exponentially thus equipartition weak sense 
[source, coding, theorem, introduce, typical, set?] best choice subset block compression definition typical set bother introducing typical set answer count typical set know elements almost iden tical probability know whole set probability almost typical set must roughly elements without help typical set similar would hard count many elements 
[source, coding, theorem, weighing, problems] exercise people first encounter weighing problem balls three outcome balance exercise think weighing six balls six balls good first weighing others say weighing six six conveys informa tion explain second group right wrong compute information gained odd ball information gained odd ball whether heavy light exercise solve weighing problem case balls one known odd exercise given balls equal weight except one either heavier lighter also given bizarre two pan balance report two outcomes two sides balance two sides balance design strategy determine odd ball uses balance possible exercise two pan balance job weigh bags flour integer weights pounds inclusive many weights need allowed put weights either pan allowed put one flour bag balance time exercise possible solve exercise weigh ing problem balls three outcome balance using sequence three fixed weighings balls chosen second weighing depend outcome first third weighing depend first second find solution general ball weighing problem exactly one balls odd show weighings odd ball identified among balls exercise given balls three outcome balance exer cise time two balls odd odd ball may heavy light know want identify odd balls direction odd copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exercises estimate many weighings required optimal strategy three odd balls answers change known regular balls weigh light balls weigh heavy ones weigh 
[source, coding, theorem, source, coding, lossy, compressor, loss] exercise let sketch function exercise let sketch function exercise physics students discuss relationship proof asymptotic equipartition principle equivalence large systems boltzmann entropy gibbs entropy 
[source, coding, theorem, distributions, don’t, obey, law, large, numbers] law large numbers used chapter shows mean set random variables probability distribution becomes narrower width increases however proved property discrete random variables real numbers taking finite set possible values many random variables continuous probability distributions also satisfy law large numbers important distributions continuous distributions mean variance exercise sketch cauchy distribution normalizing constant evaluate mean variance consider sum independent random variables cauchy distribution prob ability distribution mean probability distribution mean samples cauchy distribution 
[source, coding, theorem, asymptotic, properties] exercise chernoff bound derived weak law large numbers chebyshev inequality letting random variable inequality function random variable interested useful inequalities obtained using functions chernoff bound useful bounding tails distribution obtained letting exp show copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links source coding theorem moment generating function 
[source, coding, theorem, curious, functions, related, log, /p] exercise exercise purpose included enjoyment like mathematical curiosities sketch function hint work inverse function function closely related log 
[source, coding, theorem, solutions] solution exercise let log log log log log solution exercise ascii file reduced size factor reduction could achieved block code maps byte blocks byte blocks copying information carrying bits bytes ignoring last bit every character solution exercise pigeon hole principle states put pigeons holes without using one holes twice similarly give outcomes unique binary names length shorter log bits binary names log implies least two different inputs compressor would compress output file solution exercise cusps changes proba bility equal number elements changes one step varies logarithmically solution exercise solution found dyson lyness presented following elegant form john conway warned symbols used name balls name pans balance name outcomes name possible states odd ball label balls sequences aab aba abb abc bbc bca bcb bcc caa cab cac cca copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links solutions aab aba abb abc bbc bca bcb bcc weighings put aab caa cab cac pan aba abb abc bbc pan aba bca caa cca aab abb bcb cab given weighing pan either end canonical position assumes pans balanced position three weighings determine pan sequence three letters sequences ccc odd ball otherwise one two pans sequence among names odd ball whose weight proper one according pan weighings odd ball identified among balls way labelling non constant quences letters whose first change wth weighing putting whose wth letter pan whose wth letter pan solution exercise curves function shown figure note bits figure vertical axis horizontal binary variables solution exercise gibbs entropy runs states system entropy equivalent apart factor shannon entropy ensemble whereas gibbs entropy defined ensemble boltz mann entropy defined microcanonical ensembles probability distribution uniform set accessible states boltzmann entropy defined number cessible states microcanonical ensemble equivalent apart factor perfect information content constrained ensemble gibbs entropy microcanonical ensemble trivially equal boltzmann entropy copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links source coding theorem consider thermal distribution canonical ensemble probability state exp canonical ensemble associate corresponding microcanonical ensemble ensemble total energy fixed mean energy canonical ensemble fixed within precision fixing total energy precision equivalent fixing value within xfk definition typical set precisely consisted elements value log close mean value log canonical ensemble thus microcanonical ensemble equivalent uniform distribution typical set canonical ensemble proof asymptotic equipartition principle thus proves case system whose energy separable sum independent terms boltzmann entropy microcanonical ensemble close large gibbs entropy canonical ensemble energy microcanonical ensemble constrained equal mean energy canonical ensemble solution exercise normalizing constant cauchy dis tribution tan mean variance distribution undefined distribu tion symmetrical zero imply mean zero mean value divergent integral sum cauchy distributions probability density given convolution considerable labour using standard methods gives recognize cauchy distribution width parameter original distribution width parameter implies mean two points cauchy distribution width parameter generalizing mean samples cauchy distribution cauchy distributed parameters individual samples probability distribution mean become narrower central limit theorem apply cauchy distribution cause finite variance alternative neat method getting equation makes use fourier transform cauchy distribution biexponential convolution real space corresponds multiplication fourier space fourier transform simply reversing transform obtain equation copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links solutions solution exercise function inverse function figure shown three different scales note log log obtained tentative graph plotting along vertical axis along horizontal axis resulting graph suggests single valued looks surprisingly well behaved ordinary two valued equal infinite however might argued approach sketching partly valid define limit sequence functions sequence limit account pitchfork bifurcation sequence limit single valued lower two values sketched figure 
[symbol, codes] one might try roughly split set two continue bisecting subsets define binary tree root construction right spirit weighing problem necessarily optimal achieves copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links optimal source coding symbol codes huffman coding 
[symbol, codes, symbol, codes] binary symbol code ensemble mapping range denote codeword cor responding denote length extended code mapping obtained concatenation without punctuation corresponding codewords term mapping synonym function example symbol code ensemble defined shown margin using extended code may encode acdbac acdbac basic requirements useful symbol code first encoded string must unique decoding second symbol code must easy decode third code achieve much compression possible 
[symbol, codes, encoded, string, must, unique, decoding] code uniquely decodeable extended code two distinct strings encoding code defined example uniquely decodeable code 
[symbol, codes, symbol, code, must, easy, decode] symbol code easiest decode possible identify end codeword soon arrives means codeword prefix another codeword word prefix another word exists tail string concatenation identical example prefix show later lose performance constrain symbol code prefix code symbol code called prefix code codeword prefix codeword prefix code also known instantaneous self punctuating code encoded string decoded left right without looking ahead subsequent codewords end codeword mediately recognizable prefix code uniquely decodeable prefix codes also known prefix free codes prefix condition codes prefix codes correspond trees illustrated margin next page copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links symbol codes example code prefix code prefix prefix example let code prefix code prefix example code prefix code prefix codes represented binary trees complete prefix codes correspond binary trees unused branches incomplete code example code prefix code exercise uniquely decodeable example consider exercise figure weighing strategy identifies odd ball whether heavy light viewed assigning ternary code possible states code prefix code 
[symbol, codes, code, achieve, much, compression, possible] expected length symbol code ensemble may also write quantity example let consider code entropy bits expected length code also bits sequence symbols acdbac encoded prefix code therefore uniquely decodeable notice codeword lengths satisfy log equivalently example consider fixed length code ensemble expected length bits example consider expected length bits less code uniquely decodeable quence acdbac encodes also decoded cabdca example consider code expected length code bits sequence symbols acdbac encoded prefix code prefix copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links symbol codes uniquely decodeable obvious think might uniquely decodeable try prove finding pair strings encoding definition unique decodeability given equation certainly easy decode receive possible could start received second symbol still ambiguous could abd acd eventually unique decoding crystallizes next appears encoded stream fact uniquely decodeable comparing prefix code see codewords reverse uniquely decodeable proves since string identical string read backwards 
[symbol, codes, limit, imposed, unique, decodeability?] ask given list positive integers exist uniquely decodeable code integers codeword lengths stage ignore probabilities different symbols understand unique decodeability better reintroduce probabilities discuss make optimal uniquely decodeable symbol code examples observed take code shorten one codewords example retain unique decodeability lengthen codewords thus seems constrained budget spend codewords shorter codewords expensive let explore nature budget build code purely codewords length equal three many codewords retain unique decodeability answer chosen eight codewords way could add code another codeword length retain unique decodeability would seem make code includes length one codeword codewords length three many length three codewords restrict attention prefix codes four codewords length three namely codes way choosing codewords length give codewords intuitively think unlikely codeword length appears cost times smaller codeword length let define total budget size spend codewords set cost codeword whose length pricing system fits examples discussed codewords length cost codewords length cost spend budget codewords budget code certainly uniquely decodeable hand code may uniquely decodeable inequality kraft equality kraft inequality uniquely decodeable code binary copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links limit imposed unique decodeability alphabet codeword lengths must satisfy completeness uniquely decodeable code satisfies kraft inequality equality called complete code want codes uniquely decodeable prefix codes uniquely codeable easy decode life would simpler could restrict attention prefix codes fortunately source timal symbol code also prefix code kraft inequality prefix codes given set codeword lengths satisfy kraft inequality exists uniquely decodeable prefix code codeword lengths kraft inequality might accurately referred kraft mcmillan inequality kraft proved inequality satisfied prefix code exists given lengths mcmillan proved con verse unique decodeability implies inequality holds proof kraft inequality define consider quantity quantity exponent length encoding string every string length one term sum introduce array counts many strings encoded length defining min min max max max min assume uniquely decodeable concentrate encoded length total distinct bit strings length must case max min max min max thus max greater increases would exponentially growing function large enough exponential always exceeds polynomial max result max true therefore exercise prove result stated set code word lengths satisfying kraft inequality prefix code lengths copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links symbol codes total symbol code budget figure symbol coding budget cost codeword length indicated size box written total budget available making uniquely decodeable code think diagram showing codeword supermarket codewords arranged aisles length cost codeword indicated size box shelf cost codewords take exceeds budget code uniquely decodeable figure selections codewords made codes section copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links compression hope pictorial view kraft inequality may help solve exercise imagine choosing codewords make symbol code draw set candidate codewords supermarket displays cost codeword area box figure total budget available right hand side kraft inequality shown one side codes discussed section illustrated figure notice codes prefix codes property right selected codeword selected codewords prefix codes correspond trees notice complete prefix code corresponds complete tree unused branches ready put back symbols probabilities given set symbol probabilities english language probabilities figure example make best symbol code one smallest possible expected length smallest possible expected length obvious assign codeword lengths give short codewords probable symbols expected length might reduced hand shortening codewords necessarily causes others lengthen kraft inequality 
[symbol, codes, what’s, compression, hope, for?] wish minimize expected length code might guessed entropy appears lower bound expected length code lower bound expected length expected length uniquely decodeable code bounded proof define implicit probabilities log log use gibbs inequality log log equality kraft inequality log log log log equality achieved kraft equality satisfied codelengths satisfy log important result let say optimal source codelengths expected length minimized equal codelengths equal shannon formation contents log implicit probabilities defined codelengths conversely choice codelengths implicitly defines probability distribution codelengths would optimal codelengths code complete implicit probabilities given copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links symbol codes 
[symbol, codes, much, compress?] compress entropy close expect get entropy theorem source coding theorem symbol codes ensemble exists prefix code expected length satisfying proof set codelengths integers slightly larger optimum lengths dlog denotes smallest integer greater equal asserting optimal code necessarily uses lengths simply choosing lengths use prove theorem check prefix code lengths confirming kraft inequality satisfied dlog log confirm dlog log 
[symbol, codes, cost, using, wrong, codelengths] use code whose lengths equal optimal codelengths average message length larger entropy true probabilities use complete code lengths view lengths defining implicit probabilities con tinuing equation average length log exceeds entropy relative entropy defined 
[symbol, codes, optimal, source, coding, symbol, codes, huffman, coding] given set probabilities design optimal prefix code example best symbol code english language ensemble shown figure say optimal let assume aim figure ensemble need symbol code minimize expected length 
[symbol, codes, huffman, coding, algorithm] present beautifully simple algorithm finding optimal prefix code trick construct code backwards starting tails codewords build binary tree leaves algorithm huffman coding algorithm take two least probable symbols alphabet two symbols given longest codewords equal length differ last digit combine two symbols single symbol repeat since step reduces size alphabet one algorithm assigned strings symbols steps example let step step step step codewords obtained concatenating binary digits reverse order codelengths selected table code created huffman algorithm huffman algorithm column table cases longer cases shorter ideal codelengths shannon information contents log column expected length code bits whereas entropy bits point one way selecting two least probable symbols choice may made manner expected length code depend choice exercise prove better symbol code source huffman code example make huffman code probability distribution alphabet introduced figure result shown fig ure code expected length bits entropy ensemble bits observe disparities assigned codelengths ideal codelengths log 
[symbol, codes, constructing, binary, tree, top-down, suboptimal] previous chapters studied weighing problems built ternary binary trees noticed balanced trees ones every step two possible outcomes close possible equiprobable appeared describe efficient experiments gave intuitive motivation entropy measure information content copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links symbol codes log figure huffman code english language ensemble monogram statistics case however optimal codes always constructed greedy top method alphabet successively divided subsets near possible equiprobable example find optimal binary symbol code ensemble notice greedy top method split set two sub sets probability divided subsets prob ability greedy top method gives code shown third column table expected length huffman greedy huffman table greedily constructed code compared huffman code coding algorithm yields code shown fourth column expected length 
[symbol, codes, disadvantages, huffman, code] huffman algorithm produces optimal symbol code ensemble end story word ensemble phrase symbol code need careful attention 
[symbol, codes, changing, ensemble] wish communicate sequence outcomes one unchanging semble huffman code may convenient often appropriate copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links disadvantages huffman code ensemble changes example compressing text symbol frequencies vary context english letter much prob able figure furthermore knowledge context dependent symbol frequencies also change learn statistical properties text source huffman codes handle changing ensemble probabilities elegance one brute force approach would recompute huffman code every time probability symbols changes another attitude deny option adaptation instead run entire file advance compute good probability distribution remain fixed throughout transmission code must also communicated scenario technique cumbersome restrictive also suboptimal since initial message specifying code document partially redundant technique therefore wastes bits 
[symbol, codes, extra, bit] equally serious problem huffman codes innocuous looking tra bit relative ideal average length huffman code achieves length satisfies proved theorem huffman code thus incurs overhead bits per symbol large overhead would unimportant fractional increase many applications entropy may low one bit per symbol even smaller overhead may domi nate encoded file length consider english text contexts long strings characters may highly predictable example context strings_of_ch one might predict next nine symbols aracters_ probability traditional huffman code would obliged use least one bit per character making total cost nine bits virtually information conveyed bits total precise entropy english given good model one bit per character shannon huffman code likely highly inefficient traditional patch huffman codes uses compress blocks symbols example extended sources discussed chapter overhead per block bit overhead per symbol bits sufficiently large blocks problem extra bit may removed expenses losing elegant instantaneous decodeability simple huffman coding compute prob abilities relevant strings build associated huffman tree one end explicitly computing probabilities codes huge number strings never actually occur see exercise 
[symbol, codes, beyond, symbol, codes] huffman codes therefore although widely trumpeted optimal many defects practical purposes optimal symbol codes practi cal purposes want symbol code defects huffman codes rectified arithmetic coding dispenses restriction symbol must translate integer number bits arithmetic coding main topic next chapter copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links symbol codes 
[symbol, codes, summary] kraft inequality code uniquely decodeable lengths must satisfy lengths satisfying kraft inequality exists prefix code lengths optimal source codelengths ensemble equal shannon information contents log conversely choice codelengths defines implicit probabilities relative entropy measures many bits per symbol wasted using code whose implicit probabilities ensemble true probability distribution source coding theorem symbol codes ensemble ists prefix code whose expected length satisfies huffman coding algorithm generates optimal symbol code itera tively iteration two least probable symbols combined 
[symbol, codes, exercises] exercise code uniquely decodeable exercise ternary code uniquely decodeable exercise make huffman codes compute expected lengths com pare entropies repeat exercise exercise find probability distribution two optimal codes assign different lengths four symbols exercise continuation exercise assume four proba bilities ordered let set probability vectors two optimal codes different lengths give complete description find three probability vectors convex hull written positive copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exercises exercise write short essay discussing play game twenty questions optimally twenty questions one player thinks object player guess object using binary questions possible preferably fewer twenty exercise show probability equal integer power exists source code whose expected length equals entropy exercise make ensembles difference entropy expected length huffman code big possible exercise source alphabet eleven characters equal probability find optimal uniquely decodeable symbol code source much greater expected length optimal code entropy exercise consider optimal symbol code ensemble alphabet size symbols identical probability power show fraction symbols assigned codelengths equal dlog satisfies expected length optimal symbol code differentiating excess length respect show excess length bounded exercise consider sparse binary source dis cuss huffman codes could used compress source efficiently estimate many codewords proposed solutions require exercise scientific american carried following puzzle poisoned glass mathematicians curious birds police commissioner said wife see partly filled glasses lined rows table hotel kitchen one contained poison wanted know one searching glass fingerprints lab could test liquid glass tests take time money wanted make possible simultaneously testing mixtures small samples groups glasses university sent copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links symbol codes mathematics professor help counted glasses smiled said pick glass want commissioner test first waste test asked said part best procedure test one glass first matter one many glasses start commissioner wife asked remember somewhere exact number glasses solve puzzle explain professor fact wrong commissioner right fact optimal procedure identifying one poisoned glass expected waste relative optimum one followed professor strategy explain relationship symbol coding exercise assume sequence symbols ensemble introduced beginning chapter compressed using code imagine picking one bit random binary encoded sequence probability bit exercise binary huffman encoding scheme modified make optimal symbol codes encoding alphabet symbols also known radix 
[symbol, codes, mixture, codes] tempting idea construct metacode several symbol codes assign different length codewords alternative symbols switch one code another choosing whichever assigns shortest codeword current symbol clearly cannot free one wishes choose two codes necessary lengthen message way indicates two codes used indicate choice single leading bit found resulting code suboptimal incomplete fails kraft equality exercise prove metacode incomplete explain combined code suboptimal 
[symbol, codes, solutions] solution exercise yes uniquely decodeable even though prefix code two different strings map onto string codeword contains symbol solution exercise wish prove set codeword lengths satisfying kraft inequality prefix code lengths readily proved thinking codewords illustrated figure codeword supermarket size indicating cost imagine purchasing codewords one time starting shortest codewords biggest purchases using budget shown right figure start one side codeword supermarket say copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links solutions total symbol code budget figure codeword supermarket symbol coding budget cost codeword length indicated size box written total budget available making uniquely decodeable code symbol probability huffman rival code modified rival codewords codewords code figure proof huffman coding makes optimal symbol code assume rival code said optimal assigns unequal length codewords two symbols smallest probability interchanging codewords rival code symbol rival codelength long make code better rival code shows rival code optimal top purchase first codeword required length advance supermarket distance purchase next codeword next required length forth codeword lengths getting longer corresponding intervals getting shorter always buy adjacent codeword latest purchase wasting budget thus ith codeword advanced distance supermarket purchased codewords without running budget solution exercise proof huffman coding optimal depends proving key step algorithm decision give two symbols smallest probability equal encoded lengths cannot lead larger expected length code prove contradiction assume two symbols smallest probability called huffman algorithm would assign equal length codewords equal lengths optimal symbol code optimal symbol code rival code two codewords unequal lengths without loss generality assume code complete prefix code codelengths uniquely decodeable code realized prefix code rival code must symbol whose probability greater whose length rival code greater equal code must adjacent codeword equal greater length complete prefix code never solo codeword maximum length consider exchanging codewords figure copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links symbol codes encoded longer codeword probable gets shorter codeword clearly reduces expected length code change expected length thus contradicted assumption rival code optimal therefore valid give two symbols smallest probability equal encoded lengths huffman coding produces optimal symbol codes solution exercise huffman code code whereas entropy huffman code expected length whereas entropy huffman code maps sixteen source strings following codelengths expected length whereas entropy huffman code lengths expected length bits entropy bits huffman code shown table expected length bits entropy bits table huffman code column shows assigned codelengths column codewords strings whose probabilities identical fourth fifth receive different codelengths solution exercise set probabilities gives rise two different optimal sets codelengths second step huffman coding algorithm choose three possible pairings may either put constant length code code codes expected length another solution third solution exercise let max largest probability difference expected length entropy bigger max max gallager see exercises understand curious comes solution exercise length entropy solution exercise two ways answer problem correctly one popular way answer incorrectly let give incorrect answer first erroneous answer pick random bit first picking random source symbol probability picking random bit define fraction bits find bit copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links solutions answer wrong falls bus stop fallacy intro duced exercise buses arrive random interested average time one bus next must distinguish two possible averages average time randomly chosen bus next average time bus missed next bus second average twice big first waiting bus random time bias selection bus favour buses follow large gap unlikely catch bus comes seconds preceding bus similarly symbols get encoded longer length binary strings pick bit compressed string random likely land bit belonging would given probabilities expectation probabilities need scaled renormalized correct answer style every time symbol encoded bits added binary string expected number added per symbol expected total number bits added per symbol fraction transmitted string bit general symbol code general ensemble expectation correct answer case use powerful argument information theoretic answer encoded string output optimal compressor compresses samples pected length bits expect compress data probability bit equal would possible compress binary string using block compression code say therefore bit must equal deed probability sequence bits compressed stream taking particular value must output perfect compressor always perfectly random bits put another way probability bit equal information content per bit compressed string would would less contradicts fact recover original data information content per bit compressed string must solution exercise general huffman coding algorithm encoding alphabet symbols one difference binary case process combining symbols symbol reduces number symbols start symbols end copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links symbol codes complete ary tree mod equal otherwise know whatever prefix code make must incomplete tree number missing leaves equal modulo mod example ternary tree built eight symbols unavoidably one missing leaf tree optimal ary code made putting extra leaves longest branch tree achieved adding appropriate number symbols original source symbol set extra symbols probability zero total number leaves equal integer symbols repeatedly combined taking symbols smallest probability replacing single symbol binary huffman coding algorithm solution exercise wish show greedy metacode picks code gives shortest encoding actually suboptimal violates kraft inequality assume symbol assigned lengths candidate codes let assume alternative codes encode code used header length log bits metacode assigns lengths given log min compute kraft sum min let divide set non overlapping subsets subset contains symbols metacode sends via code one sub code satisfies kraft equality must case equality symbols would mean using one codes equality equation equality codes impossible symbols non overlapping subsets equality holding another way seeing mixture code suboptimal consider binary tree defines think special case two codes first bit send identifies code using complete code subsequent binary string valid string know using say code know follows codeword corresponding symbol whose encoding shorter code code strings invalid continuations mixture code incomplete suboptimal discussion issue relationship probabilistic modelling read bits back coding section frey 
[stream, codes] chapter discuss two data compression schemes arithmetic coding beautiful method goes hand hand philosophy compression data source entails probabilistic mod elling source best compression methods text files use arithmetic coding several state art image compression systems use lempel ziv coding universal method designed philosophy would like single compression algorithm reasonable job source fact many real life sources algorithm universal properties hold limit unfeasibly large amounts data lempel ziv compression widely used often effective 
[stream, codes, guessing, game] motivation two compression methods consider redundancy typical english text file files redundancy several levels example contain ascii characters non equal frequency certain consecutive pairs letters probable others entire words predicted given context semantic understanding text illustrate redundancy english curious way could compressed imagine guessing game english speaker repeatedly attempts predict next character text file simplicity let assume allowed alphabet consists upper case letters space game involves asking subject guess next character repeatedly feedback whether guess correct character correctly guessed correct guess note number guesses made character identified ask subject guess next character way one sentence gave following result human asked guess sentence numbers guesses listed character notice many cases next letter guessed immediately one guess cases particularly start syllables guesses needed game results offer first demonstrate redundancy english point view english speaker second game might used data compression scheme follows copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links arithmetic codes string numbers listed obtained presenting text subject maximum number guesses subject make given letter twenty seven subject performing time varying mapping twenty seven letters onto twenty seven numbers view symbols new alphabet total number symbols reduced since uses symbols much frequently others example easy compress new string symbols would uncompression sequence numbers work uncompression time original string encoded sequence imagine subject absolutely identical twin also plays guessing game knew source text stop whenever made number guesses equal given number guessed correct letter say yes right move next character alternatively identical twin available could design compression system help one human follows choose window length number characters context show human every one possible strings length ask would predict next character prediction wrong would next guesses tabulating answers questions could use two copies enormous tables encoder decoder place two human twins language model called lth order markov model systems clearly unrealistic practical compression illustrate several principles make use 
[stream, codes, arithmetic, codes] discussed variable length symbol codes optimal huffman algorithm constructing concluded pointing two practical theoretical problems huffman codes section defects rectified arithmetic codes invented elias rissanen pasco subsequently made practical witten arithmetic code probabilistic modelling clearly separated encoding operation system rather similar guessing game human predictor replaced probabilistic model source symbol produced source probabilistic model supplies predictive distribution possible values next symbol list positive numbers sum one choose model source producing symbols known distribution predictive distribution every time arithmetic coding equal ease handle complex adaptive models produce context dependent predictive distributions predictive model usually implemented computer program encoder makes use model predictions create binary string decoder makes use identical twin model guessing game interpret binary string let source alphabet let ith symbol special meaning end transmission source spits sequence source necessarily produce symbols assume computer program provided encoder assigns copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links stream codes predictive probability distribution given sequence occurred thus far receiver identical program produces predictive probability distribution figure binary strings define real intervals within real line first encountered picture like discussed symbol code supermarket chapter 
[stream, codes, concepts, understanding, arithmetic, coding] notation intervals interval numbers including binary transmission defines interval within real line example string interpreted binary real number corresponds interval binary interval base ten longer string corresponds smaller interval first string prefix new terval sub interval interval one megabyte binary file bits thus viewed specifying number precision two million decimal places two million decimal digits byte translates little two decimal digits also divide real line intervals lengths equal probabilities shown figure figure probabilistic model defines real intervals within real line may take interval subdivide intervals noted length proportional indeed length interval precisely joint probability iterating procedure interval divided sequence intervals corresponding possible finite length strings length interval equal probability string given model copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links arithmetic codes algorithm arithmetic coding iterative procedure find interval string compute cumulative probabilities 
[stream, codes, formulae, describing, arithmetic, coding] process depicted figure written explicitly follows intervals defined terms lower upper cumulative probabilities nth symbol arrives subdivide interval points defined example starting first symbol intervals algorithm describes general procedure encode string locate interval corresponding send binary string whose interval lies within interval encoding performed fly illustrate 
[stream, codes, example, compressing, tosses, bent, coin] imagine watch bent coin tossed number times example section two outcomes coin tossed denoted third possibility experiment halted event denoted end file symbol coin bent expect probabilities outcomes equal though beforehand know probable outcome encoding let source string bbba pass along string one symbol time use model compute probability distribution next copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links stream codes symbol given string thus far let probabilities context sequence thus far probability next symbol bbb bbb bbb bbb bbba bbba bbba bbba figure shows corresponding intervals interval middle interval middle forth bba bbb bbba bbbb bbb cco bbba bbbaa bbbab bbba figure illustration arithmetic coding process sequence bbba transmitted first symbol observed encoder knows encoded string start know encoder writes nothing time examines next symbol interval lies wholly within interval encoder write first bit third symbol narrows interval little quite enough lie wholly within interval next read source transmit bits interval bbba lies wholly within interval encoder adds written finally arrives need procedure terminating encoding magnifying interval bbba figure right note marked interval wholly contained bbba encoding completed appending copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links arithmetic codes exercise show overhead required terminate message never bits relative ideal message length given probabilistic model log important result arithmetic coding nearly optimal message length always within two bits shannon information content entire source string expected message length within two bits entropy entire message decoding decoder receives string passes along one symbol time first probabilities computed using identical program encoder used intervals deduced first two bits examined certain original string must started since interval lies wholly within interval decoder use model compute deduce boundaries intervals continuing decode second reach third reach forth unambiguous identification bbba whole binary string read convention denotes end message decoder knows stop decoding transmission multiple files might one use arithmetic coding communicate several distinct files binary channel character transmitted imagine decoder reset initial state transfer learnt statistics first file second file however believe relationship among files going compress could define alphabet differently introducing second end file character marks end file instructs encoder decoder continue using probabilistic model 
[stream, codes, big, picture] notice communicate string letters encoder decoder needed compute conditional probabilities proba bilities possible letter context actually encountered guessing game cost contrasted alternative using huffman code large block size order reduce possible one bit per symbol overhead discussed section block sequences could occur must considered probabilities evaluated notice flexible arithmetic coding used source alphabet encoded alphabet size source alphabet encoded alphabet change time arithmetic coding used probability distribution change utterly context context furthermore would like symbols encoding alphabet say used unequal frequency easily arranged subdividing right hand interval proportion required frequencies 
[stream, codes, probabilistic, model, might, make, predictions] technique arithmetic coding force one produce predic tive probability particular way predictive distributions might copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links stream codes aaa aab aaaa aaab aaba aabb aba abb abaa abab abba abbb baa bab baaa baab baba babb bba bbb bbaa bbab bbba bbbb figure illustration intervals defined simple bayesian probabilistic model size intervals proportional probability string model anticipates source likely biased towards one sequences lots lots larger intervals sequences length naturally produced bayesian model figure generated using simple model always assigns prob ability assigns remaining divided proportion probabilities given laplace rule number times occurred far count predictions correspond simple bayesian model expects adapts non equal frequency use source symbols within file figure displays intervals corresponding number strings length five note string far contained large number probability relative increased conversely many occur made probable larger intervals remember require fewer bits encode 
[stream, codes, details, bayesian, model] emphasized model could used arithmetic coding wedded particular set probabilities let explain simple adaptive copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links arithmetic codes probabilistic model used preceding example first encountered model exercise assumptions model described using parameters defined confused predictive probabilities particular context example baa bent coin labelled tossed number times know beforehand coin probability coming tossed parameters known beforehand source string baaba indicates sequence outcomes baaba assumed length string exponential probability distribution distribution corresponds assuming constant probability termination symbol character assumed non terminal characters string selected dependently random ensemble probabilities probability fixed throughout string unknown value could anywhere probability occur ring next symbol given knew probability given unterminated string length given string contains counts two outcomes bernoulli distribution assume uniform prior distribution define would easy assume priors beta distributions convenient handle model studied section key result require predictive distribution next symbol given string far probability next character assuming derived equation precisely laplace rule exercise compare expected message length ascii file compressed following three methods huffman header read whole file find empirical fre quency symbol construct huffman code frequen cies transmit code transmitting lengths huffman codewords transmit file using huffman code actual codewords need transmitted since use deterministic method building tree given codelengths arithmetic code using laplace model arithmetic code using dirichlet model model predic tions copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links stream codes fixed number small value corresponds responsive version laplace model probability characters expected nonuniform reproduces laplace model take care header huffman message self delimiting special cases worth considering short files hundred characters large files characters never used 
[stream, codes, efficient, generation, random, samples] arithmetic coding offers way compress strings believed come given model also offers way generate random strings model imagine sticking pin unit interval random line divided subintervals proportion probabilities probability pin lie interval generate sample model need feed ordinary random bits arithmetic decoder model infinite random bit sequence corresponds selection point random line decoder select string random assumed distribution arithmetic method guaranteed use nearly smallest number random bits possible make selection important point communities random numbers expensive joke large amounts money spent generating random bits software hardware random numbers valuable simple example use technique generation random bits nonuniform distribution exercise compare following two techniques generating random symbols nonuniform distribution standard method use standard random number generator generate integer rescale integer test whether uniformly distributed random variable less emit accordingly arithmetic coding using correct model fed standard ran dom bits roughly many random bits method use generate thousand samples sparse distribution 
[stream, codes, efficient, data-entry, devices] enter text computer make gestures sort maybe tap keyboard scribble pointer click mouse efficient text entry system one number gestures required enter given text string small writing viewed inverse process data compression data compression text bits writing text gestures compression aim map given text string small number bits text entry want small sequence gestures produce intended text inverting arithmetic coder obtain information efficient text entry device driven continuous pointing gestures ward copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links lempel ziv coding system called dasher user zooms unit interval locate interval corresponding intended string style figure language model exactly used text compression controls sizes intervals probable strings quick easy identify hour practice novice user write one finger driving dasher words per minute half normal ten finger typing speed regular keyboard even possible write words per minute hands free using gaze direction drive dasher ward mackay dasher available free software various platforms 
[stream, codes, lempel–ziv, coding] lempel ziv algorithms widely used data compression compress gzip commands different philosophy arithmetic coding separation modelling coding oppor tunity explicit modelling 
[stream, codes, basic, lempel–ziv, algorithm] method compression replace substring pointer earlier occurrence substring example string parse ordered dictionary substrings appeared follows clude empty substring first substring dictionary order substrings dictionary order emerged source every comma look along next part input sequence read substring marked ment reflection confirm substring longer one bit substring occurred earlier dictionary means encode substring giving pointer earlier occurrence pre fix sending extra bit new substring dictionary differs earlier substring nth bit enumerated substrings give value pointer dlog bits code sequence shown fourth line following table punctuation included clarity upper lines indicating source string value source substrings binary pointer bit notice first pointer send empty given one substring dictionary string bits needed convey choice substring prefix encoded string encoding simple case actually longer string source string obvious redundancy source string exercise prove uniquely decodeable code necessarily makes strings longer makes strings shorter http www inference phy cam dasher copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links stream codes one reason algorithm described lengthens lot strings inefficient transmits unnecessary bits put another way code complete substring dictionary joined children sure needed except possibly part protocol terminating message point could drop dictionary substrings shuffle along one thereby reducing length subsequent pointer messages equivalently could write second prefix dictionary point previously occupied parent second unnecessary overhead transmission new bit cases second time prefix used sure identity next bit decoding decoder involves identical twin decoding end con structs dictionary substrings data decoded exercise encode string using basic lempel ziv algorithm described exercise decode string encoded using basic lempel ziv algorithm practicalities description discussed method terminating string many variations lempel ziv algorithm exploiting idea using different procedures dictionary management etc resulting programs fast performance compression english text although useful match standards set arithmetic coding literature 
[stream, codes, theoretical, properties] contrast block code huffman code arithmetic coding methods discussed last three chapters lempel ziv algorithm defined without making mention probabilistic model source yet given ergodic source one memoryless sufficiently long timescales lempel ziv algorithm proven asymptotically compress entropy source called universal compression algorithm proof property see cover thomas achieves compression however memorizing substrings happened short name next time occur asymptotic timescale universal performance achieved may many sources unfeasibly long number typical substrings need memorizing may enormous useful performance gorithm practice reflection fact many files contain multiple repetitions particular short sequences characters form redundancy algorithm well suited copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links demonstration 
[stream, codes, common, ground] emphasized difference philosophy behind arithmetic coding lempel ziv coding common ground though prin ciple one design adaptive probabilistic models thence arithmetic codes universal models asymptotically compress source class within factor preferably entropy however practical purposes think universal models constructed class sources severely restricted general purpose compressor discover probability distribution source would general purpose artificial intelligence general purpose artificial intelli gence yet exist 
[stream, codes, demonstration] interactive aid exploring arithmetic coding dasher tcl available demonstration arithmetic coding software package written radford neal consists encoding decoding modules user adds module defining probabilistic model emphasized single general purpose arithmetic coding compressor new model written type source radford neal package includes simple adaptive model similar bayesian model demonstrated section results using laplace model viewed basic benchmark since simplest possible probabilistic model simply assumes characters file come independently fixed ensemble counts symbols rescaled rounded file read counts lie state art compressor documents containing text images djvu uses arithmetic coding uses carefully designed approximate arith metic coder binary alphabets called coder bottou much faster arithmetic coding software described one neat tricks coder uses adaptive model adapts occa sionally save computer time decision adapt pseudo randomly controlled whether arithmetic encoder emitted bit jbig image compression standard binary images uses arithmetic coding context dependent model adapts using rule similar laplace rule ppm teahan leading method text compression uses arithmetic coding many lempel ziv based programs gzip based version lempel ziv called ziv lempel compress based lzw welch experience best gzip compress inferior files bzip block sorting file compressor makes use neat hack called burrows wheeler transform burrows wheeler method based explicit probabilistic model works well files larger several thousand characters practice effective compressor files context character good predictor character http www inference phy cam mackay itprnn softwarei html ftp ftp toronto edu pub radford www software html http www djvuzone org lot information burrows wheeler transform net http dogma net datacompression bwt shtml copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links stream codes compression text file table gives computer time seconds taken compression achieved programs applied tex file containing text chapter size bytes method compression compressed size uncompression time sec age time sec laplace model gzip compress bzip bzip ppmz table comparison compression algorithms applied text file compression sparse file interestingly gzip always well table gives compres sion achieved programs applied text file containing characters either probabilities laplace model quite well matched source benchmark arithmetic coder gives good performance followed closely compress gzip worst ideal model source would compress file bytes laplace model compressor falls short performance implemented using eight bit precision ppmz compressor compresses best takes much computer time method compression compressed size uncompression time sec bytes time sec laplace model gzip gzip best compress bzip bzip ppmz table comparison compression algorithms applied random file characters 
[stream, codes, summary] last three chapters studied three classes data compression codes fixed length block codes chapter mappings fixed number source symbols fixed length binary message tiny fraction source strings given encoding codes fun identifying entropy measure compressibility little practical use copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exercises stream codes symbol codes chapter symbol codes employ variable length code symbol source alphabet codelengths integer lengths determined probabilities symbols huffman algorithm constructs optimal symbol code given set symbol probabilities every source string uniquely decodeable encoding source symbols come assumed distribution symbol code compress expected length per character lying interval statistical fluctuations source may make actual length longer shorter mean length source well matched assumed distribution mean length increased relative entropy source distribution code implicit distribution sources small entropy symbol emit least one bit per source symbol compression one bit per source symbol achieved cumbersome procedure putting source data blocks stream codes distinctive property stream codes compared symbol codes constrained emit least one bit every symbol read source stream large numbers source symbols may coded smaller number bits property could obtained using symbol code source stream somehow chopped blocks arithmetic codes combine probabilistic model encoding algorithm identifies string sub interval size equal probability string model code almost optimal sense compressed length string closely matches shannon information content given probabilistic model arithmetic codes fit philosophy good compression requires data modelling form adaptive bayesian model lempel ziv codes adaptive sense memorize strings already occurred built philoso phy know anything probability distribution source want compression algo rithm perform reasonably well whatever distribution arithmetic codes lempel ziv codes fail decode correctly bits compressed file altered compressed files stored transmitted noisy media error correcting codes essential reliable communication unreliable channels topic part 
[stream, codes, exercises, stream, codes] exercise describe arithmetic coding algorithm encode random bit strings length weight ones zeroes given case show detail intervals corresponding source substrings lengths exercise many bits needed specify selection objects objects assumed known copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links stream codes selection objects unordered might selection made random without wasteful random bits exercise binary source emits independent identically distributed symbols probability distribution find optimal uniquely decodeable symbol code string three successive samples source estimate one decimal place factor expected length optimal code greater entropy three bit string log log arithmetic code used compress string samples source estimate mean standard deviation length compressed file exercise describe arithmetic coding algorithm generate random bit strings length density bit probability one given exercise use modified lempel ziv algorithm discussed dictionary prefixes pruned writing new prefixes space occupied prefixes needed prefixes identified children added dictionary prefixes may neglect issue termination encoding use algorithm encode string highlight bits follow prefix second occasion prefix used discussed earlier bits could omitted exercise show modified lempel ziv code still complete binary strings encodings string exercise give examples simple sources low entropy would compressed well lempel ziv algorithm 
[stream, codes, exercises, data, compression] following exercises may skipped reader eager learn noisy channels exercise consider gaussian distribution dimensions exp define radius point estimate mean variance square radius may find helpful integral exp though able estimate required quantities without copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exercises data compression probability density maximized almost probability mass figure schematic representation typical set dimensional gaussian distribution assuming large show nearly probability gaussian contained thin shell radius find thickness shell evaluate probability density point thin shell origin compare use case example notice nearly probability mass located different part space region highest probability density exercise explain meant optimal binary symbol code find optimal binary symbol code ensemble compute expected length code exercise string consists two independent samples ensemble entropy construct optimal binary symbol code string find expected length exercise strings independent samples ensemble compressed using arithmetic code matched ensemble estimate mean standard deviation compressed strings lengths case exercise source coding variable length symbols chapters source coding assumed encoding binary alphabet symbols used equal frequency question plore encoding alphabet used symbols take different times transmit poverty stricken student communicates free friend using telephone selecting integer making friend phone ring times hanging middle nth ring process repeated string symbols received optimal way communicate large integers selected message takes longer communicate small integers used information content per symbol small aim maximize rate information transfer per unit time assume time taken transmit number rings redial seconds consider probability distribution defining average duration per symbol copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links stream codes entropy per symbol log show average information rate per second maximized symbols must used probabilities form satisfies implicit equation rate communication show two equations imply must set log assuming channel property seconds find optimal distribution show maximal information rate bit per second compare information rate per second achieved set symbols selected equal probability discuss relationship results derived kraft inequality source coding theory might random binary source efficiently encoded quence symbols transmission channel defined equation exercise many bits take shuffle pack cards exercise card game bridge four players receive cards deck start game looking hand bidding legal bids ascending order successive bids must follow order bid say may followed higher bids let neglect double bid players several aims bidding one aims two partners communicate much possible cards hands let concentrate task cards dealt many bits needed north convey south hand assuming bid maximum total information convey bidding assume starts bidding either stops bidding bidding stops copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links solutions exercise old arabic microwave oven buttons entering cooking times new roman microwave five tons roman microwave labelled minutes minute seconds second start abbreviate five strings symbols enter one minute twenty three seconds arabic roman figure alternative keypads microwave ovens arabic sequence roman sequence cxxiii keypads defines code mapping cooking times string symbols times produced two three symbols example produced three symbols either code two codes complete give detailed answer code name cooking time produce four symbols code cannot discuss implicit probability distributions times codes best matched concoct plausible probability distribution times real user might use evaluate roughly expected number sym bols maximum number symbols code requires discuss ways code inefficient efficient invent efficient cooking time encoding system crowave oven exercise standard binary representation positive inte gers uniquely decodeable code design binary code positive integers mapping uniquely decodeable try design codes prefix codes satisfy kraft equality motivations data file terminated special end file character mapped onto integer prefix code integers used self delimiting encoding files large files correspond large integers also one building blocks universal coding scheme coding scheme work large variety sources ability encode integers finally microwave ovens cooking times positive integers discuss criteria one might compare alternative codes inte gers equivalently alternative self delimiting codes files 
[stream, codes, solutions] solution exercise worst case situation interval represented lies inside binary interval case may choose either two binary intervals shown figure binary intervals copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links stream codes source string interval binary intervals figure termination arithmetic coding worst case two bit overhead either two binary intervals marked right hand side may chosen binary intervals smaller smaller binary encoding length greater log log two bits ideal message length solution exercise standard method uses random bits per generated symbol requires bits generate one thousand samples arithmetic coding uses average bits per gener ated symbol requires bits generate one thousand samples assuming overhead roughly two bits associated termination fluctuations number would produce variations around mean standard deviation solution exercise encoding comes parsing encoded thus solution exercise decoding solution exercise problem equivalent exercise selection objects objects requires dlog bits bits selection could made using arithmetic coding selection corresponds binary string length bits rep resent objects selected initially probability probability thereafter given emitted string thus far length contains probability probability solution exercise modified lempel ziv code still complete example five prefixes collected pointer could strings cannot thus binary strings cannot produced encodings solution exercise sources low entropy well compressed lempel ziv include copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links solutions sources symbols long range correlations inter vening random junk ideal model capture correlated compress lempel ziv compress correlated features memorizing cases intervening junk simple example consider telephone book every line contains old number new number pair number characters per line drawn character alphabet characters occur predictable sequence true information content per line assuming phone numbers seven digits long assuming random sequences bans ban information content random integer finite state language model could easily capture regularities data lempel ziv algorithm take long time compresses file bans per line however order learn string ddd always followed three digits ddd see strings near optimal compression achieved thousands lines file read figure source low entropy well compressed lempel ziv bit sequence read left right line differs line bits image width pixels sources long range correlations example two dimensional ages represented sequence pixels row row vertically adjacent pixels distance apart source stream image width consider example fax transmission line similar previous line figure true entropy per pixel probability pixel differs parent lempel ziv algorithms compress entropy strings length occurred successors memorized par ticles universe confidently say lempel ziv codes never capture redundancy image another highly redundant texture shown figure image made dropping horizontal vertical pins randomly plane contains long range vertical correlations long range horizontal correlations practical way lempel ziv fed pixel pixel scan image could capture correlations biological computational systems readily identify redundancy images images much complex thus might anticipate best data compression algorithms result development artificial intelligence methods copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links stream codes figure texture consisting horizontal vertical pins dropped random plane sources intricate redundancy files generated computers example tex file followed encoding postscript file information content pair files roughly equal information content tex file alone picture mandelbrot set picture information content equal number bits required specify range complex plane studied pixel sizes colouring rule used picture ground state frustrated antiferromagnetic ising model figure discuss chapter like figure binary image interesting correlations two directions figure frustrated triangular ising model one ground states cellular automata figure shows state history steps cellular automaton cells update rule cell new state depends state five preceding cells selected random information content equal information boundary bits propagation rule scribed bits optimal compressor thus give compressed file length essentially constant independent vertical height image lempel ziv would give zero cost compression cellular automaton entered periodic limit cycle could easily take iterations contrast jbig compression method models probability pixel given local context uses arithmetic coding would good job images solution exercise one dimensional gaussian vari ance mean value dimensions since components independent random variables copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links solutions figure step time history cellular automaton cells variance similarly times variance one dimensional gaussian variable var exp integral found equation var thus variance large central limit theorem indicates gaussian distribution mean standard deviation probability density must similarly concentrated thickness shell given turning standard deviation standard deviation small log log setting standard viation probability density gaussian point shell shell exp exp whereas probability density origin thus shell exp probability density typical radius times smaller density origin probability density origin times greater 
[codes, integers] chapter aside may safely skipped 
[codes, integers, solution, exercisep)] discuss coding integers need definitions standard binary representation positive integer denoted standard binary length positive integer length string example standard binary representation uniquely decodeable code integers since way knowing integer ended example identical would uniquely decodeable knew standard binary length integer received noticing positive integers standard binary representation starts might define another representation headless binary representation positive integer noted denotes null string representation would uniquely decodeable knew length integer make uniquely decodeable code integers two strate gies distinguished self delimiting codes first communicate somehow length integer also positive integer communicate original integer using codes end file characters code integer blocks length bits reserve one symbols special meaning end file coding integers blocks arranged reserved symbol needed purpose simplest uniquely decodeable code integers unary code viewed code end file character copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links codes integers unary code integer encoded sending string followed unary code length unary code optimal code integers probability distri bution self delimiting codes use unary code encode length binary encoding make self delimiting code code send unary code followed headless binary representation table shows codes integers overlining indicates division string parts might table equivalently view consisting string zeroes followed standard binary representation codeword length implicit probability distribution code separable product probability distribution length uniform distribution integers length otherwise code header communicates length always occupies number bits standard binary representation integer give take one expecting encounter large integers large files representation seems suboptimal since leads files occupying size double original uncoded size instead using unary code encode length could use table code send length using followed headless binary representation iterating procedure define sequence codes code code copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links codes integers 
[codes, integers, codes, end-of-file, symbols] also make byte based representations let use term byte flexibly denote fixed length string bits string length bits encode number base example decimal represent digit byte order represent digit byte need four bits leaves extra four bit symbols correspond decimal digit use end file symbols indicate end positive integer clearly redundant one end file symbol efficient code would encode integer base use sixteenth symbol punctuation character generalizing idea make similar byte based codes integers bases base form table two codes end file symbols spaces included show byte boundaries codes almost complete recall code complete satisfies kraft inequality equality codes remaining inefficiency provide ability encode integer zero empty string neither required exercise consider implicit probability distribution inte gers corresponding code end file character code eight bit blocks integer coded base mean length bits integer implicit distribution one wishes encode binary files expected size one hun dred kilobytes using code end file character optimal block size 
[codes, integers, encoding, tiny, file] illustrate codes discussed use code encode small file consisting characters claude shannon map ascii characters onto seven bit symbols decimal etc character file corresponds integer decimal unary code consists many less one zeroes followed one oceans turned ink wrote hundred bits every cubic millimeter might enough ink write standard binary representation length sequence bits exercise write describe following self delimiting represen tations number encodings shortest answer copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links codes integers 
[codes, integers, comparing, codes] one could answer question two codes superior sentence form code superior code superior contend answer misses point complete code corresponds prior optimal say code superior codes optimal priors implicit priors thought achieve best code one application notice one cannot free switch one code another choosing whichever shorter one would necessary lengthen message way indicates two codes used done single leading bit found resulting code suboptimal fails kraft equality discussed exercise another way compare codes integers consider sequence probability distributions monotonic probability distributions rank codes well encode distributions code called universal code distribution given class encodes average length within factor ideal average length let say meeting alternative world view rather figuring good prior integers advocated many orists studied problem creating codes reasonably good codes priors broad class class priors convention ally considered set priors assign monotonically decreasing probability integers finite entropy several codes discussed universal another code elegantly transcends sequence self delimiting codes elias uni versal code integers elias effectively chooses codes works sending sequence messages encodes length next message indicates single bit whether message final integer standard binary representation length positive integer positive integers begin leading omitted write loop blog halt prepend written string blog algorithm elias encoder integer encoder shown algorithm encoding generated right left table shows resulting codewords exercise show elias code actually best code prior distribution expects large integers construct ing another code specifying large must code give shorter length elias copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links codes integers table elias universal code integers examples 
[codes, integers, solutions] solution exercise use end file symbol code represents integer base corresponds belief probability current character last character number thus prior code matched puts exponential prior distribution length integer expected number characters expected length integer bits wish find log bits value satisfies constraint bit blocks roughly optimal size assuming one end file character 
[dependent, random, variables] last three chapters data compression concentrated random vectors coming extremely simple probability distribution namely separable distribution component independent others chapter consider joint ensembles random variables dependent material two motivations first data real world interesting correlations data compression well need know work models include dependences second noisy channel input output defines joint ensemble dependent independent would impossible communicate channel communication noisy channels topic chapters described terms entropy joint ensembles 
[dependent, random, variables, entropy] section gives definitions exercises entropy carrying section joint entropy log entropy additive independent random variables iff conditional entropy given entropy proba bility distribution log conditional entropy given average con ditional entropy given  log  log measures average uncertainty remains known copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links entropy marginal entropy another name entropy used contrast conditional entropies listed chain rule information content product rule probabil ities equation obtain log log log words says information content infor mation content plus information content given chain rule entropy joint entropy conditional entropy marginal entropy related words says uncertainty uncertainty plus uncertainty given mutual information satisfies measures average reduction uncertainty results learning value vice versa average amount information conveys conditional mutual information given mutual information random variables joint ensemble conditional mutual information given average conditional mutual information three term entropies defined example expres sions illegal may put conjunctions arbitrary numbers variables three spots expression example fine measures much information average convey assuming known figure shows total entropy joint ensemble broken figure important copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links dependent random variables figure relationship joint information marginal entropy conditional entropy mutual entropy 
[dependent, random, variables, exercises] exercise consider three independent random variables tropies let exercise referring definitions conditional entropy confirm example possible exceed average less data helpful increase uncertainty average exercise prove chain rule entropy equation exercise prove mutual information satisfies hint see exercise note exercise entropy distance two random variables defined difference joint entropy mutual information prove entropy distance satisfies axioms distance incidentally unlikely see good function practise inequality proving exercise joint ensemble following joint distribution joint entropy marginal entropies value conditional entropy conditional entropy conditional entropy given mutual information copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exercises exercise consider ensemble independent mod general notice ensemble related binary symmetric channel input noise output figure misleading representation entropies contrast figure 
[dependent, random, variables, three, term, entropies] solution exercise depiction entropies terms venn diagrams misleading least two reasons first one used thinking venn diagrams depicting sets sets depicted figure objects members sets think diagram encourages novice student make inappropriate analogies example students imagine copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links dependent random variables figure misleading representation entropies continued random outcome might correspond point diagram thus confuse entropies probabilities secondly depiction terms venn diagrams encourages one lieve areas correspond positive quantities special case two random variables indeed true positive quantities soon progress three variable ensembles obtain diagram positive looking areas may actually correspond negative quantities figure correctly shows relationships gives misleading impression conditional mutual information less mutual information fact area labelled correspond negative quantity consider joint ensemble independent binary variables defined mod clearly bit also bit since two variables independent mutual information zero however observed become dependent knowing given tells mod bit thus area labelled must correspond bits figure give correct answers example capricious exceptional illustration binary symmetric channel input noise output situation input noise independent see output unknown input unknown noise intimately related venn diagram representation therefore valid one aware positive areas may represent negative quantities proviso kept mind interpretation entropies terms sets helpful yeung solution exercise joint ensemble following chain rule mutual information holds case independent given using chain rule twice 
[dependent, random, variables, data-processing, theorem] data processing theorem states data processing destroy information exercise prove theorem considering ensemble state world data gathered processed data three variables form markov chain probability written show average information conveys less equal average information conveys theorem much caution definition information caution data processing copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links dependent random variables 
[dependent, random, variables, inference, information, measures] exercise three cards one card white faces one black faces one white one side black three cards shuffled orientations randomized one card drawn placed table upper face black colour lower face solve inference problem seeing top face convey information colour bottom face discuss information contents entropies situation let value upper face colour value lower face colour imagine draw random card learn entropy entropy mutual information 
[dependent, random, variables, entropies, markov, processes] exercise guessing game imagined predicting next letter document starting beginning working towards end consider task predicting reversed text predicting letter precedes already known people find harder task assuming model language using gram model says probability next character depends preceding characters difference average information contents reversed language forward language 
[dependent, random, variables, solutions] solution exercise see exercise example exceeds set prove inequality turning expression relative entropy using bayes theorem invoking gibbs inequality exercise  log  log log log log last expression sum relative entropies distributions equality independent copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links solutions solution exercise chain rule entropy follows decomposition joint probability log log log log log solution exercise symmetry mutual information log log log log expression symmetric prove mutual information positive two ways one continue log relative entropy use gibbs inequality proved asserts relative entropy equality independent use jensen inequality log log log solution exercise mod general mutual information 
[communication, noisy, channel, big, picture] noisy channel encoder decoder compressor decompressor source coding channel coding source chapters discussed source coding block codes symbol codes stream codes implicitly assumed channel compres sor decompressor noise free real channels noisy spend two chapters subject noisy channel coding fundamen tal possibilities limitations error free communication noisy channel aim channel coding make noisy channel behave like noiseless channel assume data transmitted good compressor bit stream obvious redundancy channel code makes transmission put back redundancy special sort designed make noisy received signal decodeable suppose transmit bits per second noisy channel flips bits probability rate transmission information might guess rate bits per second subtracting expected number errors per second correct recipient know errors occurred consider case noise great received symbols independent transmitted symbols corresponds noise level since half received symbols correct due chance alone information transmitted given learnt entropy seems reasonable mea sure information transmitted given mutual information source received signal entropy source minus conditional entropy source given received signal review definition conditional entropy mutual formation examine whether possible use noisy channel communicate reliably show channel non zero rate capacity information sent copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links review probability information arbitrarily small probability error 
[communication, noisy, channel, review, probability, information] example take joint distribution exercise marginal distributions shown margins joint entropy bits marginal entropies bits bits compute conditional distribution value entropy conditional distributions bits note whereas less greater cases learning increase uncertainty note also although different distribution conditional entropy equal learning changes knowledge reduce uncertainty measured entropy average though learning convey information since one may also evaluate bits mutual information bits 
[communication, noisy, channel, noisy, channels] discrete memoryless channel characterized input alphabet output alphabet set conditional probability distri butions one transition probabilities may written matrix usually orient matrix output variable indexing rows input variable indexing columns column probability vector convention obtain probability output probability distribution input right multiplication copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links communication noisy channel useful model channels binary symmetric channel binary erasure channel noisy typewriter letters letters arranged circle typist attempts type comes either probability input output forth final letter adjacent first letter ppp ppp ppp ppp ppp ppp ppp ppp ppp ppp ppp channel 
[communication, noisy, channel, inferring, input, given, output] assume input channel comes ensemble obtain joint ensemble random variables joint distribution receive particular symbol input symbol typically know certain write posterior distribution input using bayes theorem example consider binary symmetric channel probability error let input ensemble assume observe copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links information conveyed channel thus still less probable although probable exercise assume observe compute probability given example consider channel probability error let input ensemble assume observe given output become certain input exercise alternatively assume observe compute 
[communication, noisy, channel, information, conveyed, channel] consider much information communicated chan nel operational terms interested finding ways using chan nel bits communicated recovered negligible probability error mathematical terms assuming particular input semble measure much information output conveys input mutual information aim establish connection two ideas let evaluate channels 
[communication, noisy, channel, hint, computing, mutual, information] tend think much uncertainty input reduced look output computational purposes often handy evaluate instead figure relationship joint information marginal entropy conditional entropy mutual entropy figure important showing twice example consider binary symmetric channel already evaluated marginal probabil ities implicitly mutual information copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links communication noisy channel defined weighted sum value bits may contrasted entropy source bits note used binary entropy function log log throughout book log means log example channel bits entropy source bits notice mutual information channel bigger mutual information binary symmetric channel channel reliable channel exercise compute mutual information binary symmetric channel input distribution exercise compute mutual information channel input distribution 
[communication, noisy, channel, maximizing, mutual, information] observed examples mutual information input output depends chosen input ensemble let assume wish maximize mutual information conveyed channel choosing best possible input ensemble define capacity channel maximum mutual information capacity channel max distribution achieves maximum called optimal input distribution denoted may multiple optimal input distributions achieving value chapter show capacity indeed measure maxi mum amount error free information transmitted chan nel per unit time example consider binary symmetric channel considered found bits copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links noisy channel coding theorem much better symmetry optimal input distribu tion capacity figure mutual information binary symmetric channel function input distribution bsc bits justify symmetry argument later doubt symmetry argument always resort explicit maximization mutual information figure example noisy typewriter optimal input distribution uni form distribution gives log bits example consider channel identifying optimal input distribution straightforward evaluate explic itly first need compute probability easiest write mutual information figure mutual information channel function input distribution non trivial function shown figure maximized find notice optimal input distribution communicate slightly information using input symbol frequently exercise capacity binary symmetric channel general exercise show capacity binary erasure channel bec capacity general comment 
[communication, noisy, channel, noisy-channel, coding, theorem] seems plausible capacity defined may measure information conveyed channel obvious prove next chapter capacity indeed measures rate blocks data communicated channel arbitrarily small probability error make following definitions block code channel list codewords length using code encode signal number codewords integer number bits specified choosing codeword log necessarily integer copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links communication noisy channel rate code bits per channel use use definition rate channel chan nels binary inputs note however sometimes conventional define rate code channel input symbols log decoder block code mapping set length strings channel outputs codeword label extra symbol used indicate failure probability block error code decoder given channel given probability distribution encoded signal maximal probability block error max optimal decoder channel code one minimizes prob ability block error decodes output input maximum posterior probability optimal argmax uniform prior distribution usually assumed case optimal decoder also maximum likelihood decoder decoder maps output input maximum likelihood probability bit error defined assuming codeword number represented binary vector length bits average probability bit equal corresponding bit averaging bits shannon noisy channel coding theorem part one associated discrete memoryless channel non negative number achievable figure portion plane asserted achievable first part shannon noisy channel coding theorem called channel capacity following property large enough exists block code length rate decoding algorithm maximal probability block error 
[communication, noisy, channel, confirmation, theorem, noisy, typewriter, channel] case noisy typewriter easily confirm theorem create completely error free communication strategy using block code length use letters every third letter letters form non confusable subset input alphabet see figure output uniquely decoded number inputs non confusable subset error free information rate system log bits equal capacity evaluated example copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links intuitive preview proof ppp ppp ppp ppp figure non confusable subset inputs noisy typewriter figure extended channels obtained binary symmetric channel transition probability translate terms theorem following table explains theorem applies noisy typewriter associated discrete memoryless channel non negative number capacity log large enough matter set blocklength exists block code length rate block code value given log code rate log greater requested value decoding algorithm decoding algorithm maps received letter nearest letter code maximal probability block error maximal probability block error zero less given 
[communication, noisy, channel, extended, channels] prove theorem given channel consider extended channel corresponding uses channel extended channel possible inputs possible outputs extended channels obtained binary symmetric channel channel shown figures copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links communication noisy channel figure extended channels obtained channel transition probability column corresponds input row different output amp typical typical given typical amp typical figure typical outputs corresponding typical inputs subset typical sets shown overlap picture compared solution noisy typewriter figure exercise find transition probability matrices tended channel derived binary erasure channel erasure probability selecting two columns transition probability matrix define rate code channel blocklength best choice two columns decoding algorithm prove noisy channel coding theorem make use large block lengths intuitive idea large extended channel looks lot like noisy typewriter particular input likely produce output small subspace output alphabet typical output set given input find non confusable subset inputs produce essentially disjoint output sequences given let consider way generating non confusable subset inputs count many distinct inputs contains imagine making input sequence extended channel drawing ensemble arbitrary ensemble input alphabet recall source coding theorem chapter consider number probable output sequences total number typical output sequences similar probability particular typical input sequence probable sequences subsets depicted circles figure imagine restricting subset typical inputs corresponding typical output sets overlap shown figure bound number non confusable inputs dividing size typical set size typical copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exercises given typical set number non confusable inputs selected set typical inputs maximum value bound achieved ensemble maximizes case number non confusable inputs thus asymptotically bits per cycle communicated vanishing error probability sketch rigorously proved reliable communication really possible task next chapter 
[communication, noisy, channel, exercises] exercise refer back computation capacity channel less one could argue good favour input since transmitted without error also argue good favour input since often gives rise highly prized output allows certain identification input try make convincing argument case general show optimal input distribution happens noise level close exercise sketch graphs capacity channel binary symmetric channel binary erasure channel function exercise capacity five input ten output channel whose transition probability matrix                               exercise consider gaussian channel binary input real output alphabet transition probability den sity signal amplitude compute posterior probability given assuming two inputs equiprobable put answer form copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links communication noisy channel sketch value function assume single bit transmitted optimal decoder probability error express answer terms signal noise ratio error function cumulative probability function gaussian distribution note definition error function may corre spond people 
[communication, noisy, channel, pattern, recognition, noisy, channel] may think many pattern recognition problems terms communi cation channels consider case recognizing handwritten digits postcodes envelopes author digit wishes communicate message set selected message input channel comes channel pattern ink paper ink pattern represented using binary pixels channel output random variable example element alphabet shown margin exercise estimate many patterns recognizable character aim problem try demonstrate existence many patterns possible recognizable figure discuss one might model channel estimate entropy probability distribution one strategy pattern recognition create model value input use bayes theorem infer given strategy known full probabilistic modelling generative modelling essentially current speech recognition systems work addition channel model one uses prior proba bility distribution case character recognition speech recognition language model specifies probability next character word given context known grammar statistics language 
[communication, noisy, channel, random, coding] solution exercise probability people whose birthdays drawn random days distinct birthdays probability two people share birthday one minus quantity exact way answering question informative since clear value probability changes close close number pairs probability particular pair shares birthday expected number collisions answer instructive expected number collisions tiny big also approximate probability birthdays distinct small thus exp exp exp exp exp exp 
[communication, noisy, channel, solutions] solution exercise assume observe solution exercise observe solution exercise probability mutual information bits solution exercise compute mutual information using probability copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links communication noisy channel mutual information bits solution exercise symmetry optimal input distribution capacity would like find optimal input distribution without invoking sym metry computing mutual information general case input ensemble dependence first term maximized setting argument value given setting solution exercise answer symmetry optimal input distribution capacity easily evaluated writing mutual information conditional entropy known uncertain occurs probability conditional entropy binary erasure channel fails fraction time capacity precisely fraction time channel reliable result seems reasonable far obvious encode information communicate reliably channel answer alternatively without invoking symmetry assumed start input ensemble probability receive posterior probability prior probability mutual information achieves maximum value copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links solutions figure extended channel obtained binary erasure channel erasure probability block code consisting two codewords optimal decoder code solution exercise extended channel shown fig ure best code channel obtained choosing two columns minimal overlap example columns decoding algorithm returns extended channel output among top four among bottom four gives output solution exercise example showed mutual information input output channel differentiate expression respect taking care confuse log log log setting derivative zero rearranging using skills developed exer cise obtain optimal input distribution noise level tends expression tends prove using hˆopital rule values smaller rough intuition input used less input input used noisy channel injects entropy received string whereas input used noise zero entropy solution exercise capacities three channels shown figure bec channel highest bsc bec figure capacities channel binary symmetric channel binary erasure channel capacity bsc lowest solution exercise logarithm posterior probability ratio given copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links communication noisy channel using skills picked exercise rewrite form optimal decoder selects probable hypothesis done simply looking sign decode probability error 
[noisy-channel, coding, theorem, theorem] theorem three parts two positive one negative main positive result first figure portion plane proved achievable achievable every discrete memoryless channel channel capacity max following property large enough exists code length rate decoding algorithm maximal probability block error probability bit error acceptable rates achiev able rates greater achievable 
[noisy-channel, coding, theorem, jointly-typical, sequences] formalize intuitive preview last chapter define codewords coming ensemble con sider random selection one codeword corresponding channel put thus defining joint ensemble use typical set decoder decodes received signal jointly typical term defined shortly proof centre determining probabilities true input codeword jointly typical output sequence false input codeword jointly typical output show large probabilities zero long fewer codewords ensemble optimal input distribution joint typicality pair sequences length defined jointly typical tolerance respect distribution typical log typical log typical log copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links jointly typical sequences jointly typical set set jointly typical sequence pairs length example jointly typical pair length ensemble corresponds binary symmetric channel noise level notice typical probability tolerance typical differ bits typical number flips channel joint typicality theorem let drawn ensemble defined probability jointly typical tolerance tends number jointly typical sequences close precise independent samples marginal distribution probability lands jointly typical set precise proof proof parts law large numbers follows source coding theorem chapter part let pair play role source coding theorem replacing probability distribution third part cartoon jointly typical set shown figure two independent typical vectors jointly typical probability total number independent typical pairs area dashed rectangle number jointly typical pairs roughly probability hitting jointly typical pair roughly copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links noisy channel coding theorem dots figure jointly typical set horizontal direction represents set input strings length vertical direction represents set output strings length outer box contains conceivable input output pairs dot represents jointly typical pair sequences total number jointly typical sequences 
[noisy-channel, coding, theorem, analogy] imagine wish prove baby class one hundred babies weighs less individual babies difficult catch weigh shannon method solving task scoop babies figure shannon method proving one baby weighs less weigh big weighing machine find average weight smaller must exist least one baby weighs less indeed must many shannon method guaranteed reveal existence underweight child since relies tiny number elephants class use method get total weight smaller task solved 
[noisy-channel, coding, theorem, skinny, children, fantastic, codes] wish show exists code decoder small prob ability error evaluating probability error particular coding decoding system easy shannon innovation instead constructing good coding decoding system evaluating error prob ability shannon calculated average probability block error codes proved average small must exist individual codes small probability block error 
[noisy-channel, coding, theorem, random, coding, typical-set, decoding] consider following encoding decoding system whose rate fix generate codewords copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links proof noisy channel coding theorem figure random code example decodings typical set decoder sequence jointly typical codewords decoded sequence jointly typical codeword alone decoded similarly decoded sequence jointly typical one codeword decoded code random according random code shown schematically figure code known sender receiver message chosen transmitted received signal signal decoded typical set decoding typical set decoding decode jointly typical jointly typical otherwise declare failure optimal decoding algorithm good enough easier analyze typical set decoder illustrated fig ure decoding error occurs three probabilities error distinguish first probability block error particular code difficult quantity evaluate given code second average codes block error probability copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links noisy channel coding theorem fortunately quantity much easier evaluate first quantity probability decoding error step five step process previous page third maximal block error probability code max quantity interested wish show exists code required rate whose maximal block error probability small get result first finding average block error probability shown made smaller desired small number immediately deduce must exist least one code whose block error probability also less small number finally show code whose block error probability satisfactorily small whose maximal block error probability unknown could conceivably enormous modified make code slightly smaller rate whose maximal block error probability also guaranteed small modify code throwing away worst codewords therefore embark finding average probability block error 
[noisy-channel, coding, theorem, probability, error, typical-set, decoder] two sources error use typical set decoding either output jointly typical transmitted codeword codeword jointly typical symmetry code construction average probability error averaged codes depend selected value assume without loss generality probability input output jointly typical vanishes joint typicality theorem first part give name upper bound probability satisfying desired find blocklength probability jointly typical given part rival values worry thus average probability error satisfies inequality bounds total probability error tot sum probabilities sorts events sufficient cause error tot called union bound equality different events cause error never occur time average probability error made increasing almost make three modifications choose proof optimal input distribution channel condition becomes copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links communication errors capacity random code expurgated figure expurgation works typical random code small fraction codewords involved collisions pairs codewords sufficiently close probability error either codeword transmitted tiny obtain new code random code deleting confusable codewords resulting code slightly fewer codewords slightly lower rate maximal probability error greatly reduced since average probability error codes must exist code mean probability block error show average also maximal probability error made small modify code throwing away worst half codewords ones likely produce errors remain must conditional probability error less use remaining codewords define new code new code codewords reduced rate negligible reduction large achieved trick called expurgation figure resulting code may best code rate length still good enough prove noisy channel coding theorem trying conclusion construct code rate maximal probability error obtain theorem stated setting sufficiently large remaining conditions hold theorem first part thus proved 
[noisy-channel, coding, theorem, communicationwith, errors), capacity] achievable figure portion plane proved achievable first part theorem proved maximal probability block error made arbitrarily small goes bit error probability must smaller proved discrete memoryless channel achievability portion plane shown figure shown turn noisy channel essentially noiseless binary channel rate bits per cycle extend right hand boundary region achievability non zero error probabilities called rate distortion theory new trick since know make noisy channel perfect channel smaller rate sufficient consider commu nication errors noiseless channel fast communicate noiseless channel allowed make errors consider noiseless binary channel assume force communi cation rate greater capacity bit example require sender attempt communicate bits per cycle must effectively throw away half information best way aim achieve smallest possible probability bit error one simple strategy communicate fraction source bits ignore rest receiver guesses missing fraction random copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links noisy channel coding theorem average probability bit error curve corresponding strategy shown dashed line fig ure better terms minimizing spreading risk corruption evenly among bits fact achieve shown solid curve figure optimum simple figure simple bound achievable points shannon bound optimum achieved reuse tool developed namely code noisy channel turn head using decoder define lossy compressor specifically take excellent code binary symmetric channel assume code rate capable correcting errors introduced binary symmetric channel whose transition probability asymptotically rate codes exist recall attach one capacity achieving codes length binary symmetric channel probability distribution outputs close uniform since entropy output equal entropy source plus entropy noise optimal decoder code situation typically maps received vector length transmitted vector differing bits received vector take signal wish send chop blocks length yes pass block decoder obtain shorter signal length bits communicate noiseless channel decode transmission pass bit message encoder original code reconstituted message differ original message bits typically probability bit error rate lossy compressor attaching lossy compressor capacity error free commu nicator proved achievability communication curve defined reading rate distortion theory see gallager mceliece 
[noisy-channel, coding, theorem, non-achievable, regionpart, theorem)] source encoder noisy channel decoder define markov chain data processing inequality exercise must apply chain furthermore definition channel capacity assume system achieves rate bit error probability mutual information achievable achievable exercise fill details preceding argument bit errors independent copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links computing capacity complex correlations among bit errors inequality hold 
[noisy-channel, coding, theorem, computing, capacity] proved capacity channel maximum rate sections contain advanced material first time reader encouraged skip section reliable communication achieved compute capacity given discrete memoryless channel need find optimal input distri bution general find optimal input distribution computer search making use derivative mutual information respect input probabilities exercise find derivative respect input prob ability channel conditional probabilities exercise show concave function input prob ability vector since concave input distribution probability distri bution stationary must global maximum tempting put derivative routine finds local maximum input distribution lagrange multiplier associated constraint however approach may fail find right answer might maximized distribution inputs simple example given ternary confusion channel ternary confusion channel whenever input used output random inputs reliable inputs maximum information rate bit achieved making use input exercise sketch mutual information channel function input distribution pick convenient two dimensional representation optimization routine must therefore take account possibility hill may run inequality constraints exercise describe condition similar equation satisfied point maximized describe com puter program finding capacity channel copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links noisy channel coding theorem 
[noisy-channel, coding, theorem, results, may, help, finding, optimal, input, distribution] outputs must used convex function channel parameters reminder term convex means convex term concave means concave little smile frown symbols included simply remind convex concave mean may several optimal input distributions look output exercise prove output unused optimal input distri bution unless unreachable exercise prove convex function exercise prove optimal input distributions channel output probability distribution results along fact concave function input probability vector prove validity symmetry argument used finding capacity symmetric channels channel invariant group symmetry operations example interchanging input symbols interchanging output symbols given optimal input distribution symmetric invariant operations create another input distribution averaging together optimal input distribution permuted forms make applying symmetry operations original optimal input distribution permuted distributions must original symmetry new input distribution created averaging must bigger equal original distribution concavity 
[noisy-channel, coding, theorem, symmetric, channels] order use symmetry arguments help definition symmetric channel like gallager definition discrete memoryless channel symmetric channel set outputs partitioned subsets way subset matrix transition probabilities property row permutation row column permutation column example channel symmetric channel outputs partitioned matrix rewritten copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links coding theorems symmetry useful property see later chapter communication capacity achieved symmetric channels linear codes exercise prove symmetric channel number inputs uniform distribution inputs optimal input distribution exercise channels symmetric whose timal input distributions uniform find one prove none 
[noisy-channel, coding, theorem, coding, theorems] noisy channel coding theorem proved chapter quite gen eral applying discrete memoryless channel specific theorem says reliable communication error probability rate achieved using codes sufficiently large blocklength theorem say large needs achieve given values presumably smaller closer larger 
[noisy-channel, coding, theorem] figure typical random coding exponent 
[noisy-channel, coding, theorem, noisy-channel, coding, theorem, version, explicit, -dependence] discrete memoryless channel blocklength rate exist block codes length whose average probability error satisfies exp random coding exponent channel convex decreasing positive function random coding exponent also known reliability function expurgation argument also shown exist block codes maximal probability error also exponentially small definition given gallager approaches zero typical behaviour function illustrated fig ure computation random coding exponent interesting channels challenging task much effort expended even simple channels like binary symmetric channel simple pression 
[noisy-channel, coding, theorem, lower, bounds, error, probability, function, blocklength] theorem stated asserts codes smaller exp small error probability could much smaller code blocklength discrete memoryless channel probability error assuming source messages used equal probability satisfies exp copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links noisy channel coding theorem function sphere packing exponent channel convex decreasing positive function precise statement result references see gallager 
[noisy-channel, coding, theorem, noisy-channel, coding, theorems, coding, practice] imagine customer wants buy error correcting code decoder noisy channel results described allow offer following service tells properties channel desired rate desired error probability working relevant functions advise exists solution problem using particular blocklength indeed almost randomly chosen code blocklength job unfortunately found implement encoders decoders practice cost implementing encoder decoder random code large would exponentially large furthermore practical purposes customer unlikely know actly channel dealing berlekamp suggests sensible way approach error correction design encoding decoding systems plot performance variety idealized channels function channel noise level charts one illustrated page shown customer choose among systems offer without specify really thinks channel like attitude practical problem importance functions diminished 
[noisy-channel, coding, theorem, exercises] exercise binary erasure channel input output transition probability matrix   find mutual information input output general input distribution show capacity channel bits channel transition probability matrix show using code two uses channel made emulate one use erasure channel state erasure probability erasure channel hence show capacity channel satisfies bits explain result inequality rather equality copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links solutions exercise transatlantic cable contains indistinguish able electrical wires job figuring wire create consistent labelling wires end tools ability connect wires groups two test connectedness continuity tester smallest number transatlantic trips need make would solve problem larger illustration task solved two steps labelling one wire one end connecting two together crossing atlantic measuring two wires connected labelling unconnected one connecting returning across atlantic whereupon disconnecting identities deduced problem solved persistent search reason posed chapter also solved greedy approach based maximizing acquired information let unknown per mutation wires chosen set connections wires one end make measurements end measurements convey information much set connections information conveys maximized 
[noisy-channel, coding, theorem, solutions] solution exercise input distribution mutual information build good sketch function two ways careful inspection function looking special cases plots two dimensional representation use independent variables inspection use quantities two degrees freedom mutual information becomes simple converting back obtain sketch shown left function like tunnel rising direction increasing obtain required plot strip away parts tunnel live outside feasible simplex probabilities redrawing surface showing parts full plot function shown right copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links noisy channel coding theorem special cases special case channel noiseless binary channel special case term equal special case channel channel error probability know sketch previous chapter figure figure skeleton mutual information ternary confusion channel special cases allow construct skeleton shown figure solution exercise necessary sufficient conditions maximize constant related capacity log result used computer program evaluates deriva tives increments decrements probabilities proportion differences derivatives result also useful lazy human capacity finders good guessers guessed optimal input distribution one simply con firm equation holds solution exercise certainly expect nonsymmetric chan nels uniform optimal input distributions exist since inventing channel degrees freedom whereas optimal input dis tribution dimensional dimensional space perturbations around symmetric channel expect subspace perturbations dimension leave optimal input distribution unchanged explicit example bit like channel       solution exercise labelling problem solved two trips one way across atlantic key step information theoretic approach problem write information content one partition combinatorial object connecting together subsets wires wires grouped together subsets size subsets size number partitions information content one partition log quantity greedy strategy choose first partition maximize information content one game play maximize information content spect quantities treated real numbers subject constraint introducing lagrange multiplier constraint derivative log log log copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links solutions set zero leads rather nice expression optimal proportional poisson distribution solve lagrange multiplier plugging constraint gives implicit equation convenient reparameterization lagrange multiplier figure shows graph figure shows deduced non integer assignments nearby integers motivate setting first partition fgh ijk lmno pqrst partition produces random partition end figure approximate solution cable labelling problem using lagrange multipliers parameter function value highlighted non integer values function shown lines integer values motivated non integer values shown crosses information content log bits lot half total information content need acquire infer transatlantic permutation log bits contrast wires joined together pairs information content generated bits choose second partition left reader shannonesque approach appropriate picking random partition end using need ensure two partitions unlike possible labelling problem solutions particularly simple implement called knowlton graham partitions par tition disjoint sets two ways subject condition one element appears set cardinal ity set cardinality graham graham knowlton 
[error-correcting, codes, amp, real, channels] imagine gaussian channel used encoding system transmit binary source bits rate bits per channel use compare two encoding systems different rates communi cation use different powers transmitting large rate good using small power good conventional measure rate compensated signal noise ratio ratio power per source bit noise spectral density dimensionless usually reported units decibels value given log one measures used compare coding schemes gaussian channels copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links inferring input real channel 
[error-correcting, codes, amp, real, channels, gaussian, channel] popular model real input real output channel gaussian channel gaussian channel real input real output condi tional distribution given gaussian distribution exp channel continuous input output discrete time show certain continuous time channels equivalent discrete time gaussian channel channel sometimes called additive white gaussian noise awgn channel discrete channels discuss rate error free information communication achieved channel 
[error-correcting, codes, amp, real, channels, motivation, terms, continuous-time, channel] consider physical electrical say channel inputs outputs continuous time put comes transmission power cost average power transmission length may constrained thus received signal assumed differ additive noise example johnson noise model white gaussian noise magnitude noise quantified noise spectral density copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links error correcting codes real channels could channel used communicate information consider figure three basis functions weighted combination transmitting set real numbers signal duration made weighted combination orthonormal basis functions receiver compute scalars noise would equal white gaussian noise adds scalar noise estimate noise gaussian normal spectral density introduced thus continuous chan nel used way equivalent gaussian channel defined equa tion power constraint defines constraint signal amplitudes returning gaussian channel define bandwidth mea sured hertz continuous channel max max maximum number orthonormal functions produced interval length definition motivated imagining creating band limited signal duration orthonormal sine sine curves maximum frequency number orthonormal functions max definition relates nyquist sampling theorem highest frequency present signal signal fully determined values series discrete sample points separated nyquist interval seconds use real continuous channel bandwidth noise spectral density power equivalent uses per second gaussian channel noise level subject signal power constraint 
[error-correcting, codes, amp, real, channels, definition)[error-correcting, codes, amp, real, channels, ‘the, best, detection, pulses’] shannon wrote memorandum shannon problem best differentiating two types pulses known shape represented vectors given one transmitted noisy channel pattern recognition problem assumed figure two pulses represented dimensional vectors noisy version one noise gaussian probability density det exp inverse variance covariance matrix noise sym metric positive definite matrix multiple identity matrix noise white general noise coloured probability received vector given source signal either zero one det exp optimal detector based posterior probability ratio exp exp constant independent received vector detector forced make decision guess either decision minimizes probability error guess prob able hypothesis write optimal decision terms discriminant function decisions figure weight vector used discriminate guess guess guess either notice linear function received vector 
[error-correcting, codes, amp, real, channels, capacity, gaussian, channel] measured joint marginal conditional entropy discrete variables order define information conveyed continuous variables two issues must address infinite length real line infinite precision real numbers copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links error correcting codes real channels 
[error-correcting, codes, amp, real, channels, infinite, inputs] much information convey one use gaussian channel allowed put real number gaussian channel could communicate enormous string digits setting amount error free information conveyed single transmission could made arbitrarily large increasing communication could made arbitrarily reliable increasing number zeroes end usually power cost associated large inputs however mention practical limits dynamic range acceptable receiver therefore conventional introduce cost function every input constrain codes average cost less equal maximum value generalized channel coding theorem including cost function inputs proved see mceliece result channel capacity function permitted cost gaussian channel assume cost average power input constrained motivated cost function case real electrical channels physical power consumption indeed quadratic constraint makes impossible communicate infinite information one use gaussian channel 
[error-correcting, codes, amp, real, channels, infinite, precision] figure probability density question define entropy density could evaluate entropies sequence probability distributions decreasing grain size entropies tend log independent entropy goes one bit every halving log illegal integral tempting define joint marginal conditional entropies real variables simply replacing summations integrals well defined operation discretize interval smaller smaller divi sions entropy discrete distribution diverges logarithm granularity figure also permissible take logarithm dimensional quantity probability density whose dimensions one information measure however well behaved limit namely mutual information one really matters since measures much information one variable conveys another discrete case log argument log ratio two probabilities space probability densities replace sum integral log log ask questions gaussian channel probability distribution maximizes mutual information subject constraint maximal mutual information still measure maximum error free communication rate real channel discrete channel copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links capacity gaussian channel exercise prove probability distribution max imizes mutual information subject constraint gaussian distribution mean zero variance exercise show mutual information case optimized distribution log important result see capacity gaussian channel function signal noise ratio 
[error-correcting, codes, amp, real, channels, inferences, given, gaussian, input, distribution] normal normal marginal distribution normal posterior distribution input given output exp exp normal step made completing square exponent formula deserves careful study mean posterior distribution viewed weighted combination value best fits output value best fits prior weights precisions two gaussians multiplied together equation prior likelihood precision posterior distribution sum two pre cisions general property whenever two independent sources con tribute information via gaussian distributions unknown variable precisions add dual better known relationship independent variables added variances add 
[error-correcting, codes, amp, real, channels, noisy-channel, coding, theorem, gaussian, channel] evaluated maximal mutual information correspond maximum possible rate error free information transmission one way proving define sequence discrete channels derived gaussian channel increasing numbers inputs outputs prove maximum mutual information channels tends asserted noisy channel coding theorem discrete channels applies derived channels thus obtain coding theorem continuous channel alternatively make intuitive argument coding theorem specific gaussian channel copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links error correcting codes real channels 
[error-correcting, codes, amp, real, channels, geometrical, view, noisy-channel, coding, theorem, sphere, packing] consider sequence inputs corresponding output defining two points dimensional space large noise power likely close fractionally output therefore likely close surface sphere radius centred similarly original signal generated random subject average power constraint likely lie close sphere centred origin radius total average power received signal likely lie surface sphere radius centred origin volume dimensional sphere radius consider making communication system based non confusable inputs inputs whose spheres overlap significantly max imum number non confusable inputs given dividing volume sphere probable volume sphere given thus capacity bounded log log detailed argument like one used previous chapter tablish equality 
[error-correcting, codes, amp, real, channels, back, continuous, channel] recall use real continuous channel bandwidth noise spectral density power equivalent uses per second gaussian channel subject constraint substituting result capacity gaussian channel find capacity continuous channel log bits per second formula gives insight tradeoffs practical communication imag ine fixed power constraint best bandwidth make use power introducing bandwidth signal noise ratio figure shows log function capacity increases asymptote log dramatically better terms capacity fixed power transmit low signal noise ratio large bandwidth high signal noise narrow bandwidth one motivation wideband communication methods direct sequence spread spectrum approach used mobile phones course alone electromagnetic neigh bours may pleased use large bandwidth social reasons engineers often make higher power narrow bandwidth trans mitters capacity bandwidth figure capacity versus bandwidth real channel log function copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links capabilities practical error correcting codes 
[error-correcting, codes, amp, real, channels, capabilities, practical, error-correcting, codes?] nearly codes good nearly codes require exponential look tables practical implementation encoder decoder exponential blocklength coding theorem required large practical error correcting code mean one encoded decoded reasonable amount time example time scales polynomial function blocklength preferably linearly 
[error-correcting, codes, amp, real, channels, shannon, limit, achieved, practice] non constructive proof noisy channel coding theorem showed good block codes exist noisy channel indeed nearly block codes good writing explicit practical encoder coder good promised shannon still unsolved problem good codes given channel family block codes achieve arbitrarily small probability error communication rate capacity channel called good codes channel good codes code families achieve arbitrarily small probability error non zero communication rates maximum rate may less capacity given channel bad codes code families cannot achieve arbitrarily small probability error achieve arbitrarily small probability error decreasing information rate zero repetition codes example bad code family bad codes necessarily useless practical purposes practical codes code families encoded decoded time space polynomial blocklength 
[error-correcting, codes, amp, real, channels, established, codes, linear, codes] let review definition block code add definition linear block code block code channel list codewords length signal encoded comes alphabet size encoded linear block code block code codewords make dimensional subspace encoding operation represented binary matrix signal encoded binary notation vector length bits encoded signal modulo codewords defined set vectors satisfying mod parity check matrix code         example hamming code section takes signal bits transmits followed three parity check bits transmitted symbols given mod coding theory born work hamming invented fam ily practical error correcting codes able correct one error block length repetition code code copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links error correcting codes real channels simplest since established codes generalizations hamming codes bose chaudhury hocquenhem codes reed uller codes reed solomon codes goppa codes name 
[error-correcting, codes, amp, real, channels, convolutional, codes] another family linear codes convolutional codes divide source stream blocks instead read transmit bits continuously transmitted bits linear function past source bits usually rule generating transmitted bits involves feeding present source bit linear feedback shift register length transmitting one linear functions state shift register iteration resulting transmitted bit stream convolution source stream linear filter impulse response function filter may finite infinite duration depending choice feedback shift register discuss convolutional codes chapter 
[error-correcting, codes, amp, real, channels, linear, codes, ‘good’] one might ask reason shannon limit achieved practice linear codes inherently good random codes answer noisy channel coding theorem still proved linear codes least channels see chapter though proofs like shannon proof random codes non constructive linear codes easy implement encoding end decoding linear code also easy necessarily general decoding problem find maximum likelihood equation fact complete berlekamp complete problems computational problems equally difficult widely believed require expo nential computer time solve general attention focuses families codes fast decoding algorithm 
[error-correcting, codes, amp, real, channels, concatenation] one trick building codes practical decoders idea concatena tion encoder channel decoder system viewed defining super channel smaller probability error complex correlations among errors create encoder decoder super channel code consisting outer code followed inner code known concatenated code concatenated codes make use idea interleaving read data blocks size block larger blocklengths constituent codes encoding data one block using code bits reordered within block way nearby bits separated block fed second code simple example interleaver rectangular code product code data arranged block encoded horizontally using linear code vertically using linear code exercise show either two codes viewed inner code outer code example figure shows product code encode first repetition code also known hamming code copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links capabilities practical error correcting codes figure product code string encoded using concatenated code consisting two hamming codes noise pattern flips bits received vector decoding using horizontal decoder subsequently using vertical decoder decoded vector matches original decoding order three errors still remain horizontally vertically blocklength concatenated code number source bits per codeword four shown small rectangle decode conveniently though optimally using individual decoders subcodes sequence makes sense first decode code lowest rate hence greatest error correcting ability figure shows happens receive codeword fig ure errors five bits flipped shown apply decoder first decoder first decoder corrects three errors erroneously modifies third bit second row two bit errors decoder correct three errors figure shows happens decode two codes order columns one two two errors decoder introduces two extra errors corrects one error column decoder cleans four errors erroneously infers second bit 
[error-correcting, codes, amp, real, channels, interleaving] motivation interleaving spreading bits nearby one code make possible ignore complex correlations among errors produced inner code maybe inner code mess entire codeword codeword spread one bit time several codewords outer code treat errors introduced inner code independent 
[error-correcting, codes, amp, real, channels, channel, models] addition binary symmetric channel gaussian channel coding theorists keep complex channels mind also burst error channels important models practice reed solomon codes use galois fields see appendix large numbers elements input alphabets thereby automatically achieve degree burst error tolerance even successive bits corrupted successive symbols galois field representation corrupted concate nation interleaving give protection burst errors concatenated reed solomon codes used digital compact discs able correct bursts errors length bits copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links error correcting codes real channels exercise technique interleaving allows bursts errors treated independent widely used theoretically poor way protect data burst errors terms amount redundancy required explain interleaving poor method using following burst error channel example time divided chunks length clock cycles chunk burst probability burst channel nary symmetric channel burst channel error free binary channel compute capacity channel compare maximum communication rate could con ceivably achieved one used interleaving treated errors independent fading channels real channels like gaussian channels except received power assumed vary time moving mobile phone important example incoming radio signal reflected nearby objects interference patterns intensity signal received phone varies location received power easily vary decibels factor ten phone antenna moves distance similar wavelength radio signal centimetres 
[error-correcting, codes, amp, real, channels, state, art] best known codes communicating gaussian channels practical codes linear codes either based convolutional codes block codes 
[error-correcting, codes, amp, real, channels, convolutional, codes, codes, based] textbook convolutional codes facto standard error correcting code satellite communications convolutional code constraint length convolutional codes discussed chapter concatenated convolutional codes convolutional code used inner code concatenated code whose outer code reed solomon code eight bit symbols code used deep space communication systems voyager spacecraft reading reed solomon codes see lin costello code galileo code using format using longer constraint length convolutional code larger reed solomon code developed jet propulsion laboratory swan son details code unpublished outside jpl decoding possible using room full special purpose hardware best code known rate turbo codes berrou glavieux thitimajshima reported work turbo codes encoder turbo code based encoders two convolutional codes source bits fed encoder order source bits permuted random way resulting parity bits constituent code transmitted decoding algorithm involves iteratively decoding constituent code using standard decoding algorithm using output figure encoder turbo code box contains convolutional code source bits reordered using permutation fed transmitted codeword obtained concatenating interleaving outputs two convolutional codes random permutation chosen code designed fixed thereafter decoder input decoder decoding algorithm copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links summary instance message passing algorithm called sum product algorithm turbo codes discussed chapter message passing chap ters 
[error-correcting, codes, amp, real, channels, block, codes] gallager low density parity check codes best block codes known figure low density parity check matrix corresponding graph rate low density parity check code blocklength constraints white circle represents transmitted bit bit participates constraints represented squares constraint forces sum bits connected even code code outstanding performance obtained blocklength increased gaussian channels invented gallager promptly forgotten coding theory community rediscovered shown outstanding theoretical prac tical properties like turbo codes decoded message passing algorithms discuss beautifully simple codes chapter performances codes compared gaussian channels figure 
[error-correcting, codes, amp, real, channels, summary] random codes good require exponential resources encode decode non random codes tend part good random codes non random code encoding may easy even simply defined linear codes decoding problem remains difficult best practical codes employ large block sizes based semi random code constructions make use probability based decoding algorithms 
[error-correcting, codes, amp, real, channels, nonlinear, codes] practically used codes linear digital soundtracks encoded onto cinema film binary pattern likely errors affecting film involve dirt scratches produce large numbers respectively want none codewords look like easy detect errors caused dirt scratches one codes used digital cinema sound systems nonlinear code consisting binary patterns weight 
[error-correcting, codes, amp, real, channels, errors, noise] another source uncertainty receiver uncertainty tim ing transmitted signal ordinary coding theory infor mation theory transmitter time receiver time sumed perfectly synchronized receiver receives signal receiver time imperfectly known function transmitter time capacity channel commu nication reduced theory channels incomplete compared synchronized channels discussed thus far even pacity channels synchronization errors known levenshtein ferreira codes reliable communication channels synchronization errors remain active research area davey mackay copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links error correcting codes real channels 
[error-correcting, codes, amp, real, channels, reading] review history spread spectrum methods see scholtz 
[error-correcting, codes, amp, real, channels, gaussian, channel] exercise consider gaussian channel real input signal noise ratio capacity input constrained binary capacity constrained channel addition output channel thresholded using mapping capacity resulting channel plot three capacities function need numerical integral evaluate exercise large integers fraction binary error correcting codes length rate linear codes answer depend whether choose define code ordered list codewords mapping define code unordered list two codes consisting codewords identical use latter definition code set codewords encoder operates part definition code 
[error-correcting, codes, amp, real, channels, erasure, channels] exercise design code binary erasure channel decoding algorithm evaluate probability error design good codes erasure channels active research area spielman byers see also chapter exercise design code ary erasure channel whose input drawn whose output equal probability equal otherwise erasure channel good model packets transmitted internet either received reliably lost exercise redundant arrays independent disks raid work information storage systems consisting ten people say raid stands redundant array inexpensive disks think silly raid would still good idea even disks expensive disk drives two three disabled others able still able reconstruct requested file codes used far systems shannon limit problem solving would design better raid system information provided solution section see http www acnc com raid html see also chapter copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links solutions 
[error-correcting, codes, amp, real, channels, solutions] solution exercise introduce lagrange multiplier power constraint another constraint normalization make functional derivative respect final factor found using whole last term collapses puff smoke absorbed term substitute exp set derivative zero exp condition must satisfied writing taylor expansion quadratic function would satisfy constraint higher order terms would produce terms present right hand side therefore gaussian obtain optimal output distribution using gaussian input distribution solution exercise given gaussian input distribution vari ance output distribution normal since noise independent random variables variances add independent random variables mutual information log log log log log solution exercise capacity channel one minus information content noise adds information content per chunk entropy selection whether chunk bursty plus probability entropy flipped bits adds per chunk roughly accurate large per bit capacity contrast interleaving treats bursts errors independent causes channel treated binary symmetric channel whose capacity copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links error correcting codes real channels interleaving throws away useful information correlated ness errors theoretically able communicate times faster using code decoder explicitly treat bursts bursts solution exercise putting together results exercises deduce gaussian channel real input signal noise ratio capacity log input constrained binary capacity achieved using two inputs equal probability capacity reduced somewhat messy integral log log exp capacity smaller unconstrained capacity small signal noise ratio two capacities close value output thresholded gaussian channel turned binary symmetric channel whose transition probability given error function defined page capacity figure capacities top bottom graph versus signal noise ratio lower graph log log plot solution exercise several raid systems one easiest understand consists disk drives store data rate using hamming code successive four bits encoded code seven codeword bits written one disk two perhaps three disk drives others recover data effective channel model binary erasure channel assumed tell disk dead possible recover data choices three dead disk drives see exercise give example three disk drives lost lead failure raid system three lost without failure solution exercise hamming code codewords weight set three disk drives corresponding one code words lost four disks recover bits information four source bits fourth bit lost exercise binary mds codes deficit discussed section set three disk drives lost without problems corresponding four four submatrix generator matrix invertible better code would digital fountain see chapter 
[information, retrieval, information-retrieval, problem] simple example information retrieval problem task imple menting phone directory service response person name returns confirmation person listed directory person phone number details could formalize prob lem follows number names must stored directory string length number strings number possible strings figure cast characters given list binary strings length bits considerably smaller total number possible strings call superscript record number string idea runs customers order added directory name customer assume simplicity people names length name length might say bits might want store details ten million customers ignore possibility two customers identical names task construct inverse mapping make system given string returns value one exists otherwise reports exists record number look memory location separate memory full phone numbers find required number aim solving task use minimal computational resources terms amount memory used store inverse mapping amount time compute inverse mapping preferably inverse mapping implemented way new strings added directory small amount computer time 
[information, retrieval, standard, solutions] simplest dumbest solutions information retrieval problem look table raw list look table piece memory size log log amount memory required store integer locations put zero except locations correspond strings write value look table simple quick solution sufficient memory table cost looking entries copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links hash codes codes efficient information retrieval memory independent memory size definition task assumed bits amount memory required would size solution completely question bear mind number particles solar system raw list simple list ordered pairs ordered value mapping achieved searching list strings starting top comparing incoming string record match found system easy maintain uses small amount memory bits rather slow use since average five million pairwise comparisons made exercise show average time taken find required string raw list assuming original names chosen random binary comparisons note compare whole string length since comparison terminated soon mismatch occurs show need average two binary comparisons per incorrect string match compare worst case search time assuming devil chooses set strings search key standard way phone directories made improves look table raw list using alphabetically ordered list alphabetical list strings sorted alphabetical order searching entry usually takes less time needed raw list take advantage sortedness example open phonebook middle page compare name find target string target greater middle string know required string exists found second half alphabetical directory otherwise look first half iterating splitting middle proce dure identify target string establish string listed dlog string comparisons expected number binary comparisons per string comparison tend increase search progresses total number binary comparisons required greater dlog amount memory required required raw list adding new strings database requires insert correct location list find location takes dlog binary comparisons improve well established alphabetized list let consider task new viewpoints task construct mapping bits log bits pseudo invertible mapping since maps non zero customer database contains pair takes back come across idea mapping bits bits encountered idea twice first source coding studied block codes mappings strings symbols selection one label list task information retrieval similar task copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links hash codes never actually solved making encoder typical set compression code second time mapped bit strings bit strings another dimensionality studied channel codes considered codes mapped bits bits greater made theoretical progress using random codes hash codes put together two notions study random codes map bits bits smaller idea map original high dimensional space lower dimensional space one feasible implement dumb look table method rejected moment ago string length number strings size hash function bits size hash table figure revised cast characters 
[information, retrieval, hash, codes] first describe hash code works study properties idealized hash codes hash code implements solution information retrieval problem mapping help pseudo random function called hash function maps bit string bit string smaller typically chosen table size little bigger say ten times bigger example expecting million might map bit hash regardless size item hash function fixed deterministic function ideally indistinguishable fixed random code practical purposes hash function must quick compute two simple examples hash functions division method table size prime number preferably one close power hash value remainder integer divided variable string addition method method assumes string bytes table size characters added modulo hash function defect maps strings anagrams onto hash may improved putting running total fixed pseu dorandom permutation character added variable string exclusive method table size string hashed twice way initial running total set respectively algorithm result bit hash picked hash function implement information retriever follows see figure encoding piece memory called hash table created size memory units amount memory needed represent integer table initially set zero throughout memory put hash function location hash table corresponding resulting vector integer written unless entry hash table already occupied case collision earlier happen hash code collisions handled various ways discuss moment first let complete basic picture copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links hash codes codes efficient information retrieval algorithm code implementing variable string exclusive method create hash range string author thomas niemann unsigned char rand array contains random permutation int hash char pointer first char int first character unsigned char return special handling empty string initialize two hashes proceed next character rand exclusive two hashes rand put randomizer end string reached int shift left bits add int return hash concatenation hash function strings hashes hash table bits bits figure use hash functions information retrieval string hash computed value written hth row hash table blank rows hash table contain value zero table size copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links collision resolution decoding retrieve piece information corresponding target vector compute hash look corresponding location hash table zero know immediately string database cost answer cost one hash function evaluation one look table size hand non zero entry table two possibilities either vector indeed equal vector another vector happens hash code target third possibility non zero entry might something yet discussed collision resolution system check whether indeed equal take tentative answer look original forward database compare bit bit matches report desired answer successful retrieval overall cost one hash function evaluation one look table size another look table size binary comparisons may much cheaper simple solutions presented section exercise checked first bits found equal probability correct entry retrieved alternative hypothesis actually database assume original source strings random hash function random hash function many binary evaluations needed sure odds billion one correct entry retrieved hashing method information retrieval used strings arbitrary length hash function applied strings length 
[information, retrieval, collision, resolution] study two ways resolving collisions appending table storing elsewhere 
[information, retrieval, appending, table] encoding collision occurs continue hash table write value next available location memory currently contains zero reach bottom table encountering zero continue top decoding compute hash code find contained table point matches cue continue hash table either find whose match cue case done else encounter zero case know cue database method essential table substantially bigger size encoding rule become stuck nowhere put last strings 
[information, retrieval, storing, elsewhere] robust flexible method use pointers additional pieces memory collided strings stored many ways copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links hash codes codes efficient information retrieval example could store location hash table pointer must distinguishable valid record number bucket strings hash code stored sorted list encoder sorts strings bucket alphabetically hash table buckets created decoder simply look relevant bucket check short list strings brief alphabetical search method storing strings buckets allows option making hash table quite small may practical benefits may make small almost strings involved collisions buckets contain small number strings takes small number binary comparisons identify strings bucket matches cue 
[information, retrieval, planning, collisions, birthday, problem] exercise wish store entries using hash function whose output bits many collisions expect happen assuming hash function ideal random function size hash table needed would like expected number collisions smaller size hash table needed would like expected number collisions small fraction say notice similarity problem exercise 
[information, retrieval, checking, arithmetic] wish check addition done hand may find useful method casting nines casting nines one finds sum modulo nine digits numbers summed compares sum modulo nine digits putative answer little practice sums computed much rapidly full original addition example calculation shown margin sum modulo nine digits sum modulo nine calculation thus passes casting nines test casting nines gives simple example hash function addition expression form decimal numbers define sum modulo nine digits nice property decimal arithmetic hashes equal exercise evidence correct casting nines match give favour hypothesis addition done cor rectly copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links roles hash codes 
[information, retrieval, error, detection, among, friends] two files files computer could compare bit bit two files separate machines would nice way confirming two files identical without transfer one files even transfer one files would still like way confirm whether received without modifications problem solved using hash codes let alice bob holders two files alice sent file bob wish confirm received without error alice computes hash file sends bob bob computes hash file using bit hash function two hashes match bob deduce two files almost surely example probability false negative probability given two files differ two hashes nevertheless identical assume hash function random process causes files differ knows nothing hash function probability false negative bit hash gives probability false negative common practice use linear hash function called bit cyclic redundancy check detect errors files cyclic redundancy check set parity check bits similar parity check bits hamming code false negative rate smaller one billion bits plenty errors produced noise exercise simple parity check code detects errors help correct since error correcting codes exist use one get error correcting capability 
[information, retrieval, tamper, detection] differences two files simply noise introduced adversary clever forger called fiona modifies original file make forgery purports alice file alice make digital signature file bob confirm one tampered file prevent fiona listening alice signature attaching files let assume alice computes hash function file sends securely bob alice computes simple hash function file like linear cyclic redundancy check fiona knows method verifying file integrity fiona make chosen modifications file easily identify linear algebra single bits flipped restore hash function file original value linear hash functions give security forgers must therefore require hash function hard invert one construct tampering leaves hash function unaffected would still like hash function easy compute however bob hours work verify every file received hash function easy compute hard invert called one way copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links hash codes codes efficient information retrieval hash function finding functions one active research areas cryptography hash function widely used free software community confirm two files differ produces bit hash details works quite complicated involving convoluted exclusive ing ing ing even good one way hash function digital signatures described still vulnerable attack fiona access hash function fiona could take tampered file hunt tiny modification hash matches original hash alice file would take time average attempts hash function bits eventually fiona would find tampered file matches given hash secure forgery digital signatures must either enough bits random search take long hash function must kept secret fiona hash files cheat file modifications many bit hash function large enough forgery prevention another person might motivation forgery alice example might making bet outcome race without wishing broadcast prediction publicly method placing bets would send bob bookie hash bet later could send bob details bet everyone confirm bet consis tent previously publicized hash method secret publication used isaac newton robert hooke wished establish priority scientific ideas without revealing hooke hash function alphabetization illustrated conversion tensio sic vis anagram ceiiinosssttuv protocol relies assumption alice cannot change bet event without hash coming wrong big hash function need use ensure alice cannot cheat answer different size hash needed order defeat fiona alice author files alice could cheat searching two files identical hashes example like cheat placing two bets price one could make large number versions bet one differing minor details large number versions bet two hash collision hashes two bets different types submit common hash thus buy option placing either bet example hash bits big need alice good chance finding two different bets hash birthday problem like exercise montagues capulets party assigned birthday bits expected number collisions montague capulet http www freesoft org cie rfc htm copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exercises minimize number files hashed alice make equal need hash files finds two match alice hash files cheat square root number hashes fiona make alice use computers years computer taking evaluate hash bet communication system secure alice dishonesty log bits 
[information, retrieval, reading] bible hash codes volume knuth highly recommend story doug mcilroy spell program told section programming pearls bentley astonishing piece software makes use kilobyte data structure store spellings words word dictionary 
[information, retrieval, exercises] exercise shortest address typical international letter could get unique human recipient assume permitted characters long typical email addresses exercise long piece text need pretty sure human written string characters many notes new melody composed exercise pattern recognition molecules proteins produced cell regulatory role regulatory protein controls transcription specific genes genome control often involves protein binding particular dna sequence vicinity regulated gene presence bound protein either promotes inhibits transcription gene use information theoretic arguments obtain lower bound size typical protein acts regulator specific one gene whole human genome assume genome sequence nucleotides drawn four letter alphabet protein sequence amino acids drawn twenty letter alphabet hint establish long recognized dna sequence order sequence unique vicinity one gene treating rest genome random sequence discuss big protein must recognize sequence length uniquely sequences recognized dna binding regulatory pro teins consist subsequence repeated twice example sequence gccccccacccctgccccc copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links hash codes codes efficient information retrieval binding site found upstream alpha actin gene humans fact binding sites consist repeated subse quence influence answer part 
[information, retrieval, solutions] solution exercise first imagine comparing string another random string probability first bits two strings match probability second bits match suming stop comparing hit first mismatch expected number matches expected number comparisons exercise assuming correct string located random raw list compare average strings find costs binary comparisons comparing correct strings takes binary comparisons giving total expectation binary comparisons strings chosen random worst case may indeed happen practice strings similar search key lengthy sequence comparisons needed find mismatch worst case correct string last list strings differ last bit giving requirement binary comparisons solution exercise likelihood ratio two hypotheses contributed datum first bits equal datum datum first bits match likelihood ratio one finding bits match odds billion one favour assuming start even odds complete answer compute evidence given prior information hash entry found table fact gives evidence favour solution exercise let hash function output phabet size equal log would exactly enough bits entry unique hash probability one particular pair entries collide random hash function number pairs expected number collisions pairs exactly would like smaller need log need twice many bits number bits log would sufficient give entry unique name happy occasional collisions involving fraction names need since probability one particular name collided log log copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links solutions means need extra bits log important point note scaling two cases want hash function collision free must greater happy small frequency collisions needs order solution exercise posterior probability ratio two hypotheses calculation correct calculation incorrect product prior probability ratio likelihood ratio match match second factor answer question numerator match equal denominator value depends model errors know human calculator prone errors involving multiplication answer transposition adjacent digits neither affects hash value match could equal also correct match gives evidence favour assume errors random point view hash function probability false positive match correct match gives evidence favour solution exercise add tiny extra bits hash huge bit file get pretty good error detection probability error undetected less one billion error correction requires far check bits number depending expected types corruption file size example eight random bits megabyte file corrupted would take log bits specify corrupted bits number parity check bits used successful error correcting code would least number counting argument exercise solution solution exercise want know length string improbable string matches part entire writings humanity let estimate writings total one book person living book contains two million characters pages characters per page characters drawn alphabet say characters probability randomly chosen string length matches one point collected works humanity expected number matches vanishingly small log redundancy repetition humanity writings possible overestimate want write something unique sit compose string ten characters write gidnebinzz already thought string new melody focus sequence notes ignoring duration stress allow leaps octave note number choices per note pitch first note arbitrary number melodies length notes rather ugly ensemble sch onbergian tunes example length restricting permitted intervals reduce figure including duration stress increase restrict permitted intervals repetitions tones semitones reduction particularly severe melody ode joy sounds boring number recorded compositions probably less million learn new melodies per week every week life learned melodies age based copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links hash codes codes efficient information retrieval empirical experience playing game guess tune seems guess tune one player chooses melody sings gradually increasing number notes participants try guess whole melody parsons code related hash function melodies pair consecutive notes coded second note higher first repeat pitches equal otherwise find well hash function works http musipedia org whereas many four note sequences shared common melodies number collisions five note sequences rather smaller famous five note sequences unique solution exercise let dna binding protein recognize sequence length nucleotides binds preferentially dna sequence pieces dna whole genome reality recognized sequence may contain wildcard characters tataa denotes precise assuming recognized sequence contains non wildcard characters assuming rest genome random sequence con sists random nucleotides equal probability obviously untrue make much difference calculation chance occurrence target sequence whole genome length nucleotides roughly exp close one log log using require recognized sequence longer min nucleotides size protein imply weak lower bound obtained assuming information content protein sequence greater information content nucleotide sequence protein prefers bind argued must least bits gives minimum protein length log amino acids thinking realistically recognition dna sequence pro tein presumably involves protein coming contact sixteen nucleotides target sequence protein monomer must big enough simultaneously make contact sixteen cleotides dna one helical turn dna containing ten nucleotides length contiguous sequence sixteen nucleotides length diameter protein must therefore greater egg white lysozyme small globular protein length amino acids diameter suming volume proportional sequence length volume scales cube diameter protein diameter must sequence length amino acids however target sequence consists twice repeated sub sequence get much smaller protein recognizes sub sequence binds dna strongly form dimer halves bound recognized sequence halving diameter protein need protein whose length greater amino acids protein length smaller cannot serve regulatory protein specific one gene simply small able make sufficiently specific match available surface enough information content 
[binary, codes] established shannon noisy channel coding theorem general chan nel input output alphabets great deal attention coding theory focuses special case channels binary inputs first implicit choice binary symmetric channel optimal decoder code given binary symmetric channel finds codeword closest received vector closest hamming dis example hamming distance tance hamming distance two binary vectors number coordinates two vectors differ decoding errors occur noise takes transmitted codeword received vector closer codeword distances codewords thus relevant probability decoding error 
[binary, codes, distance, properties, code] distance code smallest separation two codewords example hamming code distance pairs codewords differ least bits maximum number errors correct general code distance error correcting precise term distance minimum distance code distance code often denoted min constrain attention linear codes linear code codewords identical distance properties summarize distances code codewords counting distances zero codeword weight enumerator function code defined number codewords code weight weight enumerator total figure graph hamming code weight enumerator function function also known distance distribution code example weight enumerator functions hamming code dodecahedron code shown figures 
[binary, codes, obsession, distance] since maximum number errors code guarantee correct related distance many coding theorists focus odd even distance code searching codes given size biggest possible distance much practical coding theory focused decoders give optimal decoding error patterns weight half distance codes copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links obsession distance total figure graph defining dodecahedron code circles transmitted bits triangles parity checks one redundant weight enumerator function solid lines dotted lines show average weight enumerator function random linear codes size generator matrix computed shortly lower figure shows functions log scale bounded distance decoder decoder returns closest code word received binary vector distance codeword less equal otherwise returns failure message rationale trying decode errors occurred might guarantee correct errors bother trying would interested decoder corrects error patterns weight greater others defeatist attitude example worst case ism widespread mental ailment book intended cure fact bounded distance decoders cannot reach shannon limit binary symmetric channel decoder often corrects errors state art error correcting codes decoders work way beyond minimum distance code 
[binary, codes, definitions, good, bad, distance, properties] given family codes increasing blocklength rates approach ing limit may able put family one following categories similarities categories good bad codes defined earlier sequence codes good distance tends constant greater zero sequence codes bad distance tends zero sequence codes bad distance tends constant figure graph rate low density generator matrix code rightmost transmitted bits connected single distinct parity constraint leftmost transmitted bits connected small number parity constraints example low density generator matrix code linear code whose generator matrix small number per row regardless big minimum distance code low density generator matrix codes bad distance large distance bad thing see later emphasis distance unhealthy copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links binary codes figure schematic picture part hamming space perfectly filled spheres centred codewords perfect code 
[binary, codes, perfect, codes] sphere sphere radius hamming space centred point set points whose hamming distance less equal hamming code beautiful property place spheres codewords spheres perfectly fill hamming space without overlapping saw chapter every binary vector length within distance exactly one codeword hamming code code perfect error correcting code set spheres cen tred codewords code fill hamming space without lapping see figure let recap cast characters number codewords number points entire hamming space number points hamming sphere radius code perfect parameters require times number points sphere equal perfect code equivalently perfect code number noise vectors one sphere must equal number possible syndromes hamming code satisfies numerological condition copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links perfect codes figure schematic picture hamming space perfectly filled spheres centred codewords code grey regions show points hamming distance codeword misleading picture code large high dimensions grey space spheres takes almost hamming space 
[binary, codes, happy, would, use, perfect, codes] large numbers perfect codes choose wide range blocklengths rates would perfect solution shannon problem could communicate binary symmetric channel noise level example picking perfect error correcting code blocklength chosen probability noise flips bits satisfactorily small however almost perfect codes nontrivial perfect binary codes hamming codes perfect codes blocklength defined rate hamming code approaches blocklength increases repetition codes odd blocklength perfect codes rate repetition codes goes zero one remarkable error correcting code codewords block length known binary golay code second error correcting golay code length ternary alphabet dis covered finnish football pool enthusiast called juhani virtakallio binary perfect codes shortage perfect codes precise numerological coincidences like satisfied parameters hamming code golay code rare plenty almost perfect codes spheres fill almost whole space fact picture hamming spheres centred codewords almost filling hamming space figure misleading one codes whether good codes bad codes almost hamming space taken space spheres shown grey figure established gloomy picture spend moment filling properties perfect codes mentioned copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links binary codes figure three codewords 
[binary, codes, hamming, codes] hamming code defined linear code whose parity check matrix contains columns non zero vectors length since vectors different single bit flip produces distinct syndrome single bit errors detected corrected generalize code parity constraints follows hamming codes single error correcting codes defined picking number parity check constraints blocklength parity check matrix contains columns non zero vectors length bits first hamming codes following rates checks repetition code hamming code exercise probability block error hamming code leading order code used binary symmetric channel noise density 
[binary, codes, perfectness, unattainable, first, proof] show several ways useful perfect codes exist useful means large blocklength rate close neither shannon proved given binary symmetric channel noise level exist codes large blocklength rate close like enable communication arbitrarily small error probability large number errors per block typically codes shannon almost certainly error correcting codes let pick special case noisy channel find large perfect code error correcting well let suppose code found examine three codewords remember code ought rate enormous number codewords without loss generality choose one codewords zero codeword define two overlaps shown figure second codeword differs first fraction coordinates third codeword differs first fraction second fraction fraction coordinates value zero three codewords code error correcting minimum distance must greater copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links weight enumerator function random linear codes summing three inequalities dividing two deduce impossible code cannot exist code cannot three codewords let alone conclude whereas shannon proved plenty codes communicating binary symmetric channel perfect codes study general argument indicates large perfect linear codes general rates finding typical distance random linear code 
[binary, codes, weight, enumerator, function, random, linear, codes] imagine making code picking binary entries parity check figure random binary parity check matrix matrix random weight enumerator function expect weight enumerator one particular code parity check matrix number codewords weight written sum vectors whose weight truth function equals one zero otherwise find expected value evaluating probability particular word weight codeword code averaging binary linear codes ensemble symmetry probability depends weight word details word probability entire syndrome zero found multiplying together probabilities bits syndrome zero bit syndrome sum mod random bits probability probability thus independent expected number words weight given summing words weight probability word codeword number words weight copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links binary codes large use log write log concrete example figure shows expected weight enumerator function rate random linear code figure expected weight enumerator function random linear code lower figure shows logarithmic scale 
[binary, codes, gilbert–varshamov, distance] weights expectation smaller weights expectation greater thus expect large minimum distance random linear code close distance defined definition distance gilbert varshamov distance rate blocklength gilbert varshamov conjecture widely believed asserts large possible create binary codes minimum distance significantly greater definition gilbert varshamov rate maximum rate reliably communicate bounded distance decoder defined assuming gilbert varshamov conjecture true 
[binary, codes, sphere-packing, bad, perspective, obsession, distance, inappropriate] one uses bounded distance decoder maximum tolerable noise level flip fraction min bits assuming min equal gilbert distance capacity r_gv figure contrast shannon channel capacity gilbert rate maximum communication rate achievable using bounded distance decoder function noise level given rate maximum tolerable noise level shannon twice big maximum tolerable noise level worst case ist uses bounded distance decoder crunch shannon say achievable said maximum possible rate communication capacity given rate maximum tolerable noise level according shannon given conclusion imagine good code rate chosen equations respectively define maximum noise levels tolerable bounded distance decoder shannon decoder bounded distance decoders ever cope half noise level shannon proved tolerable relate perfect codes code perfect spheres around codewords fill hamming space without overlapping copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links berlekamp bats typical random linear code used communicate nary symmetric channel near shannon limit typical number bits flipped minimum distance codewords also little bigger little shannon limit spheres around codewords overlap sufficiently sphere almost contains centre nearest neighbour reason figure two overlapping spheres whose radius almost big distance centres overlap disastrous high dimensions volume associated overlap shown shaded figure tiny fraction either sphere probability landing extremely small moral story worst case ism bad halving ability tolerate noise able decode way beyond minimum distance code get shannon limit nevertheless minimum distance code interest practice conditions minimum distance dominates errors made code 
[binary, codes, berlekamp’s, bats] blind bat lives cave flies centre cave corre sponds one codeword typical distance centre controlled friskiness parameter displacement bat centre corresponds noise vector boundaries cave made stalactites point towards centre cave figure stalactite analogous boundary home codeword codeword stalactite like shaded region figure reshaped convey idea region small volume decoding errors correspond bat intended trajectory passing inside stalactite collisions stalactites various distances centre possible friskiness small bat usually close centre cave collisions rare occur usually involve stalactites whose tips closest centre point similarly low noise conditions decoding errors rare typi cally involve low weight codewords low noise conditions minimum distance code relevant small probability error figure berlekamp schematic picture hamming space vicinity codeword jagged solid line encloses points codeword closest sphere around codeword takes small fraction space copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links binary codes friskiness higher bat may often make excursions beyond safe distance longest stalactites start collide fre quently distant stalactites owing greater number tiny number stalactites minimum distance rela tively unlikely cause errors similarly errors real error correcting code depend properties weight enumerator function high friskiness bat always long way centre cave almost collisions involve contact distant stalactites conditions bat collision frequency nothing distance centre closest stalactite 
[binary, codes, concatenation, hamming, codes] instructive play concatenation hamming codes concept first visited figure get insights notion good codes relevance otherwise minimum distance code create concatenated code binary symmetric channel noise density encoding several hamming codes succession table recaps key properties hamming codes indexed number constraints hamming codes minimum distance correct one error blocklength number source bits probability block error leading order figure rate concatenated hamming code function number concatenations make product code concatenating sequence hamming codes increasing choose parameters way rate product code tends non zero limit increases example set etc asymptotic rate figure blocklength rapidly growing function codes somewhat impractical weakness codes min imum distance good figure every one constituent figure blocklength upper curve minimum distance lower curve concatenated hamming code function number concatenations hamming codes minimum distance minimum distance cth product blocklength grows faster ratio tends zero increases contrast typical random codes ratio tends constant concatenated hamming codes thus bad distance nevertheless turns simple sequence codes yields good codes channels good codes see section recall definitions terms good good rather prove result simply explore numerically figure shows bit error probability concatenated codes assuming constituent codes decoded sequence described section one code time decoding suboptimal saw horizontal axis shows rates codes number concatenations increases rate drops error probability drops towards zero channel assumed figure binary symmetric copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links distance everything channel highest noise level tolerated using concatenated code figure bit error probabilities versus rates concatenated hamming codes binary symmetric channel labels alongside points show blocklengths solid line shows shannon limit channel bit error probability drops zero rate tends concatenated hamming codes good code family take home message story distance everything minimum distance code although widely worshipped coding theorists fundamental importance shannon mission achieving reliable communication noisy channels exercise prove exist families codes bad distance good codes 
[binary, codes, distance, isn’t, everything] let get quantitative feeling effect minimum distance code special case binary symmetric channel 
[binary, codes, error, probability, associated, one, low-weight, codeword] let binary code blocklength two codewords differ places simplicity let assume even error probability code used binary symmetric channel noise level bit flips matter places two codewords differ error probability dominated probability bits flipped happens bits irrelevant since optimal decoder ignores block error error probability associated single codeword weight plotted figure using approximation binomial coefficient figure error probability associated single codeword weight function approximate block error called bhattacharyya parameter channel consider general linear code distance block error prob ability must least independent blocklength code reason sequence codes increasing blocklength constant distance bad distance cannot block ror probability tends zero binary symmetric channel interested making superb error correcting codes tiny tiny error probability might therefore shun codes bad distance however pragmatic look carefully figure chapter argued codes disk drives need error probability smaller raw error probability disk drive error probability associated one codeword distance smaller raw error probability disk drive error probability associated one codeword distance smaller practical purposes therefore essential code good distance example codes blocklength known many codewords weight nevertheless correct errors weight tiny error probability copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links binary codes want think recommending use codes bad distance chapter discuss low density parity check codes favourite codes excellent performance good distance 
[binary, codes, union, bound] error probability code binary symmetric channel bounded terms weight enumerator function adding appropriate multiples error probability associated single codeword block error wgt inequality example union bound accurate low noise levels inaccurate high noise levels overcounts contribution errors cause confusion one codeword time exercise poor man noisy channel coding theorem pretending union bound accurate using aver age weight enumerator function random linear code section estimate maximum rate one communicate binary symmetric channel look positively using union bound inequality show communication rates possible binary symmetric channel following chapter analysing probability error syndrome decoding binary linear code using union bound prove shannon noisy channel coding theorem symmetric binary channels thus show good linear codes exist 
[binary, codes, dual, codes] concept importance coding theory though immediate use book idea dual linear error correcting code linear error correcting code thought set codewords generated adding together combinations independent basis codewords generator matrix code consists basis codewords conventionally written row vectors example hamming code generator matrix       sixteen codewords displayed table code words code linear combinations four vectors code may also described terms parity check matrix set vectors satisfy copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links dual codes one way thinking equation row specifies vector must orthogonal codeword generator matrix specifies vectors codewords built parity check matrix specifies set vectors codewords orthogonal dual code obtained exchanging generator matrix parity check matrix definition set vectors length orthogonal code words code called dual code orthogonal also orthogonal codewords orthogonal linear combination rows set linear combinations rows parity check matrix dual code hamming code parity check matrix   dual hamming code code shown table table eight codewords dual hamming code compare table possibly unexpected property pair codes dual contained within code every word dual code codeword original hamming code relationship written using set notation possibility set dual vectors overlap set codeword vectors counterintuitive think vectors real vectors vector orthogonal work modulo two arithmetic many non zero vectors indeed orthogonal exercise give simple rule distinguishes whether binary vector orthogonal three vectors 
[binary, codes, duals] general code systematic generator matrix matrix parity check matrix copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links binary codes example repetition code generator matrix parity check matrix two codewords dual code generator matrix equivalently modifying systematic form row additions call dual code simple parity code code one parity check bit equal sum two source bits dual code four codewords case vector common code dual zero codeword 
[binary, codes, goodness, duals] sequence codes good duals good examples constructed cases good codes good duals random linear codes bad codes bad duals good codes bad duals last category especially important many state art codes property duals bad classic example low density parity check code whose dual low density generator matrix code exercise show low density generator matrix codes bad family low density generator matrix codes defined two param eters column weight row weight rows columns respectively weights fixed independent example hint show code low weight codewords use argument exercise show low density parity check codes good good distance solutions see gallager mackay 
[binary, codes, self-dual, codes] hamming code property dual contained code code self orthogonal contained dual example dual hamming code self orthogonal code one way seeing overlap pair rows even codes contain duals important quantum error correction calderbank shor intriguing though necessarily useful look codes self dual code self dual dual code identical code properties self dual codes deduced copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links generalizing perfectness channels code self dual generator matrix also parity check matrix code self dual codes rate codewords even weight exercise property must matrix satisfy code generator matrix self dual examples self dual codes repetition code simple example self dual code smallest non trivial self dual code following code       exercise find relationship code hamming code 
[binary, codes, duals, graphs] let code represented graph nodes two types parity check constraints equality constraints joined edges rep resent bits code need transmitted dual code graph obtained replacing parity check nodes equality nodes vice versa type graph called normal graph forney 
[binary, codes, reading] duals important coding theory functions involving code posterior distribution codewords transformed fourier transform functions dual code accessible introduction fourier analysis finite groups see terras see also macwilliams sloane 
[binary, codes, generalizing, perfectness, channels] given search perfect codes binary symmetric channel could console changing channel could call code perfect error correcting code binary erasure channel restore erased bits never rather using perfect error correcting code binary erasure channel number redundant bits must word perfect however conventional term code maximum distance separable code mds code already noted exercise hamming code mds code recover sets erased bits bits corresponding codeword weight erased one bit information unrecoverable code poor choice raid system copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links binary codes tiny example maximum distance separable code simple parity check code whose parity check matrix code codewords even parity codewords separated distance single erased bit restored setting parity two bits repetition codes also maximum distance separable codes exercise make code parity symbols ary erasure channel decoder recover codeword symbols erased block example channel symbols code correct erasures ary erasure channel large numbers mds codes reed solomon codes famous widely used long field size bigger blocklength mds block codes rate found reading see lin costello 
[binary, codes, summary] shannon codes binary symmetric channel almost always correct errors error correcting codes 
[binary, codes, reasons, distance, code, little, relevance] shannon limit shows best codes must able cope noise level twice big maximum noise level bounded distance decoder binary symmetric channel code bounded distance decoder communicate shannon says good codes exist channels concatenation shows get good performance even dis tance bad whole weight enumerator function relevant question whether code good code relationship good codes distance properties discussed exercise 
[binary, codes, exercises] exercise codeword selected linear code transmitted noisy channel received signal assume channel memoryless channel gaus sian channel given assumed channel model two decoding problems codeword decoding problem task inferring codeword transmitted given received signal bitwise decoding problem task inferring transmitted bit likely bit one rather zero copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exercises consider optimal decoders two decoding problems prove probability error optimal bitwise decoder closely related probability error optimal codeword decoder proving following theorem theorem binary linear code minimum distance min given channel codeword bit error probability optimal bitwise decoder block error probability maxi mum likelihood decoder related min exercise minimum distances hamming code hamming code exercise let average weight enumerator function rate random linear code estimate first principles value exercise code minimum distance greater rather nice code generated generator matrix based measuring parities triplets source bits           find minimum distance weight enumerator function code exercise find minimum distance pentagonful low figure graph pentagonful low density parity check code bit nodes circles parity check nodes triangles graph known petersen graph density parity check code whose parity check matrix                               show nine ten rows independent code param eters using computer find weight enumerator function exercise replicate calculations used produce figure check assertion highest noise level correctable explore alternative concatenated sequences codes find better sequence concatenated codes better sense either higher asymptotic rate tolerate higher noise level copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links binary codes exercise investigate possibility achieving shannon limit linear block codes using following counting argument assume linear code large blocklength rate code parity check matrix rows assume code optimal decoder solves syndrome decoding problem allows reliable communication binary symmetric channel flip probability many typical noise vectors roughly many distinct syndromes since reliably deduced optimal decoder number syndromes must greater equal number typical noise vectors tell largest possible value rate given exercise linear binary codes use input symbols equal probability implicitly treating channel symmetric chan nel investigate much loss communication rate caused assumption fact channel highly asymmetric channel take example channel much smaller maximum possible rate communication using symmetric inputs capacity channel answer exercise show codes bad distance bad codes defined section exercise one linear code obtained another punctur ing puncturing means taking codeword deleting defined set bits puncturing turns code code another way make new linear codes old shortening shortening means constraining defined set bits zero deleting codewords typically shorten one bit half code codewords lost shortening typically turns code code another way make new linear code two old ones make intersection two codes codeword retained new code present two old codes discuss effect code distance properties puncturing short ening intersection possible turn code family bad distance code family good distance vice versa three manipulations exercise todd ebert hat puzzle three players enter room red blue hat placed person head colour hat determined coin toss outcome one coin toss effect others person see players hats communication sort allowed except initial strategy session group enters room chance look hats players must simultaneously guess copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links solutions hat colour pass group shares million prize least one player guesses correctly players guess incorrectly already know hat puzzle could try scottish version rules prize awarded group guess correctly reformed scottish version players must guess correctly two rounds guessing players guess round one leave room remaining players must guess round two strategy team adopt maximize chance winning game played number players general problem find strategy group maximizes chances winning prize find best strategies groups size three seven hint done three seven might able solve fifteen exercise estimate many binary low density parity check codes self orthogonal duals note expect huge number since almost low density parity check codes good low density parity check code contains dual must bad exercise figure plotted error probability associated single codeword weight function noise level binary symmetric channel make equivalent plot case gaussian channel showing error probability associated single codeword weight function rate compensated signal noise ratio depends rate choose code rate choose 
[binary, codes, solutions] solution exercise probability block error leading order solution exercise binary vector perpendicular even weight even number solution exercise self dual code two equivalent parity check matrices must equivalent row additions matrix right hand sides equation left hand sides become thus code generator matrix self dual orthogonal matrix modulo vice versa solution exercise codes intimately related code whose parity check matrix       obtained appending extra parity check bit thought parity seven bits hamming code reordering first four bits copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links binary codes solution exercise code parity symbols property decoder recover codeword symbols erased block code said maximum distance separable mds mds binary codes exist apart repetition codes simple parity codes mds codes found simple example code ary erasure channel code defined terms multiplication addition rules given appendix elements input alphabet generator matrix code resulting codewords aaaaaaaa bbbbbbbb cccccccc dddddddd eeeeeeee ffffffff abcdef badcfe abefcd bafedc cdefab dcfeba efcdab fedcba aacebfd abdfaec aaecbdf abfdace aceafdb adfbeca aecadfb afdbcea bbedfca bafcedb bacfdeb bbdecfa bcfabde bdebacf bedbafc bfcabed ccbfead cdaefbc caedcfb cbfcdea ccfbaed cdeabfc ceadcbf cfbcdae ddcafbe dcdbeaf dafbedc dbeafcd dcdebfa ddcfaeb debfacd dfaebdc eefdbac efecabd eacdbfe ebdcaef ecabdfe edbacef eefbdca efeacdb ffdaecb fecbfda fadfbce fbceadf fcbedaf fdafcbe febcfad ffadebc solution exercise quick rough proof theorem let denote difference reconstructed codeword transmitted codeword given channel output posterior distribution posterior distribution positive vectors belonging code sums follow codewords block error probability average bit error probability averaging bits codeword weight codeword weights non zero codewords satisfy min substituting inequalities definitions tain min factor two stronger right stated result making proof watertight weakened result little careful proof theorem relates performance optimal block coding algorithm optimal bitwise decoding algorithm introduce another pair decoding algorithms called block guessing decoder bit guessing decoder idea two algorithms similar optimal block decoder optimal bitwise decoder lend easily analysis define decoders let denote inferred codeword given code copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links solutions optimal block decoder returns codeword maximizes posterior probability proportional likelihood probability error decoder called optimal bit decoder returns bits value maximizes posterior probability probability error decoder called block guessing decoder returns random codeword probabil ity distribution given posterior probability probability error decoder called bit guessing decoder returns bits random bit probability distribution probability error decoder called theorem states optimal bit error probability bounded given multiple left hand inequality trivially true block correct constituent bits correct optimal block decoder outperformed optimal bit decoder could make better bit decoder block decoder prove right hand inequality establishing bit guessing decoder nearly good optimal bit decoder bit guessing decoder error probability related block guessing decoder min since min min prove two lemmas near optimality guessing consider first case single bit posterior probability optimal bit decoder probability error optimal min guessing decoder picks truth also distributed probability probability guesser truth match probability mismatch guessing error probability guess min optimal since average many error probabilities guess average corresponding optimal error probabilities optimal obtain desired relationship copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links binary codes relationship bit error probability block error probability bit guessing block guessing decoders combined single system draw sample marginal distribution drawing sample joint distribution discarding value distinguish two cases discarded value correct codeword probability bit error bit guessing decoder written sum two terms correct bit error correct incorrect bit error incorrect bit error incorrect whenever guessed incorrect true must differ least bits probability bit error cases least qed solution exercise number typical noise vectors roughly number distinct syndromes reliable communication implies terms rate bound agrees precisely capacity channel argument turned proof following chapter solution exercise three player case possible group win three quarters time three quarters time two players hats colour third player hat opposite colour group win every time happens using following strategy player looks two players hats two hats different colours passes colour player guesses hat opposite colour way every time hat colours distributed two one one player guess correctly others pass group win game hats colour however three players guess incorrectly group lose particular player guesses colour true chance guess right reason group wins time strategy ensures players guessing wrong great many guessing wrong larger numbers players aim ensure time one wrong occasionally everyone wrong game players strategy group wins every times play game players group win times figured winning strategies teams recommend thinking solution three player game terms copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links solutions locations winning losing states three dimensional hypercube thinking laterally number players optimal strategy defined using hamming code length probability winning prize player identified number two colours mapped onto state hats viewed received vector binary channel random binary vector length either codeword hamming code probability differs exactly one bit codeword player looks bits considers whether bit set colour state codeword deduced using decoder hamming code player guesses hat colour state actually codeword players guess guess wrong state non codeword one player guess guess correct quite easy train seven players follow optimal strategy cyclic representation hamming code used 
[good, linear, codes, exist] chapter use single calculation prove simultaneously source coding theorem noisy channel coding theorem binary symmet ric channel incidentally proof works much general channel models binary symmetric channel example proof reworked channels non binary outputs time varying channels chan nels memory long binary inputs satisfying symmetry property section 
[good, linear, codes, exist, coding, theorems] consider linear error correcting code binary parity check matrix matrix rows columns later proof increase keeping rate code satisfies rows independent equality follows assume equality holds eager readers may work expected rank random binary matrix close pursue effect difference rank rest proof negligible codeword selected satisfying mod binary symmetric channel adds noise giving received signal chapter denotes noise added channel input channel mod receiver aims infer using syndrome decoding approach syndrome decoding first introduced section receiver computes syndrome mod mod mod syndrome depends noise decoding problem find probable satisfies mod copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links good linear codes exist best estimate noise vector subtracted give best guess aim show long flip probability binary symmetric channel optimal decoder syndrome decoding problem vanishing probability error increases random prove result studying sub optimal strategy solving decoding problem neither optimal decoder typical set decoder would easy implement typical set decoder easier analyze typical set decoder examines typical set noise vectors set noise vectors satisfy log checking see leave xfs make typical set definition rigorous enthusiasts encouraged revisit section put details proof typical vectors satisfies observed syndrome exactly one typical vector typical set decoder reports vector hypothesized noise vector typical vector matches observed syndrome one typical set decoder reports error probability error typical set decoder given matrix written sum two terms probability true noise vector typical probability true typical least one typical vector clashes first probability vanishes increases proved first studied typical sets chapter concentrate second probability recap imagining true noise vector typical noise vectors different satisfies error use truth function whose value one statement true zero otherwise bound number type errors made noise thus number errors given number errors either zero one sum right hand side may exceed one cases several typical noise vectors equation union bound syndrome write probability type error averaging find average probability type error linear codes averaging showing average probability type error vanishes thus show exist linear codes vanishing error probability indeed almost linear codes good denote averaging binary matrices average probability type error copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links data compression linear hash codes quantity already cropped calculating expected weight enumerator function random linear codes section non zero binary vector probability averaging matrices denotes size typical set recall chapter roughly noise vectors typical set bound probability error either vanishes grows exponentially increases remembering keeping proportional increases vanishes substituting thus established noisy channel coding theorem binary symmetric channel good linear codes exist rate satisfying entropy channel noise per bit exercise redo proof general channel 
[good, linear, codes, exist, data, compression, linear, hash, codes] decoding game played also viewed uncompres sion game world produces binary noise vector source noise redundancy flip probability compress linear compressor maps bit input noise bit output syndrome uncompression task recover input output rate compressor compressor care possibility linear redundancies definition rate result found decoding problem solved almost vanishing error probability long thus instantly proves source coding theorem given binary source entropy required com pressed rate exists linear compressor mod rate equal required rate associated uncompressor virtually lossless theorem true source independent identically dis tributed symbols also source typical set fined sources memory time varying sources example required source ergodic copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links good linear codes exist 
[good, linear, codes, exist, notes] method proving codes good applied linear codes low density parity check codes mackay aji code need approximation expected weight enumerator function 
[exercises, information, theory] figure rate reliable communication function noise level desired variable rate code conservative approach would design encoding system worst case scenario installing code rate dashed line figure event lower noise level holds true managers would feeling regret wasted capacity difference possible create system transmits reliably rate whatever noise level also communicates extra lower priority bits noise level low shown figure code communicates high priority bits reliably noise levels communicates low priority bits also noise level problem mathematically equivalent previous problem degraded broadcast channel lower rate communication called rate low priority bits communicated noise level low called figure achievable region channel unknown noise level assuming two possible noise levels dashed lines show rates achievable using simple time sharing approach solid line shows rates achievable using cunning approach illustrative answer shown figure case figure also shows achievable region broadcast channel whose two half channels noise levels admit find gap simple time sharing solution cunning solution disappointingly small chapter discuss codes special class broadcast channels namely erasure channels every symbol either received without error erased codes nice property rateless number symbols transmitted determined fly reliable comunication achieved whatever erasure statistics channel exercise multiterminal information networks important practi cally intriguing theoretically consider following example two way binary channel figure two people wish talk channel want hear person saying hear signal transmitted person transmitting zero simultaneous information rates achieved everyday examples networks include vhf channels used ships computer ethernet networks devices unable hear anything two devices broadcasting simultaneously obviously achieve rates directions simple time sharing two information rates made larger finding capacity general two way channel still open problem however obtain interesting results concerning achievable points simple binary channel discussed indicated figure exist codes achieve rates boundary shown may exist better codes 
[exercises, information, theory, refresher, exercises, source, coding, noisy, channels] exercise let ensemble consider source coding using block coding every containing fewer assigned distinct codeword ignored assigned codewords length find min imum length required provide set distinct code words calculate probability getting ignored exercise let ensemble semble encoded using symbol code consider codeword corresponding large compute entropy fourth bit transmission compute conditional entropy fourth bit given third bit estimate entropy hundredth bit estimate conditional entropy hundredth bit given ninety ninth bit exercise two fair dice rolled alice sum recorded bob task ask sequence questions yes answers find number devise detail strategy achieves minimum possible average number questions exercise use coin draw straws among people exercise magic trick three participants magician assistant volunteer assistant claims paranor mal abilities soundproof room magician gives volunteer six blank cards five white one blue volunteer writes dif ferent integer card magician watching volunteer keeps blue card magician arranges five white cards order passes assistant assistant announces number blue card trick work copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exercises information theory exercise trick work ordinary pack cards shuffled random order please choose five cards pack wish let see faces give pass assistant esmerelda look esmerelda show four cards hmm nine spades six clubs four hearts ten diamonds hidden card must queen spades trick performed described pack cards use information theory give upper bound number cards trick performed exercise find probability sequence exercise consider discrete memoryless source eight letter words formed four letters find total number words typical set equation exercise consider source channel whose transition probability matrix       note source alphabet five symbols channel alphabet four assume source produces symbols exactly rate channel accepts channel sym bols given tiny explain would design system communicating source output channel aver age error probability per source symbol less explicit possible particular invoke shannon noisy channel coding theorem exercise consider binary symmetric channel code assume four codewords used probabilities decoding rule minimizes probability decoding error optimal decoding rule depends noise level binary symmetric channel give decoding rule range values exercise find capacity optimal input distribution three input three output channel whose transition probabilities   copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exercises information theory exercise input channel word bits output also word bits time used channel flips exactly one transmitted bits receiver know one seven bits received without error bits equally likely one flipped derive capacity channel show describing explicit encoder decoder possible reliably zero error probability communicate bits per cycle channel exercise channel input output conditional probability matrix       capacity exercise ten digit number cover book known isbn incorporates error detecting code number consists nine table valid isbns hyphens included legibility source digits satisfying tenth check digit whose value given mod tenth digit shown using roman numeral show valid isbn satisfies mod imagine isbn communicated unreliable human chan nel sometimes modifies digits sometimes reorders digits show code used detect correct errors one ten digits modified example show code used detect errors two jacent digits transposed example transpositions pairs non adjacent digits tected tenth digit defined mod would code work well discuss detection modifications single digits transpositions digits copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exercises information theory exercise channel input output transition proba bility matrix       assuming input distribution form write entropy output conditional entropy output given input show optimal input distribution given log log remember log write optimal input distribution capacity chan nel case comment answer exercise differences redundancies needed error detecting code reliably detect block data corrupted error correcting code detect cor rect errors 
[exercises, information, theory, tales, information, theory] following exercises give chance discover answers surprising results information theory exercise communication information correlated sources imag ine want communicate data two data sources central location via noise free one way communication channels fig ure signals strongly dependent joint information content little greater marginal information con tent either example weather collator wishes receive string reports saying whether raining allerton whether raining bognor joint probability might weather collator would like know successive values exactly since pay every bit information receives interested possibility avoiding buying bits source bits source assuming variables generated repeatedly distribution encoded rates way reconstruct variables sum information transmission rates two lines less two bits per cycle copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exercises information theory encode encode hhh achievable figure communication information dependent sources dependent sources dependence represented dotted arrow strings values variable encoded using codes rate transmissions communicated noise free channels receiver achievable rate region strings conveyed without error even though answer demonstrate indicated figure general case two dependent sources exist codes two transmitters achieve reliable communication long information rate exceeds information rate exceeds total information rate exceeds joint entropy slepian wolf case transmitter must transmit rate greater bits total rate must greater bits example exist codes achieve rates task figure try find explicit solution one sources sent plain text encoded exercise multiple access channels consider channel two sets inputs one output example shared telephone line figure simple model system two binary inputs ternary output equal arithmetic sum two inputs noise users cannot communicate cannot hear output channel output receiver certain inputs set output receiver certain inputs set output could input state users use channel messages deduced received signals fast communicate clearly total information rate receiver cannot two bits hand easy achieve total information rate one bit reliable communication achieved rates answer indicated figure practical codes multi user channels presented ratzer mackay exercise broadcast channels broadcast channel consists single transmitter two receivers properties channel fined conditional distribution assume channel memoryless task add encoder two decoders enable figure broadcast channel channel input outputs reliable communication common message rate receivers individual message rate receiver individual message rate receiver capacity region broadcast channel convex hull set achievable rate triplets simple benchmark channel given time sharing time division signaling capacities two channels considered separately copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exercises information theory achievable figure multiple access channels general multiple access channel two transmitters one receiver binary multiple access channel output equal sum two inputs achievable region devoting fraction transmission time channel channel achieve figure rates achievable simple timesharing better however analogy imagine speaking simultaneously american belarusian fluent american belarusian neither two receivers understands language receiver distinguish whether word language extra binary file conveyed recipients using bits decide whether next transmitted word american source text belarusian source text recipient concatenate words understand order receive personal message also recover binary string example broadcast channel consists two binary symmetric chan nels common input two halves channel flip prob abilities assume better half channel closely related channel degraded broadcast channel conditional probabilities random variables structure markov chain degraded version special case turns whatever information getting receiver also recovered receiver point distinguishing task find capacity region rate pair rate information reaching rate extra information reaching following exercise equivalent one solution illustrated figure exercise variable rate error correcting codes channels unknown noise level real life channels may sometimes well characterized 
[exercises, information, theory, solutions] solution exercise bits hint last part solution exists involves simple code 
[message, passing] one themes book idea complicated calculations using simple distributed hardware turns quite interesting problems solved message passing algorithms simple mes sages passed locally among simple processors whose operations lead time solution global problem 
[message, passing, counting] example consider line soldiers walking mist commander wishes perform complex calculation counting number soldiers line problem could solved two ways first solution uses expensive hardware loud booming voices commander men commander could shout soldiers report back within one minute could listen carefully men respond molesworth sir fotherington thomas sir solution relies several expensive pieces hardware must reliable communication channel every soldier commander must able listen incoming messages even hundreds soldiers must able count soldiers must well fed able shout back across possibly large distance separating commander second way finding global function number soldiers require global communication hardware high good food simply require soldier communicate single integers two adjacent soldiers line soldiers capable adding one number soldier follows rules front soldier line say number one soldier behind rearmost soldier line say number one soldier front soldier ahead behind says number add one say new number soldier side algorithm message passing rule set clever commander add one number also add two numbers together find global number soldiers simply adding together copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links message passing number said soldier front equals total number soldiers front number said com mander soldier behind number behind one count commander solution requires local communication hardware simple compu tations storage addition integers commander figure line soldiers counting using message passing rule set commander add soldier front soldier behind deduce soldiers total 
[message, passing, separation] clever trick makes use profound property total number soldiers written sum number soldiers front point number behind point two quantities computed separately two groups separated commander soldiers arranged line travelling swarm would easy separate two groups way commander jim figure swarm guerillas guerillas figure could counted using message passing rule set guerillas neighbours shown lines clear front behind furthermore since graph connections guerillas contains cycles possible guerilla cycle jim separate group two groups front behind swarm guerillas counted modified message passing algo rithm arranged graph contains cycles rule set message passing algorithm counting swarm guerillas whose connections form cycle free graph also known tree illustrated figure guerilla deduce total tree messages receive copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links counting commander jim figure swarm guerillas whose connections form tree count number neighbours keep count number messages received neighbours values messages let running total messages received number messages received equal identify neighbour sent message tell number number messages received equal number required total neighbour say neighbour number algorithm message passing rule set figure triangular grid many paths one path shown copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links message passing 
[message, passing, path-counting] profound task counting squaddies task counting number paths grid finding many paths pass given point grid figure shows rectangular grid path grid con necting points valid path one starts proceeds rightward downward moves questions many paths random path selected probability passes particular node grid say random mean paths exactly probability selected random path selected counting paths seem straightforward number paths expected pretty big even permitted grid diagonal strip three nodes wide would still possible paths figure every path enters upstream neighbour either find number paths adding number paths number computational breakthrough realize find number paths enumerate paths explicitly pick point grid consider number paths every path must come one upstream neighbours upstream meaning left number paths found adding number paths neighbours message passing algorithm illustrated figure simple grid ten vertices connected twelve directed edges start send figure messages sent forward pass ing message node received messages upstream neighbours sends sum downstream neigh bours number emerges counted number paths without enumerating sanity check figure figure five paths shows five distinct paths counted paths move challenging prob lems computing probability random path goes given vertex creating random path 
[message, passing, probability, passing, node] making backward pass well forward pass deduce many paths node divide total number paths obtain probability randomly selected path passes node figure shows backward passing messages figure messages sent forward backward passes lower right corners tables original forward passing mes sages upper left corners multiplying two numbers given vertex find total number paths passing vertex example four paths pass central vertex figure shows result computation triangular grid area blob proportional probability passing corresponding node 
[message, passing, random, path, sampling] exercise one creates random path flipping fair coin every junction choice two directions copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links finding lowest cost path resulting path uniform random sample set paths hint imagine trying grid figure neat insight like satisfaction figuring exercise run forward backward algorithms tween points grid one draw one path uniformly random figure figure probability passing node randomly chosen path message passing algorithm used count paths example sum product algorithm sum takes place node adds together messages coming predecessors product mentioned think sum weighted sum summed terms happened weight 
[message, passing, finding, lowest-cost, path] imagine wish travel quickly possible ambridge bognor various possible routes shown figure along cost hours traversing edge graph example route hhh hhh hhh hhh hhh hhh figure route diagram ambridge bognor showing costs associated edges cost hours would like find lowest cost path without explicitly evaluating cost paths efficiently finding node cost lowest cost path node quantities computed message passing starting node message passing algorithm called min sum algorithm viterbi algorithm brevity call cost lowest cost path node node cost node broadcast cost descendants knows costs possible predecessors let step algorithm hand cost zero pass news message passes along edge graph cost edge added find costs respectively figure similarly costs found respectively edge comes message path cost exists via edge learn alternative path cost figure min sum algorithm sets cost equal minimum min records smallest cost route retaining edge pruning away edges leading figure figures show remaining two iterations algorithm reveal path cost min sum algorithm encounters tie minimum cost copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links message passing path node achieved one route algorithm pick routes random recover lowest cost path backtracking following trail surviving edges back deduce lowest cost path figure min sum message passing algorithm find cost getting node thence lowest cost route 
[message, passing, applications, min–sum, algorithm] imagine manage production product raw materials via large set operations wish identify critical path process subset operations holding production operations critical path carried little faster time get raw materials product would reduced critical path set operations found using min sum algorithm chapter min sum algorithm used decoding error correcting codes 
[message, passing, summary, related, ideas] global functions separability property example number paths separates sum number paths point left number paths point functions computed efficiently message passing functions separability properties example number pairs soldiers troop share birthday size largest group soldiers share common height rounded nearest centimetre length shortest tour travelling salesman could take visits every soldier troop one challenges machine learning find low cost solutions prob lems like problem finding large subset variables approximately equal solved neural network approach hopfield brody hopfield brody neural approach trav elling salesman problem discussed section 
[message, passing, exercises] exercise describe asymptotic properties probabilities picted figure grid triangle width height exercise image processing integral image obtained image pixel coordinates defined show integral image efficiently computed mes sage passing show integral image simple functions image obtained example give expression sum image intensities rectangular region extending copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links solutions 
[message, passing, solutions] solution exercise since five paths grid figure must probability strategy based fair coin flips produce paths whose probabilities powers solution exercise make uniform random walk ward step walk chosen using different biased coin junction biases chosen proportion backward messages ema nating two options example first choice leaving message coming east coming south one east probability south probability path figure generated 
[noiseless, channels] chapter study task communicating efficiently con strained noiseless channel constrained channel strings input alphabet may transmitted make use idea introduced chapter global properties graphs computed local message passing algorithm 
[noiseless, channels, three, examples, constrained, binary, channels] constrained channel defined rules define strings permitted example channel every must followed least one channel substring forbidden valid string channel motivation model consider channel repre sented pulses electromagnetic energy device produces pulses requires recovery time one clock cycle generating pulse generate another example channel rule must come groups two must come groups two channel forbidden valid string channel motivation model consider disk drive succes sive bits written onto neighbouring points track along disk surface values represented two opposite magnetic orientations strings forbidden single isolated magnetic domain surrounded domains opposite orientation unstable might turn example example channel rule largest permitted runlength two symbol repeated channel forbidden valid string channel copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links three examples constrained binary channels physical motivation model disk drive rate rotation disk known accurately difficult distinguish string two string three represented oriented magnetizations duration respectively poorly known time taken one bit pass avoid possibility confusion resulting loss synchronization sender receiver forbid string three string three three channels examples runlength limited channels rules constrain minimum maximum numbers successive channel runlength runlength minimum maximum minimum maximum unconstrained channel runs may length runs restricted length one channel runs must length two channel runs must length one two capacity unconstrained binary channel one bit per channel use capacities three constrained channels fair defined capacity channels yet please understand pacity meaning many bits conveyed reliably per channel use 
[noiseless, channels, codes, constrained, channel] let concentrate moment channel runs may length runs restricted length one would like communicate random binary file channel efficiently possible code simple starting point code maps source bit two transmitted bits rate code respects constraints channel capacity channel least better redundant first two received bits zero know second bit also zero achieve smaller average transmitted length using code omits redundant zeroes code variable length code source symbols used equal frequency average transmitted length per source bit average communication rate capacity channel must least better two ways argue infor mation rate could increased first argument assumes comfortable entropy measure information content idea starting code reduce average message length without greatly reducing entropy copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links communication constrained noiseless channels message send decreasing fraction transmit imagine feeding stream bits frequency stream could obtained arbitrary binary file passing source file decoder arithmetic code optimal compressing binary strings density information rate achieved entropy source divided mean transmitted length thus original code without preprocessor corresponds happens perturb little towards smaller setting small negative vicinity denominator varies linearly contrast numerator second order dependence exercise find order taylor expansion function first order increases linearly decreasing must possible increase decreasing figure shows functions figure top information content per source symbol mean transmitted length per source symbol function source density bottom information content per transmitted symbol bits function indeed increase decreases maximum bits per channel use argument shown capacity channel least max exercise file containing fraction transmitted fraction transmitted stream fraction transmitted bits drive code sparse source density second fundamental approach counts many valid sequences length communicate log bits channel cycles giving one name valid sequences 
[noiseless, channels, capacity, constrained, noiseless, channel] defined capacity noisy channel terms mutual information input output proved number capac ity related number distinguishable messages could reliably conveyed channel uses channel lim log case constrained noiseless channel adopt identity definition channel capacity however name making codes noisy channels section ran messages take new role labelling states channel copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links counting number possible messages figure state diagram channel trellis section trellis connection matrix aau aau             figure state diagrams trellis sections connection matrices channels chapter denote number distinguishable messages length define capacity lim log figured capacity channel return task making practical code channel 
[noiseless, channels, counting, number, possible, messages] first let introduce representations constrained channels state diagram states transmitter represented circles labelled name state directed edges one state another indicate transmitter permitted move first state second label edge indicates symbol emitted transition made figure shows state diagram channel two states transitions state made transmitted transitions state made transmitted transitions state state possible also represent state diagram trellis section shows two successive states time two successive horizontal locations fig ure state transmitter time called set possible state sequences represented trellis shown figure valid sequence corresponds path trellis number copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links communication constrained noiseless channels figure counting number paths trellis channel counts next nodes accumulated passing left right across trellises figure counting number paths trellises channels assume start first bit preceded channels initial character permitted channel first character must channel channel aau aau aau aau aau aau aau channel aau aau aau aau aau aau aau copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links counting number possible messages log log figure counting number paths trellis channel valid sequences number paths purpose counting many paths trellis ignore labels edges summarize trellis section connection matrix edge state otherwise figure figure shows state diagrams trellis sections connection matrices channels let count number paths channel message passing trellis figure shows first steps counting process figure shows number paths ending state steps total number paths length shown along top recognize fibonacci series exercise show ratio successive terms fibonacci series tends golden ratio thus within constant factor scales capacity channel lim log constant log log describe count number paths vector obtain using state count symbols transmitted figure assumed either two symbols permitted outset total number paths limit becomes dominated principal right eigenvector constant copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links communication constrained noiseless channels principal eigenvalue find capacity constrained channel need find principal eigenvalue connection matrix log 
[noiseless, channels, back, model, channels] comparing figure figures looks channels capacity channel principal eigenvalues three trellises eigenvectors channels given bottom table indeed channels intimately related figure accumulator differentiator equivalence channels take valid string channel pass accumulator obtaining defined mod resulting string valid string channel isolated digits accumulator invertible operator similarly valid string channel mapped onto valid string channel binary differentiator mod equivalent modulo arithmetic differentiator also blurrer convolving source stream filter channel also intimately related channels exercise relationship channel channels 
[noiseless, channels, practical, communication, constrained, channels] practice since three channels equivalent concentrate channel 
[noiseless, channels, fixed-length, solutions] start explicitly enumerated codes code table achieves table runlength limited code channel rate exercise similarly enumerate strings length end zero state hence show map bits source strings transmitted bits achieve rate rate achieved mapping integer number source bits transmitted bits copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links practical communication constrained channels 
[noiseless, channels, optimal, variable-length, solution] optimal way convey information constrained channel find optimal transition probabilities points trellis make transitions probabilities discussing channel showed sparse source density driving code would achieve capacity know make sparsifiers chapter design arithmetic code optimal compressing sparse source associated decoder gives optimal mapping dense random binary strings sparse strings task finding optimal probabilities given exercise exercise show optimal transition probabilities found follows find principal right left eigenvectors solutions largest eigenvalue construct matrix whose invariant distribution proportional namely hint exercise might give helpful cross fertilization exercise show sequences generated using timal transition probability matrix entropy resulting sequence asymptotically log per symbol hint consider condi tional entropy one symbol given previous one assuming previous one distribution invariant distribution practice would probably use finite precision approximations optimal variable length solution one might dislike variable length solutions resulting unpredictability actual encoded length particular case perhaps applications would like guarantee encoded length source file size bits less given length example disk drive easier control blocks bytes known take exactly amount disk real estate constrained channels make simple modification variable length encoding offer guarantee follows find two codes two mappings binary strings variable length encodings property source string encoding first code shorter average encoding second code longer average vice versa transmit string encode whole string codes send whichever encoding shortest length prepended suitably encoded single bit convey two codes used         figure state diagrams connection matrices channels maximum runlengths equal exercise many valid sequences length starting run length limited channels shown figure capacities channels using computer find matrices generating random path trellises channel two run length limited channels shown figure copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links communication constrained noiseless channels exercise consider run length limited channel length run permitted maximum run length large number nine ninety estimate capacity channel give first two terms series expansion involving roughly form optimal matrix generating random path trellis channel focus values elements probability generating given preceding probability generating given preceding run check answer explicit computation channel maximum runlength nine 
[noiseless, channels, variable, symbol, durations] add frill task communicating constrained channels assuming symbols send different durations aim communicate maximum possible rate per unit time channels come two flavours unconstrained constrained 
[noiseless, channels, unconstrained, channels, variable, symbol, durations] encountered unconstrained noiseless channel variable symbol rations exercise solve problem done topic task determine optimal frequencies sym bols used given durations nice analogy task task designing optimal symbol code chapter make binary symbol code source unequal probabilities optimal message lengths log similarly channel whose symbols durations units time optimal probability symbols used capacity channel bits per unit time 
[noiseless, channels, constrained, channels, variable, symbol, durations] grasped preceding topics chapter able figure define find capacity trickiest constrained channels exercise classic example constrained channel variable symbol durations morse channel whose symbols dot dash short space used letters morse code long space used words constraints spaces may followed dots dashes find capacity channel bits per unit time assuming four symbols equal durations symbol durations time units respectively copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links solutions exercise well designed morse code english say probability distribution figure exercise difficult get dna narrow tube information theorist entropy associated constrained channel reveals much information conveyed sta tistical physics calculations done different reason predict thermodynamics polymers example toy example consider polymer length either sit constraining tube width open constraints open polymer adopts state drawn random set one dimensional random walks say possible directions per step entropy walk log per step figure model dna squashed narrow tube dna tendency pop tube outside tube random walk greater entropy total log free energy polymer defined times temperature tube polymer one dimensional walk directions unless wall way connection matrix example                         entropy polymer change entropy associated polymer entering tube possible obtain expression function use computer find entropy walk particular value plot probability density polymer transverse location tube notice difference capacity two channels one constrained one unconstrained directly proportional force required pull dna tube 
[noiseless, channels, solutions] solution exercise file transmitted contains aver age one third two thirds fraction solution exercise valid string channel obtained valid string channel first inverting passing accumulator operations invertible valid string also mapped onto valid string proviso comes edge effects assume first character transmitted channel preceded string zeroes first character forced figure two channels exactly equivalent assume channel first character must zero solution exercise transmitted bits largest integer number source bits encoded maximum rate fixed length code copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links communication constrained noiseless channels solution exercise let invariant distribution normalization constant entropy given assuming chapter denotes ensemble whose random variable state comes invariant distribution log log log log log log either contributions terms proportional log zero log log log log log log log solution exercise principal eigenvalues connection matrices two channels capacities log bits solution exercise channel similar unconstrained binary channel runs length greater rare large expect weak differences channel differences show contexts run length close capacity channel close one bit lower bound capacity obtained considering simple variable length code channel replaces occurrences maxi mum runlength string otherwise leaves source file unchanged average rate code invariant distribution hit add extra zero state fraction time reuse solution variable length channel exercise capacity value equation satisfied terms sum correspond possible strings emitted sum exactly given copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links solutions used anticipate little less order equal rearranging solving approximately using evaluated true capacities earlier exercise table compares approximate capacity true capacity true capacity selection values element close tiny bit larger since unconstrained binary channel run length occurred effectively choice printing let probability selecting let estimate entropy remaining characters stream function assuming rest matrix set optimal value entropy next characters stream entropy first bit plus entropy remaining characters roughly bits select first bit bits selected precisely capacity channel roughly next chars differentiating setting zero find optimal obtain log probability emitting thus decreases number emitted increases optimal matrix                               rough theory works 
[crosswords, codebreaking] chapter make random walk topics related lan guage modelling 
[crosswords, codebreaking, crosswords] rules crossword making may thought defining constrained channel fact many valid crosswords made demonstrates constrained channel capacity greater zero two archetypal crossword formats type american figure crosswords types american british crossword every row column consists succession words length separated one spaces type british crossword row column consists mixture words single characters separated one spaces every character lies least one word horizontal vertical whereas type crossword every letter lies horizontal word vertical word typical type crossword half letters half lie one word type crosswords harder create type con straint single characters permitted type crosswords gener ally harder solve fewer constraints per character 
[crosswords, codebreaking, crosswords, possible?] language redundancy letters written grid form valid crossword language high redundancy hand hard make crosswords except perhaps small number trivial ones possibility making crosswords language thus demonstrates bound redundancy language crosswords normally written genuine english written word english language consisting strings words dictionary separated spaces exercise estimate capacity word english bits per character hint think word english defining constrained channel chapter see exercise fact many crosswords made leads lower bound entropy word english simplicity model word english wenglish language troduced section consists words length entropy language per character including inter word spaces log copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links crosswords find conclusions come depend value terribly sensitive value consider large crossword size squares area let number words let number letter occupied squares typical crosswords types made words length two fractions roughly values table table factors number words number letter squares respectively smaller total number squares estimate many crosswords size using simple model wenglish assume wenglish created random gener ating strings monogram memoryless source entropy example source used characters equal probability log bits instead use chapter distribution entropy redundancy wenglish stems two sources tends use letters others words dictionary let count many crosswords imagining filling squares crossword random using distribution pro duced wenglish dictionary evaluating probability random scribbling produces valid words rows columns total number typical fillings squares crossword made probability one word length validly filled probability whole crossword made words validly filled single typical filling approximately calculation underestimates number valid wenglish crosswords counting crosswords filled typical strings monogram distribution non uniform true count dominated atypical fillings crossword friendly words appear often log number valid crosswords size estimated log log increasing function arbitrarily many crosswords made enough words wenglish dictionary plugging values table find following crossword type condition crosswords set bits assume words normal english speaker dictionary length find condition crosswords type satisfied condition crosswords type satisfied fits experience crosswords type usually contain obscure words copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links crosswords codebreaking 
[crosswords, codebreaking, reading] reading turing bletchley park see hodges good depth read cryptography schneier book highly recommended readable clear entertaining 
[crosswords, codebreaking, zipf–mandelbrot, distribution] crudest model language monogram model asserts successive word drawn independently distribution words nature distribution words zipf law zipf asserts probability rth probable word language approximately exponent value close constant according zipf log log plot frequency versus word rank show straight line slope mandelbrot modification zipf law introduces third param eter asserting probabilities given documents jane austen emma zipf mandelbrot dis tribution fits well figure documents give distributions well fitted zipf mandelbrot distribution figure shows plot frequency versus rank tex source book qualitatively graph similar straight line curve noticeable fair source file written pure english mix english maths symbols tex commands theand harriet information probability figure fit zipf mandelbrot distribution curve empirical frequencies words jane austen emma dots fitted parameters copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links simple language models probability information shannon bayes figure log log plot frequency versus rank words tex file book alpha alpha alpha alpha book figure zipf plots four languages randomly generated dirichlet processes parameter ranging also shown zipf plot book 
[crosswords, codebreaking, dirichlet, process] assuming interested monogram models languages model use one difficulty modelling language unboundedness vocabulary greater sample language greater number words encountered generative model language emulate property asked next word newly discovered work shakespeare probability distribution words must surely include non zero probability words shakespeare never used generative monogram model language also satisfy consistency rule called exchangeability imagine generating new language generative model producing ever growing corpus text statistical properties text homogeneous probability finding particular word given location stream text everywhere stream dirichlet process model model stream symbols think words satisfies exchangeability rule allows vocabulary symbols grow without limit model one parameter stream symbols produced identify new symbol unique integer seen stream length symbols define probability next symbol terms counts symbols seen far thus probability next symbol new symbol never seen probability next symbol symbol figure shows zipf plots plots symbol frequency versus rank million symbol documents generated dirichlet process priors values ranging evident dirichlet process adequate model observed distributions roughly obey zipf law copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links crosswords codebreaking figure zipf plots words two languages generated creating successive characters dirichlet process declaring one character space character two curves result two different choices space character small tweak however dirichlet processes produce rather nice zipf plots imagine generating language composed elementary symbols using dirichlet process rather small value parameter number reasonably frequent symbols declare one symbols called characters rather words space character identify strings space characters words generate language way frequencies words often come nice zipf plots shown figure character selected space character determines slope zipf plot less probable space character gives rise richer language shallower slope 
[crosswords, codebreaking, units, information, content] information content outcome whose probability defined log entropy ensemble average information content log compare hypotheses light data ten convenient compare log probability data alternative hypotheses log evidence log case two hypotheses compared evaluate log odds log also called weight evidence favour log evidence hypothesis log negative information content data data large information content given pothesis surprising hypothesis hypothesis surprised data hypothesis becomes probable information content surprise value log likelihood log evidence thing quantities logarithms probabilities weighted sums logarithms probabilities measured units units depend choice base logarithm names given units shown table copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links taste banburismus unit expression units bit log nat log ban log deciban log table units measurement information content bit unit use book word bit meanings backup name unit shannon byte bits megabyte bytes one works natural logarithms information contents weights evidence measured nats interesting units ban deciban 
[crosswords, codebreaking, history, ban] let tell factor ten probability called ban alan turing codebreakers bletchley park breaking new day enigma code task huge inference problem infer given day cyphertext three wheels enigma machines day starting positions letter substitutions use steckerboard least original german messages inferences conducted using bayesian methods course chosen units decibans half decibans deciban judged smallest weight evidence discernible human evidence favour particular hypotheses tallied using sheets paper specially printed banbury town miles bletchley inference task known banburismus units banburismus played called bans town 
[crosswords, codebreaking, taste, banburismus] details code breaking methods bletchley park kept secret long time aspects banburismus pieced together hope following description small part banburismus inaccurate much information needed number possible settings enigma machine deduce state machine therefore necessary find decibans somewhere good puts banburismus aimed deducing entire state machine figuring wheels use logic based bombes fed guesses plaintext cribs used crack settings wheels enigma machine wheels plugs put place plemented continually changing permutation cypher wandered deter ministically state space permutations enormous number messages sent day good chance ever state one machine sending one character message would another machine state sending particular char acter another message evolution machine state deterministic two machines would remain state helped descriptions given tony sale http www codesandciphers org lectures jack good worked turing bletchley copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links crosswords codebreaking rest transmission resulting correlations puts pairs machines provided dribble information content turing workers extracted daily decibans 
[crosswords, codebreaking, detect, two, messages, came, machines, common, state, sequence] hypotheses null hypothesis states machines different states two plain messages unrelated match hypothesis says machines state two plain messages unrelated attempt made infer state either machine data provided two cyphertexts let assume length alphabet size enigma probability data given two hypotheses first null hypothesis hypothesis asserts two cyphertexts given codes two unrelated time varying permutations alphabet plaintext messages exact computation probability data would depend language model plain text model enigma machine guts assume enigma machine ideal random time varying permuta tion probability distribution two cyphertexts uniform cyphertexts equally likely length hypothesis asserts single time varying permuta tion underlies probability data make assumptions plaintext language case plaintext language completely random probability would uniform would probability would equal two hypotheses would indistinguishable make progress assuming plaintext completely ran dom plaintexts written language language redun dancies assume example particular plaintext letters used often others even though two plaintext messages unrelated slightly likely use letters true two synchronized letters two cyphertexts slightly likely identical similarly language uses particular bigrams trigrams frequently two plaintext messages occasionally contain bigrams trigrams time giving rise true copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links taste banburismus little jack horner sat corner eating christmas pie put ride cock horse banbury cross see fine lady upon white horse matches table two aligned pieces english plaintext matches marked notice twelve matches including run six whereas expected number matches two completely random strings length would two corresponding cyphertexts two machines identical states would also twelve matches little burst identical letters table shows coinci dence two plaintext messages unrelated except written english codebreakers hunted among pairs messages pairs sus piciously similar counting numbers matching mono grams bigrams trigrams etc method first used polish codebreaker rejewski let look simple case monogram language model estimate long message needed able decide whether two machines state assume source language monogram english language successive letters drawn probability distribution figure probability nonuniform consider two single characters probability identical give quantity name match probability english german rather value would hold completely random language assuming ideal random permutation probability symmetry given pair cyphertexts length match places match places log evidence favour log log log log log every match contributes log favour every non match contributes log favour match probability monogram english coincidental match probability log evidence per match log log evidence per non match log matches non matches pair length example weight evidence favour would decibans likelihood ratio favour expected weight evidence line text length characters expectation depends whether true true matches expected turn rate expected weight evidence decibans per characters true copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links crosswords codebreaking spurious matches expected turn rate expected weight evidence decibans per characters typically roughly characters need inspected order weight evidence greater hundred one decibans favour one hypothesis two english plaintexts matches two random strings furthermore consecutive characters english independent bigram trigram statistics english nonuniform matches tend occur bursts consecutive matches observations also apply german using better language models evidence contributed runs matches accurately computed scoring system worked turing refined good positive results passed automated human powered codebreakers according good longest false positive arose work string consecutive matches two machines actually unrelated states 
[crosswords, codebreaking, exercises] exercise another weakness design enigma machine intended emulate perfectly random time varying permu tation never mapped letter press comes always different letter much information per character leaked design flaw long crib would needed confident crib correctly aligned cyphertext long crib would needed able confidently identify correct key crib guess plaintext imagine brits know important german travelling berlin aachen intercept enigma encoded messages sent aachen good bet one original plaintext messages contains string obersturmbannfuehrerxgrafxheinrichxvonxweizsaecker name important chap crib could used brute force approach find correct enigma key feed received messages possible engima machines see putative decoded texts match plaintext question centres idea crib also used much less expensive manner slide plaintext crib along encoded messages perfect mismatch crib encoded message found correct alignment tells lot key 
[evolution] evolution happening earth last years deniably information acquired process thanks tireless work blind watchmaker cells carry within information required outstanding spiders cells carry information required make excellent octopuses information come entire blueprint organisms planet emerged teach ing process teacher natural selection fitter individuals progeny fitness defined local environment including organisms teaching signal bits per individual individual simply smaller larger number grandchildren depending individual fitness fitness broad term could cover ability antelope run faster antelopes hence avoid eaten lion ability lion well enough camouflaged run fast enough catch one antelope per day ability peacock attract peahen mate ability peahen rear many young simultaneously fitness organism largely determined dna coding regions genes non coding regions play important role regulating transcription genes think fitness function dna sequence environment dna determine fitness information get natural selection genome well gene codes one antelope proteins defective antelope might get eaten lion early life two grandchildren rather forty information content natural selection fully contained specification spring survived children information content one bit per offspring teaching signal communicate ecosystem description imperfections organism caused fewer children bits teaching signal highly redundant throughout species unfit individuals similar failing offspring similar reasons many bits per generation acquired species whole natural selection many bits natural selection succeeded con veying human branch tree life since divergence copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links sex information acquisition evolution australopithecines apes years ago assuming generation time years reproduction generations human precursors since divergence apes assuming population individuals receiving couple bits information natural selection total number bits information responsible modifying genomes million today human genome bits however noted natural selection smart collating information dishes population great deal redundancy information population size twice great would evolve twice fast natural selection simply correcting defects twice often john maynard smith suggested rate information acquisition species independent population size order bit per generation figure would allow bits difference apes humans number much smaller total size human genome bits one human genome contains nucleotides certainly case genomic overlap apes humans huge difference small chapter develop crude model process information acquisition evolution based assumption gene two defects typically likely defective gene one defect organism two defective genes likely less fit organism one defective gene undeniably crude model since real biological systems baroque constructions complex interactions nevertheless persist simple model readily yields striking results find simple model john maynard smith figure bit per generation correct asexually reproducing population contrast species reproduces sexually rate information acquisition large bits per generation size genome also find interesting results concerning maximum mutation rate species withstand 
[evolution, model] study simple model reproducing population individuals genome size bits variation produced mutation recombina tion sex truncation selection selects fittest children generation parents next find striking differences populations recombination populations genotype individual vector bits good state bad state fitness individual simply sum bits bits genome could considered correspond either genes good alleles bad alleles nucleotides genome concentrate latter interpretation essential property fitness assuming locally roughly linear function genome many possible changes one copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links rate increase fitness could make genome small effect fitness effects combine approximately linearly define normalized fitness consider evolution natural selection two models variation variation mutation model assumes discrete generations generation every individual produces two children children genotypes differ parent random mutations natural selec tion selects fittest progeny child population reproduce new generation starts selection fittest individuals generation known truncation selection simplest model mutations child bits dependent bit small probability flipped thinking bits corresponding roughly nucleotides taken constant independent alternatively thought bits corresponding genes would model probability discovery good gene smaller number probability deleterious mutation good gene variation recombination crossover sex organisms haploid diploid enjoy sex recombination individ uals population married couples random couple children children stan dard assumption population double halve every generation children genotypes independent given parents child obtains genotype random crossover parents genotypes simplest model recombination linkage probability probability progeny born parents pass away fittest progeny selected natural selection new generation starts study two models variation detail 
[evolution, theory, mutations] assume genotype individual normalized fitness subjected mutations flip bits probability first show average normalized fitness population greater optimal mutation rate small rate acquisition information order one bit per generation since easy achieve normalized fitness simple muta tion assume work terms excess normalized fitness individual excess normalized fitness child mutation rate small probability distribution excess normalized fitness child mean child copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links sex information acquisition evolution variance population parents mean variance child population selection mean vari ance natural selection chooses upper half distribution mean fitness variance fitness next generation given mean deviation mean measured standard devia tions factor child distribution variance reduced selection numbers order case gaussian distribution assume variance dynamic equilibrium factor equation equal take results gaussian distribution approximation becomes poorest discreteness fitness becomes important small rate increase normalized fitness thus assuming maximized opt point opt rate increase fitness per generation population low fitness rate increase fitness may exceed unit per generation indeed rate increase order initial spurt last order generations rate increase fitness smaller one per generation fitness approaches optimal mutation rate tends average bits flipped per genotype rate increase fitness also equal information gained rate bits per generation takes generations genotypes individuals population attain perfection fixed fitness given copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links rate increase fitness sex sex histogram parents fitness histogram children fitness selected children fitness figure sex better sex free reproduction mutations used create variation among children unavoidable average fitness children lower parents fitness greater variation greater average deficit selection bumps mean fitness contrast recombination produces variation without decrease average fitness typical amount variation scales genome size selection average fitness rises subject constraint constant integration equal mean number bits flipped per genotype exceeds fitness approaches equilibrium value eqm theory somewhat inaccurate true probability distribu tion fitness non gaussian asymmetrical quantized integer values predictions theory grossly variance results simulations described 
[evolution, theory, sex] analysis sexual population becomes tractable two approxi mations first assume gene pool mixes sufficiently rapidly correlations genes neglected second assume homogeneity fraction bits good state given assumptions two parents fitness mate prob ability distribution children fitness mean equal parents fitness variation produced sex reduce average fitness standard deviation fitness children scales since selection increase fitness proportional standard deviation fitness increase per generation scales square root size genome shown box mean fitness evolves accordance differential equation solution equation sin constant integration sin idealized system reaches state eugenic perfection within finite time generations 
[evolution, simulations] figure shows fitness sexual population individ uals genome size starting random initial state normalized fitness also shows theoretical curve equation fits remarkably well contrast figures show evolving fitness variation produced mutation rates respectively note difference horizontal scales panel copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links sex information acquisition evolution box details theory sex depend let first assume two parents child exactly good bits homogeneity assumption bits independent random subsets bits number bits good parents roughly number good one parent roughly fitness child plus sum fair coin flips binomial distribution mean variance fitness child thus roughly distributed child normal mean variance important property distribution contrasted distribution mutation mean fitness equal parents fitness variation produced sex reduce average fitness include parental population variance write children fitnesses distributed child normal mean variance natural selection selects children upper side distribution mean increase fitness variance surviving children dynamic equilibrium factor defining constant conclude sex natural selection mean fitness population increases rate proportional square root size genome bits per generation copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links maximal tolerable mutation rate sex sex sex sex figure fitness function time genome size dots show fitness six randomly selected individuals birth population generation initial population randomly generated genomes exactly variation produced sex alone line shows theoretical curve infinite homogeneous population variation produced mutation without sex mutation rate bits per genome dashed line shows curve sex without sex sex without sex figure maximal tolerable mutation rate shown number errors per genome versus normalized fitness left panel genome size right independent genome size parthenogenetic species sex tolerate order error per genome per generation species uses recombination sex tolerate far greater mutation rates exercise dependence population size results sexual population depend population size anticipate minimum population size theory sex accurate minimum population size related exercise dependence crossover mechanism simple model sex bit taken random one two parents allow crossovers occur probability two adjacent nucleotides model affected crossover probability smaller crossovers occur exclusively hot spots located every bits along genome 
[evolution, maximal, tolerable, mutation, rate] combine two models variation maximum mutation rate tolerated species sex rate increase fitness given copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links sex information acquisition evolution positive mutation rate satisfies let compare rate result absence sex equation maximum tolerable mutation rate tolerable mutation rate sex order times greater without sex parthenogenetic non sexual species could try wriggle bound mutation rate increasing litter sizes mutation flips average bits probability bits flipped one genome roughly mother needs roughly offspring order good chance one child fitness litter size non sexual species thus exponential bigger species persist maximum tolerable mutation rate pinned close non sexual species whereas larger number order species recombination turning results around predict largest possible genome size given fixed mutation rate parthenogenetic species largest genome size order sexual species taking figure mutation rate per nucleotide per generation eyre walker keightley allowing maximum brood size predict species coding nucleotides make least occasional use recombination brood size number falls 
[evolution, fitness, increase, information, acquisition] simple model possible relate increasing fitness information acquisition bits set random fitness roughly evolution leads population individuals maximum fitness bits information acquired species namely bit species figured two states better define information acquired intermediate fitness amount selection measured bits required select perfect state gene pool let fraction population log information required find black ball urn containing black white balls ratio define information acquired log bits fractions equal log well approximated rate information acquisition thus roughly two times rate crease fitness population copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links discussion 
[evolution, discussion] results quantify well known argument species reproduce sex recombination namely recombination allows useful muta tions spread rapidly species allows deleterious muta tions rapidly cleared population maynard smith felsenstein maynard smith maynard smith athmary population reproduces recombination acquire informa tion natural selection rate order times faster partheno genetic population tolerate mutation rate order times greater genomes size coding nucleotides factor substantial enormous advantage conferred sex noted kon drashov meme kondrashov calls deterministic mutation hypothesis seem diffused throughout evolu tionary research community still numerous papers prevalence sex viewed mystery explained elaborate mecha nisms 
[evolution, ‘the, cost, males’, stability, gene, sex, parthenogenesis] people declare sex mystery main motivation mystified idea called cost males sexual reproduction disad vantageous compared asexual reproduction argued every two offspring produced sex one average useless male incapable child bearing one productive female time parthenogenetic mother could give birth two female clones put way big advantage parthenogenesis point view individual one able pass one genome one children instead thus two versions species one reproducing one without sex single mothers would expected outstrip sexual cousins simple model presented thus far include either genders ability convert sexual reproduction asexual easily modify model modify model one bits genome determines whether individual prefers reproduce parthenogenetically sex ually results depend number children single parthenogenetic mother number children born sexual couple reasonable mod els former would seem appropriate case unicellular organisms cytoplasm parents goes children latter appropriate children solely nurtured one parents single mothers many offspring sexual pair concentrate latter model since gives greatest advantage parthenogens supposedly expected outbreed sexual community parthenogens four children per generation maximum tolerable mutation rate twice expression derived fitness large maximum tolerable rate initially genomes set randomly half pop ulation gene parthenogenesis figure shows outcome learning phase evolution fitness increasing rapidly pockets parthenogens appear briefly disappear within couple generations sexual cousins overtake fitness copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links sex information acquisition evolution fitnesses sexual fitness parthen fitness sexual fitness parthen fitness ercentage figure results gene parthenogenesis interbreeding single mothers produce many children sexual couples vertical axes show fitnesses two sub populations percentage population parthenogenetic leave behind population reaches top fitness however parthenogens take mutation rate sufficiently low presence higher mutation rate however parthenogens never take breadth sexual population fit ness order mutant parthenogenetic colony arising slightly average fitness last generations fitness falls sexual cousins long popu lation size sufficiently large sexual individuals survive time sex die sufficiently unstable environment fitness function con tinually changing parthenogens always lag behind sexual commu nity results consistent argument haldane hamilton sex helpful arms race parasites parasites define effective fitness function changes time sexual population always ascend current fitness function rapidly 
[evolution, additive, fitness, function] course results depend fitness function assume model selection reasonable model fitness first order sum independent terms maynard smith argues good genes higher come pecking order example directional selection model used extensively theoretical popula tion genetic studies bulmer might expect real fitness functions involve interactions case crossover might reduce average fitness however since recombination gives biggest advantage species whose fit ness functions additive might predict evolution favoured species used representation genome corresponds fitness function weak interactions even interactions seems plausible fitness would still involve sum interacting terms number terms fraction genome size copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exercises exercise investigate fast sexual asexual species evolve fitness function interactions example let fitness sum exclusive ors pairs bits compare evolving fitnesses sexual asexual species simple additive fitness function furthermore fitness function highly nonlinear function genotype could made smooth locally linear baldwin effect baldwin effect baldwin hinton nowlan widely studied mechanism whereby learning guides evolution could also act level transcription translation consider evolution peptide sequence new purpose assume effectiveness peptide highly nonlinear function sequence perhaps small island good sequences surrounded ocean equally bad sequences organism whose transcription translation machinery flawless fitness equally nonlinear function dna sequence evolution wander around ocean making progress towards island random walk contrast organism dna sequence whose dna rna transcription rna protein translation faulty occasionally mistranslation mistranscription accidentally produce working enzyme greater probability dna sequence close good sequence one cell might produce proteins one mrna sequence enzymatic effect one one working catalyst enough cell increased fitness relative rivals whose dna sequence island good sequences reason conjecture least early evolution perhaps still genetic code implemented perfectly implemented noisily codons coding distribution possible amino acids noisy code could even switched cell cell organism multiple aminoacyl trna synthetases reliable others whilst model assumed bits genome interact ignored fact information represented redundantly assumed direct relationship phenotypic fitness genotype assumed crossover probability recombination high believe qualitative results would still hold complex models fitness crossover used relative benefit sex still scale small bred populations benefits sex expected diminished summary sex sex good bits 
[evolution, reading] high information content self replicating system ever emerge first place general area origins life tricky ques tions evolution highly recommend maynard smith athmary maynard smith athmary kondrashov may nard smith ridley dyson cairns smith hopfield 
[evolution, exercises] exercise good must error correcting machinery dna repli cation given mammals died long ago estimate probability nucleotide substitution per cell division see appendix copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links sex information acquisition evolution exercise given dna replication achieved bumbling brow nian motion ordinary thermodynamics biochemical porridge temperature astonishing error rate dna replication per replicated nucleotide reliability achieved given energetic difference correct base pairing incor rect one one two hydrogen bonds thermal energy factor four smaller free energy associated hydro gen bond ordinary thermodynamics favours correct base pairing surely frequency incorrect base pairing exp free energy difference error frequency dna replication cheated thermodynamics situation equally perplexing case protein synthesis translates mrna sequence polypeptide accordance netic code two specific chemical reactions protected errors binding trna molecules amino acids production polypep tide ribosome like dna replication involves base pairing fidelity high error rate fidelity caused energy correct final state especially low correct polypeptide sequence expected significantly lower energy sequence cells perform error correction see hopfield hopfield exercise genome acquires information natural lection rate bits per generation brain acquires information greater rate estimate rate new information stored long term memory brain think learning words new language example 
[evolution, solutions] solution exercise small enough whilst average fit ness population increases unlucky bits become frozen bad state bad genes sometimes known hitchhikers mogeneity assumption breaks eventually individuals identical genotypes mainly bits contain bits smaller population greater number frozen bits expected small population size theory sex accurate find experimentally theory based assuming homogeneity fits poorly population size smaller significantly smaller information cannot possibly acquired rate big since information content blind watchmaker decisions cannot greater bits per generation number bits required specify children get reproduce baum analyzing similar model show population size log make hitchhikers unlikely arise 
[example, inference, task, clustering] human brains good finding regularities data one way expressing regularity put set objects groups similar example biologists found objects natural world fall one two categories things brown run away things green run away first group call animals second plants call operation grouping things together clustering biologist sub divides cluster plants sub clusters would call hierarchical clustering talking hierarchical clustering yet chapter discuss ways take set objects group clusters several motivations clustering first good clustering predictive power early biologist encounters new green thing seen internal model plants animals fills predictions attributes green thing unlikely jump eat touches might get grazed stung eats might feel sick predictions uncertain useful help biologist invest resources example time spent watching predators well thus perform clustering believe underlying cluster labels meaningful lead efficient description data help choose better actions type clustering sometimes called mixture density modelling objective function measures well predictive model working information content data log second clusters useful aid communication allow lossy compression biologist give directions friend third tree right take right turn rather past large green thing red berries past large green thing thorns brief category name tree helpful sufficient identify object similarly lossy image compression aim convey bits possible reasonable reproduction picture one way divide image small patches find close match patch alphabet image templates send close fit image sending list labels matching templates task creating good library image templates equivalent finding set cluster centres type clustering sometimes called vector quantization formalize vector quantizer terms assignment rule assigning datapoints one codenames reconstruction rule aim choose functions copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links means clustering minimize expected distortion might defined ideal objective function would minimize psychologically per ceived distortion image since hard quantify distortion perceived human vector quantization lossy compression crisply defined problems data modelling lossless compression vec tor quantization necessarily believe templates natural meaning simply tools job note passing similarity assignment rule encoder vector quantization decoding problem decoding error correcting code third reason making cluster model failures cluster model may highlight interesting objects deserve special attention trained vector quantizer good job compressing satellite pictures ocean surfaces maybe patches image well compressed vector quantizer patches contain ships biologist encounters green thing sees run slither away misfit cluster model says green things run away cues pay special attention one spend one time fascinated things cluster model help sift multitude objects one world ones really deserve attention figure data points fourth reason liking clustering algorithms may serve models learning processes neural systems clustering algorithm discuss means algorithm example competitive learning algorithm algorithm works clusters compete right data points 
[example, inference, task, clustering, k-means, clustering] means algorithm algorithm putting data points name far know means clustering simply refers chosen number clusters newton followed naming policy maybe would learn school calculus variable silly name stuck dimensional space clusters cluster parameterized vector called mean data points denoted superscript runs number data points vector components assume space lives real space metric defines distances points example start means algorithm algorithm means initialized way example random values means iterative two step algorithm assignment step data point assigned nearest mean update step means adjusted match sample means data points responsible means algorithm demonstrated toy two dimensional data set figure means used assignments points two clusters indicated two point styles two means shown circles algorithm converges three iterations point assignments unchanged means remain unmoved updated means algorithm always converges fixed point copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links example inference task clustering initialization set means random values assignment step data point assigned nearest mean denote guess cluster point belongs argmin alternative equivalent representation assignment points clusters given responsibilities indicator variables assignment step set one mean closest mean datapoint otherwise zero ties expect two means exactly distance data point tie happen set smallest winning update step model parameters means adjusted match sample means data points responsible total responsibility mean means responsibilities leave mean repeat assignment step update step assign ments change algorithm means clustering algorithm copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links means clustering data assignment update assignment update assignment update figure means algorithm applied data set points means evolve stable locations three iterations run run figure means algorithm applied data set points two separate runs means reach different solutions frame shows successive assignment step exercise see prove means always converges hint find physical analogy associated lyapunov function lyapunov function function state algorithm decreases whenever state changes bounded system lyapunov function dynamics converge means algorithm larger number means demonstrated figure outcome algorithm depends initial condition first case five iterations steady state found data points fairly evenly split four clusters second case six iterations half data points one cluster others shared among three clusters 
[example, inference, task, clustering, questions, algorithm] means algorithm several hoc features update step set mean mean assigned points distance come used different measure distance choose best distance vector quantization distance copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links example inference task clustering figure means algorithm case two dissimilar clusters little large data stable set assignments means note four points belonging broad cluster incorrectly assigned narrower cluster points assigned right hand cluster shown plus signs figure two elongated clusters stable solution found means algorithm function provided part problem definition assuming interested data modelling rather vector quantization choose found multiple alternative clusterings given choose among 
[example, inference, task, clustering, cases, k-means, might, viewed, failing] questions arise look cases algorithm behaves badly compared man street would call clustering figure shows set data points generated mixture two gaussians right hand gaussian less weight one fifth data points less broad cluster figure shows outcome using means clustering means four big cluster data points assigned small cluster means end displaced left true centres clusters means algorithm takes account distance means data points representation weight breadth cluster consequently data points actually belong broad cluster incorrectly assigned narrow cluster figure shows another case means behaving badly data evidently fall two elongated clusters stable state means algorithm shown figure two clusters sliced half two examples show something wrong distance means algorithm means algorithm way representing size shape cluster final criticism means hard rather soft algorithm points assigned exactly one cluster points assigned cluster equals cluster points located near border two clusters arguably play partial role determining locations clusters could plausibly assigned means algorithm borderline point dumped one cluster copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links soft means clustering equal vote points cluster vote clusters 
[example, inference, task, clustering, soft, k-means, clustering] criticisms means motivate soft means algorithm algo rithm algorithm one parameter could term stiffness assignment step data point given soft degree signment means call degree assigned cluster responsibility responsibility cluster point exp exp sum responsibilities nth point update step model parameters means adjusted match sample means data points responsible total responsibility mean algorithm soft means algorithm version notice similarity soft means algorithm hard means algorithm update step identical difference responsibilities take values whereas assign ment means algorithm involved min distances rule assigning responsibilities soft min exercise show stiffness goes soft means algo rithm becomes identical original hard means algorithm except way means assigned points behave describe means instead sitting still dimensionally stiffness inverse length squared sociate lengthscale soft means algorithm demonstrated figure lengthscale shown radius circles surrounding four means panel shows final fixed point reached different value lengthscale 
[example, inference, task, clustering, conclusion] point may fixed problems original means algorithm introducing extra complexity control parameter set problem elongated clusters copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links example inference task clustering large small figure soft means algorithm version applied data set points implicit lengthscale parameter varied large small value picture shows state four means implicit lengthscale shown radius four circles running algorithm several tens iterations largest lengthscale four means converge exactly data mean four means separate two groups two shorter lengthscales pairs bifurcates subgroups clusters unequal weight width adding one stiffness parameter going make problems away come back questions later chapter develop mixture density modelling view clustering 
[example, inference, task, clustering, reading] vector quantization approach clustering see luttrell luttrell 
[example, inference, task, clustering, exercises] exercise explore properties soft means algorithm version assuming datapoints come single separable two dimensional gaussian distribution mean zero variances var var set assume large investigate fixed points algorithm varied hint assume exercise consider soft means algorithm applied large amount one dimensional data comes mixture two equal weight gaussians true means standard deviation example show hard means algorithm leads solution two means apart two true means discuss happens values find value soft algorithm puts two means correct places copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links solutions 
[example, inference, task, clustering, solutions] solution exercise associate energy state means algorithm connecting spring point mean responsible energy one spring proportional squared length namely stiffness spring total energy springs lyapunov function algorithm assignment step decrease energy point changes allegiance length spring would reduced update step decrease energy moving mean way minimize energy springs energy bounded second condition lyapunov function since algorithm lyapunov function converges solution exercise means initialized assignment step point location gives figure schematic diagram bifurcation largest data variance increases data variance indicated ellipse exp exp exp exp βmx updated exp βmx fixed point question stable unstable data density mean locations figure stable mean locations function constant found numerically thick lines approximation thin lines tiny taylor expand exp βmx βmx βmx small either grows decays exponentially mapping depending whether greater less fixed point stable unstable otherwise incidentally derivation shows result general holding true probability distribution variance gaussian bifurcation two stable fixed points surrounding unstable fixed point illustrate bifurcation figure shows outcome running soft means algorithm one dimensional data standard deviation various values figure shows pitchfork bifurcation point view data standard deviation fixed algorithm lengthscale varied horizontal axis data density mean locns figure stable mean locations function constant copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links example inference task clustering cheap theory model fitted parameters behave beyond bifurcation based continuing series expansion continuation series rather suspect since series necessarily expected converge beyond bifurcation point theory fits well anyway take analytic approach one term expansion exp βmx βmx βmx solve shape bifurcation leading order depends fourth moment distribution βmx βmx use fact gaussian find fourth moment map fixed point thin line figure shows theoretical approximation figure shows bifurcation function fixed figure shows bifurcation function fixed exercise pitchfork figure tend val ues give analytic expression asymp tote solution exercise asymptote mean rectified gaussian normal 
[enumeration] open toolbox methods handling probabilities discussing brute force inference method complete enumeration hypotheses evaluation probabilities approach exact method difficulty carrying motivate smarter exact approximate methods introduced following chapters 
[enumeration, burglar, alarm] bayesian probability theory sometimes called common sense amplified thinking following questions please ask common sense thinks answers see bayesian methods confirm everyday intuition earthquake burglar jalarm radio phonecall figure belief network burglar alarm problem example fred lives los angeles commutes miles work whilst work receives phone call neighbour saying fred burglar alarm ringing probability burglar house today driving home investigate fred hears radio small earthquake day near home says feeling relieved probably earthquake set alarm probability burglar house pearl let introduce variables burglar present fred house today alarm ringing fred receives phonecall neighbour porting alarm small earthquake took place today near fred house radio report earthquake heard fred probability variables might factorize follows plausible values probabilities burglar probability gives mean burglary rate every three years earthquake probability copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exact inference complete enumeration assertion earthquakes independent burglars prior probability seems reasonable unless take account opportunistic burglars strike immediately earthquakes alarm ringing probability assume alarm ring following three events happens burglar enters house trig gers alarm let assume alarm reliability burglars trigger alarm earthquake takes place triggers alarm perhaps alarms triggered earth quakes event causes false alarm let assume false alarm rate fred false alarms non earthquake causes every three years type dependence known noisy probabilities given numbers assume neighbour would never phone alarm ringing radio trustworthy reporter need specify probabilities order answer questions since outcomes give certainty respectively answer two questions burglar computing posterior probabilities hypotheses given available information let start reminding probability burglar either observed probability earthquake took place two propositions independent first know alarm ringing posterior probability becomes numerator four possible values normalizing constant sum four numbers posterior probabilities copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exact inference continuous hypothesis spaces answer question probability burglar marginalize earthquake variable nearly chance burglar present impor tant note variables independent priori dependent posterior distribution separable function fact illustrated simply studying effect learning learn posterior probability given dividing bot tom two rows sum posterior probability thus chance burglar fred house accordance everyday intuition probability pos sible cause alarm reduces fred learns earthquake alternative explanation alarm happened 
[enumeration, explaining, away] phenomenon one possible causes data data case becomes less probable another causes becomes probable even though two causes indepen dent variables priori known explaining away explaining away important feature correct inferences one artificial intelligence replicate believe neighbour radio service unreliable capricious certain alarm really ringing earthquake really happened calculations become complex explaining away effect persists arrival earthquake report simultaneously makes probable alarm truly ringing less probable burglar present summary solved inference questions burglar enu merating four hypotheses variables finding posterior probabilities marginalizing obtain required inferences exercise fred receives phone call burglar alarm hears radio report point view probability small earthquake today 
[enumeration, exact, inference, continuous, hypothesis, spaces] many hypothesis spaces consider naturally thought continuous example unknown decay length section lives continuous one dimensional space unknown mean stan dard deviation gaussian live continuous two dimensional space practical computer implementation continuous spaces neces sarily discretized however principle enumerated grid parameter values example figure plotted likelihood copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exact inference complete enumeration figure enumeration entire discretized hypothesis space one gaussian parameters horizontal axis vertical function decay length function evaluating likelihood finely spaced series points 
[enumeration, two-parameter, model] let look gaussian distribution example model two dimensional hypothesis space one dimensional gaussian distribution parameterized mean standard deviation exp normal figure shows enumeration one hundred hypotheses mean standard deviation one dimensional gaussian distribution hypotheses evenly spaced ten ten square grid covering ten values ten values hypothesis represented picture showing probability density puts examine inference figure five datapoints horizontal coordinate value datum vertical coordinate meaning given data points assumed drawn independently density imagine acquire data example five points shown fig ure evaluate posterior probability one hundred subhypotheses evaluating likelihood value likelihood values shown diagrammatically figure using line thickness encode value likelihood sub hypotheses likelihood smaller times maximum likelihood deleted using finer grid represent information plotting likelihood surface plot contour plot function figure 
[enumeration, five-parameter, mixture, model] eyeballing data figure might agree seems plau sible come single gaussian mixture two gaussians defined two means two standard deviations two mixing copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exact inference continuous hypothesis spaces figure likelihood function given data figure represented line thickness subhypotheses likelihood smaller times maximum likelihood shown mean sigma mean sigma figure likelihood function parameters gaussian distribution surface plot contour plot log likelihood function data set points mean copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exact inference complete enumeration figure enumeration entire discretized hypothesis space mixture two gaussians weight mixture components top half bottom half means vary horizontally standard deviations vary vertically coefficients satisfying exp exp let enumerate subhypotheses alternative model parameter space five dimensional becomes challenging represent single page figure enumerates subhypotheses different values five parameters means varied five values horizontal directions standard deviations take four values vertically takes two values vertically represent inference five parameters light five datapoints shown figure wish compare one gaussian model mixture two model find models posterior probabilities evaluating marginal likelihood evidence model evidence given integrating parameters integration imple mented numerically summing alternative enumerated values prior distribution grid parameter values take uniform mixture two gaussians integral five dimensional integral performed accurately grid points need much finer grids shown figures uncertainty parameters reduced say factor ten observing data brute force integration requires grid least points copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exact inference continuous hypothesis spaces figure inferring mixture two gaussians likelihood function given data figure represented line thickness hypothesis space identical shown figure subhypotheses likelihood smaller times maximum likelihood shown hence blank regions correspond hypotheses data ruled exponential growth computation model size reason complete enumeration rarely feasible computational strategy exercise imagine fitting mixture ten gaussians data twenty dimensional space estimate computational cost imple menting inferences model enumeration grid parameter values 
[maximum, likelihood, clustering] rather enumerate hypotheses may exponential number save lot time homing one good hypothesis fits data well philosophy behind maximum likelihood method identifies setting parameter vector maximizes likelihood data models maximum likelihood parameters identified instantly data complex models finding maximum like lihood parameters may require iterative algorithm model usually easiest work logarithm likelihood rather likelihood since likelihoods products probabilities many data points tend small likelihoods multiply log likelihoods add 
[maximum, likelihood, clustering, maximum, likelihood, one, gaussian] return gaussian first examples assume data log likelihood likelihood expressed terms two functions data sample mean sum square deviations likelihood depends data two quantities known sufficient statistics example differentiate log likelihood respect show standard deviation known maximum likelihood mean gaussian equal sample mean value solution copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links maximum likelihood one gaussian mean sigma mean sigma posterior mean sigma sigma sigma figure likelihood function parameters gaussian distribution surface plot contour plot log likelihood function data set points mean posterior probability various values posterior probability various fixed values shown density taylor expand log likelihood maximum fine approximate error bars maximum likelihood parameter use quadratic approximation estimate far maximum likelihood parameter setting likelihood falls standard fac tor example special case likelihood gaussian function parameters quadratic approximation exact example find second derivative log likelihood respect find error bars given data solution comparing curvature curvature log gaussian distri bution standard deviation exp deduce error bars derived likelihood function error bars property two points likelihood smaller maximum value factor example find maximum likelihood standard deviation gaus sian whose mean known light data find second derivative log likelihood respect error bars solution likelihood dependence tot tot find maximum likelihood differentiate respect often hygienic differentiate copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links maximum likelihood clustering respect rather scale variable use tot derivative zero tot second derivative tot maximum likelihood value equals error bars exercise show values jointly maximize likelihood 
[maximum, likelihood, clustering, maximum, likelihood, mixture, gaussians] derive algorithm fitting mixture gaussians one dimensional data fact algorithm important understand gentle reader get derive algorithm please work fol lowing exercise exercise random variable assumed probability distribution mixture two gaussians exp two gaussians given labels prior probability class label means two gaussians standard deviation brevity denote parameters data set consists points assumed indepen dent samples distribution let denote unknown class label nth point assuming known show posterior probability class label nth point written exp exp copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links enhancements soft means give expressions assume means known wish infer data standard deviation known remainder question derive iterative algorithm finding values maximize likelihood let denote natural log likelihood show derivative log likelihood respect given appeared equation show neglecting terms second derivative approximately given hence show initial state approximate newton raphson step updates parameters newton raphson method maximizing updates assuming sketch contour plot likelihood function function data set shown data set consists points describe peaks sketch indicate widths notice algorithm derived maximizing likelihood identical soft means algorithm section clear clustering viewed mixture density modelling able derive enhancements means algorithm rectify problems noted earlier 
[maximum, likelihood, clustering, enhancements, soft, k-means] algorithm shows version soft means algorithm corresponding modelling assumption cluster spherical gaussian width cluster algorithm updates lengthscales algorithm also includes cluster weight parame ters also update allowing accurate modelling data clusters unequal weights algorithm demonstrated figure two data sets seen second example shows copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links maximum likelihood clustering assignment step responsibilities exp exp dimensionality update step cluster parameters adjusted match data points responsible total responsibility mean algorithm soft means algorithm version figure soft means algorithm applied point data set figure little large data set figure exp numerator place algorithm soft means algorithm version corresponds model axis aligned gaussians copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links fatal flaw maximum likelihood figure soft means algorithm version applied data consisting two cigar shaped clusters figure figure soft means algorithm version applied little large data set convergence take long time eventually algorithm identifies small cluster large cluster soft means version maximum likelihood algorithm fitting mixture spherical gaussians data spherical meaning variance proof algorithm indeed maximize likelihood deferred section gaussian directions algorithm still good modelling cigar shaped clusters figure wish model clusters axis aligned gaussians possibly unequal variances replace assignment rule variance update rule rules displayed algorithm third version soft means demonstrated figure two cigars data set figure iterations algorithm correctly locates two clusters figure shows algorithm applied little large data set correct cluster locations found 
[maximum, likelihood, clustering, fatal, flaw, maximum, likelihood] finally figure sounds cautionary note fit means first toy data set sometimes find small clusters form covering one two data points pathological property soft means clustering versions exercise investigate happens one mean sits exactly top one data point show variance sufficiently small return possible becomes ever smaller figure soft means algorithm applied data set points notice convergence one small cluster formed two data points copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links maximum likelihood clustering 
[maximum, likelihood, clustering, kaboom!] soft means blow put one cluster exactly one data point let variance zero obtain arbitrarily large likelihood maximum likelihood methods break finding highly tuned models fit part data perfectly phenomenon known overfitting reason interested solutions enormous likelihood sure parameter settings may enormous posterior probability density density large small volume parameter space probability mass associated likelihood spikes usually tiny conclude maximum likelihood methods satisfactory gen eral solution data modelling problems likelihood may infinitely large certain parameter settings even likelihood infinitely large spikes maximum likelihood often unrepresentative high dimensional problems even low dimensional problems maximum likelihood solutions unrepresentative may know basic statistics maximum like lihood estimator gaussian standard deviation biased estimator topic take chapter maximum posteriori map method popular replacement maximizing likelihood maximizing bayesian posterior probability density parameters instead however multiplying likelihood prior maximizing posterior make problems away posterior density often also infinitely large spikes maximum posterior probability density often unrepresentative whole posterior distribution think back concept typicality encountered chapter high dimen sions probability mass typical set whose properties quite different points maximum probability density maxima atypical reason disliking maximum posteriori basis dependent make nonlinear change basis parameter parameter probability density transformed maximum density usually coincide maximum density figures illustrating nonlinear changes basis see next chapter seems undesirable use method whose answers change change representation 
[maximum, likelihood, clustering, reading] soft means algorithm heart automatic classification package autoclass hanson hanson 
[maximum, likelihood, clustering, exercises, maximum, likelihood, may, useful] exercise make version means algorithm models data mixture arbitrary gaussians gaussians constrained axis aligned copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exercises exercise photon counter pointed remote star one minute order infer brightness rate photons arriving counter per minute assuming number photons collected poisson distribution mean exp maximum likelihood estimate given find error bars situation assume counter detects photons star also background photons background rate photons known photons per minute assume number photons collected pois son distribution mean given detected photons maximum likelihood estimate comment answer discussing also bayesian posterior distribution unbiased estimator sampling theory exercise bent coin tossed times giving heads tails assume beta distribution prior probability heads example uniform distribution find maximum likelihood maximum posteriori values find maximum likelihood maximum posteriori values logit compare predictive distribution probability next toss come heads exercise two men looked prison bars one saw stars tried infer window frame min min max max side room look window see stars locations see window edges tally dark apart stars assuming window rectangular visible stars locations independently randomly distributed inferred values min min max max according maximum likelihood sketch likelihood function max fixed min min max exercise sailor infers location measuring bearings figure standard way drawing three slightly inconsistent bearings chart produces triangle called cocked hat sailor three buoys whose locations given chart let true bearings buoys assuming measurement bearing subject gaussian noise small standard deviation inferred location maximum likelihood sailor rule thumb says boat position taken centre cocked hat triangle produced intersection three measured bearings figure persuade maximum likelihood answer better exercise maximum likelihood fitting exponential family model assume variable comes probability distribution form exp copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links maximum likelihood clustering functions given parameters known data set points supplied show differentiating log likelihood maximum likelihood parameters satisfy left hand sum right hand sum data points shorthand result function average fitted model must equal function average found data data exercise maximum entropy fitting models constraints confronted probability distribution facts known maximum entropy principle maxent offers rule choosing distribution satisfies constraints accord ing maxent select maximizes entropy log subject constraints assuming constraints assert averages certain functions known show introducing lagrange multipliers one constraint cluding normalization maximum entropy distribution form maxent exp parameters set constraints satisfied hence maximum entropy method gives identical results max imum likelihood fitting exponential family model previous exer cise maximum entropy method sometimes recommended method assigning prior distributions bayesian modelling outcomes maximum entropy method sometimes interesting thought provoking advocate maxent approach assigning priors maximum entropy also sometimes proposed method solv ing inference problems example given mean score unfair six sided die probability distribution think bad idea use maximum entropy way give silly answers correct way solve inference problems use bayes theorem copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exercises 
[maximum, likelihood, clustering, exercises, maximum, likelihood, map, difficulties] exercise exercise explores idea maximizing proba bility density poor way find point representative density consider gaussian distribution dimensional space exp show nearly probability mass gaussian thin shell radius thickness proportional example dimen sions mass gaussian shell radius thickness however probability density origin times bigger density shell probability mass consider two gaussian densities dimensions differ radius contain equal total probability mass show maximum probability density greater centre gaussian smaller factor exp ill posed problems typical posterior distribution often weighted superposition gaussians varying means standard deviations true posterior skew peak maximum prob ability density located near mean gaussian distribution smallest standard deviation gaussian greatest weight exercise seven scientists datapoints drawn distributions gaussian common mean different unknown standard deviations maximum likelihood parameters given data example seven scientist figure seven measurements parameter seven scientists noise level scientists wildly differing experimental skills measure expect accurate work small turn wildly inaccurate answers enormous figure shows seven results reliable scientist hope agree intuitively looks pretty certain inept measurers better true value somewhere close maximizing likelihood tell exercise problems map method collection widgets property called wodge measure wid get widget noisy experiments known noise level model quantities come gaussian prior normal known prior variance flat log scenario suppose four widgets measured give fol lowing data interested inferring wodges four widgets find values maximize posterior probability log marginalize find posterior probability density given data integration skills required see mackay solution find maxima answer two maxima one error bars four parameters copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links maximum likelihood clustering obtained gaussian approximation posterior one error bars scenario suppose addition four measurements informed four widgets measured much less accurate instrument thus well determined ill determined parameters typical ill posed problem data measurements string uninformative values asked infer wodges widgets intuitively inferences well measured widgets negligibly affected vacuous information poorly measured widgets happens map method find values maximize posterior probability log find maxima answer one maximum error bars eight parameters 
[maximum, likelihood, clustering, solutions] solution exercise figure shows contour plot figure likelihood function likelihood function data points peaks pretty near centred points pretty near circular contours width peaks standard deviation peaks roughly gaussian shape solution exercise log likelihood fun part happens differentiate log malizing constant exp exp maximum likelihood 
[useful, probability, distributions] figure binomial distribution linear scale top logarithmic scale bottom bayesian data modelling small collection probability distribu tions come purpose chapter intro duce distributions intimidating encountered combat situations need memorize except perhaps gaussian distribution important enough memorize otherwise easily looked 
[useful, probability, distributions, distributions, integers] binomial poisson exponential already encountered binomial distribution poisson distribution page binomial distribution integer parameters bias number trials binomial distribution arises example flip bent coin bias times observe number heads poisson distribution parameter poisson distribution arises example count number photons arrive pixel fixed interval given mean intensity pixel corresponds average number photons figure poisson distribution linear scale top logarithmic scale bottom exponential distribution integers arises waiting problems long wait six rolled fair six sided dice rolled answer probability distribution number rolls exponential integers parameter distribution may also written copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links useful probability distributions 
[useful, probability, distributions, distributions, unbounded, real, numbers] gaussian student cauchy biexponential inverse cosh gaussian distribution normal distribution mean standard deviation exp sometimes useful work quantity called precision parameter gaussian sample standard univariate gaussian generated computing cos uniformly distributed second sample sin independent first obtained free gaussian distribution widely used often asserted common distribution real world sceptical asser tion yes unimodal distributions may common gaussian spe cial rather extreme unimodal distribution light tails log probability density decreases quadratically typical deviation respective probabilities deviates experience deviations mean four five times greater typical deviation may rare rare therefore urge caution use gaussian distributions variable modelled gaussian actually heavier tailed distribution rest model contort reduce deviations outliers like sheet paper crushed rubber band exercise pick variable supposedly bell shaped probability distribution gather data make plot variable empirical distribution show distribution histogram log scale investigate whether tails well modelled gaussian distribu tion one example variable study amplitude audio signal one distribution heavier tails gaussian mixture gaus sians mixture two gaussians example defined two means two standard deviations two mixing coefficients satisfying exp exp take appropriately weighted mixture infinite number gaussians mean obtain student distribution figure three unimodal distributions two student distributions parameters heavy line cauchy distribution light line gaussian distribution mean standard deviation dashed line shown linear vertical scales top logarithmic vertical scales bottom notice heavy tails cauchy distribution scarcely evident upper bell shaped curve πns copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links distributions positive real numbers called number degrees freedom gamma function student distribution mean mean distribution also finite variance student distribution approaches normal distribution mean standard deviation student distribution arises classical statistics sampling theoretic distribution certain statistics bayesian inference probability distribution variable coming gaussian distribution whose standard deviation sure special case student distribution called cauchy distribution distribution whose tails intermediate heaviness student gaussian biexponential distribution exp inverse cosh distribution cosh popular model independent component analysis limit large probability distribution becomes biexponential distribution limit approaches gaussian mean zero variance 
[useful, probability, distributions, distributions, positive, real, numbers] exponential gamma inverse gamma log normal exponential distribution exp arises waiting problems long wait bus pois sonville given buses arrive independently random one every minutes average answer probability distribution wait exponential mean gamma distribution like gaussian distribution except whereas gaussian goes gamma distributions gaussian distribution two parameters control mean width distribution gamma distribution two parameters product one parameter exponential distribution polynomial exponent polynomial second parameter exp copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links useful probability distributions figure two gamma distributions parameters heavy lines light lines shown linear vertical scales top logarithmic vertical scales bottom shown function left right simple peaked distribution mean variance often natural represent positive real variable terms logarithm probability density exp gamma distribution named normalizing constant odd convention seems figure shows couple gamma distributions function notice original gamma distribution may spike distribution never spike spike artefact bad choice basis limit obtain noninformative prior scale parameter prior improper prior called noninformative associated length scale characteristic value prefers values equally invariant reparameterization transform probability density density find latter density uniform exercise imagine reparameterize positive variable terms cube root probability density improper distribution probability density gamma distribution always unimodal density seen figures asymmetric gamma distribution decide work terms inverse obtain new distribution density flipped left right probability density called inverse gamma distribution exp copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links distributions periodic variables figure two inverse gamma distributions parameters heavy lines light lines shown linear vertical scales top logarithmic vertical scales bottom shown function left right gamma inverse gamma distributions crop many inference prob lems positive quantity inferred data examples include inferring variance gaussian noise noise samples infer ring rate parameter poisson distribution count gamma distributions also arise naturally distributions waiting times poisson distributed events given poisson process rate probability density arrival time mth event log normal distribution another distribution positive real number log normal distribu tion distribution results normal distri bution define median value standard deviation exp implies exp figure two log normal distributions parameters heavy line light line shown linear vertical scales top logarithmic vertical scales bottom yes really value median 
[useful, probability, distributions, distributions, periodic, variables] periodic variable real number property equivalent distribution plays periodic variables role played gaus sian distribution real variables von mises distribution exp cos normalizing constant modified bessel function copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links useful probability distributions distribution arises brownian diffusion around circle wrapped gaussian distribution normal 
[useful, probability, distributions, distributions, probabilities] beta distribution dirichlet distribution entropic distribution beta distribution probability density variable prob figure three beta distributions upper figure shows function lower shows corresponding density logit notice well behaved densities function logit ability parameters may take positive value normalizing constant beta function special cases include uniform distribution jeffreys prior improper laplace prior transform beta distribution corresponding density logit find always pleasant bell shaped density density may singularities figure 
[useful, probability, distributions, dimensions] dirichlet distribution density dimensional vector whose components positive sum beta distribution special case dirichlet distribution dirichlet distribution parameterized measure vector coefficients write normalized measure components positive dirichlet function dirac delta function restricts distribution simplex normalized normalizing constant dirichlet distribution vector mean probability distribution dirichlet working probability vector often helpful work softmax basis example three dimensional probability represented three numbers satisfying nonlinear transformation analogous transformation scale variable logit transformation single probability copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links distributions probabilities figure three dirichlet distributions three dimensional probability vector upper figures show random draws distribution showing values two axes triangle first figure simplex legal probability distributions lower figures show points softmax basis equation two axes show softmax basis ugly minus ones exponents dirichlet distribution disappear density given role parameter characterized two ways first mea sures sharpness distribution figure measures different expect typical samples distribution mean precision gaussian measures far samples stray mean large value produces distribution sharply peaked around effect higher dimensional situations visualized drawing typical sample distribution dirichlet set uniform vector making zipf plot ranked plot values components traditional plot ver tical axis rank horizontal axis logarithmic scales power law relationships appear straight lines figure shows plots single sample ensembles large plot shallow many components simi lar values small typically one component receives overwhelming share probability small probability remains shared among components another component receives similarly large share limit goes zero plot tends increasingly steep power law figure zipf plots random samples dirichlet distributions various values value one sample dirichlet distribution generated zipf plot shows probabilities ranked magnitude versus rank second characterize role terms predictive dis tribution results observe samples obtain counts possible outcomes value defines number samples required order data dominate prior predictions exercise dirichlet distribution satisfies nice additivity property imagine biased six sided die two red faces four blue faces die rolled times two bayesians examine outcomes order infer bias die make predictions one bayesian access red blue colour outcomes infers two component probability vector bayesian access full outcome see six faces came infers six component probability vector copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links useful probability distributions assuming sec ond bayesian assigns dirichlet distribution hyperparameters show order first bayesian inferences consistent second bayesian first bayesian prior dirichlet distribution hyper parameters hint brute force approach compute integral cheaper approach compute predictive distributions given arbitrary data find condition two predictive dis tributions match data entropic distribution probability vector sometimes used maximum entropy image reconstruction community exp measure positive vector log 
[useful, probability, distributions, reading] see mackay peto fun dirichlets 
[useful, probability, distributions, exercises] exercise datapoints drawn gamma distribution unknown parameters maximum likelihood parameters 
[exact, marginalization] task inferring mean standard deviation gaussian distribu tion samples familiar one though maybe everyone understands difference buttons calculator let recap formulae derive given data estimator two estimators two principal paradigms statistics sampling theory bayesian inference sampling theory also known frequentist orthodox statis tics one invents estimators quantities interest chooses estimators using criterion measuring sampling properties clear principle deciding criterion use measure performance estimator criteria systematic procedure construction optimal estimators bayesian inference contrast made explicit assumptions model data inferences mechanical whatever question wish pose rules probability theory give unique answer consistently takes account given information human designed estimators confidence intervals role bayesian inference human input ters important tasks designing hypothesis space specification model probability distributions figuring computations implement inference space answers questions probability distributions quantities interest often find estimators sampling theory emerge auto matically modes means posterior distributions choose simple hypothesis space turn handle bayesian inference copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links inferring mean variance gaussian distribution mean sigma mean sigma sigma sigma figure likelihood function parameters gaussian distribution repeated figure surface plot contour plot log likelihood function data set points mean notice maximum skew two estimators standard deviation values posterior probability various fixed values shown density posterior probability assuming flat prior obtained projecting probability mass onto axis maximum contrast maximum probabilities shows densities sampling theory estimators motivated follows unbiased estimator possible unbiased estimators smallest variance variance computed averaging ensemble imaginary experiments data samples assumed come unknown gaussian distribution estimator maximum likelihood estimator estimator biased however expectation given averaging many imagined experiments exercise give intuitive explanation estimator biased bias motivates invention sampling theory shown unbiased estimator precise unbiased estimator look bayesian inferences problem assuming non informative priors emphasis thus priors rather likelihood function concept marginalization joint posterior probability proportional likelihood function illustrated contour plot figure log likelihood given gaussian model likelihood expressed terms two functions data two quantities known sufficient statistics posterior probability using improper priors exp copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exact marginalization function describes answer question given data noninformative priors might may interest find parameter values maximize posterior probability though emphasized posterior probability maxima fundamental status bayesian inference since location depends choice basis choose basis prior flat posterior probability maximum coincides maximum likelihood saw exercise maximum likelihood solution posterior distribution mode seen figure likelihood skew peak increase width conditional distribution increases figure fix sequence values moving away sample mean obtain sequence conditional distributions whose maxima move increasing values figure posterior probability given exp normal note familiar scaling error bars let ask question given data noninformative priors might question differs first one asked interested parameter must therefore marginalized posterior probability data dependent term appeared earlier normalizing constant equation one name quantity evidence marginal likelihood obtain evidence integrating noninformative prior constant assumed call constant think prior top hat prior width gaussian integral yields first two terms best fit log likelihood log likelihood last term log occam factor penalizes smaller values discuss occam factors chapter differentiate log evidence respect find probable additional volume factor shifts maximum intuitively denominator counts number noise measurements contained quantity sum contains residuals squared effective noise measurements determination one parameter data causes one dimension noise gobbled unavoidable overfitting terminology classical copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exercises statistics bayesian best guess sets measure deviance defined equal number degrees freedom figure shows posterior probability proportional marginal likelihood may contrasted posterior prob ability fixed probable value shown figure final inference might wish make given data exercise marginalize obtain posterior marginal distri bution student distribution 
[exact, marginalization, inferring, mean, variance, gaussian, distribution] discuss one dimensional gaussian distribution parameterized mean standard deviation exp normal inferring parameters must specify prior distribution prior gives opportunity include specific knowledge independent experiments theoretical grounds example knowledge construct appropriate prior embodies supposed ignorance section assumed uniform prior range parameters plotted wish able perform exact marginalizations may useful consider conjugate priors priors whose functional form combines naturally likelihood inferences convenient form 
[exact, marginalization, conjugate, priors] conjugate prior mean gaussian introduce two perparameters parameterize prior write normal limit obtain noninformative prior location parameter flat prior noninformative invariant natural reparameterization prior const also improper prior normalizable conjugate prior standard deviation gamma distribution two parameters convenient define prior copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exact marginalization density inverse variance precision parameter exp simple peaked distribution mean variance limit obtain noninformative prior scale parameter prior noninformative invariant reparameterization prior less strange looking examine resulting density flat reminder change variables one one function probability density transforms jacobian prior expresses ignorance saying well could could could scale variables usually best represented terms logarithm noninformative prior improper following examples use improper noninformative priors using improper priors viewed distasteful circles let excuse saying sake readability included proper priors calculations could still done key points would obscured flood extra parameters 
[exact, marginalization, maximum, likelihood, marginalization)[exact, marginalization, reading] bible exact marginalization bretthorst book bayesian spec trum analysis parameter estimation 
[exact, marginalization, exercises] exercise exercise requires macho integration capabilities give bayesian solution exercise seven scientists varying capabilities measured personal noise levels interested inferring let prior broad prior example gamma distribution parameters find posterior distribution plot explore properties variety data sets one given data set hint first find posterior distribution given note normalizing constant inference marginalize find normalizing constant use bayes theorem second time find 
[exact, marginalization, solutions] solution exercise data points distributed mean squared deviation true mean sample mean unlikely exactly equal true mean sample mean value minimizes sum squared deviation data points value particular true value larger value sum squared deviation expected mean squared deviation sample mean neces sarily smaller mean squared deviation true mean 
[exact, marginalization, trellises] chapter discuss exact methods used proba bilistic modelling example discuss task decoding linear error correcting code see inferences conducted effi ciently message passing algorithms take advantage graphical structure problem avoid unnecessary duplication computations see chapter 
[exact, marginalization, trellises, decoding, problems] codeword selected linear code transmitted noisy channel received signal chapter assume channel memoryless channel gaussian channel given assumed channel model two decoding problems codeword decoding problem task inferring codeword transmitted given received signal bitwise decoding problem task inferring transmit ted bit likely bit one rather zero concrete example take hamming code chapter discussed codeword decoding problem code assuming binary symmetric channel discuss bitwise decoding problem discuss handle general channel models gaussian channel 
[exact, marginalization, trellises, solving, codeword, decoding, problem] bayes theorem posterior probability codeword likelihood function first factor numerator likeli hood codeword memoryless channel separable function example channel gaussian channel transmissions additive noise standard deviation probability density copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links decoding problems received signal two cases exp exp point view decoding matters likelihood ratio case gaussian channel exp exercise show point view decoding gaussian channel equivalent time varying binary symmetric channel known noise level depends prior second factor numerator prior probability codeword usually assumed uniform valid codewords denominator normalizing constant complete solution codeword decoding problem list codewords probabilities given equation since num ber codewords linear code often large since interested knowing detailed probabilities codewords often restrict attention simplified version codeword decoding problem map codeword decoding problem task identifying probable codeword given received signal prior probability codewords uniform task iden tical problem maximum likelihood decoding identifying codeword maximizes example chapter hamming code binary symmetric channel discussed method deducing probable codeword syndrome received signal thus solving map codeword decoding problem case would like general solution map codeword decoding problem solved exponential time order searching codewords one maximizes interested methods efficient section discuss exact method known min sum algorithm may able solve codeword decoding problem efficiently much efficiently depends properties code worth emphasizing map codeword decoding general lin ear code known complete means layman terms map codeword decoding complexity scales exponentially blocklength unless revolution computer science restrict ing attention map decoding problem necessarily made task much less challenging simply makes answer briefer report copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exact marginalization trellises 
[exact, marginalization, trellises, solving, bitwise, decoding, problem] formally exact solution bitwise decoding problem obtained equation marginalizing bits also write marginal aid truth function one proposition true zero otherwise computing marginal probabilities explicit sum codewords takes exponential time certain codes bitwise decoding problem solved much efficiently using forward backward algorithm describe algorithm example sum product algorithm moment min sum algorithm sum product algorithm widespread importance invented many times many fields 
[exact, marginalization, trellises, codes, trellises] chapters represented linear codes terms generator matrices parity check matrices case systematic block code first transmitted bits block size source bits remaining bits parity check bits means generator matrix code written parity check matrix written matrix section study another representation linear code called trellis codes trellises represent general systematic codes mapped onto systematic codes desired reordering bits block repetition code simple parity code hamming code figure examples trellises edge trellis labelled zero shown square one shown cross 
[exact, marginalization, trellises, definition, trellis] definition quite narrow comprehensive view trellises reader consult kschischang sorokine trellis graph consisting nodes also known states vertices edges nodes grouped vertical slices called times times ordered edge connects node one time node neighbouring time every edge labelled symbol leftmost rightmost states contain one node apart two extreme nodes nodes trellis least one edge connecting leftwards least one connecting rightwards copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links solving decoding problems trellis trellis times defines code blocklength follows codeword obtained taking path crosses trellis left right reading symbols edges traversed valid path trellis defines codeword number leftmost time time rightmost time number leftmost state state rightmost state total number states vertices trellis nth bit codeword emitted move time time width trellis given time number nodes time maximal width trellis sounds like trellis called linear trellis code defines linear code solely concerned linear trellises nonlinear trellises much complex beasts brevity discuss binary trellises trellises whose edges labelled zeroes ones hard generalize methods follow ary trellises figures show trellises corresponding repetition code parity code hamming code exercise confirm sixteen codewords listed table generated trellis shown figure 
[exact, marginalization, trellises, observations, linear, trellises] linear code minimal trellis one smallest number nodes minimal trellis node two edges entering two edges leaving nodes time left degree right degree width always power two minimal trellis linear code cannot width greater since every node least one valid codeword codewords furthermore define minimal trellis width everywhere less proved section notice linear trellises figure minimal trellises number times binary branch point encountered trellis traversed left right right left discuss construction trellises section know enough discuss decoding problem 
[exact, marginalization, trellises, solving, decoding, problems, trellis] view trellis linear code giving causal description probabilistic process gives rise codeword time flowing left right time divergence encountered random source source information bits communication determines way receiving end receive noisy version sequence edge labels wish infer path taken precise want identify probable path order solve codeword decoding problem want find probability transmitted symbol time zero one solve bitwise decoding problem example consider case single transmission hamming trellis shown figure copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exact marginalization trellises likelihood posterior probability figure posterior probabilities sixteen codewords received vector normalized likelihoods let normalized likelihoods ratios likelihoods etc received signal decoded threshold likelihoods turn signal nary received vector decodes using decoder binary symmetric channel chapter optimal decoding procedure optimal inferences always obtained using bayes theorem find posterior probability codewords explicit enu meration sixteen codewords posterior distribution shown figure course really interested brute force solutions aim chapter understand algorithms getting information less computer time examining posterior probabilities notice probable codeword actually string twice probable answer found thresholding using posterior probabilities shown figure also com pute posterior marginal distributions bits result shown figure notice bits quite con fidently inferred zero strengths posterior probabilities bits great example map codeword agreement bitwise decoding obtained selecting probable state bit using posterior marginal distributions always case following exercise shows copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links solving decoding problems trellis likelihood posterior marginals figure marginal posterior probabilities bits posterior distribution figure exercise find probable codeword case normalized likelihood also find estimate marginal posterior probability seven bits give bit bit decoding hint concentrate codewords largest probabil ity discuss use message passing code trellis solve decoding problems 
[exact, marginalization, trellises, min–sum, algorithm] map codeword decoding problem solved using min sum gorithm introduced section codeword code corresponds path across trellis cost journey sum costs constituent steps log likelihood codeword sum bitwise log likelihoods convention flip sign log likelihood would like maximize talk terms cost would like minimize associate edge cost log trans mitted bit associated edge received symbol min sum algorithm presented section identify prob able codeword number computer operations equal number edges trellis algorithm also known viterbi algorithm viterbi 
[exact, marginalization, trellises, sum–product, algorithm] solve bitwise decoding problem make small modification min sum algorithm messages passed trellis define probability data current point instead cost best route point replace costs edges log likelihoods replace min sum operations min sum algorithm sum product respectively let run nodes states label start state denote set states parents state likelihood associated edge node node define forward pass messages copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exact marginalization trellises messages computed sequentially left right exercise show node whose time coordinate proportional joint probability codeword path passed node first received symbols message computed end node trellis proportional marginal probability data exercise constant proportionality answer define second set backward pass messages similar manner let node end node messages computed sequentially backward pass right left exercise show node whose time coordinate proportional conditional probability given codeword path passed node subsequent received symbols finally find probability nth bit two summations products forward backward messages let run nodes time run nodes time let value associated trellis edge node node value compute posterior probability normalizing constant identical final forward message computed earlier exercise confirm sum product algorithm com pute names sum product algorithm presented forward backward algorithm bcjr algorithm belief propagation exercise codeword simple parity code transmitted received signal associated likelihoods shown table table bitwise likelihoods codeword use min sum algorithm sum product algorithm trellis figure solve map codeword decoding problem bitwise decoding problem confirm answers enumeration codewords hint use logs base min sum computations hand working sum product algorithm hand may find helpful use three colours pen one one one copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links trellises 
[exact, marginalization, trellises, trellises] discuss various ways making trellis code may safely jump section span codeword set bits contained first bit codeword non zero last bit non zero inclusive indicate span codeword binary vector shown table codeword span table codewords spans generator matrix trellis oriented form spans rows generator matrix start different columns spans end different columns 
[exact, marginalization, trellises, make, trellis, generator, matrix] first put generator matrix trellis oriented form row manipulations similar gaussian elimination example hamming code generated       matrix trellis oriented form example rows spans end column subtracting lower rows upper rows obtain equivalent generator matrix one generates set codewords follows       row generator matrix thought defining subcode code case code two codewords length first row code consists two codewords subcode defined second row consists easy construct minimal trellises subcodes shown left column figure build trellis incrementally shown figure start trellis corresponding subcode given first row generator matrix add one subcode time vertices within span new subcode duplicated edge symbols original trellis left unchanged edge symbols second part trellis flipped wherever new subcode otherwise left alone another hamming code generated       copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exact marginalization trellises figure trellises four subcodes hamming code left column sequence trellises made constructing trellis hamming code right column edge trellis labelled zero shown square one shown cross hamming code generated matrix differs permutation bits code generated systematic matrix used chapter parity check matrix corresponding permutation   trellis obtained permuted matrix given equation shown figure notice number nodes trellis smaller number nodes previous trellis hamming code figure thus observe rearranging order codeword bits sometimes lead smaller simpler trellises figure trellises permuted hamming code generated generator matrix method figure parity check matrix method page edge trellis labelled zero shown square one shown cross 
[exact, marginalization, trellises, trellises, parity-check, matrices] another way viewing trellis terms syndrome syndrome vector defined parity check matrix vector codeword syndrome zero generate codeword describe current state partial syndrome product codeword bits thus far generated state trellis partial syndrome one time coordinate starting ending states constrained zero syndrome node state represents different possible value partial syndrome since matrix syndrome bit vector need nodes state construct trellis code parity check matrix walking end generating two trees possible syndrome sequences intersection two trees defines trellis code pictures obtain construction let vertical coordinate represent syndrome horizontal edge necessarily associated zero bit since non zero bit changes syndrome copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links solutions non horizontal edge associated one bit thus rep resentation longer need label edges trellis figure shows trellis corresponding parity check matrix equation 
[exact, marginalization, trellises, solutions] likelihood posterior probability table posterior probability codewords exercise solution exercise posterior probability codewords shown table probable codeword marginal posterior probabilities seven bits likelihood posterior marginals bitwise decoding actually codeword solution exercise map codeword like lihood normalizing constant sum product algorithm intermediate left right intermediate right left bitwise decoding codewords probabilities 
[exact, marginalization, graphs] take general view tasks inference marginalization reading chapter read message passing chapter 
[exact, marginalization, graphs, general, problem] assume function set variables defined product factors follows factors function subset variables make positive function may interested second normalized function normalizing constant defined example notation introduced function three binary variables defined five factors five subsets denoted general function copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links general problem function way may recognized posterior prob ability distribution three transmitted bits repetition code section received signal channel binary sym metric channel flip probability factors respectively enforce constraints must identical must identical factors likelihood functions con tributed component function factored form depicted factor graph variables depicted circular nodes factors depicted square nodes edge put variable node factor node function dependence variable factor graph example function shown figure figure factor graph associated function 
[exact, marginalization, graphs, normalization, problem] first task solved compute normalizing constant 
[exact, marginalization, graphs, marginalization, problems] second task solved compute marginal function variable defined example function three variables marginal defined type summation except important useful special notation sum summary third task solved compute normalized marginal variable defined include suffix departing normal practice rest book would omit exercise show normalized marginal related marginal might also interested marginals subset variables tasks intractable general even every factor function three variables cost computing exact solutions marginals believed general grow exponentially number variables certain functions however marginals computed effi ciently exploiting factorization idea efficiency copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exact marginalization graphs arises well illustrated message passing examples chapter sum product algorithm review generalization message passing rule set case sum product algorithm valid graph tree like 
[exact, marginalization, graphs, notation] identify set variables mth factor depends set indices example function sets since function alone similarly define set factors variable participates denote set variable excluded introduce shorthand denote set variables excluded sum product algorithm involve messages two types passing along edges factor graph messages variable nodes factor nodes messages factor nodes variable nodes message either type sent along edge connecting factor variable always function variable two rules updating two sets messages variable factor factor variable    
[exact, marginalization, graphs, rules, apply, leaves, factor, graph] node one edge connecting another node called leaf figure factor node leaf node perpetually sends message one neighbour node factor nodes graph may connected one vari able node case set variables appearing fac tor message update empty set product functions empty product whose value fac tor node therefore always broadcasts one neighbour message similarly may variable nodes connected one factor node set empty nodes perpetually broadcast message figure variable node leaf node perpetually sends message 
[exact, marginalization, graphs, starting, finishing, method] alternatively algorithm initialized setting initial mes sages variables proceeding factor message update rule alternating variable message update rule compared method lazy initialization method leads load wasted computations whose results gradually flushed correct answers computed method number iterations equal diameter factor graph algorithm converge set messages satisfying sum product relationships exercise apply second version sum product algorithm function defined equation figure copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exact marginalization graphs reason introducing lazy method unlike method applied graphs tree like sum product algorithm run graph cycles algorithm necessarily converge certainly general compute correct marginal functions nevertheless algorithm great practical importance especially decoding sparse graph codes 
[exact, marginalization, graphs, sum–product, algorithm, on-the-fly, normalization] interested normalized marginals another version sum product algorithm may useful factor variable messages computed way variable factor messages normalized thus scalar chosen exercise apply normalized version sum product algorithm function defined equation figure 
[exact, marginalization, graphs, factorization, view, sum–product, algorithm] one way view sum product algorithm reexpresses original factored function product factors another factored function product factors factor associated factor node factor associated variable node initially time factor variable message sent factorization updated thus message computed terms using   differs assignment product exercise confirm update rules equivalent sum product rules eventually becomes marginal factorization viewpoint applies whether graph tree like copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links min sum algorithm 
[exact, marginalization, graphs, computational, tricks] fly normalization good idea computational point view product many factors values likely large small another useful computational trick involves passing logarithms messages instead computations products algorithm replaced simpler additions summations course become difficult carry return logarithm need compute softmax functions like computation done efficiently using look tables along observation value answer typically little larger max store look tables values function negative computed exactly number look ups additions scaling number terms sum look ups sorting operations cheaper exp approach costs less direct evaluation number operations reduced omitting negligible contributions smallest third computational trick applicable certain error correcting codes pass messages fourier transforms messages makes computations factor variable messages quicker simple example fourier transform trick given chapter equa tion 
[exact, marginalization, graphs, min–sum, algorithm] sum product algorithm solves problem finding marginal func tion given product analogous solving bitwise decod ing problem section decoding problems example codeword decoding problem define tasks involving solved modifications sum product algo rithm example consider task analogous codeword decoding problem maximization problem find setting maximizes product problem solved replacing two operations add mul tiply everywhere appear sum product algorithm another pair operations satisfy distributive law namely max multiply replace summation maximization notice quantity formerly known normalizing constant becomes max thus sum product algorithm turned max product algo rithm computes max solution max imization problem deduced marginal lists maximum value attain value copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exact marginalization graphs practice max product algorithm often carried negative log likelihood domain max product become min sum min sum algorithm also known viterbi algorithm 
[exact, marginalization, graphs, junction, tree, algorithm] one factor graph one interested tree several options divide exact methods approx imate methods widely used exact method handling marginaliza tion graphs cycles called junction tree algorithm algorithm works agglomerating variables together agglomerated graph cycles probably figure details complexity marginalization grows exponentially number agglomerated variables read junction tree algorithm lauritzen jordan many approximate methods visit next chapters monte carlo methods variational methods name couple however amusing way handling factor graphs sum product algorithm may applied already mentioned apply sum product algorithm simply compute messages node graph graph tree iterate cross fingers called loopy message passing great importance decoding error correcting codes come back section part 
[exact, marginalization, graphs, reading] reading factor graphs sum product algorithm see kschischang yedidia yedidia yedidia wainwright forney see also pearl good reference fundamental theory graphical models lauritzen readable introduction bayesian networks given jensen interesting message passing algorithms different capabilities sum product algorithm include expectation propagation minka survey propagation braunstein see also section 
[exact, marginalization, graphs, exercises] exercise express joint probability distribution burglar alarm earthquake problem example factor graph find marginal probabilities variables piece information comes fred attention using sum product algorithm fly normalization 
[laplace’s, method] idea behind laplace approximation simple assume unnormalized probability density whose normalizing constant interest peak point taylor expand logarithm around peak amp approximate unnormalized gaussian exp approximate normalizing constant normalizing constant amp gaussian generalize integral approximate density dimensional space matrix second derivatives maximum defined expansion generalized normalizing constant approximated det det predictions made using approximation physicists also call widely used approximation saddle point approximation copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links laplace method fact normalizing constant gaussian given exp det proved making orthogonal transformation basis transformed diagonal matrix integral separates product one dimensional integrals form exp product eigenvalues determinant laplace approximation basis dependent transformed nonlinear function density transformed general approximate normalizing constants different viewed defect since true value basis independent opportunity hunt choice basis laplace approximation accurate 
[laplace’s, method, exercises] exercise see also exercise photon counter pointed remote star one minute order infer rate photons arriving counter per minute assuming number photons collected poisson distribution mean exp assuming improper prior make laplace approxima tions posterior distribution log note improper prior transforms log constant exercise use laplace method approximate integral positive check accuracy approximation exact answer measure error log log bits exercise linear regression datapoints generated experimenter choosing world delivering noisy version linear function normal assuming gaussian priors make laplace approxima tion posterior distribution exact fact obtain predictive distribution next datapoint given see mackay reading 
[model, comparison, occam’s, razor] figure picture interpreted contains tree boxes 
[model, comparison, occam’s, razor, occam’s, razor] many boxes picture figure particular many boxes vicinity tree looked ray spectacles would see one two boxes behind trunk figure even occam razor principle states preference simple figure many boxes behind tree theories accept simplest explanation fits data thus according occam razor deduce one box behind tree hoc rule thumb convincing reason believing likely one box perhaps intuition likes argument well would remarkable coincidence two boxes height colour wish make artificial intelligences interpret data correctly must translate intuitive feeling concrete theory 
[model, comparison, occam’s, razor, motivations, occam’s, razor] several explanations compatible set observations occam razor advises buy simplest principle often advocated one two reasons first aesthetic theory mathematical beauty likely correct ugly one fits experimental data copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links model comparison occam razor evidence figure bayesian inference embodies occam razor figure gives basic intuition complex models turn less probable horizontal axis represents space possible data sets bayes theorem rewards models proportion much predicted data occurred predictions quantified normalized probability distribution probability data given model called evidence simple model makes limited range predictions shown powerful model example free parameters able predict greater variety data sets means however predict data sets region strongly suppose equal prior probabilities assigned two models data set falls region less powerful model probable model paul dirac second reason past empirical success occam razor however different justification occam razor namely coherent inference embodied bayesian probability auto matically embodies occam razor quantitatively indeed probable one box behind tree compute much probable one two 
[model, comparison, occam’s, razor, model, comparison, occam’s, razor] evaluate plausibility two alternative theories light data follows using bayes theorem relate plausibility model given data predictions made model data prior plausibility gives following probability ratio theory theory first ratio right hand side measures much initial beliefs favoured second ratio expresses well observed data predicted compared relate occam razor simpler model first ratio gives opportunity wish insert prior bias favour aesthetic grounds basis experience would correspond aesthetic empirical motivations occam razor mentioned earlier prior bias necessary second ratio data dependent factor embodies occam razor auto matically simple models tend make precise predictions complex models nature capable making greater variety predictions figure complex model must spread predictive proba bility thinly data space thus case data compatible theories simpler turn probable without express subjective dislike complex models subjective prior needs assign equal prior prob abilities possibilities simplicity complexity probability theory allows observed data express opinion let turn simple example sequence numbers task predict next two numbers infer underlying process gave rise sequence popular answer question prediction explanation add previous number alternative answer underlying rule get next number previous number evaluating copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links occam razor assume prediction seems rather less plausible second rule fits data well rule add find less plausible let give labels two general theories sequence arithmetic progression add integer sequence generated cubic function form fractions one reason finding second explanation less plausible might arithmetic progressions frequently encountered cubic func tions would put bias prior probability ratio equation let give two theories equal prior probabilities concentrate data say well theory predict data obtain must specify probability distribution model assigns parameters first depends added integer first number sequence let say numbers could anywhere since pair values first number give rise observed data probability data given evaluate must similarly say values fractions might take choose represent numbers fractions rather real numbers used real numbers model would assign relative infinitesimal probability real parameters norm however assumed rest chapter reasonable prior might state fraction numerator could number denominator number initial value sequence let leave probability distribution four ways expressing fraction prior similarly four two possible solutions respectively probability observed data given found thus comparing even prior prob abilities equal odds favour given sequence forty million one answer depends several subjective assumptions particular probability assigned free parameters theories bayesians make apologies thing inference prediction without assumptions however quantitative details prior proba bilities effect qualitative occam razor effect complex theory always suffers occam factor parameters predict greater variety data sets figure small example four data points move larger copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links model comparison occam razor gather data create alternative models fit model data assign preferences alternative models choose data gather next gather data decide whether create new models create new models choose future actions figure bayesian inference fits data modelling process figure illustrates abstraction part scientific process data collected modelled particular figure applies pattern classification learning interpolation etc two double framed boxes denote two steps involve inference two steps bayes theorem used bayes tell invent models example first box fitting model data task inferring model parameters might given model data bayesian methods may used find probable parameter values error bars parameters result applying bayesian methods problem often little different answers given orthodox statistics second inference task model comparison light data bayesian methods class second inference problem requires quantitative occam razor penalize complex models bayesian methods assign objective preferences alternative models way automatically embodies occam razor sophisticated problems magnitude occam factors typi cally increases degree inferences influenced quantitative details subjective assumptions becomes smaller 
[model, comparison, occam’s, razor, bayesian, methods, data, analysis] let relate discussion real problems data analysis countless problems science statistics technology require given limited data set preferences assigned alternative models differing complexities example two alternative hypotheses accounting planetary motion inquisition geocentric model based epicycles copernicus simpler model solar system sun centre epicyclic model fits data planetary motion least well copernican model using parameters coincidentally inquisition two extra epicyclic parameters every planet found identical period radius sun cycle around earth intuitively find copernicus theory probable 
[model, comparison, occam’s, razor, mechanism, bayesian, razor, evidence, occam, factor] two levels inference often distinguished process data mod elling first level inference assume particular model true fit model data infer values free param eters plausibly take given data results inference often summarized probable parameter values error bars parameters analysis repeated model second level inference task model comparison wish compare models light data assign sort preference ranking alternatives note levels inference distinct decision theory goal inference given defined hypothesis space particular data set assign probabilities hypotheses decision theory typically chooses alternative actions basis probabilities minimize copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links occam razor expectation loss function chapter concerns inference alone loss functions involved discuss model comparison construed implying model choice ideal bayesian predictions involve choice models rather predictions made summing alternative models weighted probabilities bayesian methods able consistently quantitatively solve inference tasks popular myth states bayesian meth ods differ orthodox statistical methods inclusion subjective priors difficult assign usually make much dif ference conclusions true first level inference bayesian results often differ little outcome orthodox tack widely appreciated bayesian performs second level inference chapter therefore focus bayesian model compar ison model comparison difficult task possible simply choose model fits data best complex models always fit data better maximum likelihood model choice would lead inevitably implausible parameterized models generalize poorly occam razor needed let write bayes theorem two levels inference described see explicitly bayesian model comparison works model assumed vector parameters model defined collection probability distributions prior distribution states values model parameters might expected take set conditional distributions one value defining predictions model makes data model fitting first level inference assume one model ith say true infer model parameters might given data using bayes theorem posterior probability parameters posterior likelihood prior evidence normalizing constant commonly ignored since irrel evant first level inference inference becomes important second level inference name evidence common practice use gradient based methods find maximum posterior defines probable value parameters usual summarize posterior distribution value error bars confidence intervals best fit parameters error bars obtained curvature pos terior evaluating hessian taylor expanding log posterior probability exp see posterior locally approximated gaussian covariance matrix equivalent error bars whether approximation good depend problem solv ing indeed maximum mean posterior distribution copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links model comparison occam razor figure occam factor figure shows quantities determine occam factor hypothesis single parameter prior distribution solid line parameter width posterior distribution dashed line single peak characteristic width occam factor fundamental status bayesian inference change nonlinear reparameterizations maximization posterior probabil ity useful approximation like equation gives good summary distribution model comparison second level inference wish infer model plausible given data posterior probability model notice data dependent term evidence appeared normalizing constant second term subjective prior hypothesis space expresses plausible thought alternative models data arrived assuming choose assign equal priors alternative models models ranked evaluating evidence normalizing constant omitted equation data modelling process may develop new models data arrived inadequacy first models detected example inference open ended continually seek probable models account data gather repeat key idea rank alternative models bayesian eval uates evidence concept general idence evaluated parametric non parametric models alike whatever data modelling task regression problem clas sification problem density estimation problem evidence transportable quantity comparing alternative models cases evidence naturally embodies occam razor 
[model, comparison, occam’s, razor, evaluating, evidence] let study evidence closely gain insight bayesian occam razor works evidence normalizing constant equation many problems posterior strong peak probable parameters figure taking simplicity one dimensional case evidence approx imated using laplace method height peak integrand times width copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links occam razor evidence best fit likelihood occam factor thus evidence found taking best fit likelihood model achieve multiplying occam factor term magnitude less one penalizes parameter 
[model, comparison, occam’s, razor, interpretation, occam, factor] quantity posterior uncertainty suppose simplicity prior uniform large interval representing range values possible priori according figure occam factor occam factor equal ratio posterior accessible volume parameter space prior accessible volume factor hypothesis space collapses data arrive model viewed consisting certain number exclusive submodels one survives data arrive occam factor inverse number logarithm occam factor measure amount information gain model parameters data arrive complex model many parameters free vary large range typically penalized stronger occam factor simpler model occam factor also penalizes models finely tuned fit data favouring models required pre cision parameters coarse magnitude occam factor thus measure complexity model relates complexity predictions model makes data space depends number parameters model also prior probability model assigns model achieves greatest evidence determined trade minimizing natural complexity mea sure minimizing data misfit contrast alternative measures model complexity occam factor model straightforward evalu ate simply depends error bars parameters already evaluated fitting model data figure displays entire hypothesis space illustrate var ious probabilities analysis three models equal prior probabilities model one parameter shown horizontal axis assigns different prior range parame ter flexible complex model assigning broadest prior range one dimensional data space shown vertical axis model assigns joint probability distribution data parameters illustrated cloud dots dots represent random samples full probability distribution total number dots three model subspaces assigned equal prior probabilities models particular data set received horizontal line infer pos terior distribution model say reading density along horizontal line normalizing posterior probability shown dotted curve bottom also shown prior distribu tion figure case model poorly copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links model comparison occam razor figure hypothesis space consisting three exclusive models one parameter one dimensional data set data set single measured value differs parameter small amount additive noise typical samples joint distribution shown dots data points observed data set single particular value shown dashed horizontal line dashed curves show posterior probability model given data set figure evidence different models obtained marginalizing onto axis left hand side figure matched data shape posterior distribution depend details tails prior likelihood curve shown case prior falls strongly obtain figure marginalizing joint distributions onto axis left hand side data set shown dotted horizontal line evidence flexible model smaller value evidence placed less predictive probability fewer dots line terms distributions model smaller evidence occam factor smaller simplest model smallest evidence best fit achieve data poor given data set probable model 
[model, comparison, occam’s, razor, occam, factor, several, parameters] posterior well approximated gaussian occam factor obtained determinant corresponding covariance matrix equation chapter det evidence best fit likelihood occam factor hessian evaluated calculated error bars equation chapter amount data collected increases gaussian approximation expected become increasingly accurate summary bayesian model comparison simple extension maximum likelihood model selection evidence obtained multiplying best fit likelihood occam factor evaluate occam factor need hessian gaussian approximation good thus bayesian method model comparison evaluating evidence computationally demanding task finding model best fit parameters error bars copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links example 
[model, comparison, occam’s, razor, example] let return example opened chapter one two boxes behind tree figure coincidences make suspicious let assume image area round trunk box size pixels trunk pixels wide different colours boxes distinguished theory says one box near trunk four free parameters three coordinates defining top three edges box one parameter giving box colour boxes could levitate would five free parameters theory says two boxes near trunk eight free parameters twice four plus ninth binary variable indicates two boxes closest viewer figure many boxes behind tree evidence model first need prior parameters evaluate evidence convenience let work pixels let assign separable prior horizontal location box width height colour height could say distinguishable values could width could location colour could values put uniform priors variables ignore parameters associated objects image since come model comparison evidence since one setting parameters fits data predicts data perfectly model six nine parameters well determined three partly constrained data left hand box furthest away example width least pixels closer two boxes width pixels assuming visible portion left hand box pixels wide get evidence need sum prior probabilities viable hypotheses exact calculation need specific data priors let get ballpark answer assuming two unconstrained real variables half values available binary variable completely undetermined exercise make explicit model work exact answer thus posterior probability ratio assuming equal prior probability data roughly favour simpler hypothesis four factors interpreted terms occam factors complex model four extra parameters sizes colours three sizes one colour pay two big occam factors highly suspicious coincidences two box heights match exactly two colours match exactly also pays two lesser occam factors two lesser coincidences boxes happened one edges conveniently hidden behind tree behind copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links model comparison occam razor figure popular view model comparison minimum description length model communicates data sending identity model sending best fit parameters model sending data relative parameters proceed complex models length parameter message increases hand length data message decreases complex model able fit data better making residuals smaller example intermediate model achieves optimum trade two trends 
[model, comparison, occam’s, razor, minimum, description, lengthmdl)] complementary view bayesian model comparison obtained replacing probabilities events lengths bits messages communicate events without loss receiver message lengths correspond probabilistic model events via relations log mdl principle wallace boulton states one prefer models communicate data smallest number bits consider two part message states model used communicates data within model pre arranged pre cision produces message length lengths different define implicit prior alter native models similarly corresponds density thus procedure assigning message lengths mapped onto posterior prob abilities log log log const principle mdl always interpreted bayesian model compar ison vice versa however simple discussion addressed one would actually evaluate key data dependent term corresponds evidence often message imagined subdivided parameter block data block figure models small number parameters short parameter block fit data well data message list large residuals long number parameters increases parameter block lengthens data message becomes shorter optimum model complexity figure sum minimized picture glosses subtle issues specified precision parameters sent precision important effect unlike precision real valued data sent assuming small relative noise level introduces additive constant decrease precision sent parameter message shortens data message typically lengthens truncated parameters match data well non trivial optimal precision simple gaussian cases possible solve optimal precision wallace freeman closely related posterior error bars parameters turns optimal parameter message length virtually identical log occam factor equation random element involved parameter truncation means encoding slightly sub optimal care therefore one replicate bayesian results mdl terms although earliest work complex model comparison involved mdl framework patrick wallace mdl apparent vantages direct probabilistic approach copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links minimum description length mdl mdl uses pedagogical tool description length concept useful motivating prior probability distributions also different ways breaking task communicating data using model give helpful insights modelling process illustrated line learning cross validation cases data consist sequence points log evidence decomposed sum line predictive perfor mances log log log log log decomposition used explain difference idence leave one cross validation measures predictive abil ity cross validation examines average value last term log random orderings data evi dence hand sums well model predicted data starting scratch 
[model, comparison, occam’s, razor, ‘bits-back’, encoding, method] another mdl thought experiment hinton van camp involves corporating random bits message data communicated using parameter block data block parameter vector sent random sam ple posterior sample sent arbitrary small granularity using message length log data encoded relative message length log data mes sage received random bits used generate sample posterior deduced receiver number bits recov ered log recovered bits need count towards message length since might use optimally encoded message random bit string thereby communicating message time net description cost therefore bits back log log log thus thought experiment yielded optimal description length bits back encoding turned practical compression method data modelled latent variable models frey 
[model, comparison, occam’s, razor, reading] bayesian methods introduced contrasted sampling theory statis tics jaynes gull loredo bayesian occam razor demonstrated model problems gull mackay useful textbooks box tiao berger one debate worth understanding question whether permis sible use improper priors bayesian inference dawid want model comparison discussed chapter essen tial use proper priors otherwise evidences occam factors copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links model comparison occam razor meaningless one intention model comparison may safe use improper priors even cases pitfalls dawid explain would agree advice always use proper priors tempered encouragement smart making calculations recognizing opportunities approximation 
[model, comparison, occam’s, razor, exercises] exercise random variables come independently probability distribution according model uniform distribu tion according model nonuniform distribution known parameter given data evidence exercise datapoints believed come straight line experimenter chooses gaussian distributed variance according model straight line horizontal according model parameter prior distribu tion normal models assign prior distribution normal given data set assuming noise level evidence model exercise six sided die rolled times numbers times face came probability die perfectly fair die assuming alternative pothesis says die biased distribution prior density uniform simplex solve problem two ways exactly using helpful dirichlet formu lae approximately using laplace method notice choice basis laplace approximation important see mackay discussion exercise exercise influence race imposition death penalty murder america much studied following three way table classifies cases defendant convicted mur der three variables defendant race victim race whether defendant sentenced death data radelet racial characteristics imposition death penalty american sociological review white defendant black defendant death penalty death penalty yes yes white victim white victim black victim black victim copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exercises seems death penalty applied much often victim white victim black victim white defendants got death penalty victim black defendants got death penalty incidentally data provide example phenomenon known simpson paradox higher fraction white defendants sentenced death overall cases involving black victims higher fraction black defendants sentenced death cases involving white victims higher fraction black defendants sentenced death figure four hypotheses concerning dependence imposition death penalty race victim race convicted murderer example asserts probability receiving death penalty depend murderer race victim quantify evidence four alternative hypotheses shown fig ure mention believe models adequate several additional variables important murder cases whether victim murderer knew whether murder premeditated whether defendant prior crim inal record none variables included table academic exercise model comparison rather serious study racial bias state florida hypotheses shown graphical models arrows showing dependencies variables victim race murderer race whether death penalty given model one free parameter probability receiving death penalty model four parameters one state variables assign uniform priors variables sensitive conclusions choice prior 
[chapter] last couple chapters assumed gaussian approximation probability distribution interested adequate already seen example clustering likelihood function multimodal nasty unboundedly high spikes certain locations parameter space maximizing posterior probability fitting gaussian always going work difficulty laplace method one motivation interested monte carlo methods fact monte carlo methods provide general purpose set tools applications bayesian data modelling many fields chapter describes sequence methods importance sampling jection sampling metropolis method gibbs sampling slice sampling method discuss whether method expected useful high dimensional problems arise inference graphical models graphical model probabilistic model dependencies inde pendencies variables represented edges graph whose nodes variables along way terminology markov chain monte carlo methods presented subsequent chapter discusses advanced methods reducing random walk behaviour details monte carlo methods theorems proofs full list references reader directed neal gilks tanner chapter use word sample following sense sample distribution single realization whose probability distribution contrasts alternative usage statistics sample refers collection realizations discuss transition probability matrices use right multipli cation convention like matrices act right preferring transition probability matrix specifies probability given current state making transition columns probability vectors write transition probability density use convention order arguments transition probability density unfortunately means get used reading right left sequence xyz probability copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links 
[monte, carlo, methods, problems, solved] monte carlo methods computational techniques make use random numbers aims monte carlo methods solve one following problems problem generate samples given probability distribu tion problem estimate expectations functions distribution example probability distribution call target density might distribution statistical physics conditional distribution arising data modelling example posterior probability model rameters given observed data generally assume dimensional vector real components sometimes con sider discrete spaces also simple examples functions whose expectations might inter ested include first second moments quantities wish predict compute means variances example quantity depends find mean variance finding expectations functions using var assumed sufficiently complex cannot evaluate expectations exact methods interested monte carlo methods concentrate first problem sampling solved solve second problem using random samples give estimator vectors generated expectation also number samples increases variance decrease variance copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links monte carlo methods figure function exp draw samples density function evaluated discrete set uniformly spaced points draw samples discrete distribution one important properties monte carlo methods accuracy monte carlo estimate depends variance dimensionality space sampled precise variance goes regardless dimensionality may dozen independent samples suffice estimate satisfactorily find later however high dimensionality cause diffi culties monte carlo methods obtaining independent samples given distribution often easy 
[monte, carlo, methods, samplingx), hard?] assume density wish draw samples evaluated least within multiplicative constant evaluate function evaluate easily solve problem general difficult obtain samples two difficulties first typically know normalizing constant second even know problem drawing samples still challenging one especially high dimensional spaces obvious way sample without enumerating possible states correct samples definition tend come places space big identify places big without evaluating everywhere high dimensional densities easy draw samples example gaussian distribution let start simple one dimensional example imagine wish draw samples density exp plot function figure mean draw samples start know normalizing constant copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links problems solved give simpler problem could discretize variable ask samples discrete probability distribution finite set uniformly spaced points figure could solve problem evaluate point compute sample probability distribution using various methods based source random bits see section cost procedure scale dimensionality space let concentrate initial cost evaluating compute visit every point space figure uniformly spaced points one dimension system dimensions say corresponding number points would unimaginable number evaluations even component took two discrete values number evaluations would number still horribly huge every electron universe gigahertz computer could evaluate trillion states every second ran computers time equal age universe seconds would still visit states wait universe ages elapse states visited systems states two penny one example collection translation american readers systems dime dozen incidentally equivalence shows correct exchange rate currencies spins fragment ising model whose probability distribution proportional exp energy function readily evaluated wish evaluate function states computer time required would function evaluations ising model simple model around long time task generating samples distribution still active research area first exact samples distribution created pioneering work propp wilson describe chapter 
[monte, carlo, methods, useful, analogy] figure lake whose depth imagine tasks drawing random water samples lake finding average plankton concentration figure depth lake assert order make analogy work plankton concentration function required average concentration integral like namely copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links monte carlo methods volume lake provided boat satellite navigation system plumbline using navigator take boat desired location map using plumbline measure point also measure plankton concentration problem draw water samples random lake way sample equally likely come point within lake problem find average plankton concentration difficult problems solve outset know nothing depth perhaps much volume lake contained figure slice lake includes canyons narrow deep underwater canyons figure case correctly sample lake correctly estimate method must implicitly discover canyons find volume relative rest lake difficult problems yes nevertheless see clever monte carlo methods solve 
[monte, carlo, methods, uniform, sampling] accepted cannot exhaustively visit every location state space might consider trying solve second problem estimating expectation function drawing random samples uniformly state space evaluating points could introduce normalizing constant defined estimate anything wrong strategy well depends functions let assume benign smoothly varying function concentrate nature learnt chapter high dimensional distribution often concentrated small region state space known typical set whose volume given entropy probability distribution almost probability mass located typical set benign function value principally determined values takes typical set uniform sampling stand chance giving good estimate make number samples sufficiently large likely hit typical set least twice many samples required let take case ising model strictly ising model may good example since necessarily typical set defined chapter definition typical set states log probability close entropy ising model would mean energy close mean energy vicinity phase transitions variance energy also known heat capacity may diverge means energy random state necessarily expected close mean energy total size state space states typical set size sample chance falling typical set copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links importance sampling log entropy temperature figure entropy spin ising model function temperature one state spin ising model number samples required hit typical set thus order min high temperatures probability distribution ising model tends uniform distribution entropy tends max bits means min order conditions uniform sampling may well satisfactory technique estimating high temperatures great interest considerably interesting intermediate tem peratures critical temperature ising model melts ordered phase disordered phase critical temperature infinite ising model melts temperature entropy ising model roughly bits figure probability dis tribution number samples required simply hit typical set order min roughly square number particles universe thus uniform sampling utterly useless study ising models modest size high dimensional problems distribution actually uniform uniform sampling unlikely useful 
[monte, carlo, methods, overview] established drawing samples high dimensional distribution difficult even easy evaluate study sequence sophisticated monte carlo methods importance sampling rejection sampling metropolis method gibbs sampling slice sampling 
[monte, carlo, methods, importance, sampling] importance sampling method generating samples prob lem method estimating expectation function problem viewed generalization uniform sampling method illustrative purposes let imagine target distribution one dimensional density let assume able evaluate density chosen point least within multiplicative constant thus evaluate function copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links monte carlo methods complicated function able sample directly assume simpler density generate samples evaluate within multiplicative constant evaluate example functions shown figure call figure functions involved importance sampling wish estimate expectation generate samples simpler distribution evaluate point sampler density importance sampling generate samples points samples could estimate equa tion generate samples values greater represented estimator points less represented take account fact sampled wrong distribution introduce weights use adjust importance point estimator thus exercise prove non zero non zero estimator converges mean value increases variance estimator asymptotically hint consider statistics numerator denominator separately estimator unbiased estimator small practical difficulty importance sampling hard estimate reliable estimator variance estimator unknown beforehand depends integral function involving variance hard estimate empirical variances quantities necessarily good guide true variances numerator denominator equation proposal density small region large quite possible even many points generated none fallen region case estimate would drastically wrong would indication empirical variance true variance estimator large figure importance sampling action using gaussian sampler density using cauchy sampler density vertical axis shows estimate horizontal line indicates true value horizontal axis shows number samples log scale 
[monte, carlo, methods, cautionary, illustration, importance, sampling] toy problem related modelling amino acid probability distribu tions one dimensional variable evaluated quantity interest ing importance sampling results using gaussian sampler cauchy sampler shown figure horizontal axis shows number copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links importance sampling samples log scale case gaussian sampler samples evaluated one might tempted call halt evidently infrequent samples make huge contribution value estimate samples wrong even million samples taken estimate still settled close true value contrast cauchy sampler suffer glitches converges scale shown samples example illustrates fact importance sampler heavy tails exercise consider situation multimodal con sisting several widely separated peaks probability distributions like arise frequently statistical data modelling discuss whether wise strategy importance sampling using sampler unimodal distribution fitted one peaks assume phi figure multimodal distribution unimodal sampler function whose mean estimated smoothly vary ing function describe typical evolution estimator function number samples 
[monte, carlo, methods, importance, sampling, many, dimensions] already observed care needed one dimensional importance sampling problems importance sampling useful technique spaces higher dimensionality say consider simple case study target density uniform distribution inside sphere proposal density gaussian centred origin normal importance sampling method trouble estimator dom inated large weights typical range values weights know discussions typical sequences part see exercise example distance origin sample quantity roughly gaussian distribution mean standard deviation thus almost samples lie typical set distance origin close let assume chosen typical set lies inside sphere radius law large numbers implies almost samples generated fall outside weight zero know samples value lies range exp thus weights typically values range exp copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links monte carlo methods figure rejection sampling functions involved rejection sampling desire samples able draw samples know value point generated random lightly shaded area curve point also lies accepted draw hundred samples typical range weights roughly estimate ratio largest weight median weight doubling standard deviation equation largest weight median weight typically ratio max med exp dimensions therefore largest weight one hundred sam ples likely roughly times greater median weight thus importance sampling estimate high dimensional problem likely utterly dominated samples huge weights conclusion importance sampling high dimensions often suffers two difficulties first need obtain samples lie typical set may take long time unless good approximation second even obtain samples typical set weights associated samples likely vary large factors probabilities points typical set although similar still differ factors order exp weights unless near perfect approximation 
[monte, carlo, methods, rejection, sampling] assume one dimensional density com plicated function able sample directly assume simpler proposal density evaluate within multiplicative factor generate samples assume know value constant schematic picture two functions shown figure generate two random numbers first generated proposal density evaluate generate uniformly distributed random variable interval two random numbers viewed selecting point two dimensional plane shown figure evaluate accept reject sample comparing value value rejected otherwise accepted means add set samples value discarded procedure generate samples proposed point comes uniform probability lightly shaded area underneath curve shown figure rejection rule rejects points lie curve points accepted uniformly distributed heavily shaded area implies copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links metropolis hastings method probability density coordinates accepted points must proportional samples must independent samples rejection sampling work best good approximation different exceed everywhere necessarily large frequency rejection large figure gaussian slightly broader gaussian scaled factor 
[monte, carlo, methods, rejection, sampling, many, dimensions] high dimensional problem likely requirement upper bound force huge acceptances rare indeed finding value may difficult since many problems know neither modes located high case study consider pair dimensional gaussian distributions mean zero figure imagine generating samples one stan dard deviation using rejection sampling obtain samples whose standard deviation let assume two standard deviations close value say larger must larger case exceeds value required dimensionality density origin exceed need set exp find exp acceptance rate value answer immediate since acceptance rate ratio volume curve volume fact normalized implies acceptance rate example general grows exponentially dimensionality acceptance rate expected exponentially small rejection sampling therefore whilst useful method one dimensional problems expected practical technique generating samples high dimensional distributions 
[monte, carlo, methods, metropolis–hastings, method] importance sampling rejection sampling work well proposal density similar large complex problems difficult create single density property figure metropolis hastings method one dimension proposal distribution shown shape changes changes though typical proposal densities used practice metropolis hastings algorithm instead makes use proposal den sity depends current state density might simple distribution gaussian centred current proposal density fixed density draw samples contrast importance sampling rejection sampling necessary look similar order algorithm practically useful example proposal density shown fig ure figure shows density two different states assume evaluate tentative new state generated proposal density decide copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links monte carlo methods whether accept new state compute quantity new state accepted otherwise new state accepted probability step accepted set step rejected set note difference rejection sampling rejection sampling rejected points discarded influence list samples collected rejection causes current state written onto list notation used superscript label points independent samples distribution superscript label sequence states markov chain important note metropolis hastings simulation iterations produce indepen dent samples target distribution samples dependent compute acceptance probability need able com pute probability ratios proposal density simple symmetrical density gaussian centred current point latter factor unity metropolis hastings method simply involves comparing value target density two points special case sometimes called metropolis method ever apologies hastings call general metropolis hastings algorithm asymmetric metropolis method since believe important ideas deserve short names 
[monte, carlo, methods, convergence, metropolis, method, target, density] shown positive probability distribution tends statement seen implying assign positive probability every point discuss examples later notice also said nothing rapidly convergence takes place metropolis method example markov chain monte carlo method abbreviated mcmc contrast rejection sampling accepted points independent samples desired distribution markov chain monte carlo methods involve markov process quence states generated sample probability distribution depends previous value since successive sam ples dependent markov chain may run considerable time order generate samples effectively independent samples difficult estimate variance importance sampling estimator difficult assess whether markov chain monte carlo method converged quantify long one wait obtain samples effectively independent samples 
[monte, carlo, methods, demonstration, metropolis, method] metropolis method widely used high dimensional problems many implementations metropolis method employ proposal distribution copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links metropolis hastings method figure metropolis method two dimensions showing traditional proposal density sufficiently small step size acceptance frequency length scale short relative longest length scale probable region figure reason choosing small length scale high dimensional problems large random step typical point sample likely end state low probability steps unlikely accepted large movement around state space occur transition low probability state actually accepted large random step chances land another probable state rate progress slow large steps used disadvantage small steps hand metropolis method explore probability distribution random walk random walk takes long time get anywhere especially walk made small steps exercise consider one dimensional random walk step state moves randomly left right equal probability show steps size state likely moved distance compute root mean square distance travelled recall first aim monte carlo sampling generate number independent samples given distribution dozen say largest length scale state space simulate random walk metropolis method time expect get sample roughly independent initial condition assuming every step accepted fraction steps accepted average time increased factor rule thumb lower bound number iterations metropolis method largest length scale space probable states metropolis method whose proposal distribu tion generates random walk step size must run least iterations obtain independent sample rule thumb gives lower bound situation may much worse example probability distribution consists several islands high probability separated regions low probability copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links monte carlo methods metropolis iterations iterations iterations independent sampling iterations iterations iterations figure metropolis method toy problem state sequence horizontal direction states vertical direction time cross bars mark time intervals duration histogram occupancy states iterations comparison histograms resulting successive points drawn independently target distribution copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links metropolis hastings method illustrate slowly random walk explores state space figure shows simulation metropolis algorithm generating samples distribution otherwise proposal distribution otherwise target distribution uniform rejections occur proposal takes state simulation started state evolution shown figure long take reach one end states since distance steps rule thumb predicts typically take time iterations reach end state confirmed present example first step end state occurs iteration long take visit end states rule thumb predicts iterations required traverse whole state space indeed first encounter end state takes place iteration thus effectively independent samples generated simulating four hundred iterations per independent sample simple example shows important try abolish random walk behaviour monte carlo methods systematic exploration toy state space could get around using step sizes twenty steps instead four hundred methods reducing random walk behaviour discussed next chapter 
[monte, carlo, methods, metropolis, method, high, dimensions] rule thumb gives lower bound number itera tions random walk metropolis method also applies higher dimensional problems consider simple case target distribution dimensional gaussian proposal distribution spherical gaussian standard deviation direction without loss generality assume target distribution separable distribution aligned axes standard deviation direction let max min largest smallest standard deviations let assume adjusted acceptance frequency close assumption variable evolves independently others executing random walk step size time taken generate effectively independent samples target distribution controlled largest lengthscale max previous section needed least iterations obtain independent sample need max big bigger smaller number comes big bigger min acceptance rate fall sharply seems plausible optimal must similar min strictly may true special cases second smallest significantly greater min optimal may closer second smallest rough conclusion simple spherical pro posal distributions used need least max min iterations obtain independent sample max min longest shortest lengthscales target distribution copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links monte carlo methods figure gibbs sampling joint density samples required starting state sampled conditional density sample made conditional density couple iterations gibbs sampling good news bad news good news unlike cases rejection sampling importance sampling catastrophic dependence dimensionality computer give useful answers time shorter age universe bad news quadratic dependence lengthscale ratio may still force make lengthy simulations fortunately methods suppressing random walks monte carlo simulations discuss next chapter 
[monte, carlo, methods, gibbs, sampling] introduced importance sampling rejection sampling metropolis method using one dimensional examples gibbs sampling also known heat bath method glauber dynamics method sampling dis tributions least two dimensions gibbs sampling viewed metropolis method sequence proposal distributions defined terms conditional distributions joint distribution assumed whilst complex draw samples directly conditional distributions tractable work many graphical models one dimensional conditional distributions straightforward sample example gaussian distribution variables unknown mean prior distribution gaussian conditional distribution given also gaussian conditional distributions standard form may still sampled adaptive rejection sampling conditional distribution satisfies certain convexity properties gilks wild gibbs sampling illustrated case two variables figure iteration start current state sampled conditional density fixed sample made conditional density using copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links gibbs sampling new value brings new state completes iteration general case system variables single iteration involves sampling one parameter time etc 
[monte, carlo, methods, convergence, gibbs, sampling, target, density] exercise show single variable update gibbs sampling viewed metropolis method target density metropolis method property every proposal always accepted gibbs sampling metropolis method probability distribution tends long pathological properties exercise discuss whether syndrome decoding problem hamming code solved using gibbs sampling syndrome decoding problem solve monte carlo approach draw samples posterior distribution noise vector normalized likelihood nth transmitted bit observed syndrome factor correct syndrome otherwise syndrome decoding problem linear error correcting code 
[monte, carlo, methods, gibbs, sampling, high, dimensions] gibbs sampling suffers defect simple metropolis algorithms state space explored slow random walk unless fortuitous rameterization chosen makes probability distribution separable say two variables strongly correlated marginal densities width conditional densities width take least iterations generate independent sample target density figure illustrates slow progress made gibbs sampling however gibbs sampling involves adjustable parameters tractive strategy one wants get model running quickly excellent software package bugs makes easy set almost arbitrary probabilistic models simulate gibbs sampling thomas http www mrc bsu cam bugs copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links monte carlo methods 
[monte, carlo, methods, terminology, markov, chain, monte, carlo, methods] spend moments sketching theory metropolis method gibbs sampling based denote probabil ity distribution state markov chain simulator visualize distribution imagine running infinite collection identical simulators parallel aim find markov chain tends desired distribution markov chain specified initial probability distribution transition probability probability distribution state iteration markov chain given example example markov chain given metropolis demonstration section figure transition proba bility initial distribution probability distribution state tth iteration shown figure equivalent sequence distributions shown figure chain begins initial state chains converge target density uniform density figure probability distribution state markov chain example 
[monte, carlo, methods, required, properties] designing markov chain monte carlo method construct chain following properties desired distribution invariant distribution chain distribution invariant distribution transition proba bility invariant distribution eigenvector transition probability matrix eigenvalue copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links terminology markov chain monte carlo methods chain must also ergodic couple reasons chain might ergodic matrix might reducible means state space contains two subsets states never reached chain many invariant distributions one would tend would depend initial condition figure probability distribution state markov chain initial condition example transition probability matrix chain one eigenvalue equal chain might periodic set means initial conditions tend invariant distribution instead tends periodic limit cycle simple markov chain property random walk dimensional hypercube chain takes state one corner randomly chosen adjacent corner unique invariant distribution chain uniform distribution states chain ergodic periodic period two divide states states odd parity states even parity notice every odd state surrounded even states vice versa initial condition time state even parity time odd times state must odd parity even times state even parity transition probability matrix chain one eigenvalue magnitude equal random walk hypercube example eigenvalues equal 
[monte, carlo, methods, methods, construction, markov, chains] often convenient construct mixing concatenating simple base transitions satisfy desired density desired density invariant distribution base transitions need individually ergodic mixture several base transitions make transition picking one base transitions random allowing determine transition probability distribution base transitions concatenation two base transitions first make transition intermediate state using make transition state using copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links monte carlo methods 
[monte, carlo, methods, detailed, balance] many useful transition probabilities satisfy detailed balance property equation says pick magic state target density make transition another state likely pick pick markov chains satisfy detailed balance also called reversible markov chains reason detailed balance property interest detailed balance implies invariance distribution markov chain necessary condition key property want mcmc simulation probability distribution chain converge exercise prove detailed balance implies invariance distri bution markov chain proving detailed balance holds often key step proving markov chain monte carlo simulation converge desired distribu tion metropolis method satisfies detailed balance example detailed balance essential condition however see later reversible markov chains useful practice may different random walk properties exercise show concatenate two base transitions satisfy detailed balance necessarily case thus defined satisfies detailed balance exercise gibbs sampling several variables updated deterministic sequence satisfy detailed balance 
[monte, carlo, methods, slice, sampling] slice sampling neal neal markov chain monte carlo method similarities rejection sampling gibbs sampling metropolis method applied wherever metropolis method applied system target density evaluated point advantage simple metropolis methods robust choice parameters like step sizes sim plest version slice sampling similar gibbs sampling consists one dimensional transitions state space however requirement one dimensional conditional distributions easy sample convexity properties required adaptive jection sampling slice sampling similar rejection sampling method asymptotically draws samples volume curve described requirement upper bounding function describe slice sampling giving sketch one dimensional sam pling algorithm giving pictorial description includes details make method valid copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links slice sampling 
[monte, carlo, methods, skeleton, slice, sampling] let assume want draw samples real number one dimensional slice sampling algorithm method making transitions two dimensional point lying curve another point lying curve probability distribution tends uniform distribution area curve whatever initial point start like uniform distribution curve produced rejection sampling section single transition one dimensional slice sampling algorithm following steps steps require elaboration evaluate draw vertical coordinate uniform create horizontal interval enclosing loop draw uniform evaluate break loop else modify interval several methods creating interval step several methods modifying step important point overall method must satisfy detailed balance uniform distribution curve invariant 
[monte, carlo, methods, ‘stepping, out’, method, step] stepping method creating interval enclosing step steps length find endpoints smaller algorithm shown figure draw uniform 
[monte, carlo, methods, ‘shrinking’, method, step] whenever point drawn lies curve shrink interval one end points original point still enclosed interval else 
[monte, carlo, methods, properties, slice, sampling] like standard metropolis method slice sampling gets around random walk whereas metropolis method choice step size copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links monte carlo methods figure slice sampling panel labelled steps algorithm executed step evaluated current point step vertical coordinate selected giving point shown box steps interval size containing created random step evaluated left end interval found larger step left size made step evaluated right end interval found smaller stepping right needed step repeated found smaller stepping halts step point drawn interval shown step establishes point step shrinks interval rejected point way original point still interval step repeated new coordinate right hand side interval gives value greater point outcome step copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links slice sampling critical rate progress slice sampling step size self tuning initial interval size small factor compared width probable region stepping procedure expands interval size cost stepping linear whereas metropolis method computer time scales square step size small chosen value large factor algorithm spends time proportional logarithm shrinking interval right size since interval typically shrinks factor ballpark time point rejected contrast metropolis algorithm responds large step size rejecting almost proposals rate progress exponentially bad rejections slice sampling probability staying exactly place small figure exercise investigate properties slice sampling applied density shown figure real variable long take typically slice sampling get peak region tail region vice versa confirm probabilities transitions yield asymptotic probability density correct 
[monte, carlo, methods, slice, sampling, used, real, problems] dimensional density may sampled help one dimensional slice sampling method presented picking sequence directions defining function replaced directions may chosen various ways example gibbs sampling directions could coordinate axes alternatively directions may selected random manner overall procedure satisfies detailed balance 
[monte, carlo, methods, computer-friendly, slice, sampling] real variables probabilistic model always represented computer using finite number bits following implementation slice sampling due skilling stepping randomization shrinking operations described terms floating point operations replaced binary integer operations assume variable slice sampled represented bit integer taking one values many correspond valid values using integer grid eliminates errors detailed balance might ensue variable precision rounding floating point numbers mapping need linear nonlinear assume function replaced appropriately transformed function example assume following operators bit integers available arithmetic sum modulo difference modulo bitwise exclusive randbits sets random bit integer slice sampling procedure integers follows copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links monte carlo methods given current point height uniform randbits define random translation binary coor dinate system set value set initial bit sampling range randbits define random move within current interval width randomize lowest bits translated coordinate system acceptable decrease try smaller perturbation termination assured translation introduced avoid permanent sharp edges example adjacent binary integers would otherwise permanently different sectors making difficult move one figure sequence intervals new candidate points drawn sequence intervals new candidate points drawn illustrated figure first point drawn entire interval shown top horizontal line subsequent draw interval halved way contain previous point preliminary stepping initial range required step replaced following similar procedure set value sets initial width randbits shrinking stepping methods shrink expand factor two per evaluation variant shrink expand one bit time setting taking step pre assigned distribution may include allows extra flexibility exercise shrinking phase unacceptable produced choice allowed depend difference slice height value without spoiling algo rithm validity prove might good idea choose larger value large investigate idea theoretically empirically feature using integer representation suitably tended number bits single integer represent two real parameters example mapping space filling curve peano curve thus multi dimensional slice sampling performed using software one dimension copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links practicalities 
[monte, carlo, methods, practicalities] predict long markov chain monte carlo simulation take equilibrate considering random walks involved markov chain monte carlo simulation obtain simple lower bounds time required convergence predicting time precisely difficult problem theoretical results giving upper bounds convergence time little practical use exact sampling methods chapter offer solution problem certain markov chains diagnose detect convergence running simulation also difficult problem practical tools available none perfect cowles carlin speed convergence time time indepen dent samples markov chain monte carlo method good news described next chapter describes hamiltonian monte carlo method overrelaxation simulated annealing 
[monte, carlo, methods, normalizing, constant, evaluated?] target density given form unnormalized density value may well interest monte carlo methods readily yield estimate quantity area active research find ways evaluating techniques evaluating include importance sampling reviewed neal annealed impor tance sampling neal thermodynamic integration simulated annealing accep tance ratio method umbrella sampling reviewed neal reversible jump markov chain monte carlo green one way dealing however may find solution one task require evaluated bayesian data modelling one might able avoid need evaluate would important model comparison one model instead using several models differing complexity example evaluating rel ative posterior probabilities one make single hierarchical model example various continuous hyperparameters play role similar played distinct models neal noting possibility computing endorsing approach normalizing constant often single important number problem think every effort devoted calculating 
[monte, carlo, methods, metropolis, method, big, models] original description metropolis method involved joint updating variables using proposal density big problems may efficient use several proposal distributions updates components proposal individually accepted rejected proposal distributions repeatedly run sequence copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links monte carlo methods exercise explain rate movement state space greater proposals considered individually sequence compared case single proposal defined concatenation assume proposal distribution acceptance rate metropolis method proposal density typically number parameters control example width parameters usually set trial error rule thumb aim rejection frequency valid width parameters dynamically updated simulation way depends history simulation modification proposal density would violate detailed balance condition guarantees markov chain correct invariant distribution 
[monte, carlo, methods, gibbs, sampling, big, models] description gibbs sampling involved sampling one parameter time described equations big problems may efficient sample groups variables jointly use several proposal distributions etc 
[monte, carlo, methods, many, samples, needed?] start chapter observed variance estimator depends number independent samples value discussed variety methods generating samples many independent samples aim many problems really need twelve independent samples imagine unknown vector amount corrosion present underground pipelines around cambridge total cost repairing pipelines distribution describes probability state given tests carried pipelines assumptions physics corrosion quantity expected cost repairs quantity variance cost measures much expect actual cost differ expectation accurately would manager like know would suggest little point knowing precision finer true cost likely differ obtain independent samples estimate precision smaller twelve samples suffice 
[monte, carlo, methods, allocation, resources] assuming decided many independent samples required important question one make use one limited computer resources obtain samples copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links summary figure three possible markov chain monte carlo strategies obtaining twelve samples fixed amount computer time time represented horizontal lines samples white circles single run consisting one long burn period followed sampling period four medium length runs different initial conditions medium length burn period twelve short runs typical markov chain monte carlo experiment involves initial riod control parameters simulation step sizes may adjusted followed burn period hope simulation converges desired distribution finally simulation continues record state vector occasionally create list states hope roughly independent samples several possible strategies figure make one long run obtaining samples make medium length runs different initial conditions obtain ing samples make short runs starting different random initial condi tion state recorded final state simulation first strategy best chance attaining convergence last strategy may advantage correlations recorded samples smaller middle path popular markov chain monte carlo experts gilks avoids inefficiency discarding burn iterations many runs still allowing one detect problems lack convergence would apparent single run finally emphasize need make points estimate nearly independent averaging dependent points fine lead bias estimates example use strategy may wish include points first last sample run course estimating accuracy estimate harder points dependent 
[monte, carlo, methods, summary] monte carlo methods powerful tool allow one sample probability distribution expressed form monte carlo methods answer virtually query related putting query form copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links monte carlo methods high dimensional problems satisfactory methods based markov chains metropolis method gibbs sam pling slice sampling gibbs sampling attractive method cause adjustable parameters use restricted cases samples generated conditional distributions slice sampling attractive whilst step length parameters performance sensitive values simple metropolis algorithms gibbs sampling algorithms although widely used perform poorly explore space slow random walk next chapter discuss methods speeding markov chain monte carlo simulations slice sampling avoid random walk behaviour automat ically chooses largest appropriate step size thus reducing bad effects random walk compared say metropolis method tiny step size 
[monte, carlo, methods, exercises] exercise study importance sampling already estab lished section importance sampling likely useless high dimensional problems exercise explores cautionary tale showing importance sampling fail even one dimension even friendly gaussian distributions imagine want know expectation function distribution expectation estimated importance sampling distribution alternatively perhaps wish estimate normalizing constant using let gaussian distributions mean zero standard deviations point drawn associated weight variance weights assume actually normalized though pretend know happens variance weights check theory simulating importance sampling problem computer exercise consider metropolis algorithm one dimensional toy problem section sampling whenever current state one end states proposal density given equation propose probability state rejected reduce waste fred modifies software responsible gen erating samples proposal density similarly always proposed copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exercises fred sets software implements acceptance rule software accepts proposed moves probability fred modified software generate samples correct acceptance rule fred proposal density order obtain samples exercise implement gibbs sampling inference single one dimensional gaussian studied using maximum likelihood section assign broad gaussian prior broad gamma prior precision parameter update involve sample gaussian distribution update requires sample gamma distribution exercise gibbs sampling clustering implement gibbs sampling inference mixture one dimensional gaussians studied using maximum likelihood section allow clusters different standard deviations assign priors means standard deviations way previous exercise either fix prior probabilities classes equal put uniform prior parameters include gibbs sampling notice similarity gibbs sampling soft means clustering algorithm algorithm alternately assign class labels given parameters update parameters given class labels assignment step involves sampling proba bility distributions defined responsibilities update step updates means variances using probability distributions centred means algorithm values experiments confirm monte carlo methods bypass fitting difficulties maximum likelihood discussed section solution exercise previous one written octave available exercise implement gibbs sampling seven scientists inference problem encountered exercise may solved exact marginalization exercise essential done latter exercise metropolis method used explore distribution actually dimensional spherical gaussian distribution standard deviation dimensions proposal density dimensional spherical gaussian distribution standard deviation roughly step size acceptance rate assuming value roughly long would method take traverse distribution generate sample independent initial condition much change typical step much vary drawn happens rather using metropolis method tries change components one instead uses concatenation metropolis updates changing one component time http www inference phy cam mackay itila copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links monte carlo methods exercise discussing time taken metropolis algo rithm generate independent samples considered distribution longest spatial length scale explored using proposal distribu tion step size another dimension mcmc method must explore range possible values log probability assuming state contains number independent random variables proportional samples drawn asymptotic equipartition principle tell value likely close entropy varying either side standard deviation scales consider metropolis method sym metrical proposal density one satisfies assuming accepted jumps either increase amount decrease small amount reasonable assumption discuss long must take generate roughly inde pendent samples discuss whether gibbs sampling similar properties exercise markov chain monte carlo methods compute parti tion functions yet allow ratios quantities like esti mated example consider random walk metropolis algorithm state space energy zero connected accessible region infinitely large everywhere else imagine accessible space chopped two regions connected one corridor states fraction times spent region equilibrium proportional volume region monte carlo method manage without measuring volumes exercise philosophy one curious defect monte carlo methods widely used bayesian statisticians non bayesian hagan involve computer experiments estimators quantities interest derived estimators depend pro posal distributions used generate samples random numbers happened come random number generator contrast alternative bayesian approach problem would use results computer experiments infer proper ties target function generate predictive distributions quantities interest approach would give answers would depend computed values points answers would depend points chosen make bayesian monte carlo method see rasmussen ghahramani practical attempt 
[monte, carlo, methods, solutions] solution exercise wish show converges expectation consider numerator denominator separately first denominator consider single importance weight copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links solutions expectation averaged distribution point expectation denominator long variance finite denominator divided converge increases fact estimate converges right answer even variance infinite long expectation well defined similarly expectation one term numerator expectation numerator divided converges increasing thus converges numerator denominator unbiased estimators respectively ratio necessarily unbiased estimator finite solution exercise true density multimodal unwise use importance sampling sampler density fitted one mode rare occasions point produced lands one modes weight associated point enormous estimates enormous variance enormous variance may evident user points modes seen solution exercise posterior distribution syndrome decoding problem pathological distribution point view gibbs sampling factor small fraction space possible vectors namely points correspond valid code words two codewords adjacent similarly single bit flip viable state take state zero probability state never move gibbs sampling general code exactly problem points corresponding valid codewords relatively number adjacent least useful code gibbs sampling use syndrome decoding two reasons first finding reasonably good hypothesis difficult long state near valid codeword gibbs sampling cannot help since none conditional distributions defined second valid hypothesis gibbs sampling never take one could attempt perform gibbs sampling using bits original message variables approach would get locked way described good code single bit flip would substantially alter reconstructed codeword one found state reasonably large likelihood gibbs sampling would take impractically large time escape solution exercise metropolis proposal take energy state amount total change energy copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links monte carlo methods proposals concatenated end point random walk steps walk might mean zero might tendency drift upwards moves increase energy decrease general latter hold acceptance rate small mean change energy one move acceptance probability concatenation moves order exp scales roughly mean square distance moved order typical step size contrast mean square distance moved moves considered individually order theory theory figure importance sampling one dimension normalizing constant gaussian distribution known fact estimated using importance sampling sampler density standard deviation horizontal axis random number seed used runs three plots show estimated normalizing constant empirical standard deviation weights weights solution exercise weights drawn mean weight assuming integral converges variance var exp integral finite coefficient exponent positive condition satisfied variance var approaches critical value variance becomes infinite figure illustrates phenomena varying random number seed used runs weights estimates follow smooth curves notice empirical standard deviation weights look quite small well behaved say true standard deviation nevertheless infinite 
[efficient, monte, carlo, methods] chapter discusses several methods reducing random walk behaviour metropolis methods aim reduce time required obtain effectively independent samples brevity say independent samples mean effectively independent samples 
[efficient, monte, carlo, methods, hamiltonian, monte, carlo] hamiltonian monte carlo method metropolis method applicable continuous state spaces makes use gradient information reduce random walk behaviour hamiltonian monte carlo method originally called hybrid monte carlo historical reasons many systems whose probability written form also gradient respect readily evaluated seems wasteful use simple random walk metropolis method gradient available gradient indicates direction one find states higher probability 
[efficient, monte, carlo, methods, overview, hamiltonian, monte, carlo] hamiltonian monte carlo method state space augmented momentum variables alternation two types proposal first proposal randomizes momentum variable leaving state changed second proposal changes using simulated hamil tonian dynamics defined hamiltonian kinetic energy two proposals used create asymptotically samples joint density exp exp exp density separable marginal distribution desired distribution exp simply discarding momentum variables obtain sequence samples asymptotically come copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links efficient monte carlo methods algorithm octave source code hamiltonian monte carlo method grade set gradient using initial finde set objective function loop times randn size initial momentum normal evaluate xnew gnew tau tau make tau leapfrog steps epsilon gnew make half step xnew xnew epsilon make step gnew grade xnew find new gradient epsilon gnew make half step endfor enew finde xnew find new value hnew enew hnew decide whether accept accept elseif rand exp accept else accept endif accept gnew xnew enew endif endfor hamiltonian monte carlo simple metropolis figure hamiltonian monte carlo used generate samples bivariate gaussian correlation comparison simple random walk metropolis method given equal computer time copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links hamiltonian monte carlo 
[efficient, monte, carlo, methods, details, hamiltonian, monte, carlo] first proposal viewed gibbs sampling update draws new momentum gaussian density exp proposal always accepted second dynamical proposal momentum vari able determines state goes gradient determines momentum changes accordance equations persistent motion direction momentum dynamical proposal state system tends move distance goes linearly computer time rather square root second proposal accepted accordance metropolis rule simulation hamiltonian dynamics numerically perfect proposals accepted every time total energy constant motion equation equal one simulation imperfect finite step sizes example dynamical proposals rejected rejection rule makes use change zero simulation perfect occasional rejections ensure asymptotically obtain samples required joint density source code figure describes hamiltonian monte carlo method uses leapfrog algorithm simulate dynamics function finde whose gradient found function grade figure shows algorithm generating samples bivariate gaussian whose ergy function corresponding variance covariance matrix figure starting state marked arrow solid line represents two successive trajectories generated hamiltonian dynamics squares show endpoints two trajectories trajectory consists tau leapfrog steps epsilon steps indicated crosses trajectory magnified inset trajectory momentum randomized trajectories accepted errors hamiltonian respectively figure shows sequence four trajectories converges initial condition indicated arrow close typical set target distribution trajectory parameters tau epsilon randomized trajectory using uniform distributions means respectively first trajectory takes new state similar energy first state second trajectory happens end state nearer bottom energy landscape since potential energy smaller kinetic energy necessarily larger start trajectory momentum randomized third trajectory kinetic energy becomes much smaller fourth copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links efficient monte carlo methods gibbs sampling overrelaxation gibbs sampling overrelaxation figure overrelaxation contrasted gibbs sampling bivariate gaussian correlation state sequence iterations iteration involving one update variables overrelaxation method excessively large value chosen make easy see overrelaxation method reduces random walk behaviour dotted line shows contour detail showing two steps making iteration time course variable iterations two methods overrelaxation method neal trajectory simulated state appears become typical target density figures show random walk metropolis method using gaussian proposal density sample gaussian distribution starting initial conditions respectively step size adjusted acceptance rate number proposals total amount computer time used similar distance moved small random walk behaviour random walk metropolis method used started initial condition given similar amount computer time 
[efficient, monte, carlo, methods, overrelaxation] method overrelaxation method reducing random walk behaviour gibbs sampling overrelaxation originally introduced systems conditional distributions gaussian example joint distribution gaussian whose conditional distributions gaussian exp copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links overrelaxation 
[efficient, monte, carlo, methods, overrelaxation, gaussian, conditional, distributions] ordinary gibbs sampling one draws new value current variable conditional distribution ignoring old value state makes lengthy random walks cases variables strongly correlated illustrated left hand panel figure figure uses correlated gaussian distribution target density adler overrelaxation method one instead samples gaussian biased opposite side conditional distribution conditional distribution normal current value adler method sets normal parameter usually set negative value positive method called relaxation exercise show individual transition leaves invariant con ditional distribution normal single iteration adler overrelaxation like one gibbs sampling updates variable turn indicated equation transition matrix defined complete update variables fixed order satisfy detailed balance individual transition one coordinate described satisfy detailed balance overall chain gives valid sampling strategy converges target density form chain applying individual transitions fixed sequence overall chain reversible temporal asymmetry key overrelaxation beneficial say two variables positively correlated short timescale evolve directed manner instead random walk shown figure may significantly reduce time required obtain independent samples exercise transition matrix defined complete update variables fixed order satisfy detailed balance updates random order would symmetric inves tigate toy two dimensional gaussian distribution assertion advantages overrelaxation lost overrelaxed updates made random order 
[efficient, monte, carlo, methods, ordered, overrelaxation] overrelaxation method generalized neal whose ordered overrelaxation method applicable system gibbs sampling used ordered overrelaxation instead taking one sample condi tional distribution create samples might set twenty often generating extra samples adds negligible computational cost initial computations required making first sample points sorted numerically current value inserted sorted list giving list points give ranks let rank current value list set value equal distance end list value rank role played adler parameter played parameter obtain ordinary gibbs sampling practical purposes neal estimates ordered overrelaxation may speed simulation factor ten twenty copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links efficient monte carlo methods 
[efficient, monte, carlo, methods, simulated, annealing] third technique speeding convergence simulated annealing simu lated annealing temperature parameter introduced large allows system make transitions would improbable temper ature temperature set large value gradually reduced procedure supposed reduce chance simulation gets stuck unrepresentative probability island asssume wish sample distribution form evaluated simplest simulated annealing method instead sample distribution decrease gradually often energy function separated two terms first term nice example separable function second nasty cases better simulated annealing method might make use distribution gradually decreasing way distribution high tem peratures reverts well behaved distribution defined simulated annealing often used optimization method aim find minimizes case temperature decreased zero rather monte carlo method simulated annealing described sample exactly right distribution guarantee probability falling one basin energy equal total prob ability states basin closely related simulated tempering method marinari parisi corrects biases introduced nealing process making temperature random variable updated metropolis fashion simulation neal annealed importance sampling method removes biases introduced annealing computing importance weights generated point 
[efficient, monte, carlo, methods, skilling’s, multi-state, leapfrog, method] fourth method speeding monte carlo simulations due john skilling similar spirit overrelaxation works dimensions method applicable sampling distribution continuous state space sole requirement energy easy evaluate gradient used leapfrog method intended used rather sequence monte carlo operators instead moving one state vector around state space case monte carlo methods discussed thus far skilling leapfrog method simultaneously maintains set state vectors copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links skilling multi state leapfrog method might six twelve aim vectors represent independent samples distribution skilling leapfrog makes proposal new state cepted rejected accordance metropolis method leapfrogging current state another state vector state vectors left acceptance probability depends change energy vector partner leapfrog event chosen various ways simplest method select partner random vectors might better choose selecting one nearest neighbours nearest chosen distance function long one uses acceptance rule ensures detailed balance checking whether point still among nearest neighbours new point 
[efficient, monte, carlo, methods, leapfrog, good, idea] imagine target density strong correlations example density might needle like gaussian width length emphasized motion around density standard methods proceeds slow random walk imagine set points lurking initially location probable density inappropriately small ball size skilling leapfrog method typical first move take point little outside current ball perhaps doubling distance centre ball points chance move ball increased size moves accepted ball bigger factor two dimensions rejection moves mean ball containing points probably elongated needle long direction factor say two another cycle points ball grown long direction another factor two typical distance travelled long dimension grows exponentially number iterations maybe factor two growth per iteration optimistic side even ball grows factor let say per iteration growth nevertheless exponential take number iterations proportional log log long dimension explored exercise discuss effectiveness skilling method scales dimensionality using correlated dimensional gaussian distri bution example find expression rejection probability assuming markov chain equilibrium also discuss scales strength correlation among gaussian variables hint skilling method invariant affine transformations rejec tion probability equilibrium found looking case separable gaussian method similarity adaptive direction sampling method gilks leapfrog method simpler applied greater variety distributions copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links efficient monte carlo methods 
[efficient, monte, carlo, methods, monte, carlo, algorithms, communication, channels] may helpful perspective thinking speeding monte carlo methods think information communicated two communications take place sample generated first selection particular necessarily requires least log random bits consumed recall use inverse arith metic coding method generating samples given distributions section second generation sample conveys information subroutine able evaluate subroutines access properties consider dumb metropolis method example dumb metropolis method proposals nothing properties involved algorithm acceptance step ratio computed channel true distribution user interested computing properties thus passes bottleneck information conveyed string acceptances rejections replaced different distribution way change would influence string acceptances rejections would changed aware much use made information theoretic view monte carlo algorithms think instructive viewpoint aim obtain information properties presumably helpful identify channel information flows maximize rate information transfer example information theoretic viewpoint offers simple justification widely adopted rule thumb states parameters dumb metropolis method adjusted acceptance rate one half let call acceptance history binary string accept reject decisions information learned algorithm run steps less equal information content since information mediated information content upper bounded acceptance rate bound information acquired maximized setting another helpful analogy dumb metropolis method evolutionary one proposal generates progeny current state two individuals compete metropolis method uses noisy survival fittest rule progeny fitter parent assuming factor unity progeny replaces parent survival rule also allows less fit progeny replace parent sometimes insights rate evolution thus applied monte carlo methods exercise let let separable distribution example let proposal density dumb metropolis algorithm involve flipping fraction bits state analyze long takes chain copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links multi state methods converge target density function find optimal deduce long metropolis method must run compare result results evolving population natural selection found chapter insight fastest progress standard metropolis method make information terms one bit per iteration gives strong motivation speeding algorithm chapter already reviewed several methods reducing random walk behaviour methods also speed rate information acquired exercise gibbs sampling smart metropolis method whose proposal distributions depend allow information leak rate faster one bit per iteration find toy examples question precisely investigated exercise hamiltonian monte carlo another smart metropolis method proposal distributions depend hamiltonian monte carlo extract information rate faster one bit per iteration exercise importance sampling weight floating point number computed retained end computation contrast dumb metropolis method ratio reduced single bit bigger smaller random number thus principle importance sampling preserves information dumb metropolis find toy example extra information indeed lead faster convergence importance sampling metropolis design markov chain monte carlo algorithm moves around adaptively like metropolis method retains useful formation value like importance sampling chapter noticed evolving population individuals make faster evolutionary progress individuals engage sexual reproduc tion observation motivates looking monte carlo algorithms multiple parameter vectors evolved interact 
[efficient, monte, carlo, methods, multi-state, methods] multi state method multiple parameter vectors maintained evolve individually moves metropolis gibbs also interactions among vectors intention either eventually vectors samples illustrated skilling leapfrog method information associated final vectors allow approximate expectations importance sampling 
[efficient, monte, carlo, methods, genetic, methods] genetic algorithms often described proponents monte carlo algorithms think correct categorization ideal genetic algorithm would one proved valid monte carlo algorithm converges specified density copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links efficient monte carlo methods use denote number vectors population aim genetic algorithm involves moves two three types first individual moves one state vector perturbed could performed using monte carlo methods mentioned far second allow crossover moves form typical crossover move progeny receives half state vector one parent half secret success genetic algorithm parameter must encoded way crossover two independent states good fitness reasonably good chance producing progeny equally fit constraint hard one satisfy many problems genetic algorithms mainly talked hyped rarely used serious experts introduced crossover move need choose acceptance rule one easy way obtain valid algorithm accept reject crossover proposal using metropolis rule target density involves comparing fitnesses crossover using ratio crossover operator reversible easy proof procedure satisfies detailed balance valid component chain converging exercise discuss whether two operators individual varia tion crossover metropolis acceptance rule give efficient monte carlo method standard method one state vector crossover reason sexual community could acquire information faster asexual community chapter crossover operation produced diversity standard deviation blind watchmaker able convey lots information fitness function killing less fit offspring two operators offer speed compared standard monte carlo methods killing required order obtain speed two things multiplication death least one must operate selectively either must kill less fit state vectors must allow fit state vectors give rise offspring easy sketch ideas hard define valid method exercise design birth rule death rule chain converges believe still open research problem 
[efficient, monte, carlo, methods, particle, filters] particle filters particularly popular inference problems involving temporal tracking multistate methods mix ideas importance sampling markov chain monte carlo see isard blake isard blake berzuini berzuini gilks doucet copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links methods necessarily help 
[efficient, monte, carlo, methods, methods, necessarily, help] common practice use many initial conditions particular markov chain figure worried sampling well complicated density ensure states produced simulations well distributed typical set ensuring initial points well distributed whole state space answer unfortunately hierarchical bayesian models example large number parameters may coupled together via parameter known hyperparameter example quantities might independent noise signals might inverse variance noise source joint distribution might broad distribution describing igno rance noise level simplicity let leave variables data might involved realistic problem let imagine want sample effectively gibbs sampling alter nately sampling conditional distribution sampling conditional distributions resulting marginal distribution asymptotically broad distribution large conditional distribution given particular setting tightly concentrated particular probable value width proportional progress axis therefore take place slow random walk steps size initialization strategy finesse slow convergence problem using initial conditions located state space sadly distribute points widely actually favouring initial value noise level large random walk parameter thus tend first drawing always start one end axis 
[efficient, monte, carlo, methods, reading] hamiltonian monte carlo method duane reviewed neal excellent tome also reviews huge range monte carlo methods including related topics simulated annealing free energy estimation 
[efficient, monte, carlo, methods, exercises] exercise important detail hamiltonian monte carlo method simulation hamiltonian dynamics may accurate must perfectly reversible sense initial con dition goes simulator must take inaccurate dynamics must conserve state space vol ume leapfrog method algorithm satisfies rules explain rules must satisfied create example illus trating problems arise copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links efficient monte carlo methods exercise multi state idea slice sampling investigate follow ing multi state method slice sampling skilling multi state leapfrog method section maintain set state vectors update one state vector one dimensional slice sampling rection determined picking two state vectors random setting investigate method toy problems highly correlated multivariate gaussian distribution bear mind smaller number dimensions method ergodic may need mixed methods classes problems better solved slice sampling method standard methods picking cycling coordinate axes picking random gaussian distribution 
[efficient, monte, carlo, methods, solutions] solution exercise consider spherical gaussian distribution components mean zero variance one dimension nth leapfrogs obtain proposed coordinate assuming gaussian random variables normal gaussian normal change energy contributed one dimension typical change energy positive change bad news dimensions typical change energy leapfrog move made equilibrium thus probability acceptance move scales implies skilling method described effective high dimensional problems least convergence occurred nev ertheless impressive advantage convergence properties independent strength correlations variables property even hamiltonian monte carlo overrelaxation methods offer 
[ising, models] expect happen case ground states infinite system two checkerboard patterns figure copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links ising models monte carlo simulation energy energy mean square magnetization heat capacity figure detail monte carlo simulations rectangular ising models mean energy fluctuations energy function temperature fluctuations energy standard deviation mean square magnetization heat capacity temperature heat capacity var figure schottky anomaly heat capacity fluctuations energy function temperature two level system separation copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links ising models energy per spin like ground states model analogy pressed moment reflection confirm two systems figure two ground states rectangular ising model figure two states rectangular ising models identical energy equivalent checkerboard symmetry operation take infinite system state flip spins lie black squares infinite checkerboard set figure energy unchanged magnetization changes course thermodynamic properties two systems expected identical case zero applied field subtlety lurking spotted simu lating finite grids periodic boundary conditions size grid direction odd checkerboard operation longer symme try operation relating checkerboard match boundaries means systems odd size ground state system degeneracy greater energy ground states low per spin expect qualitative differences cases odd sized systems differences expected prominent small systems frustrations introduced boundaries length boundary grows square root system size fractional influence boundary related frustration energy entropy system crease figure compares energies ferromagnetic antiferromagnetic models difference striking energy temperature temperature figure monte carlo simulations rectangular ising models mean energy fluctuations energy function temperature 
[ising, models, relevance, ising, models] ising models relevant three reasons ising models important first models magnetic systems phase transition theory universality statistical physics shows systems dimension two symmetries equivalent critical properties scaling laws shown phase transitions identical studying ising models find magnetic phase transitions also phase transitions many systems second generalize energy function couplings applied fields constant obtain family models known spin glasses physicists hopfield copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links ising models networks boltzmann machines neural network community models spins declared neighbours case physicists call system infinite range spin glass networkers call fully connected network third ising model also useful statistical model right chapter study ising models using two different computational techniques 
[ising, models, remarkable, relationships, statistical, physics] would like get much information possible computations consider example heat capacity system defined exp work heat capacity system might naively guess increase temperature measure energy change heat capacity however intimately related energy fluctuations constant temperature let start partition function exp mean energy obtained differentiation respect exp differentiation spits variance energy exp var heat capacity also derivative respect temperature var system temperature var var thus observe variance energy system equilibrium estimate heat capacity find almost paradoxical relationship consider system finite set states imagine heating high temperature states equiprobable mean energy essentially constant heat capacity essentially zero hand states equiprobable certainly fluctuations energy heat capacity related fluctuations answer words essentially zero heat capacity quite zero high temperature tends zero tends zero var copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links ising models quantity var tending constant high temperatures behaviour heat capacity finite systems high temperatures thus general factor viewed accident history tem perature scales defined using definition heat capacity would var heat capacity fluctuations would identical quantities exercise call entropy physical system rather statistical physics chapter set entropy system whose states temperature exp show mean energy system show free energy 
[ising, models, ising, models, monte, carlo, simulation] section study two dimensional planar ising models using simple gibbs sampling method starting initial state spin selected random probability given state spins temperature computed exp local field factor appears equation two spin states rather spin set probability otherwise next spin update selected random sufficiently many iterations procedure converges equilibrium distribution alternative gibbs sampling formula metropolis algorithm consider change energy results flipping chosen spin current state adopt change configuration probability accept exp copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links ising models monte carlo simulation procedure roughly double probability accepting energetically unfavourable moves may efficient sampler low tem peratures relative merits gibbs sampling metropolis algorithm may subtle figure rectangular ising model 
[ising, models, rectangular, geometry] first simulated ising model rectangular geometry shown fig ure periodic boundary conditions line two spins indicates neighbours set external field con sidered two cases ferromagnet antiferromagnet respectively started large temperature changed temper ature every iterations first decreasing gradually increasing gradually back large temperature procedure gives crude check whether equilibrium reached tem perature expect see hysteresis graphs plot also gives idea reproducibility results assume two runs decreasing increasing temperature effectively independent temperature recorded mean energy per spin standard deviation energy mean square value magnetization one tricky decision made soon start taking figure sample states rectangular ising models sequence temperatures measurements new temperature established difficult detect equilibrium even give clear definition system equilibrium chapter see solution problem crude strategy let number iterations temperature hundred times number spins discard first iterations found needed iterations reach equilibrium given temperature 
[ising, models, results, small] simulated grid let quick think results expect low temperatures system expected ground state rectangular ising model two ground states state state energy per spin either ground state high temperatures spins independent states equally probable energy expected fluctuate around mean standard deviation proportional let look results figures temperature shown basic picture emerges spins figure top energy rises monotonically increase number spins figure bottom new details emerge first expected fluctuations large temperature decrease second fluctuations intermediate temperature become relatively bigger signature collective phenomenon case phase transition systems infinite show true phase transitions getting hint critical fluctuations figure shows details graphs figure shows sequence typical states simulation spins sequence decreasing temperatures copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links ising models mean energy fluctuations mean square magnetization energy temperature mean square magnetization temperature energy temperature mean square magnetization temperature figure monte carlo simulations rectangular ising models mean energy fluctuations energy function temperature left mean square magnetization function temperature right top row bottom even larger see later figures contrast schottky anomaly figure schematic diagram explain meaning schottky anomaly curve shows heat capacity two gases function temperature lower curve shows normal gas whose heat capacity increasing function temperature upper curve small peak heat capacity known schottky anomaly least cambridge peak produced gas magnetic degrees freedom finite number accessible states peak heat capacity function temperature occurs system finite number energy levels peak evidence phase transition peaks viewed anomalies classical thermody namics since normal systems infinite numbers energy levels particle box heat capacities either constant increasing functions temperature contrast systems finite number levels produced small blips heat capacity graph figure let refresh memory simplest system two level system states energy energy mean energy exp exp exp derivative respect exp exp heat capacity exp exp fluctuations energy given var evaluated heat capacity fluctuations plotted figure take home message point whilst schottky anomalies peak heat capacity peak fluctuations variance energy simply increases monotonically temperature value proportional number independent spins thus peak fluctuations interesting rather peak heat capacity ising model peak fluctuations seen second row figure 
[ising, models, triangular, ising, model] repeat computations triangular ising model expect triangular ising model show different physical properties rectangular ising model presumably model broadly similar properties rectangular counterpart case radically different gone think unfrustrated ground state state must frustrations pairs neighbours sign unlike case rectangular model odd size frustrations introduced periodic boundary conditions every set three mutually neighbouring spins must state frustration shown figure solid lines show happy couplings contribute energy dashed lines show unhappy couplings contribute thus certainly expect different behaviour low temperatures fact might expect system non zero entropy absolute zero triangular model violates third law thermodynamics let look results sample states shown figure figure antiferromagnetic triangular ising model three neighbouring spins frustrated eight possible configurations three spins six energy two energy figure shows energy fluctuations heat capacity copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links direct computation partition function ising models note different results peak standard deviation energy case indicates antiferromagnetic system phase transition state long range order energy temperature energy temperature energy temperature energy temperature heat capacity temperature heat capacity temperature figure monte carlo simulations triangular ising models mean energy fluctuations energy function temperature fluctuations energy standard deviation heat capacity 
[ising, models, direct, computation, partition, function, ising, models] examine completely different approach ising models trans fer matrix method exact abstract approach obtains physical properties model partition function exp summation states inverse temperature usual let free energy given number states direct computation partition function possible large avoid enumerating global states explicitly use trick similar sum product algorithm discussed chapter concentrate models form long thin strip width periodic boundary conditions directions iterate along length model working set partial partition functions one location terms partial partition functions previous location iteration involves summation states boundary operation exponential width strip final clever trick copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links ising models figure sample states triangular ising models high temperatures top low bottom copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links direct computation partition function ising models note system translation invariant along length need one iteration order find properties system length computational task becomes evaluation matrix number microstates need considered boundary computation eigenvalues eigenvalue largest magnitude gives partition function infinite length thin strip detailed explanation label states columns thin strip integer rth bit indicates whether spin row column partition function exp exp appropriately defined energy want periodic boundary conditions defined one definition figure illustration help explain definition counts contributions energy rectangle total energy given stepping rectangle along horizontal bond inside rectangle counted vertical bond half inside rectangle half inside adjacent rectangle half energy included factor appears second term run nodes column bond visited twice state shown horizontal bonds contribute vertical bonds contribute left right assuming periodic boundary conditions top bottom definition energy nice property rectangular ising model defines matrix symmetric two indices factors needed vertical links counted four times let define exp continuing equation trace eigenvalues length strip increases becomes dominated largest eigenvalue max max free energy per spin limit infinite thin strip given max max really neat thermodynamic properties long thin strip obtained largest eigenvalue matrix 
[ising, models, computations] computed partition functions long thin strip ising models geometries shown figure last section set applied field zero considered two cases ferromagnet antiferromagnet respectively computed free energy per spin widths function copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links ising models rectangular triangular figure two long thin strip ising models line two spins indicates neighbours strips width infinite length computational ideas largest eigenvalue needed several ways getting quantity example iterative multiplication matrix initial vec tor matrix positive know principal eigenvector positive frobenius perron theorem reasonable initial vector iterative procedure may faster explicit computation eigenvalues computed anyway advantage find free energy finite length strips using equation well infinite ones free energy temperature ferromagnets width triangular rectangular temperature antiferromagnets width triangular rectangular figure free energy per spin long thin strip ising models note non zero gradient case triangular antiferromagnet 
[ising, models, comments, graphs] large temperatures ising models show behaviour free energy entropy dominated entropy per spin mean energy per spin goes zero free energy per spin tend free energies shown figure one interesting properties obtain free energy degeneracy ground state temperature goes zero boltzmann distribution becomes concentrated ground state ground state degenerate multiple ground states identical entropy temperature triangular rectangular triangular figure entropies nats width ising systems function temperature obtained differentiating free energy curves figure rectangular ferromagnet antiferromagnet identical thermal properties triangular systems upper curve denotes antiferromagnet lower curve ferromagnet copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links direct computation partition function ising models triangular rectangular triangular figure mean energy versus temperature long thin strip ising models width compare figure heat capacity temperature rectangular ferromagnet width width temperature triangular ising models width width width width figure heat capacities rectangular model triangular models different widths denoting ferromagnet antiferromagnet compare figure energy entropy non zero find entropy free energy using entropy triangular antiferromagnet absolute zero appears half high temperature value figure mean energy function temperature plotted figure evaluated using identity hei figure shows estimated heat capacity taking raw derivatives mean energy function temperature triangular models widths figure shows fluctuations energy function temperature figures show smooth graphs roughness curves due inaccurate numerics nature phase transition obvious graphs seem compatible assertion ferromagnet shows antiferromagnet show phase transition pictures free energy figure give insight could predict transition temperature see two phases ferromagnetic systems simple free energies straight sloping line high temperature phase horizontal line low temperature phase slope line shows entropy per spin phase phase transition occurs roughly intersection lines predict transition temperature linearly related ground state energy var temperature rectangular ferromagnet width width temperature triangular ising models width width width width figure energy variances per spin rectangular model triangular models different widths denoting ferromagnet antiferromagnet compare figure copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links ising models 
[ising, models, comparison, monte, carlo, results] agreement results two experiments seems good two systems simulated long thin strip periodic square quite identical one could accurate comparison finding eigenvalues strip width computing get partition function patch 
[ising, models, exercises] exercise would best way extract entropy monte carlo simulations would best way obtain entropy heat capacity partition function computation exercise ising model may generalized coupling spins value could different special case couplings positive know system two ground states states general setting conceivable could many ground states imagine required make spin system whose local minima given list states think way setting chosen states low energy states allowed adjust whatever values wish 
[exact, monte, carlo, sampling, problem, monte, carlo, methods] high dimensional problems widely used random sampling meth ods markov chain monte carlo methods like metropolis method gibbs sampling slice sampling problem methods yes given algorithm guaranteed produce samples target density asymptotically chain converged equilibrium distribution one runs chain short time samples come distribution long must markov chain run converged mentioned chapter question usually hard answer however pioneering work propp wilson allows one certain chains answer question furthermore propp wilson show obtain exact samples target density 
[exact, monte, carlo, sampling, exact, sampling, concepts] propp wilson exact sampling method also known perfect simulation coupling past depends three ideas 
[exact, monte, carlo, sampling, coalescence, coupled, markov, chains] first several markov chains starting different initial conditions share single random number generator trajectories state space may coalesce coalesced separate initial condi tions lead trajectories coalesce single trajectory sure markov chain forgotten initial condition figure shows twenty one markov chains identical one described section samples using metropolis algorithm figure chains different initial condition driven single random number generator chains coalesce steps figure shows markov chains different random number seed case coalescence occur steps elapsed shown figure shows similar markov chains identical proposal density section figure figure proposed move step left right obtained way chains timestep independent current state coupling chains changes statistics coalescence two neighbouring paths merge rejection occurs rejections occur walls particular markov chain coalescence occur chains leftmost state rightmost state copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exact monte carlo sampling figure coalescence first idea behind exact sampling method time runs bottom top leftmost panel coalescence occurred within steps different coalescence properties obtained depending way state uses random numbers supplied two runs metropolis simulator random bits determine proposed step depend current state different random number seed used case simulator random proposal left right states panel one paths one starting location highlighted copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exact sampling concepts 
[exact, monte, carlo, sampling, coupling, past] use coalescence property find exact sample equilibrium distribution chain state system moment complete coalescence occurs valid sample equilibrium distribution example figure final coalescence always occurs state one two walls trajectories merge walls sampling forward time coalescence occurs valid method second key idea exact sampling obtain exact samples sampling time past present coalescence occurred present sample unbiased sample equilibrium distribution restart simulation time past reusing random numbers simulation repeated sequence ever distant times doubling one run next convenient choice coalescence occurs time present record exact sample equilibrium distribution markov chain figure shows two exact samples produced way leftmost panel figure start twenty one chains possible initial condi tions run forward time coalescence occur restart simulation possible initial conditions reset random number generator way random num bers generated time particular identical first run notice trajectories pro duced runs started identical subset trajectories first simulation coalescence still occur double time trajectories coalesce obtain exact sample shown arrow pick earlier time trajectories must still end point since every trajectory must pass state states lead final point ran markov chain infinite time past initial condition would end state figure shows exact sample produced way markov chains figure method called coupling past important allows obtain exact samples equilibrium distribution described little practical use since obliged simulate chains starting initial states examples shown twenty one states realistic sampling problem utterly enormous number states think states system binary spins example whole point introducing monte carlo methods try avoid visit states system 
[exact, monte, carlo, sampling, monotonicity] established obtain valid samples simulating forward times past starting possible states times third trick propp wilson makes exact sampling method useful practice idea markov chains may possible detect coalescence trajectories without simulating trajectories property holds example chain figure property two trajectories never cross simply track two tra jectories starting leftmost rightmost states know copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exact monte carlo sampling figure coupling past second idea behind exact sampling method copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exact sampling concepts figure ordering states third idea behind exact sampling method trajectories shown left right trajectories figure order establish state time zero need run simulations point coalescence occurs two exact samples target density generated method different random number seeds initial times required respectively copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exact monte carlo sampling coalescence trajectories occurred two trajectories alesce figure illustrates idea showing left right trajectories figure figure shows two act samples equilibrium distribution generated running coupling past method starting two end states alone two runs coalesced starting necessary try times achieve coalescence 
[exact, monte, carlo, sampling, exact, sampling, interesting, distributions] toy problem studied states could put one dimensional order two trajectories crossed states many interesting state spaces also put partial order coupled markov chains found respect partial order example partial order four possible states two spins states ordered systems show coalescence occurred merely verifying coalescence occurred histories whose initial states maximal minimal states state space compute draw uniform else algorithm gibbs sampling coupling method markov chains coupled together chains update spin time step chains share common sequence random numbers example consider gibbs sampling method applied ferro magnetic ising spin system partial ordering states defined thus state greater equal state spins maximal minimal states states markov chains coupled together shown algorithm propp wilson show exact samples generated system though time find exact samples large ising model critical temperature since gibbs sampling method slowly mixing conditions propp wilson improved method ising model using markov chain called single bond heat bath algorithm sample related model called random cluster model show exact samples random cluster model obtained rapidly converted exact samples ising model ground breaking paper includes exact sample million spin ising model critical temperature sample smaller ising model shown figure figure exact sample ising model critical temperature produced wilson samples produced within seconds ordinary computer exact sampling 
[exact, monte, carlo, sampling, generalization, exact, sampling, method, ‘non-attractive’, distri-, butions] method propp wilson ising model sketched applied probability distributions call tractive rather define term let say means practical purposes method applied spin systems cou plings positive ferromagnet special spin systems negative couplings already observed chapter rect angular ferromagnet antiferromagnet equivalent cannot applied general spin systems couplings negative systems trajectories followed states guaranteed upper lower bounds set trajecto ries fortunately however need strict possible express propp wilson algorithm way generalizes case spin systems negative couplings idea summary state version exact sampling still keep track bounds set copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exact sampling interesting distributions trajectories detect bounds equal find exact samples bounds actual trajectories necessarily tight bounds instead simulating two trajectories moves state space simulate one trajectory envelope augmented state space symbol denotes either call state augmented system summary state example summary state six spin system summary state shorthand set states update rule step markov chain takes single spin enu merates possible states neighbouring spins compatible current summary state local scenarios computes new value spin using gibbs sampling coupled random number algorithm new values agree new value updated spin summary state set unanimous value otherwise new value spin summary state initial condition time given setting spins summary state corresponds considering possible start configurations case spin system positive couplings summary state simulation identical simulation uppermost state low ermost states style propp wilson coalescence occuring symbols disappeared summary state method applied general spin systems couplings shortcoming method envelope may describe unnecessarily large set states guarantee summary state algorithm con verge time coalescence detected may considerably larger actual time taken underlying markov chain coalesce summary state scheme applied exact sampling belief networks harvey neal triangular antiferromagnetic ising model childs summary state methods first intro duced huber also names sandwiching methods bounding chains 
[exact, monte, carlo, sampling, reading] reading impressive pictures exact samples distribu tions generalizations exact sampling method browse perfectly random sampling website beautiful exact sampling demonstrations running live web browser see jim propp website 
[exact, monte, carlo, sampling, uses, coupling] idea coupling together markov chains share random number generator applications beyond exact sampling pinto neal shown accuracy estimates obtained markov chain monte carlo simulation second problem discussed section using estimator http www dbwilson com exact http www math wisc edu propp tiling www applets copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exact monte carlo sampling figure perfectly random tiling hexagon lozenges provided propp wilson improved coupling chain interest converges second chain generates samples second simpler distribution coupling must set way states two chains strongly correlated idea first estimate expectations function interest normal way compare estimate true value expectation assume evaluated exactly overes timate likely overestimate difference thus used correct 
[exact, monte, carlo, sampling, exercises] exercise relationship probability dis tribution time taken trajectories coalesce equi libration time markov chain prove relationship find single chain realized two different ways different coalescence times exercise imagine fred ignores requirement random bits used time every run increasingly distant times must identical makes coupled markov chain simulator uses fresh random numbers every time changed describe happens fred applies method markov chain intended sample uniform distribution states using metropolis method driven random bit source figure exercise investigate application perfect sampling linear gression holmes mallick holmes denison try generalize exercise concept coalescence many applications sur names frequent others die altogether make copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links solutions model process long take everyone surname similarly variability particular portion human genome forms basis forensic dna fingerprinting inherited like surname dna fingerprint like string surnames fact surnames subject coalescences surnames chance prevalent others affect way dna fingerprint evidence used court exercise use coin create random ranking people construct solution uses exact sampling example could apply exact sampling markov chain coin repeatedly used alternately decide whether switch first second whether switch second third exercise finding partition function probability distribution difficult problem many markov chain monte carlo methods produce valid samples distribution without ever finding probability distribution markov chain either time taken produce perfect sample number random bits used create perfect sample related value situations time coalescence conveys information 
[exact, monte, carlo, sampling, solutions] solution exercise perhaps surprising rect relationship equilibration time time coalescence prove using example uniform distribution inte gers markov chain converges distribution exactly one iteration chain probability state given uniform distribution chain coupled random number generator two ways could draw random integer set equal regardless could draw random integer set equal mod method would produce cohort trajectories locked together similar trajec tories figure except coalescence ever occurs thus equilibration times methods one coalescence times respectively one infinity seems plausible hand coalescence time provides sort upper bound equilibration time 
[variational, methods] variational methods important technique approximation com plicated probability distributions applications statistical physics data modelling neural networks 
[variational, methods, variational, free, energy, minimization] one method approximating complex distribution physical system mean field theory mean field theory special case general variational free energy approach feynman bogoliubov study key piece mathematics needed understand method gibbs inequality repeat gibbs inequality first appeared equation see also exercise relative entropy two probability distributions defined alphabet log relative entropy satisfies gibbs inequality equality general chapter replace log measure divergence nats 
[variational, methods, probability, distributions, statistical, physics] statistical physics one often encounters probability distributions form exp example state vector energy function partition function normalizing constant exp probability distribution equation complex unbearably complex evaluate particular time copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links variational free energy minimization polynomial number spins evaluating normalizing constant difficult saw chapter describing properties probability distribution also hard knowing value arbitrary points example gives useful information average properties system evaluation would particularly desirable derive thermodynamic properties system variational free energy minimization method approximating complex distribution simpler ensemble parameterized adjustable parameters adjust parameters get best approximate sense product approximation lower bound 
[variational, methods, variational, free, energy] objective function chosen measure quality approximation variational free energy exp expression manipulated couple interesting forms first average energy function distribution entropy distribution set one definition identical definition entropy part second use definition write true free energy defined relative entropy approximating distribution true distribution thus gibbs inequality variational free energy bounded attains value strategy thus vary way minimized approximating distribution gives simplified approximation true distribution may useful value upper bound equivalently lower bound 
[variational, methods, evaluated?] already agreed evaluation various interesting sums intractable example partition function exp copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links variational methods energy hei exp entropy presumed impossible evaluate suppose objective function also defined terms sum convenient quantity deal well range interesting energy functions sufficiently simple approximating distributions variational free energy efficiently evaluated 
[variational, methods, variational, free, energy, minimization, spin, systems] example tractable variational free energy given spin system whose energy function given equation approximate separable approximating distribution exp variational parameters variational free energy components vector evaluate variational free energy need entropy distribution mean energy entropy separable approximating distribution simply sum entropies individual spins exercise probability spin exp mean energy easy obtain sum terms involving product two independent random variables self couplings define mean value given tanh copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links variational free energy minimization spin systems obtain variational free energy given figure variational free energy two spin system whose energy function two variational parameters inverse temperature function plotted notice fixed function convex respect fixed convex respect consider minimizing function respect variational parameters derivative entropy obtain derivative equal zero extremized point satisfies equation tanh variational free energy may multimodal function case stationary point maximum minimum saddle satisfy equa tions one way using equations case system arbitrary coupling matrix update parameter corresponding value using equation one time asynchronous updating parameters guaranteed decrease equations may recognized mean field equa tions spin system variational parameter may thought strength fictitious field applied isolated spin equation describes mean response spin equation describes field set response mean state spins variational free energy derivation helpful viewpoint mean field theory two reasons approach associates objective function mean field equations objective function useful help identify alternative dynamical systems minimize function copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links variational methods figure solutions variational free energy extremization problem ising model three different applied fields horizontal axis temperature vertical axis magnetization critical temperature found mean field theory mft theory readily generalized approximating distributions imagine introducing complex approximation might example capture correlations among spins instead mod elling spins independent one could evaluate variational free energy optimize parameters complex approx imation degrees freedom approximating distribution tighter bound free energy becomes however complexity approximation increased evaluation either mean energy entropy typically becomes challenging 
[variational, methods, example, mean, field, theory, ferromagnetic, ising, model] simple ising model studied chapter every coupling equal neighbours zero otherwise applied field spins simple approximating distribution one single variational parameter defines separable distribution exp spins independent probability exp mean magnetization tanh equation defines minimum variational free energy becomes number couplings spin involved case rectangular two dimensional ising model solve equations numerically fact easiest vary solve obtain graphs free energy minima maxima function temperature shown figure solid line shows versus case pitchfork bifurcation critical temperature mft pitchfork bifurcation transition like one shown solid lines copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links variational methods inference data modelling figure system one minimum function right system left two minima one maximum maximum middle one three lines solid lines look like pitchfork temperature one minimum variational free energy minimum corresponds approximating distribution uniform states critical temperature two minima corresponding approximating distributions symmetry broken spins likely spins likely state persists stationary point variational free energy local maximum variational free energy global variational free energy minimum temperature positive value shown upper dotted curves figure long also second local minimum free energy temperature sufficiently small second minimum cor responds self preserving state magnetization opposite direction applied field temperature second minimum appears smaller mft appears accompanied saddle point located two minima name given type bifurcation saddle node bifurcation variational free energy per spin given exercise sketch variational free energy function one parameter variety values temperature applied field figure reproduces key properties real ising system critical temperature system long range order adopt one two macroscopic states however probing little reveal inadequacies variational approximation start critical temperature mft nearly factor greater true critical temperature also variational model equivalent properties number dimensions including true system phase transition bifurcation mft described phase transition case follow trajectory global minimum function find entropy heat capacity fluctuations proximating distribution compare real fragment using matrix method chapter shown figure one biggest differences fluctuations energy real system large fluctuations near critical temperature whereas approximating distri bution correlations among spins thus energy variance scales simply linearly number spins 
[variational, methods, variational, methods, inference, data, modelling] statistical data modelling interested posterior probability distribution parameter vector given data model assumptions traditional approaches model fitting single parameter vector timized find mode distribution really interest copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links variational methods free energy energy mean field theory real system mean field theory real system entropy heat capacity mean field theory real system mean field theory real system fluctuations var mean field theory real system figure comparison approximating distribution properties real fragment notice variational free energy approximating distribution indeed upper bound free energy real system quantities shown per spin copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links case unknown gaussian whole distribution may also interested normalizing constant wish model comparison probability distribution often complex distribution variational approach ference introduce approximating probability distribution rameters optimize distribution varying param eters approximates posterior distribution parameters well one objective function may choose measure quality proximation variational free energy denominator within multiplicative constant posterior probability variational free energy viewed sum relative entropy bounded attains value certain models certain approximating distributions free energy derivatives respect approximating distribution parameters evaluated approximation posterior probability distributions using variational free energy minimization provides useful approach approximating bayesian inference number fields ranging neural networks decoding error correcting codes hinton van camp hinton zemel dayan neal hinton mackay method sometimes called ensemble learning contrast traditional learning processes single parameter vector optimized another name variational bayes let examine ensemble learning works simple case gaussian distribution 
[variational, methods, distribution] fit approximating ensemble posterior distribution studied chapter exp make single assumption approximating ensemble separable form restrictions functional form made write variational free energy find optimal separable distribution considering separately optimization fixed optimization fixed copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links variational methods figure optimization approximating distribution posterior distribution figure shown solid contours initial condition approximating distribution dotted contours arbitrary separable distribution updated using equation updated using equation updated updated converged approximation iterations arrows point peaks two distributions 
[variational, methods, optimization)[variational, methodsµ)] functional denote constants depend dependence thus collapses simple dependence mean recognize function logarithm gaussian identical posterior distribution particular value since relative entropy minimized setting immediately write distribution opt minimizes fixed opt normal 
[variational, methodsσ)] problem need resort iterative computation find optimal approximating ensemble equations define optimum implicitly must simultaneously solution similar true posterior distribution gamma distri bution see equation true posterior also mean value satisfying difference approximating distribution parameter large approximations given variational free energy minimization always tend compact true distribution conclusion ensemble learning gives approximation posterior agrees nicely conventional estimators approximate poste rior distribution gamma distribution mean corresponding variance approximate posterior dis tribution gaussian mean standard deviation variational free energy minimization approach nice prop erty parameterization independent avoids problem basis dependence map methods laplace method suffer convenient software package automatic implementation variational inference graphical models vibes bishop plays role variational inference bugs plays monte carlo inference 
[variational, methods, direct, solution, joint, optimum)[variational, methodsµ)q)[variational, methods, interlude] one students asked ever come useful approximating distribution given true distribution complex compute directly let answer question context bayesian data modelling let true distribution interest posterior probability distribution set parameters standard data modelling practice find single best fit setting parameters example finding maximum likelihood function posterior distribution copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links variational methods one interpretation standard practice full description knowledge approximated delta function probability distribution concentrated perspective approximating distribution matter crummy improvement spike produced standard method even use simple gaussian approximation well study application variational approach realistic example data clustering 
[variational, methods, rithm, variational, method] chapter introduced soft means clustering algorithm version chapter introduced versions algorithm motivated algorithm maximum likelihood algorithm means clustering example expectation maximization algorithm two steps called assignment update known step step respectively give general view means clustering due neal hinton algorithm shown optimize variational objective function neal hinton derivation applies algorithm 
[variational, methods, probability, everything] let parameters mixture model means standard deviations weights denoted data point missing variable also known latent variable class label point probability everything given assumed model posterior probability everything given data proportional probability everything approximate posterior distribution separable distribution define variational free energy usual way bounded minus evidence make iterative algorithm assignment step update step assignment step adjusted reduce fixed update step adjusted reduce fixed wish obtain exactly soft means algorithm impose constraint approximating distribution constrained delta function centred point estimate copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links variational methods free energy minimization upper bound exp lower bound exp figure illustration jaakkola jordan variational method upper lower bounds logistic function solid line upper lower bounds exponential gaussian functions easier integrate graph shows sigmoid function upper lower bounds unfortunately distribution contributes variational free energy infinitely large integral better leave term treating additive constant using delta function good idea aim minimize moving aim derive soft means algorithm exercise show given optimal sense minimizing separable distribution probabil ity given responsibility exercise show given separable described timal sense minimizing obtained update step soft means algorithm assume uniform prior exercise instantly improve infinitely large value achieved soft means clustering allowing general distribution delta function derive update step allowed separable distribution product discuss whether generalized algorithm still suffers soft means kaboom problem algorithm glues ever shrinking gaussian one data point sadly sounds like promising generalization algorithm allow non delta function kaboom problem goes away artefacts arise approximate inference method involving local minima reading see mackay mackay 
[variational, methods, variational, methods, free, energy, minimization] strategies approximating complicated distribution addition based minimizing relative entropy approximating distribution one approach pioneered jaakkola jordan create adjustable upper lower bounds illustrated figure bounds unnormalized densities parameterized variational parameters adjusted order obtain tightest possible fit lower bound adjusted maximize upper bound adjusted minimize copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links variational methods using normalized versions optimized bounds compute proximations predictive distributions reading methods found references jaakkola jordan jaakkola jor dan jaakkola jordan gibbs mackay 
[variational, methods, bethe, kikuchi, free, energies] chapter discussed sum product algorithm functions factor graph form factor graph tree like sum product algo rithm converges correctly computes marginal function variable also yield joint marginal function subsets variables appear common factor sum product algorithm may also applied factor graphs tree like algorithm converges fixed point shown fixed point stationary point usually minimum function messages called kikuchi free energy special case factors factor graph functions one two variables kikuchi free energy called bethe free energy articles idea new approximate inference algorithms tivated see yedidia yedidia welling teh yuille yedidia yedidia 
[variational, methods, exercises] exercise exercise explores assertion made approximations given variational free energy minimization ways tend compact true distribution consider two dimensional gaussian distribution axes aligned directions let variances two directions optimal variance distribution approximated spherical gaussian variance optimized variational free energy minimization instead optimized objec tive function would optimal value sketch contour true distribution two approximating distributions case note general possible evaluate objective func tion integrals true distribution usually intractable exercise think idea using variational method optimize approximating distribution use proposal density importance sampling exercise define relative entropy kullback leibler divergence tween two probability distributions state gibbs inequality consider problem approximating joint distribution separable distribution show objec copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links solutions tive function approximation log minimal value achieved equal marginal distributions consider alternative objective function log probability distribution shown margin proximated separable distribution state value set marginal distribu tions show three distinct minima identify minima evaluate 
[variational, methods, solutions] solution exercise need know relative entropy tween two one dimensional gaussian distributions normal normal normal normal approximate whose variances whose variances find differentiating zero thus set approximating distribution inverse variance mean inverse variance target distribution case obtain factor larger pretty much independent value larger standard deviation variational free energy minimization typically leads approximating distributions whose length scales match shortest length scale target distribution approximating distribution might viewed compact copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links variational methods figure two separable gaussian approximations dotted lines bivariate gaussian distribution solid line approximation minimizes variational free energy approximation minimizes objective function figure lines show contours inverse covariance matrix gaussian contrast use objective function find constant constant depends differentiating zero thus set approximating distribution variance mean variance target distribution case obtain factor smaller independent value two approximations shown scale figure solution exercise best possible variational approximation course target distribution assuming possible good variational approximation compact true distribution contrast good sampler heavy tailed true distribution compact distribution would lousy sampler large variance 
[latent, variable, modelling, latent, variable, models] many statistical models generative models models specify full probability density variables situation make use latent variables describe probability distribution observables examples latent variable models include chapter mixture models model observables coming superposed mixture simple probability distributions latent variables unknown class labels examples hidden markov models rabiner juang durbin factor analysis decoding problem error correcting codes also viewed terms latent variable model figure case encoding matrix normally known advance latent variable modelling parameters equivalent usually known must inferred data along latent variables figure error correcting codes latent variable models latent variables independent source bits give rise observables via generator matrix usually latent variables simple distribution often separable distribution thus fit latent variable model finding scription data terms independent components independent component analysis algorithm corresponds perhaps simplest possible latent variable model continuous latent variables 
[latent, variable, modelling, generative, model, independent, component, analysis] set observations assumed generated follows dimensional vector linear mixture underlying source signals matrix mixing coefficients known simplest algorithm results assume number sources equal number observations aim recover source variables within multiplicative factors possibly per muted put another way aim create inverse within post multiplicative factor given set examples assume latent variables independently distributed marginal distributions denotes assumed form model assumed probability distributions latent variables probability observables hidden variables given copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links independent component analysis latent variable modelling     assume vector generated without noise assumption usually made latent variable modelling since noise free data rare makes inference problem far simpler solve 
[latent, variable, modelling, likelihood, function] learning data relevant quantity likelihood function product factors obtained marginalizing latent variables marginalize delta functions remember adopt summation convention point example single factor likelihood given det det obtain maximum likelihood algorithm find gradient log likelihood introduce log likelihood contributed single example may written det assume det positive omit absolute value sign need following identities det let define copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links generative model independent component analysis repeat datapoint put linear mapping put nonlinear map popular choice tanh adjust weights accordance algorithm independent component analysis online steepest ascents version see also algorithm preferred indicates direction needs change make probability data greater may obtain gradient respect using equations alternatively derivative respect choose change ascend gradient obtain learning rule algorithm far summarized algorithm 
[latent, variable, modelling, choices] choice function defines assumed prior distribution latent variable let first consider linear choice implicitly via equation assumes gaussian distribution latent variables gaussian distribution latent variables invariant rotation latent variables evidence favouring particular alignment latent variable space linear algorithm thus uninteresting never recover matrix original sources hope thus sources non gaussian thankfully real sources non gaussian distributions often heavier tails gaussians thus move popular tanh nonlinearity tanh implicitly assuming cosh heavier tailed distribution latent variables gaussian distribution copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links independent component analysis latent variable modelling figure illustration generative models implicit learning algorithm distributions two observables generated cosh distributions latent variables compact distribution broader distribution contours generative distributions latent variables cauchy distributions learning algorithm fits amoeboid object empirical data way maximize likelihood contour plot adequately represent heavy tailed distribution part tails cauchy distribution giving contours times density origin data one generative distributions illustrated tell samples created fell plotted region could also use tanh nonlinearity gain tanh whose implicit probabilistic model cosh limit large nonlinearity becomes step function probabil ity distribution becomes biexponential distribution exp limit approaches gaussian mean zero variance heavier tailed distributions may also used student cauchy distributions spring mind 
[latent, variable, modelling, example, distributions] figures illustrate typical distributions generated independent components model components cosh cauchy distribu tions figure shows samples cauchy model cauchy distribution heavy tailed gives clearest picture predictive distribution depends assumed generative parameters 
[latent, variable, modelling, covariant, simpler, faster, learning, algorithm] thus derived learning algorithm performs steepest descents likelihood function algorithm work quickly even toy data algorithm ill conditioned illustrates nicely general advice finding gradient objective function splendid idea ascending gradient directly may fact algorithm ill conditioned seen fact involves matrix inverse arbitrarily large even undefined 
[latent, variable, modelling, covariant, optimization, general] principle covariance says consistent algorithm give results independent units quantities measured knuth copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links covariant simpler faster learning algorithm prime example non covariant algorithm popular steepest descents rule dimensionless objective function defined deriva tive respect parameters computed changed rule popular equation dimensionally inconsistent left hand side equation dimensions right hand side dimensions behaviour learning algorithm covariant respect linear rescaling vector dimensional inconsistency end world success numerous gradient descent algorithms demon strated indeed decreases line learning number iterations munro robbins theorem bishop shows parameters asymptotically converge maximum likelihood parameters non covariant algorithm may take large number iterations achieve convergence indeed many former users steepest descents algorithms prefer use algorithms conjugate gradients adaptively figure curvature objective function defense equation points could dimensional constant untenable parameters dimensions algorithm would covariant form positive definite matrix whose element dimensions obtain matrix two sources matrices metrics curvatures 
[latent, variable, modelling, metrics, curvatures] natural metric defines distances parameter space matrix obtained metric often natural choice special case known quadratic metric defining length vector matrix obtained quadratic form example length natural matrix steepest descents appropriate another way finding metric look curvature objective function defining matrix give covariant algorithm algorithm newton algorithm recognize alleviate one principal difficulties steepest descents namely slow convergence minimum objective function ill conditioned newton algorithm converges minimum single step quadratic problems may curvature consists data dependent terms data independent terms case one might choose define metric using data independent terms gull resulting algorithm still covariant implement exact newton step obviously many covariant algorithms unique choice covariant algorithms small subset set algorithms copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links independent component analysis latent variable modelling 
[latent, variable, modelling, back, independent, component, analysis] present maximum likelihood problem evaluated gradient respect gradient respect steepest ascents covariant let construct alternative covariant algorithm help curvature log likelihood taking second derivative log likelihood respect obtain two terms first data independent second data dependent sum derivative tempting drop data dependent term define matrix however matrix positive definite least one non positive eigenvalue poor approximation curvature log likelihood must positive definite neighbourhood maximum likelihood solution must therefore consult data dependent term inspiration aim find convenient approximation curvature obtain covariant algorithm necessarily implement exact newton step average value true value make several severe approximations replace present value replace correlated average ihz variance covariance matrix latent variables assumed exist typical value curvature given sources assumed independent diagonal matrices approximations motivate trix given simplicity assume sources similar homogeneous lead algorithm covariant respect linear rescaling data respect linear rescaling latent variables thus use multiplying matrix gradient equation obtain following covariant learning algorithm notice expression require inversion matrix additional computation computed single back ward pass weights compute quantity copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links covariant simpler faster learning algorithm repeat datapoint put linear mapping put nonlinear map popular choice tanh put back adjust weights accordance algorithm independent component analysis covariant version terms covariant algorithm reads quantity right hand side sometimes called natural gradient covariant independent component analysis algorithm summarized algorithm 
[latent, variable, modelling, reading] ica originally derived using information maximization approach bell sejnowski another view ica terms energy functions motivates general models given hinton another generalization ica found pearlmutter parra enormous literature applications ica variational free energy minimization approach ica like models given miskin miskin mackay miskin mackay reading blind separation including non ica algorithms found jutten herault comon hendin amari hojen sorensen 
[latent, variable, modelling, infinite, models] latent variable models finite number latent variables widely used often case beliefs situation would accurately captured large number latent variables consider clustering example attack speech recognition mod elling words using cluster model many clusters use number possible words unbounded section would really like use model always possible new clusters arise furthermore careful job modelling cluster corresponding one english word probably find cluster one word modelled composed clusters indeed hierarchy copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links independent component analysis latent variable modelling clusters within clusters first levels hierarchy would divide male speakers female would separate speakers different regions india britain europe forth within clusters would subclusters different accents within region subclusters could subsubclusters right level villages streets families thus would often like infinite numbers clusters cases clusters would hierarchical structure cases hierarchy would flat infinite models implemented finite computers set bayesian models avoid getting silly answers infinite mixture models categorical data presented neal along monte carlo method simulating inferences predictions infinite gaussian mixture models flat hierarchical structure pre sented rasmussen neal shows use dirichlet diffusion trees define models hierarchical clusters ideas build dirichlet process section remains active research area rasmussen ghahramani beal 
[latent, variable, modelling, exercises] exercise repeat derivation algorithm assume small amount noise term joint probability replaced probability distribution mean show noise distribution sufficiently small standard deviation identical algorithm results exercise implement covariant ica algorithm apply toy data exercise create algorithms appropriate situations cludes substantial gaussian noise measurements latent variables fewer measurements latent variables factor analysis assumes observations described terms independent latent variables independent additive noise thus observable given noise vector whose components separable probability distri bution factor analysis often assumed probability distributions zero mean gaussians noise terms may different variances exercise make maximum likelihood algorithm inferring data assuming generative model correct independent gaussian distributions include parameters describe variance maximize likelihood respect let variance exercise implement infinite gaussian mixture model rasmussen 
[random, inference, topics, know, ignorant?] example real variable measured accurate experiment example might half life neutron wavelength light emitted firefly depth lake vostok mass jupiter moon probability value starts like charge electron units boltzmann constant probability starts like faraday constant mol second digit probability mantissa starts probability starts solution expert neutrons fireflies antarctica jove might able predict value thus predict first digit confidence someone knowledge topic probability distribution corresponding knowing nothing one way attack question notice units specified half life neutron measured fortnights instead seconds number would divided measured years would divided knowledge particular knowledge first digit affected change units expert answer yes let take someone truly ignorant answer predictions first digit independent units arbitrariness units corresponds invariance probability distribution multiplied number metres inches feet figure viewed logarithmic scale scales using different units translated relative know units quantity measured probability first digit must proportional length corresponding piece logarithmic scale probability first digit number thus log log log log log log copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links random inference topics without needing calculator log log generally probability first digit log log log log log observation initial digits known benford law ignorance correspond uniform probability distribution exercise pin thrown tumbling air probability distribution angle pin vertical moment air tumbling pin photographed probability distribution angle pin vertical imaged photograph exercise record breaking consider keeping track world record quantity say earthquake magnitude longjump distances jumped world championships assume attempts break record take place steady rate assume lying probability distribution outcome changing assumption think unlikely true case sports endeavours interesting assumption consider nonetheless assuming knowledge predicted successive intervals dates records broken 
[random, inference, topics, uck, distribution] exercise landmark paper demonstrating bacteria could mutate virus sensitivity virus resistance luria delbr uck wanted estimate mutation rate exponentially growing pop ulation total number mutants found end experi ment problem difficult quantity measured number mutated bacteria heavy tailed probability distribution mutation occuring early experiment give rise huge number mutants unfortunately luria delbr uck know bayes theorem way coping heavy tailed distribution involves arbitrary hacks leading two different estimators mutation rate one estimators based mean number mutated bacteria averaging several experiments appallingly large variance yet sampling theorists continue use base confidence intervals around kepler oprea exercise inference right culture single bacterium resistant gives rise generations descendants clones except differences arising mutations final culture exposed virus number resistant bacteria measured according accepted mutation hypothesis resistant bacteria got resistance random mutations took place growth colony mutation rate per cell per generation one hundred million total number opportunities mutate since bacterium mutates ith generation descendants inherit mutation final number resistant bacteria contributed one ancestor copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links inferring causation given separate experiments colony size created measured numbers resistant bacteria infer mutation rate make inference given following dataset luria delbr uck small amount computation required solve problem 
[random, inference, topics, inferring, causation] exercise bayesian graphical model community task inferring way arrows point nodes parents children one much written inferring causation tricky likelihood equivalence two graph ical models likelihood equivalent setting parameters either exists setting parameters two joint probability distributions observables identical example pair likelihood equivalent models model asserts parent sloppy terminology causes example situation true case variable burglar house variable alarm ringing literally true causes choice words confusing applied another example denotes rained morning denotes pavement dry causes confusing therefore use words parent denote causation statistical meth ods use likelihood alone unable use data distinguish likelihood equivalent models bayesian approach hand two likelihood equivalent models may nevertheless somewhat distinguished light data since likelihood equivalence force bayesian use priors assign equivalent densities two parameter spaces models however many bayesian graphical modelling folks perhaps sym pathy non bayesian colleagues latent urge appear different deliberately discard potential advantage bayesian methods ability infer causation data skewing models ability goes away widespread orthodoxy holds one identify choices prior prior equivalence holds priors models likelihood equivalent also identical posterior probabilities one use one priors inference prediction argument motivates use prior probability vectors specially constructed dirichlet distributions view philosophical error use priors causation cannot inferred priors set describe one assump tions done likely interesting inferences causation made data exercise make example inference consider toy problem binary variables two models asserts marginal probabil ity comes beta distribution parameters uni form distribution two conditional distributions also come independently beta distributions parameters model assigns similar priors marginal probability conditional distributions given data gathered copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links random inference topics counts given outcomes posterior probabilities two hypotheses hint good idea work exercise symbolically order spot simplifications emerge topic inferring causation complex one fact bayesian inference sensibly used infer directions arrows graphs seems neglected view certainly whole story see pearl discussion many aspects causality 
[random, inference, topics, exercises] exercise photons arriving photon detector believed emit ted poisson process time varying rate exp sin parameters known data collected time given photons arrived times discuss inference reading gregory loredo exercise data file consisting two columns numbers printed way boundaries columns unclear resulting strings discuss probable given data correct parsing item etc etc parsing string grammatical interpretation string example punch bores could parsed punch noun bores verb punch imperative verb bores plural noun exercise experiment measured quantities come inde pendently biexponential distribution mean exp copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links solutions normalizing constant mean known example distribution shown figure figure biexponential distribution assuming four datapoints data tell include detailed sketches answer give range plausible values 
[random, inference, topics, solutions] solution exercise population size opportunities mutate probability number mutations occurred roughly poisson slightly inaccurate descendants mutant cannot selves undergo mutation mutation gives rise number final mutant cells depends generation time mutation multiplication went like clockwork probability would probability would probability would powers two expect mutant progeny divide exact synchrony know pre cise timing end experiment compared division times smoothed version distribution permits integers occur distribution moments wrong since never exceed cares moments sampling theory statisticians barking wrong tree constructing unbiased estimators log error introduce likelihood function using approximation negligible observed number mutants sum probability distribution given convolution identical distributions form example probability distribution given need bayesian inference given summing quantity evaluated analytically small easy evaluate desired numerical precision explicitly summing copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links random inference topics max also found max explicit convolutions required values max max largest value encountered data computed exactly question data max plenty accurate result used max make graphs figure octave source code available figure likelihood mutation rate linear scale log scale given luria delbruck data vertical axis likelihood horizontal axis incidentally data sets like one exercise substantial number zero counts little lost making luria delbruck second approximation retain count many equal zero many non zero likelihood function found using weakened data set scarcely distinguishable likelihood computed using full information solution exercise six terms form factors cancel remains data data modest evidence favour three probabilities inferred hypothesis roughly typical prior three probabilities inferred statement sounds absurd think priors uniform three probabilities surely uniform prior settings probabilities equally probable natural basis logit basis prior proportional posterior probability ratio estimated exactly right illustrate preference coming www inference phy cam itprnn code octave luria 
[decision, theory] decision theory trivial apart computational details like playing chess choice various actions world may one many states one occurs may influenced action world state probability distribution finally utility function specifies payoff receive world state chose action task decision theory select action maximizes expected utility computational problem maximize pes simists may prefer define loss function instead utility function minimize expected loss anything said decision theory well real problem choice appropriate utility function may quite difficult furthermore sequence actions taken action providing information take account effect anticipated information may subsequent tions resulting mixture forward probability inverse probability computations decision problem distinctive realistic problem playing board game tree possible cogitations actions must considered becomes enormous right thing simple expected utility action cannot computed exactly russell wefald baum smith baum smith let explore example 
[decision, theory, rational, prospecting] suppose task choosing site tanzanite mine final action select site list sites nth site net value called return initially unknown found exactly site chosen equals revenue earned selling tanzanite site minus costs buying site paying staff forth outset return probability distribution based information already available take final action opportunity prospecting prospecting nth site cost yields data reduce uncertainty assume returns copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links decision theory sites unrelated prospecting one site yields information site affect return site decision problem given initial probability distributions first decide whether prospect sites light prospecting results choose site mine simplicity let make everything problem gaussian focus notation normal indicates gaussian distribution mean variance question whether prospect assume utility function linear wish maximize expected return utility function prospecting done chosen action site prospect ing done utility site prospecting took place prior distribution return site normal prospect site datum noisy version normal exercise given assumptions show prior probability dis tribution normal mnemonic independent variables add variances add posterior distribution given normal mnemonic gaussians multiply precisions add start let evaluate expected utility prospecting choose site immediately evaluate expected utility first prospect one site make choice two results able decide whether prospect zero times prospect site first consider expected utility without prospecting exercise show optimal action assuming prospecting select site biggest mean argmax expected utility action optimal max intuition says surely optimal decision take count different uncertainties answer question reasonable utility function nonlinear copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links reading exciting bit prospect prospected site choose site using decision rule value mean replaced updated value given makes problem exciting yet know value know action indeed whole value prospecting comes fact outcome may alter action one would taken absence experimental information expression new mean terms known variance compute probability distribution key quantity work expected utility integrating possible outcomes associated actions exercise show probability distribution new mean gaussian mean variance consider prospecting site let biggest mean sites obtain new value mean choose site get expected return choose site get expected return expected utility prospecting site picking best site prospect normal difference utility prospecting prospecting quantity interest depends would done without prospecting depends whether bigger prospecting prospect prospecting normal normal plot change expected utility due prospecting omitting function difference horizontal axis initial standard deviation vertical axis figure noise variance figure contour plot gain expected utility due prospecting contours equally spaced steps decide whether worth prospecting site find contour equal cost prospecting points contour worthwhile 
[decision, theory, reading] world act little complicated prospecting problem example multiple iterations prospecting possible cost prospecting uncertain finding optimal balance exploration exploitation becomes much harder computational problem reinforcement learning addresses approximate methods problem sut ton barto copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links decision theory 
[decision, theory, exercises] exercise four doors problem new game show uses rules similar three doors exer cise four doors host explains first point one doors open one doors guaranteeing choose non winner decide whether stick original pick switch one remaining doors open another non winner never current pick make final decision sticking door picked previous decision switching remaining door optimal strategy switch first opportu nity switch second opportunity exercise one challenges decision theory figuring actly utility function utility money example notoriously nonlinear people fact behaviour many people cannot captured coher ent utility function illustrated allais paradox runs follows choices find attractive million guaranteed chance million chance million chance nothing consider choices chance nothing chance million chance nothing chance million many people prefer time prove preferences inconsistent utility function money exercise optimal stopping large queue potential partners waiting door asking marry arrived random order meet partner decide spot based information far whether marry say potential partner desirability find meet must marry one allowed back anyone said several ways define precise problem assuming aim maximize desirability utility function partner selected strategy use assuming wish much marry desirable person utility function achieve zero wise strategy use copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exercises assuming wish much marry desirable person strategy strategy strategy meet first partners say memorize maximum desirability max among meet others sequence waiting partner max comes along marry none desirable comes along marry final partner feel miserable optimal value exercise regret objective function preceding exercise parts involved utility function based regret one married tenth desirable candidate utility function asserts one would feel regret chosen desirable many people working learning theory decision theory use mini mizing maximal possible regret objective function make sense action buy outcome buy win wins table utility lottery ticket problem imagine fred bought lottery ticket offers sell known whether ticket winner simplicity say probability ticket winner winner worth fred offers sell ticket buy possible actions buy buy utilities four possible action outcome pairs shown table assumed utility small amounts money linear buy ticket utility zero regardless whether ticket proves winner buy ticket end either losing action buy outcome buy win wins table regret lottery ticket problem one pound probability gaining nine probability minimax regret community actions chosen mini mize maximum possible regret four possible regret outcomes shown table buy ticket win regret bought would better buy ticket wins regret bought would better action minimizes maximum possible regret thus buy ticket discuss whether use regret choose actions philosophi cally justified problem turned investment portfolio decision problem imagining given one pound invest two possible funds one day fred lottery fund cash fund put fred lottery fund fred promises return lottery ticket winner otherwise nothing remaining kept cash best investment show minimax regret community invest money high risk high return lottery fund cash investment method justified exercise gambling oddities cover thomas horse race involving horses occurs repeatedly obliged bet money time bet time represented copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links decision theory normalized probability vector multiplied money odds offered bookies horse wins return assuming bookies odds fair assuming probability horse wins work optimal betting strategy aim cover aim namely maximize expected value log show optimal strategy sets equal independent bookies odds show strategy used money expected grow exponentially log bet optimal strategy different think optimal strategy makes sense think optimal common language ignore bookies odds conclude cover aim exercise two ordinary dice thrown repeatedly outcome throw sum two numbers joe shark says lucky numbers bets even money thrown first thrown gambler would take bet probability winning joe bets even money thrown first thrown would take bet gained confidence joe suggests combining two bets single bet bets larger sum still even odds thrown two thrown would take bet probability winning 
[bayesian, inference, sampling, theory] two schools statistics sampling theorists concentrate methods guaranteed work time given minimal assumptions bayesians try make inferences take account available informa tion answer question interest given particular data set probably gathered strongly recommend use bayesian methods sampling theory widely used approach statistics pers journals report experiments using quantities like confidence intervals significance levels values value prob ability given null hypothesis probability distribution data outcome would extreme extreme observed come untrained readers perhaps worryingly authors many papers usually interpret value bayesian probability example posterior probability null hypothesis interpretation sampling theorists bayesians would agree incorrect chapter study couple simple inference problems order compare two approaches statistics cases answers bayesian approach sam pling theory similar also find cases significant differences already seen example exercise sampling theorist got value smaller viewed strong evidence null hypothesis whereas data actually favoured null hypothesis simplest alternative another example given value smaller mystical value yet data favoured null hypothesis thus cases sampling theory trigger happy declaring results sufficiently improbable null hypothesis rejected results actually weakly sup port null hypothesis see also inference problems sampling theory fails detect significant evidence bayesian approach everyday intuition agree evidence strong telling inference problems significance assigned sampling theory changes depending irrelevant factors concerned design experiment chapter provided readers curious sampling theory bayesian methods debate find chapter tough understand please skip point trying understand debate use bayesian methods much easier understand debate copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links bayesian inference sampling theory 
[bayesian, inference, sampling, theory, medical, example] trying reduce incidence unpleasant disease called microsoftus two vaccinations tested group volunteers vaccination control treatment placebo treatment active ingredients subjects randomly assigned treatment given control treatment observe subjects one year vaccinations group one contracts microsoftus group three contract microsoftus treatment better treatment 
[bayesian, inference, sampling, theory, sampling, theory] standard sampling theory approach question better construct statistical test test usually compares hypothesis different effectivenesses null hypothesis exactly effectivenesses novice might object want compare hypothesis better alternative better objections welcome sampling theory two hypotheses defined first hypothesis scarcely mentioned attention focuses solely null hypothesis makes laugh write true null hypothesis accepted rejected purely basis unexpected data much better predicted data one chooses statistic measures much data set deviates null hypothesis example standard statistic use would one called chi squared compute take difference data measurement expected value assuming null hypothesis true divide square difference variance measurement assuming null hypothesis true present problem four data measurements integers number subjects given treatment contracted microsoftus number subjects given treatment forth definition actually elementary statistics book spiegel find yates cor rection recommended want know yates correction read sampling theory textbook point chapter teach sampling theory merely mention yates correction professional sampling theorist might use case given null hypothesis treatments equally effective rates two outcomes expected counts copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links medical example test accepts rejects null hypothesis basis big make test precise give significance level work sampling distribution taking account fact sampling distribution statistic probability distribution value repetitions experiment assuming null hypothesis true four data points independent satisfy two constraints fact parameters known three constraints reduce number degrees freedom data four one want learn computing number degrees freedom read sampling theory book bayesian methods need know quantities equivalent number degrees freedom pop straight bayesian analysis appropriate sampling distributions tabulated sampling theory gnomes come accompanied warnings conditions accurate example standard tabulated distributions accurate expected numbers data arrive sampling theorists estimate unknown parameters null hypothesis data evaluate point sampling theory school divides two camps one camp uses following protocol first looking data pick significance level test determine critical value null hypothesis rejected significance level fraction times statistic would exceed critical value null hypothesis true evaluate compare critical value declare outcome test significance level fixed beforehand second camp looks data finds looks table distributions significance level observed value would critical value result test reported giving value fraction times result extreme one observed extreme would expected arise null hypothesis true let apply two methods first camp let pick signifi cance level critical value one degree freedom estimated values expected values four measurements defined equation since value exceeds reject null hypothesis two treat ments equivalent significance level however use yates correction find therefore accept null hypothesis copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links bayesian inference sampling theory camp two runs finger across table found back good sampling theory book finds interpolating camp two reports value notice answer say much effective simply says significantly different significant means statistically significant practically significant man street reading statement treatment sig nificantly different control might come conclusion chance treatments differ effectiveness actually means experiment many times two treatments equal effectiveness time would find value extreme one happened almost nothing want know likely treatment better 
[bayesian, inference, sampling, theory, let, through, i’m, bayesian] let infer really want know scrap hypothesis two treatments exactly equal effectivenesses since believe two unknown parameters probabilities people given treatments respectively contract disease given data infer two probabilities answer questions interest examining posterior distribution posterior distribution likelihood function prior distribution use prior distribution gives opportunity include knowledge experiments prior belief two parameters different expected similar values use simplest vanilla prior distribution uniform distri bution parameter plot posterior distribution given assumption sepa rable prior posterior distribution also separable two posterior distributions shown figure except graphs normalized joint posterior probability shown figure want know answer question probable smaller answer exactly question computing posterior probability data copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links medical example figure posterior probabilities two effectivenesses treatment solid line dotted line figure joint posterior probability two effectivenesses contour plot surface plot integral joint posterior probability data figure proposition true points shaded triangle find probability proposition integrate joint posterior probability data figure region shown figure region shaded triangle figure value integral obtained straightfor ward numerical integration likelihood function relevant region data thus chance given data prior assumptions treatment superior treatment conclusion according bayesian model data contracted disease vaccination contracted disease vaccination give strong evidence one treatment superior treatment bayesian approach also easy answer relevant questions example want know likely treatment ten times effective treatment integrate joint posterior proba bility data region figure figure proposition true points shaded triangle 
[bayesian, inference, sampling, theory, model, comparison] situation really want compare two hypotheses course directly bayesian methods also example consider data set one subject given treatment subsequently contracted microsoftus one subject given treatment treatment got disease total treated copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links bayesian inference sampling theory strongly data set favour answer question computing evidence hypothesis let assume uniform priors unknown parameters models first hypothesis one unknown parameter let call use uniform prior two parameters model used probability data model normalizing constant inference given probability data model given simple two dimensional integral thus evidence ratio favour model asserts two effectivenesses unequal prior probability two hypotheses posterior probability favour easy get sensible answers well posed questions using bayesian methods sampling theory answer question would involve identical significance test used preceding problem test would yield significant result think greatly preferable acknowledge obvious intuition namely data give weak evidence favour bayesian methods quantify weak evidence 
[bayesian, inference, sampling, theory, dependence, p-values, irrelevant, information] expensive laboratory bloggs tosses coin labelled twelve times outcome string aaabaaaabaab contains three nine evidence data give coin biased favour copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links dependence values irrelevant information bloggs consults sampling theory friend says let num ber total number tosses view random variable find probability taking value extreme value assuming null hypothesis true thus computes reports significance level significant evidence bias favour friend prefers report values rather simply compare would report value conventionally viewed significantly small two tailed test seemed appropriate might compute two tailed area twice probability report value significantly small focus issue choice one tailed two tailed tests bigger fish catch bloggs pays careful attention calculation responds random variable experiment decided running experiment would keep tossing coin saw three random variable thus experimental designs unusual experiments error correcting codes often simulate decoding code chosen number block errors occurred since error inferred value log goes roughly independent exercise find bayesian inference bias coin given data determine whether bayesian inferences depend stopping rule force according sampling theory different calculation required order assess significance result probability distribution given probability first tosses contain exactly nth toss sampling theorist thus computes reports back bloggs value significant evidence bias think bloggs publish result marvellous value one journals insists exper imental results significance assessed using sampling theory boot sampling theorist door seek coherent method assessing significance one depend stopping rule point audience divides two half audience intuitively feel stopping rule irrelevant need convincing answer exercise inferences depend stopping rule half perhaps account thorough copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links bayesian inference sampling theory training sampling theory intuitively feel bloggs stopping rule stopped tossing moment third appeared may biased experiment somehow second group encourage reflect situation hope eventually come round view consistent likelihood principle stopping rule relevant learned thought experiment consider onlookers order save money spying bloggs experiments time tosses coin spies update values spies eager make inferences data soon new result occurs spies beliefs bias coin depend bloggs intentions regarding continuation experiment fact values sampling theory depend stopping rule indeed whole volumes sampling theory literature concerned task assessing significance complicated stopping rule required sequential probability ratio tests example seems com pelling argument nothing values bayesian solution inference problem given sections exer cise would help clarify issue added one scene story janitor eavesdropping bloggs conversation comes says happened notice stopped experi ments coin officer whimsical departmental rules ordered immediate destruction coins coin therefore destroyed departmental safety officer way could continued experiment much beyond tosses seems need recompute value 
[bayesian, inference, sampling, theory, confidence, intervals] experiment data obtained system unknown parameter standard concept sampling theory idea confidence interval interval min max associated confidence level informally interpreted probability lies confidence interval let make precise confidence level really means give example confidence interval function min max data set confidence level confidence interval property compute data arrive imagine generating many data sets particular true value calculating interval min max checking whether true value lies interval averaging imagined repetitions experiment true value lies confidence interval fraction time property holds true values confidence level confidence interval example mean gaussian distribution known standard deviation sample gaussian min max confidence interval let look simple example meaning confidence level becomes clearer let parameter integer let data pair points drawn independently following distribution values copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links compromise positions example could expect following data sets probability probability probability probability consider following confidence interval min max min min example confidence interval would min max let think confidence interval confidence level considering four possibilities shown see chance confidence interval contain true value confidence interval therefore confidence level definition data acquire well compute confidence interval shall report interval associated confidence level would correct rules sampling theory make sense actually know case intuitively bayes theorem clear could either possibilities equally likely prior probabilities equal posterior probability data case confidence interval still associated confidence level case bayes theorem common sense sure neither case probability lies confidence interval equal thus way many people interpret confidence levels sampling theory incorrect given data people usually want know whether know bayesian posterior probability distribution examples contrived making fuss nothing sceptical dogmatic views expressed encourage look case study look depth exercise reference kepler oprea sampling theory estimates confidence intervals mutation rate constructed try methods simulated data bayesian approach based simply computing likelihood function confidence interval sampling theory let know find bayesian answer always better sampling theory answer often much much better suboptimality sampling theory achieved great effort passionate bayesian methods bayesian methods straightforward optimally use information data 
[bayesian, inference, sampling, theory, compromise, positions] let end conciliatory note many sampling theorists pragmatic happy choose selection statistical methods choosing whichever best long run properties contrast problem copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links bayesian inference sampling theory idea one answer well posed problem essential convert sampling theorists viewpoint instead offer bayesian estimators bayesian confidence intervals request sampling theoretical properties methods evaluated need mention methods derived bayesian per spective sampling properties good pragmatic sampling theorist choose use bayesian methods indeed case many bayesian methods good sampling theoretical properties perhaps surprising method gives optimal answer indi vidual case also good long run another piece common ground conceded believe well posed inference problems unique correct answer found bayesian methods problems well posed common question arising data modelling using appropriate model model criticism hunting defects current model task may aided sampling theory tests null hypothesis current model correct well defined alternative model specified one could use sampling theory measures values guide one search aspects model need scrutiny 
[bayesian, inference, sampling, theory, reading] favourite reading topic includes jaynes gull loredo berger jaynes treatises bayesian statistics statistics community include box tiao hagan 
[bayesian, inference, sampling, theory, exercises] exercise traffic survey records traffic two successive days friday morning vehicles one hour saturday morn ing vehicles half hour assuming vehicles poisson distributed rates vehicles per hour respec tively greater factor bigger smaller exercise write program compare treatments given data described section outputs program probability treatment effective treatment probability probability 
[introduction, neural, networks] field neural networks study properties networks idealized neurons three motivations underlie work broad interdisciplinary field biology task understanding brain works one standing unsolved problems science neural network models intended shed light way computation memory performed brains engineering many researchers would like create machines learn perform pattern recognition discover patterns data complex systems third motivation interested neural net works complex adaptive systems whose properties interesting right emphasize several points outset book gives taste field many interesting neural network models time touch models discuss intended faithful models biological systems relevant biology relevance abstract level describe neural network methods widely used nonlinear data modelling able give full description state art wish solve real problems neural networks please read relevant papers 
[introduction, neural, networks, memories] next chapters meet several neural network models come simple learning algorithms make function memories perhaps dwell moment conventional idea memory digital computation memory string bits describing name person image face say stored digital computer address retrieve memory need know address address nothing memory notice properties scheme address based memory associative imagine know half memory say someone face would like recall rest copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links memories memory name memory address based get memory without knowing address computer scientists devoted effort wrapping traditional address based memories inside cunning software produce content addressable memories content addressability come naturally added address based memory robust fault tolerant one bit mis take made specifying address completely different mem ory retrieved one bit memory flipped whenever memory retrieved error present course mod ern computers error correcting codes used memory small numbers errors detected corrected error tolerance intrinsic property memory system minor damage occurs certain hardware implements memory retrieval likely functionality catastrophically lost address based memory distributed serial computer accessing particular memory tiny fraction devices participate memory recall cpu circuits storing required byte millions devices machine sitting idle models truly parallel computation multiple vices participate computations present day parallel computers scarcely differ serial computers point view memory retrieval works way control computation process resides cpus simply cpus devices sit idle time biological memory systems completely different biological memory associative memory recall content addressable given person name often recall face vice versa memories apparently recalled spontaneously request cpu biological memory recall error tolerant robust errors cues memory recall corrected example asks recall american politician intelligent whose politician father like broccoli many people think president bush even though one cues contains error hardware faults also tolerated brains noisy lumps meat continual state change cells damaged natural processes alcohol boxing cells brains proteins cells continually changing many memories persist unaffected biological memory parallel distributed completely distributed throughout whole brain appear functional specialization parts brain memories stored seems many neurons participate storage multiple mem ories properties biological memory systems motivate study arti ficial neural networks parallel distributed computational systems consisting copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links introduction neural networks many interacting simple elements hope model systems might give hints neural computation achieved real bio logical neural networks 
[introduction, neural, networks, terminology] time describe neural network algorithm typically specify three things terminology hard understand probably best dive straight next chapter architecture architecture specifies variables involved network topological relationships example variables involved neural net might weights connections neurons along activities neurons activity rule neural network models short time scale dynamics local rules define activities neurons change response typically activity rule depends weights parameters network learning rule learning rule specifies way neural net work weights change time learning usually viewed taking place longer time scale time scale dynamics activity rule usually learning rule depend activities neurons may also depend values target values supplied teacher current value weights rules come often activity rules learning rules invented imaginative researchers alternatively activity rules learning rules may derived carefully chosen objective functions neural network algorithms roughly divided two classes supervised neural networks given data form inputs tar gets targets teacher specification neural net work response input unsupervised neural networks given data undivided form sim ply set examples learning algorithms intended simply memorize data way examples recalled future algorithms intended generalize discover patterns data extract underlying features unsupervised algorithms able make predictions exam ple algorithms fill missing variables example also viewed supervised networks 
[single, neuron, classifier, single, neuron] study single neuron two reasons first many neural network models built single neurons good understand detail second single neuron capable learning indeed various standard statistical methods viewed terms single neurons model serve first example supervised neural network 
[single, neuron, classifier, definition, single, neuron] start defining architecture activity rule single neuron derive learning rule figure single neuron architecture single neuron number inputs one output call see figure associated input weight may additional parameter neuron called bias may view weight associated input permanently set single neuron feedforward device connections directed inputs output neuron activity rule activity rule two steps first response imposed inputs compute activa tion neuron sum bias otherwise second output set function activation output also called activity neuron confused activation several possible activation activation activity functions popular deterministic activation functions linear sigmoid logistic function copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links single neuron classifier iii sigmoid tanh tanh threshold function stochastic activation functions stochastically selected heat bath probability otherwise metropolis rule produces output way depends previous output state compute flip state else flip state probability 
[single, neuron, classifier, basic, neural, network, concepts] neural network implements function output network nonlinear function inputs function parameterized weights study single neuron produces output following function exercise contexts encountered function already 
[single, neuron, classifier, motivations, linear, logistic, function] section studied best detection pulses assuming one two signals transmitted gaussian channel variance covariance matrix found probability source signal rather given received signal exp linear function received vector linear logistic function motivated several ways see exercises copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links basic neural network concepts 
[single, neuron, classifier] figure output simple neural network function input 
[single, neuron, classifier, input, space, weight, space] convenience let study case input vector param eter vector two dimensional spell function performed neuron thus figure shows output neuron function input vector two horizontal axes figure inputs output vertical axis notice line perpendicular output constant along line direction output sigmoid function introduce idea weight space parameter space network case two parameters weight space two dimensional weight space shown figure selection values parameter vector smaller inset figures show function performed network set values smaller figures equivalent figure thus point space corresponds function notice gain sigmoid function gradient ramp increases magnitude increases central idea supervised neural networks given examples relationship input vector target hope make neural network learn model relationship successfully trained network given give output close sense target value training network involves searching weight space network value produces function fits provided training data well typically objective function error function defined function measure well network weights set solves task objective function sum terms one input target pair measuring close output target training process exercise function minimization adjusting way find minimizes objective function many function minimization algorithms make use objective function also gradient respect parameters general feedforward neural networks backpropagation algorithm efficiently evaluates gradient output respect parameters thence gradient objective function respect copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links single neuron classifier figure weight space copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links training single neuron binary classifier 
[single, neuron, classifier, training, single, neuron, binary, classifier] assume data set inputs binary labels neuron whose output bounded write following error function term objective function may recognized information content one outcome may also described relative entropy tween empirical probability distribution probability distribution implied output neuron objective func tion bounded zero attains value differentiate objective function respect exercise backpropagation algorithm show derivative given notice quantity error example difference target output simplest thing gradient error function descend even though often mensionally incorrect since gradient dimensions parameter whereas change parameter dimensions parameter since derivative sum terms defined obtain simple line algorithm putting input network one time adjusting little direction opposite summarize whole learning algorithm 
[single, neuron, classifier, on-line, gradient-descent, learning, algorithm] architecture single neuron number inputs one output associated input weight activity rule first response received inputs may arbitrary real numbers compute activation neuron sum bias otherwise second output set sigmoid function activation output might viewed stating probability according neuron given input class rather class copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links single neuron classifier learning rule teacher supplies target value says correct answer given input compute error signal adjust weights direction would reduce magnitude error ηex learning rate commonly set trial error constant value decreasing function simulation time activity rule learning rule repeated input target pair presented fixed data set size cycle data multiple times 
[single, neuron, classifier, batch, learning, versus, on-line, learning] described line learning algorithm change weights made every example presented alternative paradigm batch examples computing outputs errors accumulating changes specified equation made end batch 
[single, neuron, classifier, batch, learning, single, neuron, classifier] input target pair compute exp define compute weight let batch learning algorithm gradient descent algorithm whereas line algorithm stochastic gradient descent algorithm source code implementing batch learning given algorithm algorithm demonstrated figure neuron two inputs weights bias performing function bias included contrast figure omitted neuron trained data set ten labelled examples copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links training single neuron binary classifier figure single neuron learning classify gradient descent neuron two weights bias learning rate set batch mode gradient descent performed using code displayed algorithm training data evolution weights function number iterations log scale evolution weights weight space objective function function number iterations magnitude weights function time function performed neuron shown three contours iterations contours shown corresponding namely also shown vector proportional larger weights bigger vector becomes closer together contours e_w copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links single neuron classifier algorithm octave source code gradient descent optimizer single neuron batch learning optional weight decay rate alpha octave notation instruction causes matrix consisting input vectors multiplied weight vector giving vector listing activations input vectors means transpose single command sigmoid computes sigmoid function elements vector global matrix containing input vectors global vector length containing targets loop times compute activations sigmoid compute outputs compute errors compute gradient vector eta alpha make step using learning rate eta weight decay alpha endfor function sigmoid exp endfunction figure influence weight decay single neuron learning objective function learning method figure evolution weights evolution weights weight space shown points contrasted trajectory followed case zero weight decay shown thin line figure notice problem weight decay effect similar early stopping objective function error function function number iterations function performed neuron iterations copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links beyond descent error function regularization 
[single, neuron, classifier, beyond, descent, error, function, regularization] parameter set appropriate value algorithm works algorithm finds setting correctly classifies many examples possible examples fact linearly separable neuron finds lin ear separation weights diverge ever larger values simulation continues seen happening figure exam ple overfitting model fits data well generalization performance likely adversely affected behaviour may viewed undesirable rectified hoc solution overfitting use early stopping use algorithm originally intended minimize error function prevent halting algorithm point principled solution overfitting makes use regularization reg ularization involves modifying objective function way corporate bias sorts solution dislike example dislike development sharp decision bound ary figure sharp boundary associated large weight values use regularizer penalizes large weight values modify objective function simplest choice regularizer weight decay regularizer regularization constant called weight decay rate additional term favours small values decreases tendency model overfit fine details training data quantity known hyperparameter hyperparameters play role learning algorithm play role activity rule network exercise compute derivative respect regularizer known weight decay regularizer gradient descent source code algorithm implements weight decay gradient descent algorithm demonstrated figure using weight decay rates weight decay rate increased solution becomes biased towards broader sigmoid functions decision boundaries closer origin 
[single, neuron, classifier, note] gradient descent step size general efficient way minimize function modification gradient descent known momentum improving convergence also recommended neural network experts use advanced optimizers conjugate gradient algorithms please confuse momentum sometimes given symbol weight decay copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links single neuron classifier 
[single, neuron, classifier, motivations, linear, neuron] exercise consider task recognizing two gaussian distri butions vector comes unlike case studied section distributions different means common variance covariance matrix assume two distributions actly mean different variances let probability given normal variance source symbol show written form exp appropriate function exercise noisy led consider led display elements numbered shown state display vector controller wants display show character number element either adopts intended state probability flipped probability let call two states assuming intended character actually probability given state show written form exp compute values weights case assuming one prior probabilities probability given state put answer form functions could make better alphabet characters noisy led alphabet less susceptible confusion table alternative character alphabet element led display exercise error correcting code consists two codewords source bit proba bility distribution used select one two codewords transmission binary symmetric channel noise level copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exercises received vector show posterior probability given written form exp give expressions coefficients bias describe diagram optimal decoder expressed terms neuron 
[capacity, single, neuron] learning algorithm figure neural network learning viewed communication 
[capacity, single, neuron, neural, network, learning, communication] many neural network models involve adaptation set weights response set data points example set target values given locations adapted weights used process subsequent input data process viewed communication process sender examines data creates message depends data receiver uses example receiver might use weights try reconstruct data neural network parlance using neuron memory rather generalization generalizing means extrapolating observed data value new location disk drive communication channel adapted network weights therefore play role communication channel conveying information training data future user neural net question address capacity channel much information stored training neural network learning algorithm either produces network whose sponse inputs network whose response inputs depending training data weights allow distinguish tween two sorts data set maximum information learning algorithm could convey data therefore bit information con tent achieved two sorts data set equiprobable much information conveyed make full use neural network ability represent functions 
[capacity, single, neuron, capacity, single, neuron] look simplest case single binary threshold neuron find capacity neuron two bits per weight neuron inputs store bits information obtain interesting result lay rules exclude less interesting answers capacity neuron infinite copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links capacity single neuron weights real number convey infinite number bits exclude answer saying receiver able examine weights directly receiver allowed probe weights observing output neuron arbitrarily chosen inputs constrain receiver observe output neuron fixed set points training set matters many different distinguishable functions neuron produce given observe function points many different binary labellings points linear threshold function produce number compare maximum possible number binary labellings nearly labellings realized neuron communication channel convey bits target values small probability error identify capacity neuron maximum value probability error small departing little definition capacity chapter thus examine following scenario sender given neuron inputs data set labelling points sender uses adaptive algorithm try find reproduce labelling exactly assume algorithm finds exists receiver evaluates threshold function input values probability bits correctly reproduced large become given without probability becoming substantially less one 
[capacity, single, neuron, general, position] one technical detail needs pinned set inputs considering answer might depend choice assume points general position definition set points dimensional space general position subset size linearly independent lie dimensional plane dimensions example set points general position three points colinear four points coplanar intuitive idea points general position like random points space terms linear dependences points expect three random points three dimensions lie straight line 
[capacity, single, neuron, linear, threshold, function] neuron consider performs function bias capacity neuron bias obtained replacing final result considering one inputs fixed input points would general position derivation still works copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links counting threshold functions figure one data point two dimensional input space two regions weight space give two alternative labellings point 
[capacity, single, neuron, counting, threshold, functions] let denote number distinct threshold functions points general position dimensions derive formula start let work cases hand 
[capacity, single, neuron, dimension] points lie line changing sign one weight label points right side origin others vice versa thus two distinct threshold functions 
[capacity, single, neuron, point] one point realize possible labellings setting thus 
[capacity, single, neuron, dimensions] use weight space visualization study three dimensional case let imagine adding one point time count number thresh old functions weight space divided two perplanes four regions one region vectors produce function input vectors thus adding third point general position produces third plane space distinguishable regions three bisecting planes shown figure point matters become slightly tricky figure illus trates fourth plane three dimensional space cannot transect eight sets created first three planes six existing regions cut two remaining two unaffected two copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links counting threshold functions figure weight space illustrations three hyperplanes corresponding three points general position divide space regions shown colouring relevant part surface hollow semi transparent cube centred origin four hyperplanes divide space regions figure shows region view right hand face compare figure regions coloured white cut two table values deduced hand figure illustration cutting process going eight regions figure one added hyperplane regions coloured white cut two hollow cube made solid see regions cut fourth plane front half cube cut away figure shows new two dimensional hyperplane divided six regions three one dimensional hyperplanes lines cross regions corresponds one three dimensional regions figure cut two new hyperplane shows figure compared figure copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links capacity single neuron binary functions points dimensions cannot realized linear threshold function filled values shown table obtain insights derivation order fill rest table greater six six number regions new hyperplane bisected space figure equivalently look dimensional subspace hyperplane subspace divided six regions previous hyperplanes figure concept met compare figure figure many regions created hyperplanes dimensional space course present case look previous section 
[capacity, single, neuron, dimensions, point, view, weight, space] another way visualizing problem instead visualizing plane separating points two dimensional input space consider two dimensional weight space colouring regions weight space different colours label given datapoints differently count number threshold functions counting many distinguishable regions weight space consider first set weight vectors weight copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links capacity single neuron figure two data points two dimensional input space four regions weight space give four alternative labellings figure three data points two dimensional input space six regions weight space give alternative labellings points case labellings cannot realized three points general position always two labellings cannot realized space classify particular example example figure shows single point two dimensional space figure shows two corresponding sets points space one set weight vectors occupy half space others occupy figure added second point input space possible labellings figure shows two hyperplanes separate sets weight vectors produce labellings figure weight space divided three hyperplanes six regions eight conceivable labellings realized thus 
[capacity, single, neuron, recurrence, relation] generalizing picture see add hyperplane dimensions bisect regions created previous hyperplanes therefore total number regions obtained adding hyperplane since regions split two plus remaining regions split nth hyperplane gives following equation remains solve recurrence relation given boundary conditions recurrence relation look familiar maybe remember building pascal triangle adding together two adjacent numbers one row get number element pascal triangle equal table pascal triangle combinations satisfy equation adopting convention satisfies required recurrence relation mean since many functions satisfy one recurrence relation copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links counting threshold functions figure fraction functions points dimensions linear threshold functions shown various viewpoints see dependence approximately error function passing fraction reaches see dependence drops sharply panel shows dependence sudden drop fraction realizable labellings panel shows values log log function figures plotted using approximation error function log log perhaps express linear superposition combination functions form comparing tables see satisfy boundary conditions simply need translate pascal triangle right superpose add multiply two drop whole table one line thus using fact row pascal triangle sums simplify cases 
[capacity, single, neuron, interpretation] natural compare total number binary functions points ratio tells probability arbitrary labelling memorized neuron two functions equal line thus special line defining maximum number points arbitrary labelling realized number points referred vapnik chervonenkis dimension dimension class functions dimension binary threshold function dimensions thus copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links capacity single neuron interesting large number points almost labelling realized ratio still greater large ratio close purposes sum equation well approximated error function exp figure shows realizable fraction function take home message shown figure although fraction less negligibly less catastrophic drop zero tiny fraction binary labellings realized threshold function 
[capacity, single, neuron, conclusion] capacity linear threshold neuron large bits per weight single neuron almost certainly memorize random binary labels perfectly almost certainly fail memorize 
[capacity, single, neuron, exercises] exercise finite set distinct points two dimensional space split half straight line points general position points general position points dimensional space split half dimensional hyperplane exercise four points selected random surface sphere probability lie single hemi sphere question relate exercise consider binary threshold neuron dimensions set points find parameter vector neuron memorizes labels find unrealizable labelling exercise chapter constrained hyperplanes origin exercise remove constraint figure three lines plane create seven regions many regions plane created lines general position exercise estimate bits total sensory experience life visual information auditory information etc estimate much information memorized estimate information content works shakespeare compare capacity brain assuming neurons making synaptic connections capacity result one neuron two bits per connection applies brain full yet copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links solutions exercise capacity axon spiking neuron viewed communication channel bits per second see mackay mcculloch early publication topic multiply number axons optic nerve cochlear nerve per ear estimate rate acquisition sensory experience 
[capacity, single, neuron, solutions] solution exercise probability four points lie single hemisphere 
[learning, inference, neural, network, learning, inference] chapter trained simple neural network classifier minimizing objective function made error function regularizer neural network learning process given following probabilistic interpretation interpret output neuron literally defining parameters specified probability input belongs class rather alternative thus value defines different hypothesis probability class relative class function define observed data targets inputs assumed given modelled infer given data require likelihood function prior probability likelihood function measures well parameters predict observed data probability assigned observed values model parameters set two equations rewritten single equation exp error function interpreted minus log likelihood exp similarly regularizer interpreted terms log prior proba bility distribution parameters exp copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links illustration neuron two weights quadratic defined corresponding prior distribution gaussian variance equal number parameters vector objective function corresponds inference parameters given data exp found locally minimizing interpreted locally probable parameter vector refer natural interpret error functions log probabilities error functions usually additive example sum information con tents sum squared weights probabilities hand multiplicative independent events joint probability logarithmic mapping maintains correspondence interpretation log probability numerous benefits discuss moment 
[learning, inference, illustration, neuron, two, weights] case neuron two inputs bias plot posterior probability exp imag ine receive data shown left column figure data point consists two dimensional input vector value indicated likelihood function exp shown function second column product functions form product traditional learning point space estimator maximizes posterior probability density contrast bayesian view product learning ensemble plausible parameter values bottom right figure choose one particular hypothesis rather evaluate posterior probabilities posterior distribution obtained multiplying likelihood prior distribution space shown broad gaussian upper right figure posterior ensemble within multiplicative constant shown third column figure contour plot fourth column amount data increases top bottom posterior ensemble becomes increasingly concentrated around probable value 
[learning, inference, beyond, optimization, making, predictions] let consider task making predictions neuron trained classifier section neuron two inputs bias copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links learning inference figure bayesian interpretation generalization traditional neural network learning evolution probability distribution parameters data arrive data set likelihood probability parameters constant figure making predictions function performed optimized neuron shown three contours trained weight decay figure contours shown corresponding namely predictions reasonable contours shown posterior probability schematic bayesian predictions shown obtained averaging together predictions made possible value weights value receiving vote proportional probability posterior ensemble method used create described section samples copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links beyond optimization making predictions last played trained minimizing objective function resulting optimized function case reproduced fig ure consider task predicting class corresponding new input common practice making predictions simply use neural network weights fixed optimized value optimal seen intuitively considering predictions shown figure reasonable predictions consider new data arriving points best fit model assigns examples probability class value really knew equal predictions would correct know parameters uncertain intuitively might inclined assign less confident probability closer shown figure since point far training data best fit parameters often give confident predictions non bayesian approach problem downweight predictions uniformly empirically determined factor copas ideal since intuition suggests strength predictions downweighted bayesian viewpoint helps understand cause problem provides straightforward solution nutshell obtain bayesian predictions taking account whole posterior ensemble shown schematically figure bayesian prediction new datum involves marginalizing parameters anything else uncertain simplicity let assume weights uncertain quantities weight decay rate model assumed fixed sum rule predictive probability new target location dimensionality three toy problem thus predictions obtained weighting prediction possible weight given posterior probability recently wrote equation posterior probability exp exp summary get bayesian predictions find way computing integral exp average output neuron posterior distribution copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links learning inference dumb metropolis gradient descent langevin figure one step langevin method two dimensions contrasted traditional dumb metropolis method gradient descent proposal density langevin method given gradient descent noise 
[learning, inference, implementation] shall compute integral toy problem weight space three dimensional realistic neural network dimensionality might thousands bayesian inference general data modelling problems may imple mented exact methods chapter monte carlo sampling chapter deterministic approximate methods example methods make gaussian approximations using laplace method chap ter variational methods chapter neural networks exact methods two main approaches implementing bayesian inference neural networks monte carlo methods developed neal gaussian approximation methods developed mackay 
[learning, inference, monte, carlo, implementation, single, neuron] first use monte carlo approach task evaluating integral solved treating function whose mean compute using samples posterior distribution exp equation obtain samples using metropolis method section aside possible disadvantage monte carlo approach poor way estimating probability improbable event close zero improbable event likely occur conjunction improbable parameter values generate samples radford neal introduced hamil tonian monte carlo method neural networks met sophisticated metropolis method makes use gradient information chapter method demonstrate simple version hamiltonian monte carlo called langevin monte carlo method 
[learning, inference, langevin, monte, carlo, method] langevin method algorithm may summarized gradient scent added noise shown pictorially figure noise vector generated gaussian unit variance gradient computed copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links monte carlo implementation single neuron algorithm octave source code langevin monte carlo method obtain hamiltonian monte carlo method repeat four lines marked multiple times algorithm gradm set gradient using initial findm set objective function loop times randn size initial momentum normal evaluate epsilon make half step wnew epsilon make step gnew gradm wnew find new gradient epsilon gnew make half step mnew findm wnew find new objective function hnew mnew evaluate new value hnew decide whether accept accept elseif rand exp accept compare uniform else accept variate endif accept gnew wnew mnew endif endfor function gradm gradient objective function compute activations sigmoid compute outputs compute errors compute gradient alpha endfunction function findm objective function log log alpha endfunction copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links learning inference figure single neuron learning langevin monte carlo method evolution weights function number iterations evolution weights weight space also shown line evolution weights using optimizer figure error function function number iterations also shown error function optimization figure objective function function number iterations see also figures langevin optimizer langevin optimizer step made given xfp notice xfp term omitted would simply gradient descent learning rate step accepted rejected depending change value objective function change gradient probability acceptance detailed balance holds langevin method one free parameter controls typical step size set large value moves may rejected set small value progress around state space slow 
[learning, inference, demonstration, langevin, method] langevin method demonstrated figures objective function figures include comparison results previous optimization method using gradient descent objective function figure seen mean evolution similar evolution parameters gradient descent monte carlo method appears converged posterior distribution iterations average acceptance rate simulation proposed moves rejected probably faster progress around state space would made larger step size used value chosen descent rate matched step size earlier simulations 
[learning, inference, making, bayesian, predictions] iteration weights sampled every itera tions corresponding functions plotted figure considerable variety plausible functions obtain monte carlo proximation bayesian predictions averaging thirty functions together result shown figure contrasted predic tions given optimized parameters bayesian predictions become satisfyingly moderate move away region highest data density copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links monte carlo implementation single neuron figure samples obtained langevin monte carlo method learning rate set weight decay rate step size given function performed neuron shown three contours every iterations iteration contours shown corresponding namely also shown vector proportional figure bayesian predictions found langevin monte carlo method compared predictions using optimized parameters predictive function obtained eraging predictions samples uniformly spaced iterations shown figure contours shown corresponding namely contrast predictions given probable setting neuron parameters given optimization copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links learning inference algorithm octave source code hamiltonian monte carlo method algorithm identical langevin method algorithm except replacement four lines marked algorithm fragment shown wnew gnew tau tau epsilon gnew make half step wnew wnew epsilon make step gnew gradm wnew find new gradient epsilon gnew make half step endfor langevin hmc figure comparison sampling properties langevin monte carlo method hamiltonian monte carlo hmc method horizontal axis number gradient evaluations made figure shows weights first iterations rejection rate hamiltonian monte carlo simulation bayesian classifier better able identify points classi fication uncertain pleasing behaviour results simply mechanical application rules probability 
[learning, inference, optimization, typicality] final observation concerns behaviour functions monte carlo sampling process compared values optimum figure function fluctuates around value though symmetrical way function also fluctuates fluctuate around obviously cannot minimized could smaller furthermore rarely drops close language information theory typical set different properties probable state general message therefore emerges applicable data models neural networks one cautious making use optimized parameters properties optimized parameters may unrepresen tative properties typical plausible parameters predictions obtained using optimized parameters alone often unreasonably confident 
[learning, inference, reducing, random, walk, behaviour, using, hamiltonian, monte, carlo] final study monte carlo methods compare langevin monte carlo method big brother hamiltonian monte carlo method change hamiltonian monte carlo simple implement shown algo rithm single proposal makes use multiple gradient evaluations copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links implementing inference gaussian approximations along dynamical trajectory space extra momentum variables langevin hamiltonian monte carlo methods num ber steps tau set random number trajectory step size kept fixed retain comparability simulations gone recommended one randomize step size practical applications however figure compares sampling properties langevin hamil tonian monte carlo methods autocorrelation state hamil tonian monte carlo simulation falls much rapidly simulation time langevin method toy problem hamiltonian monte carlo least ten times efficient use computer time 
[learning, inference, implementing, inference, gaussian, approximations] physicists love take nonlinearities locally linearize love approximate probability distributions gaussians approximations offer alternative strategy dealing integral exp evaluated using monte carlo methods start making gaussian approximation posterior probability minimum using gradient based optimizer taylor expand matrix second derivatives also known hessian defined thus define gaussian approximation det exp think matrix defining error bars precise normal distribution whose variance covariance matrix exercise show second derivative respect given first derivative computed hessian task perform integral using gaussian approximation copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links learning inference figure marginalized probability approximation function evaluated numerically functions defined text shown function mackay figure gaussian approximation weight space approximate predictions input space projection gaussian approximation onto plane weight space one two standard deviation contours shown also shown trajectory optimizer monte carlo method samples predictive function obtained gaussian approximation equation figure 
[learning, inference, calculating, marginalized, probability] output depends scalar reduce dimensionality integral finding probability density assuming locally gaussian posterior probability distribution exp single neuron activation linear function activation gaussian distributed exercise assuming gaussian distributed mean variance covariance matrix show probability distribution normal exp means marginalized output normal contrasted output prob able network integral sigmoid times gaussian approximated figure 
[learning, inference, demonstration] figure shows result fitting gaussian approximation timum results using gaussian approximation equa copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links implementing inference gaussian approximations tion make predictions comparing predictions langevin monte carlo method figure observe whilst quali tatively two clearly numerically different least one two methods completely accurate exercise gaussian approximation heavy tailed light tailed may help consider function one parameter think two distributions logarithmic scale discuss conditions gaussian approximation accurate 
[learning, inference, marginalize?] output immediately used make decision costs asso ciated error symmetrical use marginalized outputs gaussian approximation make difference performance classifier compared using outputs given probable param eters since functions pass bayesian outputs make difference example option saying know addition saying guess guess even two choices costs associated error unequal decision boundary contour contour boundary affected marginalization 
[hopfield, networks] spent three chapters studying single neuron time come connect multiple neurons together making output one neuron input another make neural networks neural networks divided two classes basis con nectivity figure feedforward network feedback network feedforward networks feedforward network connections directed network forms directed acyclic graph feedback networks network feedforward network called feedback network chapter discuss fully connected feedback network called hopfield network weights hopfield network constrained symmetric weight neuron neuron equal weight neuron neuron hopfield networks two applications first act associative memories second used solve optimization problems first discuss idea associative memory also known content addressable memory 
[hopfield, networks, hebbian, learning] chapter discussed contrast traditional digital memories biological memories perhaps striking difference associative nature biological memory simple model due donald hebb captures idea associa tive memory imagine weights neurons whose activities positively correlated increased correlation imagine stimulus present example smell banana activity neuron increases neuron associated copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links hopfield networks another stimulus example sight yellow object two stimuli yellow sight banana smell occur environment hebbian learning rule increase weights means later occasion stimulus occurs isolation mak ing activity large positive weight cause neuron also activated thus response sight yellow object automatic association smell banana could call pattern completion teacher required associative memory work signal needed indicate correlation detected sociation made unsupervised local learning algorithm unsupervised local activity rule spontaneously produce associative memory idea seems simple effective must relevant memories work brain 
[hopfield, networks, definition, binary, hopfield, network] convention weights convention general denotes connection neuron neuron architecture hopfield network consists neurons fully connected symmetric bidirectional connections weights self connections biases may included may viewed weights neuron whose activity permanently denote activity neuron output activity rule roughly hopfield network activity rule neu ron update state single neuron threshold activation function since feedback hopfield network every neuron output input neurons specify order updates occur updates may synchronous asynchronous synchronous updates neurons compute activations update states simultaneously asynchronous updates one neuron time computes activa tion updates state sequence selected neurons may fixed sequence random sequence properties hopfield network may sensitive choices learning rule learning rule intended make set desired memo ries stable states hopfield network activity rule memory binary pattern copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links definition continuous hopfield network moscow russia lima peru london england tokyo japan edinburgh scotland ottawa canada oslo norway stockholm sweden paris france moscow moscow russia canada ottawa canada otowa canada ottawa canada egindurrh sxotland edinburgh scotland figure associative memory schematic list desired memories first purpose associative memory pattern completion given partial pattern second purpose memory error correction weights set using sum outer products hebb rule unimportant constant prevent largest possible weight growing might choose set exercise explain value important hopfield network defined 
[hopfield, networks, definition, continuous, hopfield, network] using identical architecture learning rule define hopfield network whose activities real numbers activity rule hopfield network activity rule neuron date state single neuron sigmoid activation function updates may synchronous asynchronous volve equations tanh learning rule binary hopfield network value becomes relevant alternatively may fix introduce gain activation function tanh exercise encountered equations 
[hopfield, networks, convergence, hopfield, network] hope hopfield networks defined perform associa tive memory recall shown schematically figure hope activity rule hopfield network take partial memory corrupted memory perform pattern completion error correction restore original memory expect pattern stable activity rule let alone desired memories address continuous hopfield network since binary network special case already encountered activity rule copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links hopfield networks discussed variational methods section approximated spin system whose energy function separable distribution exp optimized latter minimize variational free energy found pair iterative equations tanh guaranteed decrease variational free energy simply replace see equations hopfield network identical set mean field equations minimize general name function decreases dynamical evolution system bounded function lyapunov function system useful able prove existence lyapunov functions system lyapunov function dynamics bound settle fixed point local minimum lyapunov function limit cycle along lyapunov function constant chaotic behaviour possible system lyapunov function system lyapunov function state space divided basins attraction one basin associated attractor continuous hopfield network activity rules implemented asyn chronously lyapunov function lyapunov function convex function parameter hopfield network dynamics always converge stable fixed point convergence proof depends crucially fact hopfield network connections symmetric also depends updates made asynchronously exercise show constructing example feedback network symmetric connections dynamics may fail converge fixed point exercise show constructing example hopfield network updated synchronously initial conditions may fail converge fixed point copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links convergence hopfield network figure binary hopfield network storing four memories four memories weight matrix initial states differ one two three four even five bits desired memory restored memory one two iterations initial conditions far memories lead stable states four memories stable state looks like mixture two memories stable state like mixture find corrupted version memory two bits distant corrupted version four bits distant state looks spurious recognize inverse stable state copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links hopfield networks 
[hopfield, networks, associative, memory, action] figure shows dynamics unit binary hopfield network learnt four patterns hebbian learning four patterns displayed five five binary images figure twelve initial conditions panels show state network iteration iteration units updated asynchronously iteration initial condition randomly perturbed memory often takes one iteration errors corrected network stable states addition four desired memories inverse stable state also stable state several stable states interpreted mixtures memories 
[hopfield, networks, brain, damage] network severely damaged still work fine associative memory take weights network shown figure randomly set zero still find desired memories attracting stable states imagine digital computer still works fine even components destroyed exercise implement hopfield network confirm amazing robust error correcting capability 
[hopfield, networks, memories] squash memories network figure shows set five memories train network hebbian learning five memories stable states even weights randomly deleted shown weight matrix however basins attraction smaller figures show dynamics resulting randomly chosen starting states close memories bits flipped three memories recovered correctly try store many patterns associative memory fails catas trophically add sixth pattern shown figure one patterns stable others flow one two spurious stable states 
[hopfield, networks, continuous-time, continuous, hopfield, network] fact hopfield network properties robust minor change asynchronous synchronous updates might cause con cern model useful model biological networks turns move continuous time version hopfield networks issue melts away assume neuron activity continuous function time activations computed instantaneously accordance neuron response activation assumed mediated differential equation copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links continuous time continuous hopfield network figure hopfield network storing five memories suffering deletion weights five memories weights network deleted weights shown initial states differ three random bits memory restored converge states desired memories figure overloaded hopfield network trained six memories stable copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links hopfield networks figure failure modes hopfield network highly schematic list desired memories resulting list attracting stable states notice memories retained small number errors desired memories completely lost attracting stable state desired memory near spurious stable states unrelated original list spurious stable states confabulations desired memories desired memories moscow russia lima peru london england tokyo japan edinburgh scotland ottawa canada oslo norway stockholm sweden paris france attracting stable states moscow russia lima peru londog englard tonco japan edinburgh scotland oslo norway stockholm sweden paris france wzkmhewn xqwqwpoq paris sweden ecnarf sirap activation function example tanh steady activation activity relaxes exponentially time constant nice result long weight matrix symmetric system variational free energy lyapunov function exercise computing prove variational free energy lyapunov function continuous time hopfield network particularly easy prove function lyapunov function system dynamics perform steepest descent case continuous time continuous hopfield network quite simple every component sign means appropriately defined metric hopfield network dynamics perform steepest descents 
[hopfield, networks, capacity, hopfield, network] one way viewed learning single neuron communica tion communication labels training data set one point time later point time found capacity linear threshold neuron bits per weight similarly might view hopfield associative memory commu nication channel figure list desired memories encoded set weights using hebb rule equation perhaps learning rule receiver receiving weights finds stable states hopfield network interprets original mem ories communication system fail various ways illustrated figure individual bits memories might corrupted sta ble state hopfield network displaced little desired memory entire memories might absent list attractors net work stable state might present small basin attraction use pattern completion error correction spurious additional memories unrelated desired memories might present spurious additional memories derived desired memories erations mixing inversion may also present copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links capacity hopfield network failure modes modes clearly undesirable mode espe cially mode might matter much long desired memories large basin attraction fourth failure mode might contexts actually viewed beneficial example network required memorize examples valid sentences john loves mary john gets cake might happy find john loves cake also stable state network might call behaviour generalization capacity hopfield network neurons might defined number random patterns stored without failure mode substantial probability also require failure mode tiny probability resulting capacity much smaller study alternative definitions capacity 
[hopfield, networks, capacity, hopfield, network, stringent, definition] first explore information storage capabilities binary hopfield network learns using hebb rule considering stability one bit one desired patterns assuming state network set desired pattern assume patterns stored randomly selected binary patterns activation particular neuron weights split two terms first contribute signal reinforcing desired memory second noise substituting activation first term times desired state term would keep neuron firmly clamped desired state second term sum random quantities moment reflection confirms quantities independent random binary variables mean variance thus considering statistics ensemble random pat terns conclude mean variance brevity assume large enough neglect distinction restate conclusion gaussian distributed mean variance figure probability density activation case probability bit becomes flipped area tail probability selected bit stable put network state probability bit flip first iteration hopfield network dynamics unstable copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links hopfield networks figure overlap desired memory stable state nearest function loading fraction overlap defined scaled inner product recall perfect zero stable state bits flipped abrupt transition overlap drops zero important quantity ratio number patterns stored number neurons example try store patterns hopfield network chance specified bit specified pattern unstable first iteration position derive first capacity result case corruption desired memories permitted exercise assume wish desired patterns completely stable want bits flip network put desired pattern state total probability error required less small number using approximation error function large show maximum number patterns stored max max however allow small amount corruption memories occur number patterns stored increases 
[hopfield, networks, statistical, physicists’, capacity] analysis led equation tells try store patterns hopfield network starting desired memory bits unstable first iteration analysis shed light expected happen subsequent iterations flipping bits might make bits unstable causing increasing number bits flipped process might lead avalanche network state ends long way desired memory fact large avalanches happen small tend stable state near desired memory limit large amit used methods statistical physics find numerically transition two behaviours sharp discontinuity crit copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links improving capacity hebb rule critical value likely stable state near every desired memory small fraction bits flipped exceeds system spurious stable states known spin glass states none correlated desired memories critical value fraction bits flipped desired memory evolved associated stable state figure shows overlap desired memory nearest stable state function transitions properties model occur additional values summarized stable spin glass states exist uncorrelated desired memories spin glass states stable states stable states close desired memories stable states associated desired memories lower energy spurious spin glass states spin glass states dominate spin glass states lower energy stable states associated desired memories additional mixture states combina tions several desired memories stable states low energy stable states associated desired memories conclusion capacity hopfield network neurons define capacity terms abrupt discontinuity discussed random binary patterns length received bits flipped bits capacity expression capacity omits smaller negative term order log bits associated arbitrary order memories bits since weights network also express capacity bits per weight 
[hopfield, networks, improving, capacity, hebb, rule] capacities discussed previous section capacities hop field network whose weights set using hebbian learning rule better hebb rule defining objective function measures well network stores memories minimizing associative memory useful must able correct least one flipped bit let make objective function measures whether flipped bits tend restored correctly intention every neuron network weights neuron satisfy rule every pattern neurons set correctly activation neuron preferred output rule familiar idea yes precisely wanted single neuron chapter pattern defines input target pair single neuron defines input target pair neurons copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links hopfield networks algorithm octave source code optimizing weights hopfield network works associative memory algorithm data matrix columns rows matrix identical except replaced initialize weights using hebb rule loop times ensure self weights zero end compute activations sigmoid compute outputs compute errors compute gradients symmetrize gradients eta alpha make step endfor defined objective function training single neuron classifier define exp steal algorithm algorithm wrote single neuron write algorithm optimizing hopfield network algorithm convenient syntax octave requires changes extra lines enforce constraints self weights zero weight matrix symmetrical expected learning algorithm better job one shot hebbian learning rule six patterns figure cannot memorized hebb rule learned using algorithm six patterns become stable states exercise implement learning rule investigate empirically capacity memorizing random patterns also compare avalanche properties hebb rule 
[hopfield, networks, hopfield, networks, optimization, problems] since hopfield network dynamics minimize energy function natural ask whether map interesting optimization problems onto hopfield networks biological data processing problems often involve element constraint satisfaction scene interpretation example one might wish infer spatial location orientation brightness texture visible element visible elements connected together objects inferences constrained given data prior knowledge continuity objects copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links hopfield networks optimization problems place tour city place tour city figure hopfield network solving travelling salesman problem cities two solution states neuron network activites represented black white tours corresponding network states negative weights node nodes weights enforce validity tour negative weights embody distance objective function hopfield tank suggested one might take interesting constraint satisfaction problem design weights binary contin uous hopfield network settling process network would minimize objective function problem 
[hopfield, networks, travelling, salesman, problem] classic constraint satisfaction problem hopfield networks applied travelling salesman problem set cities given matrix distances cities task find closed tour cities visiting city smallest total distance travelling salesman problem equivalent difficulty complete problem method suggested hopfield tank represent tentative lution problem state network neurons arranged square neuron representing hypothesis particular city comes particular point tour convenient consider states neurons rather two solution states four city travelling salesman problem shown figure weights hopfield network play two roles first must define energy function minimized state network represents valid tour valid state one looks like permutation matrix exactly one every row one every column rule enforced putting large negative weights pair neurons row column setting positive bias neurons ensure neurons turn figure shows negative weights connected one neuron represents statement city comes second tour second weights must encode objective function want minimize total distance done putting negative weights proportional appropriate distances nodes adja cent columns example nodes adjacent columns weight would negative weights connected neu ron shown figure result network valid state total energy total distance corresponding copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links hopfield networks figure evolution state continuous hopfield network solving travelling salesman problem using aiyer graduated non convexity method state network projected two dimensional space cities located finding centre mass point tour using neuron activities mass function travelling scholar problem shortest tour linking cambridge colleges engineering department university library sree aiyer house aiyer tour plus constant given energy associated biases since hopfield network minimizes energy hoped binary continuous hopfield network dynamics take state minimum valid tour might optimal tour hope fulfilled large travelling salesman problems however without careful modifications specified size weights enforce tour validity relative size distance weights setting scale factor poses difficulties large validity enforcing weights used network dynamics rattle valid state little regard distances small validity enforcing weights used possible distance weights cause network adopt invalid state lower energy valid state original formulation energy function puts objective function solution validity potential conflict difficulty resolved work sree aiyer showed modify distance weights would interfere solution validity define continuous hopfield network whose dynamics times confined valid subspace aiyer used graduated non convexity deterministic annealing approach find good solutions using hopfield networks deterministic annealing approach involves gradually increasing gain neurons network point state network corresponds valid tour sequence trajectories generated applying method thirty city travelling salesman problem shown figure solution travelling scholar problem found aiyer using con tinuous hopfield network shown figure copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exercises 
[hopfield, networks, exercises] exercise storing two memories two binary memories stored heb bian learning hopfield network using biases set zero network put state evaluate activation neuron show written form comparing signal strength magnitude noise strength show stable state dynamics network network put state differing places perturbation satisfies number components non zero non zero defining overlap evaluate activation neuron show dynamics network restore number flipped bits satisfies number compare maximum number flipped bits corrected optimal decoder assuming vector either noisy version exercise hopfield network collection binary classifiers ercise explores link unsupervised networks supervised networks hopfield network desired memories attracting stable states every neuron network weights going solve classification problem personal neuron take set memories write form denotes components let denote vector weights using know capacity single neuron show almost certainly impossible store random memories hopfield network neurons copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links hopfield networks 
[hopfield, networks, lyapunov, functions] exercise erik puzzle stripped version conway game life cells arranged square grid cell either alive dead live cells die dead cells become alive two immediate neighbours alive neighbours north south east west smallest number live cells needed order rules lead entire square alive figure erik dynamics dimensional version game rule neigh bours alive come life smallest number live cells needed order entire hypercube becomes alive live cells arranged 
[hopfield, networks, southeast, puzzle] figure southeast puzzle southeast puzzle played semi infinite chess board starting northwest top left corner three rules starting position one piece placed northwest square figure permitted one piece given square step remove one piece board replace two pieces one square immediately east one square immediately south illustrated figure every step increases number pieces board one move made either piece may selected next move figure shows outcome moving lower piece next move either lowest piece middle piece three may selected uppermost piece may selected since would violate rule move selected middle piece pieces may moved except leftmost piece puzzle exercise possible obtain position ten squares closest northwest corner marked figure empty hint puzzle connection data compression 
[hopfield, networks, solutions] solution exercise take binary feedback network neu rons let whenever neuron updated match neuron whenever neuron updated flip opposite state neuron stable state copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links solutions solution exercise take binary hopfield network neu rons let let initial condition dynamics synchronous every iteration neurons flip state dynamics converge fixed point solution exercise key problem notice similarity construction binary symbol code starting empty string build binary tree repeatedly splitting codeword two every codeword implicit probability depth codeword binary tree whenever split codeword two create two new codewords whose length increased one two new codewords implicit probability equal half old codeword complete binary code kraft equality affirms sum implicit probabilities similarly southeast associate weight piece board assign weight piece sitting top left square weight piece square whose distance top left one weight piece whose distance top left two forth distance city block distance every legal move southeast leaves unchanged total weight pieces board lyapunov functions come two flavours function may function state whose value known stay constant may function state bounded whose value always decreases stays constant total weight lyapunov function second type starting weight powerful tool conserved function state possible find position ten highest weight squares vacant total weight total weight squares board occupied figure total figure possible position southeast puzzle weight would equal impossible empty ten squares 
[boltzmann, machines, hopfield, networks, boltzmann, machines] noticed binary hopfield network minimizes energy func tion continuous hopfield network activation function tanh viewed approximating probability distribution asso ciated energy function exp exp observations motivate idea working neural network model actually implements probability distribution stochastic hopfield network boltzmann machine hinton jnowski following activity rule activity rule boltzmann machine computing activa tion set probability else set rule implements gibbs sampling probability distribution 
[boltzmann, machines, boltzmann, machine, learning] given set examples real world might interested adjusting weights generative model exp well matched examples derive learning algorithm writing bayes theorem obtain posterior probability weights given data copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links hopfield networks boltzmann machines concentrate first term numerator likelihood derive maximum likelihood algorithm though might advantages pursuing full bayesian approach case single neuron differentiate logarithm likelihood respect bearing mind defined symmetric exercise show derivative respect exercise similar exercise derivative log likelihood therefore data gradient proportional difference two terms first term empirical correlation data second term correlation current model first correlation data readily evaluated empirical correlation activities real world second correlation easy evaluate estimated monte carlo methods observing average value tivity rule boltzmann machine equation iterated special case evaluate gradient exactly symmetry correlation must zero weights adjusted gradient descent learning rate one iteration weights precisely value weights given hebb rule equation trained hopfield network 
[boltzmann, machines, interpretation, boltzmann, machine, learning] one way viewing two terms gradient waking sleeping rules network awake measures correlation real world weights increased proportion copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links boltzmann machines network asleep dreams world using generative model measures correlations model world correlations determine proportional decrease weights second order correlations dream world match correlations real world two terms balance weights change figure shifter ensembles four samples plain shifter ensemble four corresponding samples labelled shifter ensemble 
[boltzmann, machines, criticism, hopfield, networks, simple, boltzmann, machines] point discussed hopfield networks boltzmann machines neurons correspond visible variables result probabilistic model optimized capture second order statistics environment second order statistics ensemble expected values pairwise products real world however often higher order correlations must included description effective often second order correlations may carry little useful information consider example ensemble binary images chairs imagine images chairs various designs four legged chairs comfy chairs chairs five legs wheels wooden chairs cushioned chairs chairs rockers instead legs child easily learn distinguish images images carrots parrots expect second order statistics raw data useless describing ensemble second order statistics capture whether two pixels likely state higher order concepts needed make good generative model images chairs simpler ensemble images high order statistics important shifter ensemble comes two flavours figure shows samples plain shifter ensemble image bottom eight pixels copy top eight pixels either shifted one pixel left unshifted shifted one pixel right top eight pixels set random ensemble simple model visual signals two eyes arriving early levels brain signals two eyes similar may differ small translations varying depth visual world ensemble simple describe second order statistics convey useful information correlation one pixel three pixels correlation two pixels zero figure shows samples labelled shifter ensemble problem made easier including extra three neu rons label visual image instance either shift left shift shift right sub ensemble extra information ensemble still learnable using second order statistics alone second order correlation label neuron image neuron zero need models capture higher order statistics environment develop models one idea might create models directly capture higher order correlations exp   ijk higher order boltzmann machines equally easy simulate using stochastic updates learning rule higher order parameters ijk equivalent learning rule copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links boltzmann machine hidden units exercise derive gradient log likelihood respect ijk possible spines found biological neurons responsible detecting correlations small numbers incoming signals however capture statistics high enough order describe ensemble images chairs well would require unimaginable number terms capture merely fourth order statistics pixel image need parameters measuring moments images good way describe derlying structure perhaps need instead addition hidden variables also known statisticians latent variables important innovation introduced hinton sejnowski idea high order correlations among visible variables described includ ing extra hidden variables sticking model second order interactions variables hidden variables induce higher order correlations visible variables 
[boltzmann, machines, boltzmann, machine, hidden, units] add hidden neurons stochastic model neurons correspond observed variables free play role probabilistic model defined equation might actually take interpretable roles effectively performing feature extraction 
[boltzmann, machines, learning, boltzmann, machines, hidden, units] activity rule boltzmann machine hidden units identical original boltzmann machine learning rule derived maximum likelihood need take account fact states hidden units unknown denote states visible units states hidden units generic state neuron either visible hidden state network visible neurons clamped state likelihood given single data example exp exp equation may also written exp differentiating likelihood find derivative spect weight difference waking term sleeping term copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links boltzmann machines first term correlation boltzmann machine simulated visible variables clamped hidden variables freely sampling conditional distribution second term correlation boltzmann machine generates samples model distribution hinton sejnowski demonstrated non trivial ensembles labelled shifter ensemble learned using boltzmann machine hidden units hidden units take role feature detectors spot patterns likely associated one three shifts boltzmann machine time consuming simulate compu tation gradient log likelihood depends taking difference two gradients found monte carlo methods boltzmann machines widespread use area active research create models embody capabilities using efficient computations hinton dayan hinton ghahramani hinton hinton teh 
[boltzmann, machines, exercise] exercise bars stripes ensemble figure learned figure four samples bars stripes ensemble sample generated first picking orientation horizontal vertical row spins orientation bar stripe respectively switching spins probability boltzmann machine hidden units may surprised 
[networks, multilayer, perceptrons] course neural networks could complete without discussion pervised multilayer networks also known backpropagation networks multilayer perceptron feedforward network input neurons hidden neurons output neurons hidden neurons may arranged sequence layers common multilayer perceptrons single hidden layer known two layer networks number two counting number layers neurons including inputs feedforward network defines nonlinear parameterized mapping input output output continuous function input parameters architecture net functional form mapping denoted feedforward networks trained perform regression classification tasks 
[networks, regression, networks] hiddens inputs outputs figure typical two layer network six inputs seven hidden units three outputs line represents one weight case regression problem mapping network one hidden layer may form hidden layer output layer example tanh runs inputs runs hidden units runs puts weights biases together make parameter vector nonlinear sigmoid function hidden layer gives neu ral network greater computational flexibility standard linear regression model graphically represent neural network set layers connected neurons figure 
[networks, sorts, functions, networks, implement?] explored weight space single neuron chapter examining functions could produce let explore weight space multilayer network figures take network one input one output large number hidden units set biases figure samples prior functions one input network sequence values bias bias one random function shown hyperparameters network copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links supervised learning multilayer networks hidden layer input output bias output input bias figure properties function produced random network vertical scale typical function produced network random weights order horizontal range function varies significantly order bias shortest horizontal length scale order function shown produced making random network hidden units gaussian weights bias weights random values plot resulting function set hidden units biases random values gaussian zero mean standard deviation bias input hidden weights random values standard deviation bias output weights random values standard deviation sort functions obtain depend values bias weights biases made bigger obtain complex functions features greater sensitivity input variable vertical scale typical function produced network random weights order horizontal range function varies significantly order bias shortest horizontal length scale order radford neal also shown limit statistical properties functions generated randomizing weights independent number hidden units interestingly complexity functions becomes independent number parameters model determines complexity typical functions characteristic magnitude weights thus anticipate fit models real data important way controlling complexity fitted function control characteristic magnitude weights figure one sample prior two input network bias figure shows one typical function produced network two inputs one output contrasted function produced traditional linear regression model flat plane neural networks create functions complexity linear regression 
[networks, regression, network, traditionally, trained] network trained using data set adjusting minimize error function objective function sum terms one input target pair measuring close output target minimization based repeated evaluation gradient gradient efficiently computed using backpropagation algorithm rumelhart uses chain rule find derivatives copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links neural network learning inference often regularization also known weight decay included modifying objective function example additional term favours small values decreases tendency model overfit noise training data rumelhart showed multilayer perceptrons trained gradient descent discover solutions non trivial problems deciding whether image symmetric networks successfully applied real world tasks varied pronouncing english text sejnowski rosenberg focussing multiple mirror telescopes angel 
[networks, neural, network, learning, inference] neural network learning process given following proba bilistic interpretation repeat generalize discussion chap ter error function interpreted defining noise model negative log likelihood exp thus use sum squared error corresponds assump tion gaussian noise target variables parameter defines noise level similarly regularizer interpreted terms log prior probability distribution parameters exp quadratic defined corresponding prior distribution gaussian variance probabilistic model specifies architecture network likelihood prior objective function corresponds inference parameters given data exp found locally minimizing interpreted locally probable parameter vector interpretation log probability adds little new stage new tools emerge proceed inferences first though let establish probabilistic interpretation classification net works tools apply copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links supervised learning multilayer networks 
[networks, binary, classification, networks] targets data set binary classification labels natural use neural network whose output bounded interpreted probability example network one hidden layer could described feedforward equations error function replaced negative log likelihood total objective function note includes parameter gaussian noise 
[networks, multi-class, classification, networks] multi class classification problem represent targets vector single element set indicating correct class elements set case appropriate use softmax network coupled outputs sum one interpreted class probabilities last part equation replaced negative log likelihood case case regression network minimization objective function corresponds inference form variety useful results built interpretation 
[networks, neural, networks] statistical perspective supervised neural networks nothing nonlinear curve fitting devices curve fitting trivial task however effective complexity interpolating model crucial importance illustrated figure consider control parameter influences complexity model example regularization constant weight decay parameter control parameter varied increase complexity model descending figure going left right across figure best fit training data model achieve becomes increasingly good however empirical performance model test error first decreases increases complex model overfits data generalizes poorly problem may also complicate choice architecture multilayer perceptron radius basis functions radial basis function network choice input vari ables multidimensional regression problem finding values model control parameters appropriate data therefore important non trivial problem copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links benefits bayesian approach supervised feedforward neural networks model control parameters training error test error model control parameters log probability training data control parameters figure optimization model complexity panels show radial basis function model interpolating simple data set one input variable one output variable regularization constant varied increase complexity model interpolant able fit training data increasingly well beyond certain point generalization ability test error model deteriorates probability theory allows optimize control parameters without needing test set overfitting problem solved using bayesian approach control model complexity give probabilistic interpretation model evaluate evidence alternative values control parameters explained chapter complex models turn less probable evidence data control parameters used objective function optimization model control parameters figure setting maximizes evidence displayed figure bayesian optimization model control parameters four important vantages test set validation set involved available training data devoted model fitting model comparison reg ularization constants optimized line simultaneously optimization ordinary model parameters bayesian objective func tion noisy contrast cross validation measure gradient evidence respect control parameters evaluated making possible simultaneously optimize large number control parameters probabilistic modelling also handles uncertainty natural manner offers unique prescription marginalization incorporating uncertainty parameters predictions procedure yields better predictions saw chapter figure shows error bars predictions trained neural network figure error bars predictions trained regression network solid line gives predictions best fit parameters multilayer perceptron trained data points error bars dotted lines produced uncertainty parameters notice error bars become larger data sparse 
[networks, implementation, bayesian, inference] mentioned chapter bayesian inference multilayer networks may implemented monte carlo sampling deterministic methods employing gaussian approximations neal mackay copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links supervised learning multilayer networks within bayesian framework data modelling easy improve probabilistic models example believe input variables problem may irrelevant predicted quantity know define new model multiple hyperparameters captures idea uncertain input variable relevance mackay neal mackay models infer automatically data relevant input variables problem 
[networks, exercises] exercise measure classifier quality written new classification algorithm want measure well performs test set compare classifiers performance measure use several standard answers let assume classifier gives output input discuss true target value simplest discussions classifiers binary variables might care consider cases general objects also widely used measure performance test set error rate fraction misclassifications made classifier measure forces classifier give output ignores additional information classifier might able offer example indication firmness prediction unfortunately error rate necessarily measure informative classifier output consider frequency tables showing joint frequency output classifier horizontal axis true variable vertical axis numbers show percentages error rate sum two diagonal numbers could call false positive rate false negative rate following three classifiers error rate greater error rate classifier classifier classifier clearly classifier simply guesses outcome cases conveying information whereas classifier informative output sure really zero chance compared prior probability classifier slightly less informative still much useful information free classifier common sense ranks classifiers best worst error rate ranks classifiers best worst one way improve error rate performance measure report pair false positive error rate false negative error rate classifiers especially important distinguish two error probabilities applications two sorts error different associated costs however couple problems error rate pair first simply told classifier error rates error rates would immediately evident classifier actually utterly worthless surely performance measure gives worst possible score copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exercises second turn multiple class classification problem digit recognition number types error increases two one possible confusion class would nice sensible way collapsing numbers single rankable number makes sense error rate another reason liking error rate give classifier credit accurately specifying uncertainty consider classifiers three outputs available rejection class indicates classifier sure consider classifiers following frequency tables percentages classifier classifier classifiers equally good classifiers compare classifier two classifiers equiva lent disguise could make taking output tossing coin says order decide whether give output equal thus inferior compare justify suggestion informative classifier thus superior yet scores error rate rejection rate figure error reject curve people use area curve measure classifier quality people often plot error reject curves also known roc curves roc stands receiver operating characteristic show total versus allowed vary use curves compare classifiers figure special case binary classification problems may plotted versus instead seen error rates undiscerning performance measures plotting one error rate function another make weakness error rates away exercise either construct explicit example demonstrating error reject curve area necessarily good ways compare classifiers prove suggested alternative method comparing classifiers consider mutual information output target log measures many bits classifier output conveys target evaluate mutual information classifiers investigate performance measure discuss whether useful one practical drawbacks 
[gaussian, processes] publication rumelhart hinton williams paper supervised learning neural networks surge interest empirical modelling relationships high dimensional data using nonlinear parametric models multilayer perceptrons radial basis functions bayesian interpretation modelling methods nonlinear func tion parameterized parameters assumed underlie data adaptation model data corresponds inference function given data denote set input vectors set corresponding target values vector inference described posterior probability distribution two terms right hand side first probability target values given function case regression problems often assumed separable gaussian distribution second term prior distribution functions assumed model prior implicit choice parametric model choice regularizers used model fitting prior typically specifies function expected continuous smooth less high frequency power low frequency power precise meaning prior somewhat obscured use parametric model prediction future values matters sumed prior assumed noise model parameterization function irrelevant idea gaussian process modelling place prior directly space functions without parameterizing simplest type prior functions called gaussian process thought generalization gaussian distribution finite vector space function space infinite dimension gaussian distribution fully specified mean covariance matrix gaussian process specified mean covariance function mean function often take zero function covariance function expresses expected covariance values function points function one data modelling problem assumed single sample gaussian distribution gaussian processes already well established models various spatial temporal problems example brownian motion langevin processes wiener processes examples gaussian processes kalman filters widely used copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links gaussian processes model speech waveforms also correspond gaussian process models method kriging geostatistics gaussian process regression method 
[gaussian, processes, reservations, gaussian, processes] might thought possible reproduce interesting prop erties neural network interpolation methods something simple gaussian distribution shall see many popular nonlinear inter polation methods equivalent particular gaussian processes use term interpolation cover problem regression fitting curve noisy data task fitting interpolant passes exactly given data points might also thought computational complexity inference work priors infinite dimensional function spaces might infinitely large concentrating joint probability distribution observed data quantities wish predict possible make predictions resources scale polynomial functions number data points 
[gaussian, processes, problem] given data points inputs vec tors fixed input dimension targets either real numbers case task regression interpolation task categorical variables example case task clas sification task concentrate case regression time assuming function underlies observed data task infer function given data predict function value value observation new point 
[gaussian, processes, parametric, approaches, problem] parametric approach regression express unknown function terms nonlinear function parameterized parameters example fixed basis functions using set basis functions write basis functions nonlinear functions radial basis functions centred fixed points exp nonlinear function however since dependence parameters linear might sometimes refer linear model neural network terms model like multilayer network whose connections input layer nonlinear hidden layer fixed output weights adaptive possible sets fixed basis functions include polynomials integer powers depend copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links standard methods nonlinear regression example adaptive basis functions alternatively might make func tion basis functions depend additional parameters included vector two layer feedforward neural network nonlinear hidden units linear output function written tanh dimensionality input space weight vector consists input weights hidden unit biases output weights output bias model dependence nonlinear chosen parameterization infer function inferring parameters posterior probability parameters factor states probability observed data points parameters hence function known proba bility distribution often taken separable gaussian data point differing underlying value additive noise factor specifies prior probability distribution parameters often taken separable gaussian distribution dependence nonlinear posterior distribution general gaussian distribution inference implemented various ways laplace method minimize objective function respect locating locally probable parameters use curvature define error bars alternatively use general markov chain monte carlo techniques create samples posterior distribution obtained one representations inference given data predictions made marginalizing parameters gaussian representation posterior integral typically evaluated directly alternative monte carlo approach generates samples intended samples posterior distribution approximate predictive distribution copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links gaussian processes 
[gaussian, processes, nonparametric, approaches] nonparametric methods predictions obtained without explicitly rameterizing unknown function lives infinite dimensional space continuous functions one well known nonparametric proach regression problem spline smoothing method kimeldorf wahba spline solution one dimensional regression problem described follows define estimator function minimizes functional pth derivative positive number set resulting function cubic spline piecewise cubic function knots discontinuities second derivative data points estimation method interpreted bayesian method iden tifying prior function const probability data measurements assuming inde pendent gaussian noise const constants equations functions spectively strictly prior improper since addition arbitrary polynomial degree constrained impropriety easily rectified addition appropriate terms given interpretation functions equation equal nus log posterior probability within additive constant splines estimation procedure interpreted yielding bayesian map estimate bayesian perspective allows additionally put error bars splines estimate draw typical samples posterior distribution gives automatic method inferring hyperparameters 
[gaussian, processes, comments] assuming finding derivatives priors straightforward search however two problems need aware firstly illustrated figure evidence may multimodal suitable priors sensible optimization strategies often eliminate poor tima secondly perhaps importantly evaluation gradi ent log likelihood requires evaluation exact inversion method cholesky decomposition decomposition gauss jordan elimination associated computational cost order calculating gradients becomes time consuming large training data sets proximate methods implementing predictions equations gradient computation equation active research area one approach based ideas skilling makes approxima tions trace using iterative methods cost gibbs mackay gibbs references topic given end chapter copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links classification 
[gaussian, processes, linear, models] let consider regression problem using fixed basis functions example one dimensional radial basis functions defined equation let assume list input points specified define matrix matrix values basis functions points define vector vector values points prior distribution gaussian zero mean normal linear function also gaussian distributed mean zero covariance matrix hyy hrww hww prior distribution normal normal result vector function values gaussian distribu tion true selected points defining property gaussian process probability distribution function gaus sian process finite selection points density gaussian number basis functions smaller number data points matrix full rank case probability distribution might thought flat elliptical pancake confined dimensional subspace dimensional space lives target values target assumed differ additive gaussian noise variance corresponding function value also gaussian prior distribution normal denote covariance matrix whether full rank covariance matrix full rank since full rank covariance matrix look like general entry copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links parametric models gaussian processes entry otherwise example let take example one dimensional case radial basis functions expression becomes simplest assume uniformly spaced basis functions basis function labelled cen tred point take limit sum becomes integral avoid covariance diverges better make scale number basis functions per unit length axis constant max min max min exp exp let limits integration solve integral exp arriving new perspective interpolation problem instead specifying prior distribution functions terms basis functions priors parameters prior summarized simply covariance function exp given new name constant front generalizing particular case vista interpolation methods opens given valid covariance function discuss moment valid means define covariance matrix function values locations matrix given covariance matrix corresponding target values assuming gaus sian noise matrix given conclusion prior probability target values data set normal samples gaussian process simple gaussian processes displayed figure copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links gaussian processes exp exp exp sin exp figure samples drawn gaussian process priors panel shows two functions drawn gaussian process prior four corresponding covariance functions given plot decrease lengthscale produces rapidly fluctuating functions periodic properties covariance function seen covariance function contains non stationary term corresponding covariance straight line typical functions include linear trends gibbs 
[gaussian, processes, multilayer, neural, networks, gaussian, processes] figures show random samples prior distribution functions defined selection standard multilayer perceptrons large numbers hidden units samples seem million miles away gaussian process samples figure indeed neal showed properties neural network one hidden layer equation converge gaussian process number hidden neurons tends infinity standard weight decay priors assumed covariance function gaussian process depends details priors assumed weights network activation functions hidden units 
[gaussian, processes, using, given, gaussian, process, model, regression] spent time talking priors return data problem prediction make predictions gaussian process formed covariance matrix defined equation task infer given observed vector joint density gaussian conditional distribution also gaussian distinguish different sizes covariance matrix subscript covariance copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links examples covariance functions matrix vector define submatrices follows             posterior distribution given exp evaluate mean standard deviation posterior distribution brute force inversion elegant expression predictive distribution however useful whenever predictions made number new points basis data set size write terms using partitioned inverse equations barnett substitute matrix equation find exp predictive mean new point given defines error bars prediction notice need invert order make predictions needs inverted thus gaussian processes allow one implement model number basis functions much larger number data points com putational requirement order independent discuss ways reducing cost later predictions produced gaussian process depend entirely covariance matrix discuss sorts covariance functions one might choose define automate selection covariance function response data 
[gaussian, processes, examples, covariance, functions] constraint choice covariance function must gen erate non negative definite covariance matrix set points copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links gaussian processes denote parameters covariance function covariance matrix entries given covariance function noise model might stationary spatially varying example input independent noise exp input dependent noise continuity properties determine continuity properties typical samples gaussian process prior encyclopaedic paper gaus sian processes giving many valid covariance functions written abrahamsen 
[gaussian, processes, stationary, covariance, functions] stationary covariance function one translation invariant satisfies function covariance function separation also known autocovariance function additionally depends magnitude distance covariance function said homogeneous stationary covariance functions may also described terms fourier transform function known power spectrum gaussian process fourier transform necessarily positive function frequency one way constructing valid stationary covariance function invent positive function frequency define inverse fourier transform example let power spectrum gaussian function frequency since fourier transform gaussian gaussian autoco variance function corresponding power spectrum gaussian function separation argument rederives covariance function derived equation generalizing slightly popular form hyperparameters exp dimensional vector lengthscale associated input lengthscale direction expected vary significantly large lengthscale means expected essentially constant function input input could said irrelevant automatic relevance determination method neural networks mackay neal hyperparameter defines vertical scale variations typical function hyperparameter allows whole function offset away zero unknown constant understand term examine equation consider basis function another stationary covariance function exp copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links adaptation gaussian process models figure multimodal likelihood functions gaussian processes data set five points modelled simple covariance function one hyperparameter controlling noise variance panels show probable interpolant error bars hyperparameters set two different values locally maximize likelihood panel shows contour plot likelihood function two maxima shown crosses gibbs special case previous covariance function typical functions prior smooth analytic functions typical functions continuous smooth covariance function models function periodic known period input direction exp     sin   figure shows random samples drawn gaussian processes variety different covariance functions 
[gaussian, processes, nonstationary, covariance, functions] simplest nonstationary covariance function one corresponding linear trend consider plane gaussian distributions zero mean variances respectively plane covariance function lin example random sample functions incorporating linear term seen figure 
[gaussian, processes, adaptation, gaussian, process, models] let assume form covariance function chosen depends undetermined hyperparameters would like learn copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links gaussian processes hyperparameters data learning process equivalent inference hyperparameters neural network example weight decay hyperparameters complexity control problem one solved nicely bayesian occam razor ideally would like define prior distribution hyperparameters integrate order make predictions would like find integral usually intractable two approaches take approximate integral using probable values hyperparameters perform integration numerically using monte carlo methods williams rasmussen neal either approaches implemented efficiently gradient posterior probability evaluated 
[gaussian, processes, gradient] posterior probability log first term evidence hyperparameters det derivative respect hyperparameter trace 
[gaussian, processes, classification] gaussian processes integrated classification modelling identify variable sensibly given gaussian process prior binary classification problem define quantity probability class rather large positive values correspond probabilities close one large neg ative values define probabilities close zero classifica tion problem typically intend probability smoothly varying function embody prior belief defining gaussian process prior 
[gaussian, processes, implementation] easy perform inferences adapt gaussian process model data classification model regression problems like lihood function gaussian function posterior distribution given observations gaussian normal ization constant cannot written analytically barber williams implemented classifiers based gaussian process priors using laplace approximations chapter neal implemented monte carlo approach implementing gaussian process classifier gibbs mackay implemented another cheap cheerful approach based methods jaakkola jordan section varia tional gaussian process classifier obtain tractable upper lower bounds unnormalized posterior density bounds parameterized variational parameters adjusted order obtain tightest possible fit using normalized versions optimized bounds compute approximations predictive distributions multi class classification problems also solved monte carlo methods neal variational methods gibbs 
[gaussian, processes, discussion] gaussian processes moderately simple implement use parameters model need determined hand generally priors hyperparameters gaussian processes useful tools automated tasks fine tuning problem possible appear sacrifice performance simplicity easy construct gaussian processes particular desired properties example make straightforward automatic relevance determination model one obvious problem gaussian processes computational cost associated inverting matrix cost direct methods inversion becomes prohibitive number data points greater 
[gaussian, processes, thrown, baby, bath, water?] according hype neural networks meant intelligent models discovered features patterns data gaussian processes copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links gaussian processes contrast simply smoothing devices gaussian processes possi bly replace neural networks neural networks hyped underestimated power smoothing methods think propositions true success gaussian processes shows many real world data modelling problems perfectly well solved sensible smoothing methods interesting problems task feature discovery example ones gaussian processes solve maybe multilayer perceptrons solve either perhaps fresh start needed approaching problem machine learning paradigm different supervised feedforward mapping 
[gaussian, processes, reading] study gaussian processes regression far new time series analysis performed astronomer thiele using gaussian processes lauritzen wiener kolmogorov pre diction theory introduced prediction trajectories military targets wiener within geostatistics field matheron proposed framework regression using optimal linear estimators called krig ing krige south african mining engineer framework identical gaussian process approach regression kriging developed considerably last thirty years see cressie view including several bayesian treatments omre kitanidis however geostatistics approach gaussian process model con centrated mainly low dimensional problems largely ignored probabilistic interpretation model kalman filters widely used implement inferences stationary one dimensional gaussian processes popular models speech music modelling bar shalom fort mann generalized radial basis functions poggio girosi arma models wahba variable metric kernel methods lowe closely related gaussian processes see also hagan idea replacing supervised neural networks gaussian processes first explored williams rasmussen neal thorough comparison gaussian processes methods neural networks mars made rasmussen methods reducing complexity data modelling gaussian processes remain active research area poggio girosi luo wahba tresp williams seeger smola bartlett rasmussen seeger opper winther longer review gaussian processes mackay review paper regression complexity control using hierarchical bayesian models mackay gaussian processes support vector learning machines scholkopf vapnik lot common kernel based predictors kernel another name covariance function bayesian version support vectors exploiting connection found chu chu chu chu 
[deconvolution, optimal, linear, filters] many imaging problems data measurements linearly related underlying image vector denotes inevitable noise corrupts real data case camera produces blurred picture vector denotes true image denotes blurred noisy picture linear operator convolution defined point spread function camera special case true image data vector reside space important maintain distinction use subscript run data measurements subscripts run image pixels one might speculate since blur created linear operation perhaps might deblurred another linear operation derive optimal linear filter two ways 
[deconvolution, bayesian, derivation] assume linear operator known noise gaussian independent known standard deviation exp assume prior probability image also gaussian scale parameter det exp   assume correlations among pixels symmetric full rank matrix equal identity matrix sophisticated intrinsic correlation function model uses convolution takes imaginary hidden image uncorrelated real correlated image intrinsic correlation function confused point spread function defines image data mapping copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links deconvolution zero mean gaussian prior clearly poor assumption known elements image positive let proceed write posterior probability image given data words posterior likelihood prior evidence evidence normalizing constant posterior distribution unimportant used sophisticated analysis compare example different values different point spread functions since posterior distribution product two gaussian functions also gaussian therefore summarized mean also probable image covariance matrix log defines joint error bars equation symbol denotes differentiation respect image parameters find differentiating log posterior solving derivative zero obtain operator called optimal linear filter term neglected optimal linear filter pseudoinverse term regularizes ill conditioned inverse optimal linear filter also manipulated form optimal linear filter 
[deconvolution, minimum, square, error, derivation] non bayesian derivation optimal linear filter starts assuming estimate true image linear function data linear operator optimized minimizing expected sum squared error unknown true image following equa tions summations repeated indices implicit expectation statistics random variables ensemble images expect bump assume noise zero mean uncorrelated second order everything else hei copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links traditional image reconstruction methods differentiating respect introducing bayesian derivation find optimal linear filter opt rfr identify obtain optimal linear filter bayesian derivation hoc assumptions made derivation choice quadratic error measure decision use linear estimator interesting without explicit assumptions gaussian distributions derivation reproduced estimator bayesian posterior mode advantage bayesian approach criticize sumptions modify order make better reconstructions 
[deconvolution, image, models] better matched model images real world bet ter image reconstructions less data need answer given question gaussian models lead optimal linear filter spectacularly poorly matched real world example gaussian prior fails specify pixel intensities image positive omission leads pronounced artefacts age observation high contrast large black patches optimal linear filters applied astronomical data give reconstructions negative areas corresponding patches sky suck energy telescopes maximum entropy model image deconvolution gull daniell great success principally model forced reconstructed image positive spurious negative areas complementary spu rious positive areas eliminated quality reconstruction greatly enhanced classic maximum entropy model assigns entropic prior classic exp skilling model enforces positivity parameter defines characteristic dynamic range pixel values expected differ default image intrinsic correlation function maximum entropy model gull introduces expectation spatial correlations prior writing convolution intrinsic correlation function putting classic maxent prior underlying hidden image 
[deconvolution, probabilistic, movies] found probable image also error bars one task visualize error bars whether use monte carlo methods infer correlated random walk around posterior distribution used visualize uncertainties correlations gaussian posterior distribution create correlated sequence unit normal random vectors using copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links deconvolution unit normal random vector controls persistent memory sequence render image sequence defined cholesky decomposition 
[deconvolution, supervised, neural, networks, image, deconvolution] neural network researchers often exploit following strategy given prob lem currently solved standard algorithm interpret computations performed algorithm parameterized mapping input output call mapping neural network adapt parameters data produce another mapping solves task better construction neural network reproduce standard algorithm data driven adaptation make performance better several reasons standard algorithms bettered way algorithms often designed optimize real objective func tion example speech recognition hidden markov model designed model speech signal fitted maximize generative probability given known string words training data real objective discriminate different words inadequate model used neural net style training model focus limited resources model aspects relevant discrimination task discriminative training hidden markov models speech recognition improve performance neural network flexible standard model adaptive parameters might viewed fixed features original designers flexible network find properties data included original model 
[deconvolution, deconvolution, humans] huge fraction brain devoted vision one neglected features visual system raw image falling retina severely blurred people see resolution arcminute one sixtieth degree daylight conditions bright dim image retina blurred point spread function width large arcminutes wald griffin howarth bradley amazing able resolve pixels twenty five times smaller area blob produced retina point source isaac newton aware conundrum hard make lens chromatic aberration cornea lens like lens made ordinary glass refract blue light strongly red typically eyes focus correctly middle visible spectrum green look single white dot made red green blue light image retina consists sharply focussed green dot surrounded broader red blob superposed even broader blue blob width red blue blobs proportional diameter pupil largest dim lighting conditions blobs roughly concentric though people slight bias one eye red blob centred tiny distance copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links deconvolution humans left blue centred tiny distance right eye way round slight bias explains look blue red writing dark background people perceive blue writing slightly greater depth red minority people small bias way round red blue depth perception reversed effect many people aware noticed cinemas example tiny compared chromatic aberration discussing vividly demonstrate enormous chromatic aber ration eye help sheet card colour computer screen impressive results guarantee amazed use dim room light apart computer screen pretty strong effect still seen even room daylight coming long bright sunshine cut slit wide card screen display small coloured objects black background especially recommend thin vertical objects coloured pure red pure blue magenta red plus blue white red plus blue plus green include little black white text screen stand sit sufficiently far away read text perhaps distance four metres normal vision hold slit vertically front one eyes close eye hold slit near eye brushing eyelashes look waggle slit slowly left right slit alternately front left right sides pupil see see red objects waggling fro blue objects waggling fro huge distances opposite directions white objects appear stay still negligibly distorted thin magenta objects seen splitting constituent red blue parts measure large motion red blue objects minutes arc dim room check sharply see conditions look text screen example case see whole pupil features far smaller distance red blue components waggling yet using whole pupil falling retina must image blurred blurring diameter equal waggling amplitude one main functions early visual processing must deconvolve chromatic aberration neuroscientists sometimes conjecture rea son retinal ganglion cells cells lateral geniculate nucleus main brain area retinal ganglion cells project centre surround receptive fields colour opponency long wavelength centre medium wavelength surround example order perform fea ture extraction edge detection think view mistaken reason centre surround filters first stage visual processing fovea least huge task deconvolution chromatic aber ration speculate mccollough effect extremely long lasting associ ation colours orientation mccollough mackay mackay produced adaptation mechanism tunes chromatic aberration deconvolution circuits deconvolution circuits need rapidly tuneable point spread function eye changes pupil diameter change within seconds indeed mccollough effect induced within seconds time effect long lasting http www inference phy cam mackay itila files html copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links deconvolution eye covered interests deconvolution circuits stay well tuned sleep see sharply instant wake also wonder whether main reason evolved colour vision order see fruit better able see black white sharper deconvolving chromatic aberration easier even entirely black white world one access chromatic information image final speculation eyes make micro saccades look things miniature eye movements angular size big ger spacing cones fovea spaced roughly minute arc perceived resolution eye typical size microsaccade minutes arc ratliff riggs coincidence size chromatic aberration surely micro saccades must play essential role deconvolution mechanism delivers high resolution vision 
[deconvolution, exercises] exercise blur image circular top hat point spread func tion add noise deconvolve blurry noisy image using optimal linear filter find error bars visualize making probabilistic movie 
[low-density, parity-check, codes] low density parity check code gallager code block code parity check matrix every row column sparse regular gallager code low density parity check code every column weight every row weight reg ular gallager codes constructed random subject constraints low density parity check code illustrated figure figure low density parity check matrix corresponding graph rate low density parity check code blocklength constraints white circle represents transmitted bit bit participates constraints represented squares constraint forces sum bits connected even 
[low-density, parity-check, codes, theoretical, properties] low density parity check codes lend theoretical study fol lowing results proved gallager mackay low density parity check codes spite simple construction good codes given optimal decoder good codes sense section furthermore good distance sense section two results hold column weight furthermore sequences low density parity check codes increases gradually way ratio still goes zero good good distance however optimal decoder decoding low density parity check codes complete problem practice 
[low-density, parity-check, codes, practical, decoding] given channel output wish find codeword whose likelihood biggest effective decoding strategies low density parity check codes message passing algorithms best algorithm known sum product algorithm also known iterative probabilistic decoding belief propagation assume channel memoryless channel though com plex channels easily handled running sum product algorithm complex graph represents expected correlations among errors worthen stark memoryless channel two approaches decoding problem lead generic problem find maximizes separable distribution binary vector another binary vector two approaches represents decoding problem terms factor graph chapter copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links low density parity check codes prior distribution codewords variable nodes transmitted bits node represents factor mod posterior distribution codewords upper function node represents likelihood factor joint probability noise syndrome top variable nodes noise bits added variable nodes base syndrome values definition mod enforced factor figure factor graphs associated low density parity check code codeword decoding viewpoint first note prior distribution codewords mod represented factor graph figure factorization mod omit mod posterior distribution code words given multiplying prior likelihood introduces another factors one received bit factor graph corresponding function shown figure graph prior except addition likelihood dongles transmitted bits viewpoint received signal live alphabet matters values syndrome decoding viewpoint alternatively view channel output terms binary received vector noise vector probability distribution derived channel properties whatever additional information available channel outputs example binary symmetric channel define noise syndrome noise model channels gaussian channel output may define received copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links decoding sum product algorithm binary vector however wish obtain effective binary noise model exercises joint probability noise syndrome factored factor graph function shown figure variables also drawn belief network also known bayesian network causal network influence diagram similar figure arrows edges upper circular nodes represent variables lower square nodes represent variables say every bit parent checks check child bits decoding viewpoints involve essentially graph either ver sion decoding problem expressed generic decoding problem find maximizes codeword decoding viewpoint codeword syndrome decoding viewpoint noise syndrome matter viewpoint take apply sum product algorithm two decoding algorithms isomorphic give equiva lent outcomes unless numerical errors intervene tend use syndrome decoding viewpoint one advantage one need implement encoder code order able simulate decoding problem realistically talk terms generic decoding problem 
[low-density, parity-check, codes, decoding, sum–product, algorithm] aim given observed checks compute marginal posterior proba bilities hard compute exactly graph contains many cycles however interesting implement decoding algorithm would appropriate cycles assumption errors introduced might relatively small proach ignoring cycles used artificial intelligence literature frowned upon produces inaccurate probabilities ever decoding good error correcting code care accurate marginal probabilities want correct codeword also posterior probability case good code communicating achievable rate expected typically hugely concentrated probable decoding dealing distinctive probability distribution experience gained fields may apply sum product algorithm presented chapter write explicitly works solving decoding problem mod brevity reabsorb dongles hanging nodes fig ure modify sum product algorithm accordingly graph copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links low density parity check codes live original graph figure whose edges defined graph contains nodes two types call checks bits graph connecting checks bits bipartite graph bits connect checks vice versa iteration prob ability ratio propagated along edge graph bit node updates probability state denote set bits participate check similarly define set checks bit participates denote set bit excluded algorithm two alternating parts quantities associated edge graph iteratively updated quantity meant probability bit value given information obtained via checks check quantity meant probability check satisfied bit considered fixed bits separable distribution given probabilities algorithm would produce exact posterior probabilities bits fixed number iterations bipartite graph defined matrix contained cycles initialization let prior probability bit let taking syndrome decoding viewpoint channel binary symmetric channel equal noise level varies known way example channel binary input gaussian channel real output initialized appropriate normalized likelihood every variables initialized values respectively horizontal step horizontal step algorithm horizontal point view matrix run checks compute two probabilities first probability observed value arising given bits separable distribution given probabilities defined second probability observed value arising defined conditional probabilities summations either zero one pending whether observed matches hypothesized values probabilities computed various obvious ways based equation computations may done efficiently large regarding final state markov chain states chain started state undergoing transitions corresponding additions various transition probabilities given corresponding probabilities observed value given either found efficiently use forward backward algorithm section copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links decoding sum product algorithm particularly convenient implementation method uses forward backward passes products differences computed obtain identity identity derived iterating following observation mod probabilities thus recover using transformations differences back may viewed fourier transform inverse fourier transformation vertical step vertical step takes computed values updates values probabilities compute chosen products efficiently computed downward pass upward pass also compute pseudoposterior probabilities iteration given quantities used create tentative decoding consistency used decide whether decoding algorithm halt halt point algorithm repeats horizontal step stop done decoding method recommended decod ing procedure set see checks mod satisfied halting declaring failure maximum number iterations occurs without successful decoding event failure may still report flag whole block failure note passing difference decoding procedure widespread practice turbo code community decoding algorithm run fixed number iterations irrespective whether decoder finds consistent state earlier time practice wasteful computer time blurs distinction undetected detected errors procedure undetected errors occur decoder finds copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links low density parity check codes figure demonstration encoding rate gallager code encoder derived sparse parity check matrix three per column figure code creates transmitted vectors consisting source bits parity check bits source sequence altered changing first bit notice many parity check bits changed parity bit depends half source bits transmission case vector difference modulo transmissions dilbert image copyright united feature syndicate inc used permission 
[low-density, parity-check, codes, parity, bits] satisfying mod equal true detected errors occur algorithm runs maximum number iterations without finding valid decoding undetected errors scientific interest reveal distance properties code engineering practice would seem preferable blocks known contain detected errors labelled practically possible cost brute force approach time create generator matrix scales block size encoding time scales encoding involves binary arithmetic block lengths studied takes considerably less time simulation gaussian channel decoding involves approximately floating point multiplies per iteration total number operations per decoded bit assuming iterations independent blocklength codes presented next section operations encoding complexity reduced clever encoding tricks invented richardson urbanke specially constructing parity check matrix mackay decoding complexity reduced small loss perfor mance passing low precision messages place real numbers richardson urbanke 
[low-density, parity-check, codes, pictorial, demonstration, gallager, codes] figures illustrate visually conditions low density parity check codes give reliable communication binary symmetric channels gaussian channels demonstrations may viewed animations world wide web http www inference phy cam mackay codes gifs copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links pictorial demonstration gallager codes figure low density parity check matrix columns weight rows weight 
[low-density, parity-check, codes, encoding] figure illustrates encoding operation case gallager code whose parity check matrix matrix three per col umn figure high density generator matrix illustrated figure showing change transmitted vector one source bits altered course source images shown highly redundant images really compressed encoding redundant images chosen demonstrations make easier see correction process iterative decoding decod ing algorithm take advantage redundancy source vector would work exactly way irrespective choice source vector 
[low-density, parity-check, codes, iterative, decoding] transmission sent channel noise level received vector shown upper left figure subsequent pictures figure show iterative probabilistic decoding process sequence figures shows best guess bit bit given iterative decoder iterations decoder halts iteration best guess violates parity checks final decoding error free case unusually noisy transmission decoding algorithm fails find valid decoding code channel failures happen every transmissions figure shows error rate compared block error rates classical error correcting codes copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links low density parity check codes figure iterative probabilistic decoding low density parity check code transmission received channel noise level sequence figures shows best guess bit bit given iterative decoder iterations decoder halts iteration best guess violates parity checks final decoding error free received decoded probability decoder error rate shannon limit low density parity check code figure error probability low density parity check code error bars binary symmetric channel compared algebraic codes squares repetition codes hamming code points reed muller bch codes copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links pictorial demonstration gallager codes figure demonstration gallager code gaussian channel received vector transmission gaussian channel greyscale represents value normalized likelihood transmission perfectly decoded sum product decoder empirical probability decoding failure probability distribution output channel two possible inputs received transmission gaussian channel corresponds shannon limit probability distribution output channel two possible inputs figure performance rate gallager codes gaussian channel vertical axis block error probability horizontal axis signal noise ratio dependence blocklength codes left right dashed lines show frequency undetected errors measurable blocklength small dependence column weight codes blocklength 
[low-density, parity-check, codes, gaussian, channel] figure left picture shows received vector transmission gaussian channel greyscale represents value normalized likelihood signal noise ratio noise level rate gallager code communicates reliably probability error show close shannon limit right panel shows received vector signal noise ratio reduced corresponds shannon limit codes rate 
[low-density, parity-check, codes, variation, performance, code, parameters] figure shows parameters affect performance low density parity check codes shannon would predict increasing blocklength leads improved performance dependence follows different pattern given optimal decoder best performance would obtained codes closest random codes codes largest however sum product decoder makes poor progress dense graphs best performance obtained small value among values copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links low density parity check codes figure schematic illustration constructions completely regular gallager code nearly regular gallager code rate notation integer represents number permutation matrices superposed surrounding square diagonal line represents identity matrix figure monte carlo simulation density evolution following decoding process curve shows average entropy bit function number iterations estimated monte carlo algorithm using samples per iteration noise level binary symmetric channel increases steps bottom graph top graph evidently threshold algorithm cannot determine mackay shown figure best blocklength block error probability observation motivates construction gallager codes col umns weight construction columns weight shown figure many columns weight code becomes much poorer code discuss later even better making code even irregular 
[low-density, parity-check, codes, density, evolution] one way study decoding algorithm imagine running infinite tree like graph local topology gallager code graph figure local topology graph gallager code column weight row weight white nodes represent bits black nodes represent checks edge corresponds larger matrix closer decoding properties approach infinite graph imagine infinite belief network loops every bit connects checks every check connects bits figure consider iterative flow information network examine average entropy one bit function number iterations iteration bit accumulated information local network radius equal number iterations successful decoding occur average entropy bit decreases zero number iterations increases iterations infinite belief network simulated monte carlo methods technique first used gallager imagine network radius total number iterations centred one bit aim compute conditional entropy central bit given state checks radius evaluate probability central bit given particular syndrome involves step propagation outside network centre ith iteration probabilities copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links improving gallager codes radius transformed radius way depends states unknown bits radius monte carlo method rather simulating network exactly would take time grows exponentially create iteration representative sample size say values case iteration iteration figure tree fragment constructed monte carlo simulation density evolution fragment appropriate regular gallager code regular network parameters new pair list ith iteration created drawing new distribution drawing random replacement pairs list iteration assembled tree fragment figure sum product algorithm run top bottom find new value associated new node example results runs noise densities using samples iteration shown figure runs low enough noise level show collapse zero entropy small number iterations high noise level decrease non zero entropy corresponding failure decode boundary two behaviours called threshold decoding algorithm binary symmetric channel figure shows monte carlo simulation threshold regular codes richardson urbanke derived thresholds regular codes tour force direct analytic methods thresholds shown table max table thresholds max regular low density parity check codes assuming sum product decoding algorithm richardson urbanke shannon limit rate codes max 
[low-density, parity-check, codes, approximate, density, evolution] practical purposes computational cost density evolution reduced making gaussian approximations probability distributions messages density evolution updating parameters approximations information techniques produce diagrams known exit charts see ten brink chung ten brink 
[low-density, parity-check, codes, improving, gallager, codes] since rediscovery gallager codes two methods found enhancing performance binary table translation binary message symbols 
[low-density, parity-check, codes, clump, bits, checks, together] first make gallager codes variable nodes grouped together metavariables consisting say binary variables check nodes similarly grouped together metachecks sparse graph constructed connecting metavariables metachecks lot freedom details variables checks within wired one way set wiring work finite field define low density parity check matrices using elements translate binary messages using mapping one given table messages passed decoding messages probabilities likelihoods conjunctions binary variables example clump contains three binary variables likelihoods describe likelihoods eight alternative states bits carefully optimized constructions resulting codes binary table translation binary matrix entries parity check matrix turned binary parity check matrix way copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links low density parity check codes algorithm fourier transform fourier transform function given transforms viewed sequence binary transforms dimensions inverse transform identical fourier transform except also divide figure comparison regular binary gallager codes irregular codes codes outstanding codes rate left best performance right irregular low density parity check code blocklength bits davey jpl turbo code jpl blocklength regular low density parity check blocklength bits davey mackay irregular binary low density parity check code blocklength bits davey luby irregular binary low density parity check code blocklength bits jpl code galileo best known code rate regular binary low density parity check code blocklength bits mackay shannon limit even better sparse graph codes constructed empirical bit error probability signal noise ratio turbo irreg reg luby irreg reg gallileo perform nearly one decibel better comparable binary gallager codes computational cost decoding scales log propriate fourier transform used check nodes update rule check variable message   convolution quantities summation replaced product fourier transforms followed inverse fourier transform fourier transform shown algorithm 
[low-density, parity-check, codes, make, graph, irregular] second way improving gallager codes introduced luby make graphs irregular instead giving variable nodes degree variable nodes degree degree check nodes also given unequal degrees helps improve performance erasure channels turns gaussian channel best graphs regular check degrees figure illustrates benefits offered two methods proving gallager codes focussing codes rate making binary code irregular gives win switching gives copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links fast encoding low density parity check codes difference set cyclic codes gallager dsc figure algebraically constructed low density parity check code satisfying many redundant constraints outperforms equivalent random gallager code table shows distance row weight difference set cyclic codes highlighting codes large small large comparison gallager code rate identical difference set cyclic code vertical axis block error probability horizontal axis signal noise ratio matthew davey code combines features irregular gives win regular binary gallager code methods optimizing profile gallager code number rows columns degree developed richardson led low density parity check codes whose performance decoded sum product algorithm within hair breadth shannon limit 
[low-density, parity-check, codes, algebraic, constructions, gallager, codes] performance regular gallager codes enhanced third man ner designing code redundant sparse constraints difference set cyclic code example code satisfies low weight constraints figure impossible make random gallager codes anywhere near much redundancy among checks difference set cyclic code performs better equivalent random gallager code open problem discover codes sharing remarkable properties difference set cyclic codes different blocklengths rates call task tanner challenge 
[low-density, parity-check, codes, fast, encoding, low-density, parity-check, codes] discuss methods fast encoding low density parity check codes faster standard method generator matrix found gaussian elimination cost order block encoded multiplying cost order 
[low-density, parity-check, codes, staircase, codes] certain low density parity check matrices columns weight less encoded easily linear time example matrix staircase structure illustrated right hand side         copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links low density parity check codes data loaded first bits parity bits computed left right linear time call two parts matrix describe encoding operation two steps first compute intermediate parity vector pass accumulator create cost encoding method linear sparsity exploited computing sums 
[low-density, parity-check, codes, fast, encoding, general, low-density, parity-check, codes] richardson urbanke demonstrated elegant method encoding cost low density parity check code reduced straightforward method cost gap hopefully small constant worst cases scales small fraction figure parity check matrix approximate lower triangular form first step parity check matrix rearranged row interchange column interchange approximate lower triangular form shown figure original matrix sparse six matrices also sparse matrix lower triangular everywhere diagonal source vector length encoded transmission follows compute upper syndrome source vector done linear time find setting second parity bits upper syn drome zero vector found linear time back substitution com puting first bit second third forth copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links reading compute lower syndrome vector done linear time get clever bit define matrix find inverse computation needs done cost order inverse dense matrix invertible either full rank else column permutations produce invertible set first parity bits operation cost order claim point found correct setting first parity bits discard tentative parity bits find new upper syndrome done linear time find setting second parity bits upper syndrome zero vector found linear time back substitution 
[low-density, parity-check, codes, reading] low density parity check codes codes first studied gallager generally forgotten coding theory community tanner generalized gallager work introducing general constraint nodes codes called turbo product codes fact called tanner product codes since tanner proposed colleagues karplus krit implemented hardware publications gallager codes contributing rebirth include wiberg mackay neal mackay neal wiberg mackay spielman sipser spielman low precision decoding algorithms fast encoding algorithms gallager codes discussed richardson banke richardson urbanke mackay davey showed low density parity check codes outperform reed solomon codes even reed solomon codes home turf high rate short block lengths important papers include luby luby luby davey mackay richardson chung useful tools design irregular low density parity check codes include chung urbanke see wiberg frey mceliece discussion sum product algorithm view low density parity check code decoding terms group theory coding theory see forney offer soljanin offer copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links low density parity check codes soljanin background reading topic see hartmann rudolph terras growing literature prac tical design low density parity check codes mao banihashemi mao banihashemi ten brink adopted applications hard drives satellite communications low density parity check codes applicable quantum error correction see mackay 
[low-density, parity-check, codes, exercises] exercise hyperbolic tangent version decoding algorithm section sum product decoding algorithm low density parity check codes presented first terms quantities terms quantities third description replaced log probability ratios show tanh derive update rules exercise sometimes asked decode linear codes example algebraic codes transforming parity check matrices low density applying sum product algorithm recall linear combination rows valid parity check matrix code long matrix invertible many parity check matrices one code explain random linear code low density parity check matrix low density means row weight small constant exercise show low density parity check code columns weight say columns code words weight order log exercise section found expected value weight enumerator function averaging ensemble random linear codes calculation also carried ensemble low density parity check codes gallager mackay litsyn shevelev plausible however mean value always good indicator typical value ensemble example particular value codes might say typical value zero mean found find typical weight enumerator function low density parity check codes 
[low-density, parity-check, codes, solutions] solution exercise consider codes rate blocklength source bits parity check bits let copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links solutions codes bits ordered first bits independent could wish put code systematic form number distinct linear codes number matrices expressed distinct low density log parity check codes number low density parity check matrices row weight number distinct codes define much smaller pigeon hole principle log log possible every random linear code map low density 
[convolutional, codes, turbo, codes] chapter follows tightly chapter makes use ideas codes trellises forward backward algorithm 
[convolutional, codes, turbo, codes, introduction, convolutional, codes] studied linear block codes described three ways generator matrix describes turn string arbitrary source bits transmission bits parity check matrix specifies parity check con straints valid codeword satisfies trellis code describes valid codewords terms paths trellis labelled edges fourth way describing block codes algebraic approach covered book well covered numerous books coding theory part book discusses state art error correcting codes makes little use algebraic coding theory competent teach subject describe convolutional codes two ways first terms mechanisms generating transmissions source bits second terms trellises describe constraints satisfied valid transmissions 
[convolutional, codes, turbo, codes, linear-feedback, shift-registers] generate transmission convolutional code putting source stream linear filter filter makes use shift register linear output functions possibly linear feedback draw shift register right left orientation bits roll right left time goes figure shows three linear feedback shift registers could used define convolutional codes rectangular box surrounding bits indicates memory filter also known state three filters one input two outputs clock cycle source sup plies one bit filter outputs two bits concatenating together bits obtain source stream trans mission stream two transmitted bits every source bit codes shown figure rate copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links linear feedback shift registers octal name figure linear feedback shift registers generating convolutional codes rate symbol indicates copying delay one clock cycle symbol denotes linear addition modulo delay filters systematic nonrecursive nonsystematic nonrecursive systematic recursive filters require bits memory codes define known constraint length codes convolutional codes come three flavours corresponding three types filter figure 
[convolutional, codes, turbo, codes, systematic, nonrecursive] filter shown figure feedback also property one output bits identical source bit encoder thus called systematic source bits reproduced transparently transmitted stream nonrecursive feedback transmitted bit linear function state filter one way describing function dot product modulo two binary vectors length binary vector state vector include state vector bit put first bit memory next cycle vector every tap downward pointing arrow state bit transmitted bit convenient way describe binary tap vectors octal thus filter makes use tap vector drawn delay lines table taps delay line converted octal right left make easy relate diagrams octal numbers 
[convolutional, codes, turbo, codes, nonsystematic, nonrecursive] filter shown figure also feedback systematic makes use two tap vectors create two transmitted bits encoder thus nonsystematic nonrecursive added complexity nonsystematic codes error correcting abilities superior systematic nonrecursive codes constraint length copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links convolutional codes turbo codes 
[convolutional, codes, turbo, codes, systematic, recursive] filter shown figure similar nonsystematic nonrecursive filter shown figure uses taps formerly made make linear signal fed back shift register along source bit output linear function state vector output filter systematic recursive code conventionally identified octal ratio fig ure code denoted figure two rate convolutional codes constraint length non recursive recursive two codes equivalent 
[convolutional, codes, turbo, codes, equivalence, systematic, recursive, nonsystematic, nonrecursive, codes] two filters figure code equivalent sets code words define identical every codeword nonsystematic nonrecursive code choose source stream encoder output identical vice versa prove denote quantity shown fig ure shows pair smaller otherwise equivalent filters two transmissions equivalent equal figures every cycle source bit systematic code must must simply confirm choice systematic code shift register follow state sequence nonsystematic code assuming states match initially figure nonrecursive whereas figure recursive substituting using immediately find recursive nonrecursive thus codeword nonsystematic nonrecursive code codeword systematic recursive code taps taps sense vertical arrows places figures though one arrows points instead two codes equivalent two encoders behave dif ferently nonrecursive encoder finite impulse response one puts string zeroes except single one resulting output stream contains finite number ones one bit passed states memory delay line returns zero state figure shows state sequence resulting source string figure shows trellis recursive code figure response filter source string filter infinite impulse response response settles periodic state period equal three clock cycles exercise input recursive filter state sequence transmission nonrecursive filter hint see figure copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links linear feedback shift registers transmit source transmit source figure trellises rate convolutional codes figure assumed initial state filter time horizontal axis state filter time step vertical coordinate line segments shown emitted symbols stars boxes paths taken trellises source sequence highlighted solid line light dotted lines show state trajectories possible source sequences transmit source figure source sequence systematic recursive code produces path trellis nonsystematic nonrecursive case copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links convolutional codes turbo codes received figure trellis code painted likelihood function received vector equal codeword one bit flipped three line styles depending value likelihood thick solid lines show edges trellis match corresponding two bits received string exactly thick dotted lines show edges match one bit mismatch thin dotted lines show edges mismatch bits general linear feedback shift register bits memory impulse response periodic period corresponding filter visiting every non zero state state space incidentally cheap pseudorandom number generators cheap crypto graphic products make use exactly periodic sequences though larger values random number seed cryptographic key lects initial state memory thus close connection certain cryptanalysis problems decoding convolutional codes 
[convolutional, codes, turbo, codes, decoding, convolutional, codes] receiver receives bit stream wishes infer state sequence thence source stream posterior probability bit found sum product algorithm also known forward backward bcjr algorithm introduced section probable state sequence found using min sum algorithm section also known viterbi algorithm nature task illustrated figure shows cost associated edge trellis case sixteen state code channel assumed binary symmetric channel received vector equal codeword except one bit flipped three line styles depending value likelihood thick solid lines show edges trellis match corresponding two bits received string exactly thick dotted lines show edges match one bit mismatch thin dotted lines show edges mismatch bits min sum algorithm seeks path trellis uses many solid lines possible precisely minimizes cost path cost zero solid line one thick dotted line two thin dotted line exercise spot probable path flipped bit copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links turbo codes transmit source transmit source figure two paths differ two transmitted bits figure terminated trellis codeword completed filter state 
[convolutional, codes, turbo, codes, unequal, protection] defect convolutional codes presented thus far offer equal protection source bits figure shows two paths trellis differ two transmitted bits last source bit less well protected source bits unequal protection bits motivates termination trellis terminated trellis shown figure termination slightly reduces number source bits used per codeword four source bits turned parity bits memory bits must returned zero 
[convolutional, codes, turbo, codes, turbo, codes] turbo code defined number constituent convolutional encoders often two equal number interleavers permutation matrices without loss generality take first interleaver identity matrix string source bits encoded feeding figure encoder turbo code box contains convolutional code source bits reordered using permutation fed transmitted codeword obtained concatenating interleaving outputs two convolutional codes constituent encoder order defined associated interleaver transmitting bits come constituent encoder often first constituent encoder chosen systematic encoder like recursive filter shown figure second non systematic one rate emits parity bits transmitted codeword consists copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links convolutional codes turbo codes figure rate rate turbo codes represented factor graphs circles represent codeword bits two rectangles represent trellises rate convolutional codes systematic bits occupying left half rectangle parity bits occupying right half puncturing constituent codes rate turbo code represented lack connections half parity bits trellis source bits followed parity bits generated first convolutional code parity bits second resulting turbo code rate turbo code represented factor graph two trellises represented two large rectangular nodes figure source bits first parity bits participate first trellis source bits last parity bits participate second trellis codeword bit participates either one two trellises depending whether parity bit source bit trellis node contains trellis exactly like terminated trellis shown figure except one thousand times long factor graph representations turbo codes make use elementary nodes factor graph given yields standard version sum product algorithm used turbo codes turbo code smaller rate required standard modifica tion rate code puncture parity bits figure turbo codes decoded using sum product algorithm described chapter first iteration trellis receives channel likelihoods runs forward backward algorithm compute bit relative likelihood given information bits likelihoods passed across trellis multiplied channel likelihoods way ready second iteration forward backward algorithm run trellis using updated probabilities ten twenty iterations hoped correct decoding found common practice stop fixed number iterations better stopping criterion following procedure used every iter ation time step trellis identify probable edge according local messages probable edges join two valid paths one trellis two paths consistent reasonable stop subsequent iterations unlikely take decoder away codeword maximum number iterations reached without stopping criterion satisfied decoding error reported stopping procedure recommended several reasons allows big saving decoding time loss error probability allows decoding failures detected decoder identified knowing particular block definitely corrupted surely useful information receiver distinguish detected undetected rors undetected errors give helpful insights low weight codewords copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links parity check matrices convolutional codes turbo codes code may improve process code design turbo codes described excellent performance decoded error probabilities randomly constructed turbo codes tend error floor starting level error floor caused low weight codewords reduce height error floor one attempt modify random construction increase weight low weight codewords tweaking turbo codes black art never succeeds totalling eliminating low weight codewords precisely low weight codewords eliminated sacrificing turbo code excellent per formance contrast low density parity check codes rarely error floors long number weight columns large exercise 
[convolutional, codes, turbo, codes, parity-check, matrices, convolutional, codes, turbo, codes] figure schematic pictures parity check matrices convolutional code rate turbo code rate notation diagonal line represents identity matrix band diagonal lines represent band diagonal circle inside square represents random permutation columns square number inside square represents number random permutation matrices superposed square horizontal vertical lines indicate boundaries blocks within matrix close discussing parity check matrix rate convolutional code viewed linear block code adopt convention bits one block made bits followed bits exercise prove convolutional code low density parity check matrix shown schematically figure hint easiest figure parity constraints satisfied convo lutional code thinking nonsystematic nonrecursive encoder figure consider putting filter stream convolutional filter vice versa compare two resulting streams ignore termination trellises parity check matrix turbo code written listing constraints satisfied two constituent trellises figure turbo codes also special cases low density parity check codes turbo code punctured longer necessarily low density parity check matrix always generalized parity check matrix sparse explained next chapter 
[convolutional, codes, turbo, codes, reading] reading convolutional codes johannesson zigangirov highly recommended one topic would liked include sequential decoding sequential decoding explores promising paths trellis backtracks evidence accumulates wrong turning taken sequential decoding used trellis big able apply maximum likelihood algorithm min sum algorithm read sequential decoding johannesson zigangirov information use sum product algorithm turbo codes rarely used highly recommended stopping criteria halting decoding frey essential reading lots good stuff book 
[convolutional, codes, turbo, codes, solutions] solution exercise first bit flipped probable path upper one figure 
[repeat–accumulate, codes] figure generator matrix generalized parity check matrix non systematic low density generator matrix code code rate 
[repeat–accumulate, codes, encoder] take source bits repeat bit three times giving bits permute bits using random permutation fixed random permutation one every codeword call permuted string transmit accumulated sum mod mod mod 
[repeat–accumulate, codes, graph] figure shows graph repeat accumulate code using four types node equality constraints intermediate binary variables black circles parity constraints transmitted bits white circles source sets values black bits bottom three time accumulator computes transmitted bits along top copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links decoding figure factor graphs repeat accumulate code rate using elementary nodes white circle represents transmitted bit constraint forces sum bits connected even black circle represents intermediate binary variable constraint forces three variables connected equal factor graph normally used decoding top rectangle represents trellis accumulator shown inset total undetected figure performance six rate repeat accumulate codes gaussian channel blocklengths range vertical axis block error probability horizontal axis dotted lines show frequency undetected errors graph factor graph prior probability codewords circles binary variable nodes squares representing two types factor nodes usual contributes factor form mod contributes factor form 
[repeat–accumulate, codes, decoding] repeat accumulate code normally decoded using sum product algo rithm factor graph depicted figure top box represents trellis accumulator including channel likelihoods first half iteration top trellis receives likelihoods every transition trellis runs forward backward algorithm produce likelihoods variable node second half iteration likelihoods multiplied together nodes produce new likelihood messages send back trellis gallager codes turbo codes stop done decoding method applied possible distinguish undetected errors caused low weight codewords code detected errors decoder gets stuck knows failed find valid answer figure shows performance six randomly constructed repeat accumulate codes gaussian channel one mind error floor kicks block error probability performance staggeringly good simple code figure copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links repeat accumulate codes total detected undetected iii iii figure histograms number iterations find valid decoding repeat accumulate code source block length transmitted blocklength block error probability versus signal noise ratio code histogram iii iii fits power laws 
[repeat–accumulate, codes, empirical, distribution, decoding, times] interesting study number iterations sum product algo rithm required decode sparse graph code given one code set channel conditions decoding time varies randomly trial trial find histogram decoding times follows power law large power depends signal noise ratio becomes smaller distribution heavy tailed signal noise ratio decreases observed power laws repeat accumulate codes irregular regular gallager codes figures iii show distribution decoding times repeat accumulate code two different signal noise ratios power laws extend several orders magnitude exercise investigate power laws density evolution predict design code used manipulate power law useful way 
[repeat–accumulate, codes, generalized, parity-check, matrices] find helpful relating sparse graph codes use common representation forney introduced idea normal graph nodes variable nodes degree one two variable nodes degree two represented edges connect node node generalized parity check matrix graphical way representing normal graphs parity check matrix columns transmitted bits rows linear constraints generalized parity check matrix additional columns may included represent state variables transmitted one way thinking state variables punctured code transmission state variables indicated horizontal line corresponding columns pieces diagrammatic notation generalized parity copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links generalized parity check matrices figure generator matrix parity check matrix generalized parity check matrix repetition code rate check matrices mackay mackay diagonal line square indicates part matrix contains identity matrix two parallel diagonal lines indicate band diagonal matrix corresponding number per row horizontal ellipse arrow indicates corresponding columns block randomly permuted vertical ellipse arrow indicates corresponding rows block randomly permuted integer surrounded circle represents number superposed random permutation matrices definition generalized parity check matrix pair binary matrix list punctured bits matrix defines set valid vectors satisfying valid vector codeword obtained puncturing bits indicated one code many generalized parity check matrices rate code generalized parity check matrix estimated follows punctures bits selects bits transmission effective number constraints codeword number source bits rate greater equal copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links repeat accumulate codes 
[repeat–accumulate, codes, examples] repetition code generator matrix parity check matrix generalized parity check matrix simple rate repetition code shown figure systematic low density generator matrix code systematic low density generator matrix code state variables transmitted codeword length given denoting identity matrix sparse matrix parity check matrix code case rate code parity check matrix might represented shown figure non systematic low density generator matrix code non systematic low density generator matrix code transmitted codeword length given sparse matrix generalized parity check matrix code corresponding generalized parity check equation whereas parity check matrix simple code typically com plex dense matrix generalized parity check matrix retains underlying simplicity code case rate code generalized parity check matrix might represented shown figure low density parity check codes linear codes parity check matrix figure generalized parity check matrices rate gallager code columns weight rate linear code rate low density parity check code shown figure copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links generalized parity check matrices figure generalized parity check matrices convolutional code rate rate turbo code built parallel concatenation two convolutional codes linear code non systematic low density parity check code state bits code source bits figure shows generalized parity check matrix rate linear code convolutional codes non systematic non recursive convolutional code source bits play role state bits fed delay line two linear functions delay line transmitted figure two parity streams shown two successive vectors length common interleave two parity streams bit reordering relevant illustrated concatenation parallel concatenation two codes represented one diagrams aligning matrices two codes way source bits line adding blocks zero entries matrix state bits parity bits two codes occupy separate columns example given turbo code follows serial concatenation columns corresponding transmitted bits first code aligned columns corresponding source bits second code turbo codes turbo code parallel concatenation two convolutional codes generalized parity check matrix rate turbo code shown figure repeat accumulate codes generalized parity check matrices rate figure generalized parity check matrix repeat accumulate code rate repeat accumulate code shown figure repeat accumulate codes equivalent staircase codes section intersection generalized parity check matrix intersection two codes made stacking generalized parity check matrices top way transmitted bits columns correctly aligned punctured bits associated two component codes occupy separate columns 
[digital, fountain, codes] digital fountain codes record breaking sparse graph codes channels erasures channels erasures great importance example files sent internet chopped packets packet either received without error received simple channel model describing situation ary erasure channel inputs input alphabet probability transmitting input without error probability delivering output alphabet size number bits packet common methods communicating channels employ feed back channel receiver sender used control retransmission erased packets example receiver might send back messages identify missing packets retransmitted alternatively receiver might send back messages acknowledge received packet sender keeps track packets acknowledged retransmits others packets acknowledged simple retransmission protocols advantage work regardless erasure probability purists learned shannon theory feel retransmission protocols wasteful erasure probability large number feedback messages sent first protocol large second protocol likely receiver end receiving multiple redundant copies packets heavy use made feedback channel according shannon need feedback channel capacity forward channel bits whether feedback wastefulness simple retransmission protocols especially evi dent case broadcast channel erasures channels one sender broadcasts many receivers receiver receives random fraction packets every packet missed one receivers retransmitted retransmissions terribly dundant every receiver already received retransmitted packets would like make erasure correcting codes require feed back almost feedback classic block codes erasure correction called reed solomon codes reed solomon code alpha bet size ideal property transmitted symbols received original source symbols recovered see berlekamp lin costello information reed solomon codes exist reed solomon codes disadvantage practical small standard copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links digital fountain codes plementations encoding decoding cost order log packet operations furthermore reed solomon code block code one must estimate erasure probability choose code rate transmission unlucky larger expected receiver receives fewer symbols like simple way extend code fly create lower rate code reed solomon codes fly method exists better way pioneered michael luby company digital fountain first company whose business based sparse graph codes digital fountain codes describe codes invented luby idea digital fountain code follows encoder stands luby transform fountain produces endless supply water drops encoded packets let say original source file size bits drop contains encoded bits anyone wishes receive encoded file holds bucket fountain collects drops number drops bucket little larger recover original file digital fountain codes rateless sense number encoded packets generated source message potentially limitless number encoded packets generated determined fly regardless statistics erasure events channel send many encoded packets needed order decoder recover source data source data decoded set encoded packets slightly larger practice larger digital fountain codes also fantastically small encoding decod ing complexities probability packets communicated average encoding decoding costs order packet operations luby calls codes universal simultaneously near optimal every erasure channel efficient file length grows overhead order 
[digital, fountain, codes, digital, fountain’s, encoder] encoded packet produced source file follows randomly choose degree packet degree distri bution appropriate choice depends source file size discuss later choose uniformly random distinct input packets set equal bitwise sum modulo packets sum done successively exclusive ing packets together encoding operation defines graph connecting encoded packets source packets mean degree significantly smaller graph sparse think resulting code irregular low density generator matrix code decoder needs know degree packet received source packets connected graph information communicated decoder various ways example sender receiver synchronized clocks could use identical pseudo random copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links decoder number generators seeded clock choose random degree set connections alternatively sender could pick random key given degree connections determined pseudo random process send key header packet long packet size much bigger key size need bits key introduces small overhead cost 
[digital, fountain, codes, decoder] decoding sparse graph code especially easy case erasure chan nel decoder task recover matrix associated graph simple way attempt solve prob lem message passing think decoding algorithm sum product algorithm wish messages either completely certain messages completely certain messages uncertain messages assert message packet could value equal probability certain messages assert particular value probability one figure example decoding digital fountain code source bits encoded bits simplicity messages allows simple description decoding process call encoded packets check nodes find check node connected one source packet check node decoding algorithm halts point fails recover source packets set add checks connected remove edges connected source packet repeat determined decoding process illustrated figure toy case packet one bit three source packets shown upper circles four received packets shown lower check symbols values start algorithm first iteration check node connected sole source bit first check node panel set source bit accordingly panel discard check node add value checks connected panel disconnecting graph start second iteration panel fourth check node connected sole source bit set panel add two checks connected panel finally find two check nodes connected agree value would hope restored panel 
[digital, fountain, codes, designing, degree, distribution] probability distribution degree critical part design occasional encoded packets must high degree similar order ensure source packets connected one many packets must low degree decoding process copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links digital fountain codes get started keep going total number addition operations involved encoding decoding kept small given degree distribution statistics decoding process predicted appropriate version density evolution rho tau figure distributions case gives distribution largest ideally avoid redundancy like received graph prop erty one check node degree one iteration itera tion check node processed degrees graph reduced way one new degree one check node appears expectation ideal behaviour achieved ideal soliton distribution expected degree distribution roughly exercise derive ideal soliton distribution first iteration let number packets degree show expected number packets degree degree reduced tth iteration packets recovered number packets degree expected number packets degree degree reduced hence show order expected number packets degree satisfy must start generally recursion solve upwards degree distribution works poorly practice fluctuations around expected behaviour make likely point decoding process degree one check nodes furthermore source nodes receive connections small modification fixes problems robust soliton distribution two extra parameters designed ensure expected number degree one checks rather throughout decoding process parameter bound probability decoding fails run completion certain number packets received parameter constant order aim prove luby main theorem codes practice however viewed free parameter value somewhat smaller giving good results define positive function see figure exercise add ideal soliton distribu tion normalize obtain robust soliton distribution number encoded packets required receiving end ensure decoding run completion proba bility least delta delta delta delta delta delta figure number degree one checks upper figure quantity lower figure function two parameters luby main theorem proves exists value given received packets decoding algorithm recover source packets probability copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links applications luby analysis explains small end role ensuring decoding process gets started spike included ensure every source packet likely connected check least luby key result appropriate value figure histograms actual number packets required order recover file size packets parameters follows top histogram middle bottom constant receiving checks ensures packets recovered probability least illustrative figures set allowable decoder failure probability quite large actual failure probability much smaller suggested luby conservative analysis practice codes tuned file original size packets recovered overhead figure shows tograms actual number packets required couple settings parameters achieving mean overheads smaller respec tively 
[digital, fountain, codes, applications] digital fountain codes excellent solution wide variety situations let mention two 
[digital, fountain, codes, storage] wish make backup large file aware magnetic tapes hard drives unreliable sense catastrophic failures stored packets permanently lost within one device occur rate something like per day store file digital fountain used spray encoded packets place every storage device available recover backup file whose size packets one simply needs find packets anywhere corrupted packets matter simply skip find packets elsewhere method storage also advantages terms speed file covery hard drive standard practice store file successive sectors hard drive allow rapid reading file occasion ally happens packet lost owing reading head track moment giving burst errors cannot corrected packet error correcting code whole revolution drive must performed bring back packet head second read time taken one revolution produces undesirable delay file system files instead stored using digital fountain principle digital drops stored one consecutive sectors drive one would never need endure delay reading packet packet loss would become less important hard drive could consequently operated faster higher noise level fewer resources devoted noisy channel coding exercise compare digital fountain method robust storage multiple hard drives raid redundant array independent disks 
[digital, fountain, codes, broadcast] imagine ten thousand subscribers area wish receive digital movie broadcaster broadcaster send movie packets copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links digital fountain codes broadcast network example wide bandwidth phone line satellite imagine packets received houses let say lost house standard approach file transmitted plain sequence packets encoding house would notify broadcaster missing packets request retransmitted ten thousand subscribers requesting retransmissions would retransmission request almost every packet thus broadcaster would repeat entire broadcast twice order ensure subscribers received whole movie users would wait roughly twice long ideal time download complete broadcaster uses digital fountain encode movie sub scriber recover movie packets broadcast needs last say packets every house likely successfully recovered whole file another application broadcasting data cars imagine want send updates car navigation databases satellite hundreds thousands vehicles receive data open road feedback channels standard method sending data put carousel broadcasting packets fixed periodic sequence yes car may tunnel miss hundred packets able collect missed packets hour later carousel gone full revolution hope maybe following day instead satellite uses digital fountain car needs receive amount data equal original file size plus 
[digital, fountain, codes, reading] encoders decoders sold digital fountain even higher efficiency codes described work well blocklengths large lengths shokrollahi presents raptor codes extension codes linear time encoding coding 
[digital, fountain, codes, exercises] exercise understanding robust soliton distribution repeat analysis exercise aim expected number packets degree instead show initial required number packets reason truncating second term beyond replac ing spike see equation ensure decoding complexity grow larger estimate expected number packets expected number edges sparse graph determines decoding complexity histogram packets given compare expected numbers packets edges robust soliton distribution used copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links exercises exercise show spike equation ade quate replacement tail high weight packets exercise investigate experimentally necessary spike equation successful decoding investigate also whether tail beyond necessary happens high weight degrees removed spike tail beyond exercise fill details proof luby main theorem receiving checks ensures source packets recovered probability least exercise optimize degree distribution digital fountain code file packets pick sensible objective function optimization minimizing mean number packets required complete decoding percentile histogram figure exercise make model situation data stream broad cast cars quantify advantage digital fountain carousel method exercise construct simple example illustrate fact digital fountain decoder section suboptimal sometimes gives even though information available sufficient decode whole file cost optimal decoder compare exercise every transmitted packet created adding together source packets random probability source packet included show probability received packets suffice optimal decoder able recover source packets little put another way probability random matrix full rank show packets received probability suffice optimal decoder roughly exercise implement optimal digital fountain decoder uses method richardson urbanke derived fast encod ing sparse graph codes section handle matrix inversion required optimal decoding changed decoder reoptimize degree distribution using higher weight packets much reduce overhead confirm assertion approach makes digital fountain codes viable erasure correcting codes blocklengths large blocklengths codes excellent exercise digital fountain codes excellent rateless codes erasure channels make rateless code channel erasures noise copyright cambridge university press screen viewing permitted printing permitted http www cambridge org buy book pounds see http www inference phy cam mackay itila links digital fountain codes 
[digital, fountain, codes, summary, sparse-graph, codes] simple method designing error correcting codes noisy channels first pioneered gallager recently rediscovered generalized communication theory transformed practical performance gallager low density parity check codes modern cousins vastly better performance codes textbooks filled intervening years sparse graph code best noisy channel depends cho sen rate blocklength permitted encoding decoding complexity question whether occasional undetected errors acceptable low density parity check codes versatile easy make compet itive low density parity check code almost rate blocklength low density parity check codes virtually never make undetected errors special case erasure channel sparse graph codes best digital fountain codes 
[digital, fountain, codes, conclusion] best solution communication problem combine simple pseudo random code message passing decoder 
