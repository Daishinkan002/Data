[using, neural, nets, recognize, handwritten, digits] human visual system one wonders world consider following sequence handwritten digits people effortlessly recognize digits ease deceptive hemisphere brain humans primary visual cortex also known containing million neurons tens billions connections yet human vision involves entire series visual cortices progressively complex image processing carry heads supercomputer tuned evolution hundreds millions years superbly adapted understand visual world recognizing handwritten digits easy rather humans stupendously astoundingly good making sense eyes show nearly work done unconsciously usually appreciate tough problem visual systems solve difficulty visual pattern recognition becomes apparent attempt write computer program recognize digits like seems easy suddenly becomes extremely difficult simple intuitions recognize shapes loop top vertical stroke bottom right turn simple express algorithmically try make rules precise quickly get lost morass exceptions caveats special cases seems hopeless neural networks approach problem different way idea take large number handwritten digits known training examples using neural nets recognize handwritten digits develop system learn training examples words neural network uses examples automatically infer rules recognizing handwritten digits furthermore increasing number training examples network learn handwriting improve accuracy shown training digits perhaps could build better handwriting recognizer using thousands even millions billions training examples chapter write computer program implementing neural network learns recognize handwritten digits program lines long uses special neural network libraries short program recognize digits accuracy percent without human intervention furthermore later chapters develop ideas improve accuracy percent fact best commercial neural networks good used banks process cheques post offices recognize addresses focusing handwriting recognition excellent prototype problem learning neural networks general prototype hits sweet spot challenging small feat recognize handwritten digits difficult require extremely complicated solution tremendous computational power furthermore great way develop advanced techniques deep learning throughout book return repeatedly problem handwriting recognition later book discuss ideas may applied problems computer vision also speech natural language processing domains course point chapter write computer program recognize handwritten digits chapter would much shorter along way develop many key ideas neural networks including two important types artificial neuron perceptron sigmoid neuron standard learning algorithm neural networks known stochastic gradient descent throughout focus explaining things done way building neural networks intuition requires lengthier discussion presented basic mechanics going worth deeper understanding attain amongst payoffs end chapter position understand deep learning matters 
[using, neural, nets, recognize, handwritten, digits, perceptrons] neural network get started explain type artificial neuron called perceptron perceptrons scientist frank perceptrons rosenblatt inspired earlier warren mcculloch walter pitts today common use models artificial neurons book much modern work neural networks main neuron model used one called sigmoid neuron get sigmoid neurons shortly understand sigmoid neurons defined way worth taking time first understand perceptrons perceptrons work perceptron takes several binary inputs produces single binary output example shown perceptron three inputs general could fewer inputs rosenblatt proposed simple rule compute output introduced weights real numbers expressing importance respective inputs output neuron output determined whether weighted sum less greater threshold value like weights threshold real number parameter neuron put precise algebraic terms output threshold threshold perceptron works basic mathematical model way think perceptron device makes decisions weighing evidence let give example realistic example easy understand soon get realistic examples suppose weekend coming heard going cheese festival city like cheese trying decide whether festival might make decision weighing three factors weather good boyfriend girlfriend want accompany festival near public transit car represent three factors corresponding binary variables instance weather good weather bad similarly boyfriend girlfriend wants similarly public transit suppose absolutely adore cheese much happy festival even boyfriend girlfriend uninterested festival hard get perhaps really loathe bad weather way festival weather bad use perceptrons model kind decision making one way choose weight weather conditions larger value indicates weather matters lot much whether boyfriend girlfriend joins nearness public transit finally suppose choose threshold perceptron choices perceptron implements desired decision making model outputting whenever using neural nets recognize handwritten digits weather good whenever weather bad makes difference output whether boyfriend girlfriend wants whether public transit nearby varying weights threshold get different models decision making example suppose instead chose threshold perceptron would decide festival whenever weather good festival near public transit boyfriend girlfriend willing join words different model decision making dropping threshold means willing festival obviously perceptron complete model human decision making example illustrates perceptron weigh different kinds evidence order make decisions seem plausible complex network perceptrons could make quite subtle decisions network first column perceptrons call first layer perceptrons making three simple decisions weighing input evidence perceptrons second layer perceptrons making decision weighing results first layer decision making way perceptron second layer make decision complex abstract level perceptrons first layer even complex decisions made perceptron third layer way many layer network perceptrons engage sophisticated decision making incidentally defined perceptrons said perceptron single output network perceptrons look like multiple outputs fact still single output multiple output arrows merely useful way indicating output perceptron used input several perceptrons less unwieldy drawing single output line splits let simplify way describe perceptrons condition threshold cumbersome make two notational changes simplify first change write dot product vectors whose components weights inputs respectively second change move threshold side inequality replace known perceptron bias threshold using bias instead threshold perceptron rule rewritten output think bias measure easy get perceptron output put biological terms bias measure easy get perceptrons perceptron fire perceptron really big bias extremely easy perceptron output bias negative difficult perceptron output obviously introducing bias small change describe perceptrons see later leads notational simplifications remainder book use threshold always use bias described perceptrons method weighing evidence make decisions another way perceptrons used compute elementary logical functions usually think underlying computation functions nand example suppose perceptron two inputs weight overall bias perceptron see input produces output since positive introduced symbol make multiplications explicit similar calculations show inputs produce output input produces output since negative perceptron implements nand gate nand example shows use perceptrons compute simple logical functions fact use networks perceptrons compute logical function reason nand gate universal computation build computation nand gates example use nand gates build circuit adds two bits requires computing bitwise sum well carry bit set carry bit bitwise product get equivalent network perceptrons replace nand gates perceptrons two inputs weight overall bias resulting network note moved perceptron corresponding bottom right nand gate little make easier draw arrows diagram using neural nets recognize handwritten digits one notable aspect network perceptrons output leftmost per ceptron used twice input bottommost perceptron defined perceptron model say whether kind double output place allowed actu ally much matter want allow kind thing possible simply merge two lines single connection weight instead two connections weights find obvious stop prove equivalent change network looks follows unmarked weights equal biases equal single weight marked drawing inputs like variables floating left network perceptrons fact conventional draw extra layer perceptrons input layer encode inputs notation input perceptrons output inputs shorthand actually mean perceptron inputs see suppose perceptron inputs weighted sum would always zero perceptron would output perceptron would simply output fixed value desired value example better think input perceptrons really perceptrons rather special units simply defined output desired values adder example demonstrates network perceptrons used simulate circuit containing many nand gates nand gates universal computation follows perceptrons also universal computation computational universality perceptrons simultaneously reassuring disap pointing reassuring tells networks perceptrons powerful sigmoid neurons computing device also disappointing makes seem though perceptrons merely new type nand gate hardly big news however situation better view suggests turns devise learning algorithms automatically tune weights biases network artificial neurons tuning happens response external stimuli without direct intervention programmer learning algorithms enable use artificial neurons way radically different conventional logic gates instead explicitly laying circuit nand gates neural networks simply learn solve problems sometimes problems would extremely difficult directly design conventional circuit 
[using, neural, nets, recognize, handwritten, digits, sigmoid, neurons] learning algorithms sound terrific devise algorithms neural network suppose network perceptrons like use learn solve problem example inputs network might raw pixel data scanned handwritten image digit like network learn weights biases output network correctly classifies digit see learning might work suppose make small change weight bias network like small change weight cause small corresponding change output network see moment property make learning possible schematically want obviously network simple handwriting recognition true small change weight bias causes small change output could use fact modify weights biases get network behave manner want example suppose network mistakenly classifying image could figure make small change weights biases network gets little closer classifying image repeat changing weights biases produce better better output network would learning problem happens network contains perceptrons fact small change weights bias single perceptron network sometimes cause output perceptron completely flip say flip may cause behaviour rest network completely change using neural nets recognize handwritten digits complicated way might classified correctly behaviour network images likely completely changed hard control way makes difficult see gradually modify weights biases network gets closer desired behaviour perhaps clever way getting around problem immediately obvious get network perceptrons learn overcome problem introducing new type artificial neuron called sigmoid neuron sigmoid neurons similar perceptrons modified small changes weights bias cause small change output crucial fact allow network sigmoid neurons learn okay let describe sigmoid neuron depict sigmoid neurons way depicted perceptrons like perceptron sigmoid neuron inputs instead inputs also take values instance valid input sigmoid neuron also like perceptron sigmoid neuron weights input overall bias output instead called sigmoid defined put little explicitly output sigmoid neuron inputs weights bias exp first sight sigmoid neurons appear different perceptrons algebraic form sigmoid function may seem opaque forbidding already familiar fact many similarities perceptrons sigmoid neurons algebraic form sigmoid function turns technical detail true barrier understanding understand similarity perceptron model suppose large positive number words large positive output sigmoid neuron approximately would perceptron suppose hand negative negative behaviour sigmoid incidentally sometimes called logistic function new class neurons called logistic neurons useful remember terminology since terms used many people working neural nets however stick sigmoid terminology sigmoid neurons neuron also closely approximates perceptron modest size much deviation perceptron model algebraic form understand fact exact form important really matters shape function plotted shape sigmoid function shape smoothed version step function step function fact step function sigmoid neuron would perceptron since output would depending whether positive using actual function get already implied smoothed perceptron indeed smoothness function crucial fact detailed form smoothness means small changes weights bias produce small change output output neuron fact calculus tells output well approximated output output output actually perceptron outputs step function outputs strictly speaking need modify step function one point get idea using neural nets recognize handwritten digits sum weights output output denote partial derivatives output respect respectively panic comfortable partial derivatives expression looks complicated partial derivatives actually saying something simple good news output linear function changes weights bias linearity makes easy choose small changes weights biases achieve desired small change output sigmoid neurons much qualitative behavior perceptrons make much easier figure changing weights biases change output shape really matters exact form use particular form used equation fact later book occasionally consider neurons output activation function main thing changes use different activation function particular values partial derivatives equation change turns compute partial derivatives later using simplify algebra simply exponentials lovely properties differentiated case commonly used work neural nets activation function use often book interpret output sigmoid neuron obviously one big difference perceptrons sigmoid neurons sigmoid neurons output output real number values legitimate outputs useful example want use output value represent average intensity pixels image input neural network sometimes nuisance suppose want output network indicate either input image input image obviously easiest output perceptron practice set convention deal example deciding interpret output least indicating output less indicating always explicitly state using convention cause confusion 
[using, neural, nets, recognize, handwritten, digits, exercises] prove assertion last paragraph hint already familiar may find helpful familiarize explained gradient descent function two variables function two variables happens function one variable provide geometric interpretation gradient descent one dimensional case people investigated many variations gradient descent including variations closely mimic real physical ball ball mimicking variations advantages also major disadvantage turns necessary compute second partial derivatives quite costly see costly suppose want compute second partial derivatives million variables need compute something like trillion million squared second partial going computationally costly said tricks avoiding kind problem finding alternatives gradient descent active area investigation book use gradient descent variations main approach learning neural networks apply gradient descent learn neural network idea use gradient descent find weights biases minimize cost equation see works let restate gradient descent update rule weights biases replacing variables words position components gradient vector corresponding components actually like half trillion since still get point using neural nets recognize handwritten digits writing gradient descent update rule terms components repeatedly applying update rule roll hill hopefully find minimum cost function words rule used learn neural network number challenges applying gradient descent rule look depth later chapters want mention one problem understand problem let look back quadratic cost equation notice cost function form average costs individual training examples practice compute gradient need compute gradients separately training input average unfortunately number training inputs large take long time learning thus occurs slowly idea called stochastic gradient descent used speed learning idea estimate gradient computing small sample randomly chosen training inputs averaging small sample turns quickly get good estimate true gradient helps speed gradient descent thus learning make ideas precise stochastic gradient descent works randomly picking small number randomly chosen training inputs label random training inputs refer mini batch provided sample size large enough expect average value roughly equal average second sum entire set training data swapping sides get confirming estimate overall gradient computing gradients randomly chosen mini batch connect explicitly learning neural networks suppose denote weights biases neural network stochastic gradient descent works picking randomly chosen mini batch training inputs training learning gradient descent sums training examples current mini batch pick another randomly chosen mini batch train exhausted training inputs said complete epoch training point start new training epoch incidentally worth noting conventions vary scaling cost function mini batch updates weights biases equation scaled overall cost function factor people sometimes omit summing costs individual training examples instead averaging particularly useful total number training examples known advance occur training data generated real time instance similar way mini batch update rules sometimes omit term front sums conceptually makes little difference since equivalent rescaling learning rate detailed comparisons different work worth watching think stochastic gradient descent like political polling much easier sample small mini batch apply gradient descent full batch carrying poll easier running full election example training set size mnist choose mini batch size say means get factor speedup estimating gradient course estimate perfect statistical fluctuations need perfect really care moving general direction help decrease means need exact computation gradient practice stochastic gradient descent commonly used powerful technique learning neural networks basis learning techniques develop book 
[using, neural, nets, recognize, handwritten, digits, architecture, neural, networks] next section introduce neural network pretty good job classifying handwritten digits preparation helps explain terminology lets name different parts network suppose network architecture neural networks mentioned earlier leftmost layer network called input layer neurons within layer called input neurons rightmost output layer contains output neurons case single output neuron middle layer called hidden layer since neurons layer neither inputs outputs term hidden perhaps sounds little mysterious first time heard term thought must deep philosophical mathematical significance really means nothing input output network single hidden layer networks multiple hidden layers example following four layer network two hidden layers somewhat confusingly historical reasons multiple layer networks times called multilayer perceptrons mlps despite made sigmoid neurons perceptrons going use mlp terminology book since think confusing wanted warn existence design input output layers network often straightforward example suppose trying determine whether handwritten image depicts natural way design network encode intensities image pixels input neurons image greyscale image input neurons intensities scaled appropriately output layer contain single neuron output values less indicating input image values greater indicating input image design input output layers neural network often straight forward quite art design hidden layers particular using neural nets recognize handwritten digits possible sum design process hidden layers simple rules thumb instead neural networks researchers developed many design heuristics hidden layers help people get behaviour want nets example heuristics used help determine trade number hidden layers time required train network meet several design heuristics later book discussing neural networks output one layer used input next layer networks called feedforward neural networks means loops network information always fed forward never fed back loops end situations input function depended output hard make sense allow loops however models artificial neural networks feedback loops possible models called recurrent neural networks idea models neurons fire limited duration time becoming quiescent firing stimulate neurons may fire little later also limited duration causes still neurons fire time get cascade neurons firing loops cause problems model since neuron output affects input later time instantaneously recurrent neural nets less influential feedforward networks part learning algorithms recurrent nets least date less powerful recurrent networks still extremely interesting much closer spirit brains work feedforward networks possible recurrent networks solve important problems solved great difficulty feedforward networks however limit scope book going concentrate widely used feedforward networks 
[using, neural, nets, recognize, handwritten, digits, simple, network, classify, handwritten, digits] defined neural networks let return handwriting recognition split problem recognizing handwritten digits two sub problems first like way breaking image containing many digits sequence separate images containing single digit example like break image six separate images humans solve segmentation problem ease challenging computer program correctly break image image segmented program needs classify individual digit instance like program recognize first digit simple network classify handwritten digits focus writing program solve second problem classifying individual digits turns segmentation problem difficult solve good way classifying individual digits many approaches solving segmentation problem one approach trial many different ways segmenting image using individual digit classifier score trial segmentation trial segmentation gets high score individual digit classifier confident classification segments low score classifier lot trouble one segments idea classifier trouble somewhere probably trouble segmentation chosen incorrectly idea variations used solve segmentation problem quite well instead worrying segmentation concentrate developing neural network solve interesting difficult problem namely recognizing individual handwritten digits recognize individual digits use three layer neural network input layer network contains neurons encoding values input pixels discussed next section training data network consist many pixel images scanned handwritten digits input layer contains neurons simplicity omitted input neurons diagram input pixels greyscale value representing white value representing black values representing gradually darkening shades grey second layer network hidden layer denote number neurons using neural nets recognize handwritten digits hidden layer experiment different values example shown illustrates small hidden layer containing neurons output layer network contains neurons first neuron fires output indicate network thinks digit second neuron fires indicate network thinks digit little precisely number output neurons figure neuron highest activation value neuron say neuron number network guess input digit output neurons might wonder use output neurons goal network tell digit corresponds input image seemingly natural way use output neurons treating neuron taking binary value depending whether neuron output closer four neurons enough encode answer since possible values input digit network use neurons instead inefficient ultimate justification empirical try network designs turns particular problem network output neurons learns recognize digits better network output neurons leaves wondering using output neurons works better heuristic would tell advance use output encoding instead output encoding understand helps think neural network first principles consider first case use output neurons let concentrate first output neuron one trying decide whether digit weighing evidence hidden layer neurons hidden neurons well suppose sake argument first neuron hidden layer detects whether image like following present heavily weighting input pixels overlap image lightly weighting inputs similar way let suppose sake argument second third fourth neurons hidden layer detect whether following images present may guessed four images together make image saw line digits shown earlier four hidden neurons firing conclude digit course sort evidence use conclude image could legitimately get many ways say translations learning gradient descent images slight distortions seems safe say least case conclude input supposing neural network functions way give plausible explanation better outputs network rather outputs first output neuron would trying decide significant bit digit easy way relate significant bit simple shapes like shown hard imagine good historical reason component shapes digit closely related say significant bit output said heuristic nothing says three layer neural network operate way described hidden neurons detecting simple component shapes maybe clever learning algorithm find assignment weights lets use output neurons heuristic way thinking described works pretty well save lot time designing good neural network architectures 
[using, neural, nets, recognize, handwritten, digits, exercise] try creating network two layers input output layer hidden layer neurons respectively train network using stochastic gradient descent classification accuracy achieve earlier skipped details mnist data loaded pretty straightforward completeness code data structures used store mnist data described documentation strings straightforward stuff tuples lists numpy ndarray objects think vectors familiar ndarrays mnist_loader library load mnist image data details data structures returned see doc strings load_data load_data_wrapper practice load_data_wrapper function usually called neural network code libraries standard library import cpickle import gzip third party libraries import numpy def load_data return mnist data tuple containing training data validation data test data training_data returned tuple two entries first entry contains actual training images numpy ndarray entries entry turn numpy ndarray values representing pixels single mnist image second entry training_data tuple numpy ndarray containing entries entries digit values corresponding images contained first entry tuple validation_data test_data similar except contains images nice data format use neural networks helpful modify format training_data little done wrapper function load_data_wrapper see gzip open data mnist pkl training_data validation_data test_data cpickle load close return training_data validation_data test_data implementing network classify digits def load_data_wrapper return tuple containing training_data validation_data test_data based load_data format convenient use implementation neural networks particular training_data list containing tuples dimensional numpy ndarray containing input image dimensional numpy ndarray representing unit vector corresponding correct digit validation_data test_data lists containing tuples case dimensional numpy ndarry containing input image corresponding classification digit values integers corresponding obviously means using slightly different formats training data validation test data formats turn convenient use neural network code tr_d va_d te_d load_data training_inputs reshape tr_d training_results vectorized_result tr_d training_data zip training_inputs training_results validation_inputs reshape va_d validation_data zip validation_inputs va_d test_inputs reshape te_d test_data zip test_inputs te_d return training_data validation_data test_data def vectorized_result return dimensional unit vector jth position zeroes elsewhere used convert digit corresponding desired output neural network zeros return said program gets pretty good results mean good compared informative simple non neural network baseline tests compare understand means perform well simplest baseline course randomly guess digit right ten percent time much better less trivial baseline let try extremely simple idea look dark image instance image typically quite bit darker image pixels blackened following examples illustrate suggests using training data compute average darknesses digit presented new image compute dark image guess whichever digit closest average darkness simple procedure easy code explicitly write code interested big improvement random guessing getting test images correct percent accuracy difficult find ideas achieve accuracies percent range work bit harder get percent get much higher accuracies helps use established machine learning algorithms let try using one best known algorithms support vector machine svm familiar svms worry going need understand details svms work instead use python library called provides simple python interface fast based library svms known run scikit learn svm classifier using default settings gets test images correct code available big improvement naive approach classifying image based dark indeed means svm performing roughly well neural networks little worse later chapters introduce new techniques enable improve neural networks perform much better svm end story however result scikit learn default settings svms svms number tunable parameters possible search parameters improve box performance explicitly search instead refer andreas müller like know mueller shows work optimizing svm parameters possible get performance percent accuracy words well tuned svm makes error one digit pretty good neural networks better fact present well designed neural networks outperform every technique solving mnist including svms current record classifying images correctly done wan matthew zeiler sixin zhang yann lecun rob fergus see techniques used later book level performance close human equivalent arguably better since quite mnist images difficult even humans recognize confidence example trust agree tough classify images like mnist data set remarkable neural networks accurately classify test images usually programming believe solving complicated problem like recognizing mnist digits requires sophisticated algorithm even neural networks wan paper mentioned involve quite simple algorithms variations algorithm seen chapter complexity learned automatically training data sense moral results sophisticated papers problems sophisticated algorithm simple learning algorithm good training data toward deep learning figure credits ester inbar unknown nasa esa illingworth magee oesch university california santa cruz bouwens leiden university hudf team 
[using, neural, nets, recognize, handwritten, digits, learning, gradient, descent] design neural network learn recognize digits first thing need data set learn called training data set use contains tens thousands scanned images handwritten digits together correct classifications mnist name comes fact modified subset two data sets collected nist united states national institute standards technology images mnist using neural nets recognize handwritten digits see digits fact shown beginning chapter challenge recognize course testing network ask recognize images training set mnist data comes two parts first part contains images used training data images scanned handwriting samples people half census bureau employees half high school students images greyscale pixels size second part mnist data set images used test data greyscale images use test data evaluate well neural network learned recognize digits make good test performance test data taken different set people original training data albeit still group split census bureau employees high school students helps give confidence system recognize digits people whose writing see training use notation denote training input convenient regard training input dimensional vector entry vector represents grey value single pixel image denote corresponding desired output dimensional vector example particular training image depicts desired output network note transpose operation turning row vector ordinary column vector like algorithm lets find weights biases output network approximates training inputs quantify well achieving goal define cost denotes collection weights network biases total number training inputs vector outputs network input sum training inputs course output depends keep notation simple explicitly indicated dependence notation denotes usual length function vector call quadratic cost function also sometimes known mean squared error mse inspecting form quadratic cost function see non negative since every term sum non negative furthermore cost becomes small precisely approximately equal output training inputs training algorithm done good job find weights biases contrast well large would mean close output large number inputs aim training algorithm minimize cost function weights biases words want find set weights biases make cost small possible using algorithm known gradient descent sometimes referred loss objective function use term cost function throughout book note terminology since often used research papers discussions neural networks learning gradient descent introduce quadratic cost primarily interested number images correctly classified network try maximize number directly rather minimizing proxy measure like quadratic cost problem number images correctly classified smooth function weights biases network part making small changes weights biases cause change number training images classified correctly makes difficult figure change weights biases get improved performance instead use smooth cost function like quadratic cost turns easy figure make small changes weights biases get improvement cost focus first minimizing quadratic cost examine classification accuracy even given want use smooth cost function may still wonder choose quadratic function used equation works perfectly well understanding basics learning neural networks stick recapping goal training neural network find weights biases minimize quadratic cost function well posed problem got lot distracting structure currently posed interpretation weights biases function lurking background choice network architecture mnist turns understand tremendous amount ignoring structure concentrating minimization aspect going forget specific form cost function connection neural networks instead going imagine simply given function many variables want minimize function going develop technique called gradient descent used solve minimization problems come back specific function want minimize neural networks okay let suppose trying minimize function could real valued function many variables note replaced notation emphasize could function specifically thinking neural networks context minimize helps imagine function two variables call like find achieves global minimum course function plotted eyeball graph find minimum sense using neural nets recognize handwritten digits perhaps shown slightly simple function general function may complicated function many variables usually possible eyeball graph find minimum one way attacking problem use calculus try find minimum analytically could compute derivatives try using find places extremum luck might work function one variables turn nightmare many variables neural networks often want far variables biggest neural networks cost functions depend billions weights biases extremely complicated way using calculus minimize work asserting gain insight imagining function two variables turned around twice two paragraphs said hey function many two variables sorry please believe say really help imagine function two variables happens sometimes picture breaks last two paragraphs dealing breakdowns good thinking mathematics often involves juggling multiple intuitive pictures learning appropriate use picture okay calculus work fortunately beautiful analogy suggests algorithm works pretty well start thinking function kind valley squint little plot hard imagine ball rolling slope valley everyday experience tells ball eventually roll bottom valley perhaps use idea way find minimum function randomly choose starting point imaginary ball simulate motion ball rolled bottom valley could simulation simply computing derivatives perhaps second derivatives derivatives would tell everything need know local shape valley therefore ball roll based written might suppose trying write newton equations motion ball considering effects friction gravity actually going take ball rolling analogy quite seriously devising algorithm minimize developing accurate simulation laws physics ball eye view meant stimulate imagination constrain thinking rather get messy details physics let simply ask declared god day could make laws physics dictating ball roll law laws motion could pick would make ball always rolled bottom valley make question precise let think happens move ball small amount direction small amount direction calculus tells changes follows going find way choosing make negative choose ball rolling valley figure make choice helps define vector changes transpose operation turning row vectors column vectors also define learning gradient descent gradient vector partial derivatives denote gradient vector moment rewrite change terms gradient getting though want clarify something sometimes gets people hung gradient meeting notation first time people sometimes wonder think symbol exactly mean fact perfectly fine think single mathematical object vector defined happens written using two symbols point view piece notational flag waving telling hey gradient vector advanced points view viewed independent mathematical entity right example differential operator need points view definitions expression rewritten equation helps explain called gradient vector relates changes changes expect something called gradient really exciting equation lets see choose make negative particular suppose choose small positive parameter known learning rate equation tells guarantees always decrease never increase change according prescription exactly property wanted take equation compute value move ball position amount use update rule make another move keep keep decreasing hope reach global minimum summing way gradient descent algorithm works repeatedly compute gradient move opposite direction falling slope valley visualize like using neural nets recognize handwritten digits notice rule gradient descent reproduce real physical motion real life ball momentum momentum may allow roll across slope even momentarily roll uphill effects friction set ball guaranteed roll valley contrast rule choosing says right still pretty good rule finding minimum make gradient descent work correctly need choose learning rate small enough equation good approximation might end obviously would good time want small since make changes tiny thus gradient descent algorithm work slowly practical implementations often varied equation remains good approximation algorithm slow see later works explained gradient descent function two variables fact everything works well even function many variables suppose particular function variables change produced small change gradient vector two variable case choose guaranteed approximate expression negative gives way following gradient minimum even function many learning gradient descent variables repeatedly applying update rule think update rule defining gradient descent algorithm gives way repeatedly changing position order find minimum function rule always work several things wrong prevent gradient descent finding global minimum point return explore later chapters practice gradient descent often works extremely well neural networks find powerful way minimizing cost function helping net learn indeed even sense gradient descent optimal strategy searching minimum let suppose trying make move position decrease much possible equivalent minimizing constrain size move small fixed words want move small step fixed size trying find movement direction decreases much possible proved choice minimizes determined size constraint gradient descent viewed way taking small steps direction immediately decrease 
[using, neural, nets, recognize, handwritten, digits, exercize] extreme version gradient descent use mini batch size given training input update weights biases according rules choose another training input update weights biases repeatedly procedure known online line incremental learning online learning neural network learns one training input time human beings name one advantage one disadvantage online learning compared stochastic gradient descent mini batch size say let conclude section discussing point sometimes bugs people new gradient descent neural networks cost course function many variables weights biases sense defines surface high dimensional space people get hung thinking hey able visualize extra dimensions may start worry think four dimensions let alone five five million special ability missing ability real supermathematicians course answer even professional mathematicians visualize four dimensions especially well trick use instead develop ways representing going exactly used algebraic rather visual representation figure move decrease people good thinking high dimensions mental library containing many different techniques along lines algebraic trick one example techniques may simplicity accustomed visualizing three dimensions build library techniques get pretty good thinking high dimensions detail using neural nets recognize handwritten digits interested may enjoy reading discussion techniques professional mathematicians use think high dimensions techniques discussed quite complex much best content intuitive accessible could mastered anyone 
[using, neural, nets, recognize, handwritten, digits, implementing, network, classify, digits] alright let write program learns recognize handwritten digits using stochastic gradient descent mnist training data short python program lines code first thing need get mnist data git user obtain data cloning code repository book git clone https github com mnielsen neural networks deep learning git use git download data code incidentally described mnist data earlier said split training images test images official mnist description actually going split data little differently leave test images split image mnist training set two parts set images use train neural network separate image validation set use validation data chapter later book find useful figuring set certain hyper parameters neural network things like learning rate directly selected learning algorithm although validation data part original mnist specification many people use mnist fashion use validation data common neural networks refer mnist training data referring image data set original image data apart mnist data also need python library called numpy fast linear algebra already numpy installed get let explain core features neural networks code giving full listing centerpiece network class use represent neural network code use initialize network object class network object def __init__ self sizes self num_layers len sizes self sizes sizes self biases random randn sizes self weights random randn zip sizes sizes code list sizes contains number neurons respective layers example want create network object neurons first layer neurons second layer neuron final layer code noted earlier mnist data set based two data sets collected nist united states national institute standards technology construct mnist nist data sets stripped put convenient format yann lecun corinna cortes christopher burges see details data set repository form makes easy load manipulate mnist data python obtained particular form data lisa machine learning laboratory university montreal implementing network classify digits net network biases weights network object initialized randomly using numpy random randn function generate gaussian distributions mean standard deviation random initialization gives stochastic gradient descent algorithm place start later chapters find better ways initializing weights biases note network initialization code assumes first layer neurons input layer omits set biases neurons since biases ever used computing outputs later layers note also biases weights stored lists numpy matrices example net weights numpy matrix storing weights connecting second third layers neurons first second layers since python list indexing starts since net weights rather verbose let denote matrix matrix weight connection neuron second layer neuron third layer ordering indices may seem strange surely make sense swap indices around big advantage using ordering means vector activations third layer neurons quite bit going equation let unpack piece piece vector activations second layer neurons obtain multiply weight matrix add vector biases apply function elementwise every entry vector gives result earlier rule equation computing output sigmoid neuron 
[using, neural, nets, recognize, handwritten, digits, toward, deep, learning] neural network gives impressive performance performance somewhat mysterious weights biases network discovered automatically means immediately explanation network find way understand principles network classifying handwritten digits given principles better put questions starkly suppose decades hence neural networks lead artificial intelligence understand intelligent networks work perhaps networks opaque weights biases understand learned automatically early days research people hoped effort build would also help understand principles behind intelligence maybe functioning human brain perhaps outcome end understanding neither brain artificial intelligence works address questions let think back interpretation artificial neurons gave start chapter means weighing evidence suppose want determine whether image shows human face could attack problem way attacked handwriting recognition using pixels image input neural network output network single neuron indicating either yes face face let suppose using learning algorithm instead going try design network hand choosing appropriate weights biases might forgetting neural networks entirely moment heuristic could use decompose problem sub problems image eye top left eye top right nose middle mouth bottom middle hair top answers several questions yes even probably yes conclude image likely face conversely answers questions image probably face course rough heuristic suffers many deficiencies maybe person bald hair maybe see part face face angle facial features obscured still heuristic suggests solve sub problems using neural networks perhaps build neural network face detection combining networks sub problems using neural nets recognize handwritten digits possible architecture rectangles denoting sub networks note intended realistic approach solving face detection problem rather help build intuition networks function architecture also plausible sub networks decomposed suppose considering question eye top left decomposed questions eyebrow eyelashes iris course questions really include positional information well eyebrow top left iris kind thing let keep simple network answer question eye top left decomposed questions broken multiple layers ultimately working sub networks answer questions simple easily answered level single pixels questions might example presence absence simple shapes particular points image questions answered single neurons connected raw pixels image toward deep learning end result network breaks complicated question image show face simple questions answerable level single pixels series many layers early layers answering simple specific questions input image later layers building hierarchy ever complex abstract concepts networks kind many layer structure two hidden layers called deep neural networks course said recursive decomposition sub networks certainly practical hand design weights biases network instead like use learning algorithms network automatically learn weights biases thus hierarchy concepts training data researchers tried using stochastic gradient descent backpropagation train deep networks unfortunately except special architectures much luck networks would learn slowly practice often slowly useful since set techniques developed enable learning deep neural nets deep learning techniques based stochastic gradient descent back propagation also introduce new ideas techniques enabled much deeper larger networks trained people routinely train networks hidden layers turns perform far better many problems shallow neural networks networks single hidden layer reason course ability deep nets build complex hierarchy concepts bit like way conventional programming languages use modular design ideas abstraction enable creation complex computer programs comparing deep network shallow network bit like comparing programming language ability make function calls stripped language ability make calls abstraction takes different form neural networks conventional programming important using neural nets recognize handwritten digits 
[backpropagation, algorithm, works] backpropagation algorithm based common linear algebraic operations things like vector addition multiplying vector matrix one operations little less commonly used particular suppose two vectors dimension use denote elementwise product two vectors thus components example kind elementwise multiplication sometimes called hadamard product schur product refer hadamard product good matrix libraries usually provide fast implementations hadamard product comes handy implementing backpropagation 
[backpropagation, algorithm, works, warm, up, fast, matrix-based, approach, computing, output, neural, network] discussing backpropagation let warm fast matrix based algorithm compute output neural network actually already briefly saw algorithm near end last chapter section described quickly worth revisiting detail particular good way getting comfortable notation used backpropagation familiar context let begin notation lets refer weights network unam biguous way use denote weight connection neuron layer neuron layer example diagram shows weight connection fourth neuron second layer second neuron third layer network notation cumbersome first take work master little effort find notation becomes easy natural one quirk notation ordering indices might think makes sense use refer input neuron output neuron vice versa actually done explain reason quirk use similar notation network biases activations explicitly use bias neuron layer use activation neuron layer following diagram shows examples notations use notations activation neuron layer related activations layer equation compare equation surrounding discussion last chapter warm fast matrix based approach computing output neural network sum neurons layer rewrite expression matrix form define weight matrix layer entries weight matrix weights connecting layer neurons entry row column similarly layer define bias vector probably guess works components bias vector values one component neuron layer finally define activation vector whose components activations last ingredient need rewrite matrix form idea vectorizing function met vectorization briefly last chapter recap idea want apply function every element vector use obvious notation denote kind elementwise application function components example function vectorized form effect vectorized squares every element vector notations mind equation rewritten beautiful compact vectorized form expression gives much global way thinking activations one layer relate activations previous layer apply weight matrix activations add bias vector finally apply global view often easier succinct involves fewer indices neuron neuron view taken think way escaping index hell remaining precise going expression also useful practice matrix libraries provide fast ways implementing matrix multiplication vector addition vectorization indeed code see last chapter made implicit use expression compute behaviour network using equation compute compute intermediate quantity along way quantity turns useful enough worth naming call weighted input neurons layer make considerable use weighted input later chapter equation sometimes written terms weighted input also worth noting components weighted input activation function neuron layer way expression motivates quirk notation mentioned earlier used index input neuron index output neuron need replace weight matrix equation transpose weight matrix small change annoying lose easy simplicity saying thinking apply weight matrix activations backpropagation algorithm works 
[backpropagation, algorithm, works, two, assumptions, need, cost, function] goal backpropagation compute partial derivatives cost function respect weight bias network backpropagation work need make two main assumptions form cost function stating assumptions though useful example cost function mind use quadratic cost function last chapter equation notation last section quadratic cost form total number training examples sum individual training examples corresponding desired output denotes number layers network vector activations output network input okay assumptions need make cost function order backpropagation applied first assumption need cost function written average cost functions individual training examples case quadratic cost function cost single training example assumption also hold true cost functions meet book reason need assumption backpropagation actually lets compute partial derivatives single training example recover averaging training examples fact assumption mind suppose training example fixed drop subscript writing cost eventually put back notational nuisance better left implicit second assumption make cost written function outputs neural network example quadratic cost function satisfies requirement since quadratic cost single training example may written thus function output activations course cost function also depends hadamard product desired output may wonder regarding cost also function remember though input training example fixed output also fixed parameter particular something modify changing weights biases way something neural network learns makes sense regard function output activations alone merely parameter helps define function 
[backpropagation, algorithm, works, four, fundamental, equations, behind, backpropagation] backpropagation understanding changing weights biases network changes cost function ultimately means computing partial derivatives compute first introduce intermediate quantity call error neuron layer backpropagation give procedure compute error relate understand error defined imagine demon neural network demon sits neuron layer input neuron comes demon messes neuron operation adds little change neuron weighted input instead outputting neuron instead outputs change propagates later layers network finally causing overall cost change amount backpropagation algorithm works demon good demon trying help improve cost trying find makes cost smaller suppose large value either positive negative demon lower cost quite bit choosing opposite sign contrast close zero demon improve cost much perturbing weighted input far demon tell neuron already pretty near heuristic sense measure error neuron motivated story define error neuron layer per usual conventions use denote vector errors associated layer backpropagation give way computing every layer relating errors quantities real interest might wonder demon changing weighted input surely natural imagine demon changing output activation result using measure error fact things work quite similarly discussion turns make presentation backpropagation little algebraically complicated stick measure plan attack backpropagation based around four fundamental equations together equations give way computing error gradient cost function state four equations warned though expect instantaneously assimilate equations expectation lead disappointment fact backpropagation equations rich understanding well requires considerable time patience gradually delve deeper equations good news patience repaid many times discussion section merely beginning helping way thorough understanding equations preview ways delve deeply equations later chapter give helps explain true restate equations algorithmic form see pseudocode implemented real final section chapter develop intuitive picture backpropagation equations mean someone along way return repeatedly four fundamental equations deepen understanding equations come seem comfortable perhaps even beautiful natural equation error output layer components given case small changes course assume demon constrained make small changes classification problems like mnist term error sometimes used mean classification failure rate neural net correctly classifies percent digits error percent obviously quite different meaning vectors practice trouble telling meaning intended given usage four fundamental equations behind backpropagation natural expression first term right measures fast cost changing function output activation example depend much particular output neuron small expect second term right measures fast activation function changing notice everything easily computed particular compute computing behaviour network small additional overhead compute exact form course depend form cost function however provided cost function known little trouble computing example using quadratic cost function obviously easily computable equation componentwise expression perfectly good expression matrix based form want backpropagation however easy rewrite equation matrix based form bpa defined vector whose components partial derivatives think expressing rate change respect output activations easy see equations interchangeably refer equations example case quadratic cost fully matrix based form becomes see everything expression nice vector form easily computed using library numpy equation error terms error next layer particular transpose weight matrix layer equation appears complicated element nice interpretation suppose know error layer apply transpose weight matrix think intuitively moving error backward network giving sort measure error output layer take hadamard product moves error backward activation function layer giving error weighted input layer combining compute error layer network start using compute apply equation compute equation compute way back network equation rate change cost respect bias net work particular error exactly equal rate change great news since backpropagation algorithm works already told compute rewrite shorthand understood evaluated neuron bias equation rate change cost respect weight network particular tells compute partial derivatives terms quantities already know compute equation rewritten less index heavy notation understood activation neuron input weight error neuron output weight zooming look weight two neurons connected weight depict nice consequence equation activation small gradient term also tend small case say weight learns slowly meaning changing much gradient descent words one consequence weights output low activation neurons learn slowly insights along lines obtained let start looking output layer consider term recall graph sigmoid function last chapter function becomes flat approximately occurs lesson weight final layer learn slowly output neuron either low activation high activation case common say output neuron saturated result weight stopped learning learning slowly similar remarks hold also biases output neuron obtain similar insights earlier layers particular note term means likely get small neuron near saturation turn means weights input saturated neuron learn summing learnt weight learn slowly either input neuron low activation output neuron saturated either high low activation none observations greatly surprising still help improve mental model going neural network learns furthermore turn type reasoning around four fundamental equations turn hold activation function standard sigmoid function see moment reasoning hold large enough entries compensate smallness speaking general tendency four fundamental equations behind backpropagation proofs use special properties use equations design activation functions particular desired learning properties example give idea suppose choose non sigmoid activation function always positive never gets close zero would prevent slow learning occurs ordinary sigmoid neurons saturate later book see examples kind modification made activation function keeping four equations mind help explain modifications tried impact 
[backpropagation, algorithm, works, problem] fully matrix based approach backpropagation mini batch imple mentation stochastic gradient descent loops training examples mini batch possible modify backpropagation algorithm computes gradients training examples mini batch simultaneously idea instead beginning single input vector begin matrix backpropagation algorithm works whose columns vectors mini batch forward propagate multiply ing weight matrices adding suitable matrix bias terms applying sigmoid function everywhere backpropagate along similar lines explicitly write pseudocode approach backpropagation algorithm modify network uses fully matrix based approach advantage approach takes full advantage modern libraries linear algebra result quite bit faster looping mini batch laptop example speedup factor two run mnist classification problems like considered last chapter practice serious libraries backpropagation use fully matrix based approach variant 
[backpropagation, algorithm, works, proof, four, fundamental, equations, optional] prove four fundamental equations four consequences chain rule multivariable calculus comfortable chain rule strongly encourage attempt derivation reading let begin equation gives expression output error prove equation recall definition applying chain rule express partial derivative terms partial derivatives respect output activations sum neurons output layer course output activation neuron depends weighted input neuron vanishes result simplify previous equation recalling second term right written equation becomes gives equation error terms error next layer want rewrite terms using chain rule last line interchanged two terms right hand side substituted definition evaluate first term last line note differentiating obtain backpropagation algorithm substituting back obtain written component form final two equations want prove also follow chain rule manner similar proofs two equations leave exercise 
[backpropagation, algorithm, works, exercise] prove equations completes proof four fundamental equations backpropagation proof may seem complicated really outcome carefully applying chain rule little less succinctly think backpropagation way computing gradient cost function systematically applying chain rule multi variable calculus really backpropagation rest details 
[backpropagation, algorithm, works, backpropagation, algorithm] backpropagation equations provide way computing gradient cost function let explicitly write form algorithm input set corresponding activation input layer feedforward compute output error compute vector backpropagate error compute output gradient cost function given examining algorithm see called back propagation compute error vectors backward starting final layer may seem peculiar going network backward think proof backpropagation backward movement consequence fact cost function outputs network understand cost varies earlier weights biases need repeatedly apply chain rule working backward layers obtain usable expressions 
[backpropagation, algorithm, works, exercises] backpropagation single modified neuron suppose modify single neuron feedforward network output neuron given function sigmoid modify backpropagation algorithm case backpropagation linear neurons suppose replace usual non linear function throughout network rewrite backpropagation algorithm case described backpropagation algorithm computes gradient cost function single training example practice common combine backpropagation learning algorithm stochastic gradient descent compute gradient many training examples particular given mini batch backpropagation algorithm works training examples following algorithm applies gradient descent learning step based mini batch input set training examples training example set corresponding input activation perform following steps feedforward compute output error compute vector backpropagate error compute gradient descent update weights according rule biases according rule course implement stochastic gradient descent practice also need outer loop generating mini batches training examples outer loop stepping multiple epochs training omitted simplicity 
[backpropagation, algorithm, works, code, backpropagation] understood backpropagation abstract understand code used last chapter implement backpropagation recall chapter code contained update_mini_batch backprop methods network class code methods direct translation algorithm described particular update_mini_batch method updates network weights biases computing gradient current mini_batch training examples class network object def update_mini_batch self mini_batch eta update network weights biases applying gradient descent using backpropagation single mini batch mini_batch list tuples eta learning rate nabla_b zeros shape self biases nabla_w zeros shape self weights mini_batch delta_nabla_b delta_nabla_w self backprop nabla_b dnb dnb zip nabla_b delta_nabla_b nabla_w dnw dnw zip nabla_w delta_nabla_w self weights eta len mini_batch zip self weights nabla_w self biases eta len mini_batch zip self biases nabla_b work done line delta_nabla_b delta_nabla_w self backprop uses backprop method figure partial derivatives backprop method follows algorithm last section closely one small change use slightly different approach indexing layers change made take advantage feature python namely use negative list indices count backward end list third last entry list code backprop together helper functions used compute function derivative derivative cost function code backpropagation inclusions able understand code self contained way something tripping may find helpful consult original description complete listing code class network object def backprop self return tuple nabla_b nabla_w representing gradient cost function c_x nabla_b nabla_w layer layer lists numpy arrays similar self biases self weights nabla_b zeros shape self biases nabla_w zeros shape self weights feedforward activation activations list store activations layer layer list store vectors layer layer zip self biases self weights dot activation append activation sigmoid activations append activation backward pass delta self cost_derivative activations sigmoid_prime nabla_b delta nabla_w dot delta activations transpose note variable loop used little differently notation chapter book means last layer neurons second last layer renumbering scheme book used take advantage fact python use negative indices lists xrange self num_layers sigmoid_prime delta dot self weights transpose delta nabla_b delta nabla_w dot delta activations transpose return nabla_b nabla_w def cost_derivative self output_activations return vector partial derivatives partial c_x partial output activations return output_activations def sigmoid sigmoid function return exp def sigmoid_prime derivative sigmoid function return sigmoid sigmoid 
[backpropagation, algorithm, works, sense, backpropagation, fast, algorithm] sense backpropagation fast algorithm answer question let consider another approach computing gradient imagine early days neural networks research maybe first person world think using gradient descent learn make idea work need way computing gradient cost function think back knowledge calculus decide see use chain rule compute gradient playing around bit algebra looks complicated get discouraged try find another approach decide regard cost function weights alone get back biases moment number weights want compute particular weight obvious way use approximation small positive number unit vector direction words estimate computing cost two slightly different values applying equation idea let compute partial derivatives respect biases approach looks promising simple conceptually extremely easy implement using lines code certainly looks much promising idea using chain rule compute gradient unfortunately approach appears promising implement code turns extremely slow understand imagine million weights network distinct weight need compute order compute means compute gradient need compute cost function million different times requiring million forward passes network per training example need compute well total million one passes network clever backpropagation enables simultaneously compute partial derivatives using one forward pass network followed one backward pass network roughly speaking computational cost backpropagation big picture backward pass forward total cost backpropagation roughly making two forward passes network compare million one forward passes needed approach based even though backpropagation appears superficially complex approach based actually much much faster speedup first fully appreciated greatly expanded range problems neural networks could solve turn caused rush people using neural networks course backpropagation panacea even late people ran limits especially attempting use backpropagation train deep neural networks networks many hidden layers later book see modern computers clever new ideas make possible use backpropagation train deep neural networks 
[backpropagation, algorithm, works, backpropagation, big, picture] explained backpropagation presents two mysteries first algorithm really developed picture error backpropagated output deeper build intuition going matrix vector multiplications second mystery someone could ever discovered backpropagation first place one thing follow steps algorithm even follow proof algorithm works mean understand problem well could discovered algorithm first place plausible line reasoning could led discover backpropagation algorithm section address mysteries improve intuition algorithm let imagine made small change weight network change weight cause change output activation corresponding neuron plausible requires analysis make careful statement plausible dominant computational cost forward pass multiplying weight matrices backward pass multiplying transposes weight matrices operations obviously similar computational cost backpropagation algorithm works turn cause change activations next layer changes turn cause changes next layer next way causing change final layer cost function change cost related change weight equation suggests possible approach computing carefully track small change propagates cause small change careful express everything along way terms easily computable quantities able compute let try carry change causes small change activation backpropagation big picture neuron layer change given change activation cause changes activations next layer layer concentrate way single one activations affected say fact cause following change substituting expression equation get course change turn cause changes activations next layer fact imagine path way network change activation causing change next activation finally change cost output path goes activations resulting expression picked type term additional neuron passed well term end represents change due changes activations along particular path network course many paths change propagate affect cost considering single path compute total change plausible sum possible paths weight final cost mnp backpropagation algorithm works summed possible choices intermediate neurons along path comparing see mnp equation looks complicated however nice intuitive interpretation computing rate change respect weight network equation tells every edge two neurons network associated rate factor partial derivative one neuron activation respect neuron activation edge first weight first neuron rate factor rate factor path product rate factors along path total rate change sum rate factors paths initial weight final cost procedure illustrated single path providing heuristic argument way thinking going perturb weight network let sketch line thinking could use develop argument first could derive explicit expressions individual partial derivatives equation easy bit calculus done could try figure write sums indices matrix multiplications turns tedious requires persistence extraordinary insight simplifying much possible discover end exactly backpropagation algorithm think backpropagation algorithm providing way computing sum rate factor paths put slightly differently backpropagation algorithm clever way keeping track small perturbations weights biases propagate network reach output affect cost going work messy requires considerable care work details challenge may enjoy attempting even hope line thinking gives insight backpropagation accomplishing mystery backpropagation could discovered first place fact follow approach sketched discover proof backpropagation unfortunately proof quite bit longer complicated one described earlier chapter short mysterious proof discovered find write details long proof backpropagation big picture fact several obvious simplifications staring face make simplifications get shorter proof write several obvious simplifications jump repeat result iterations proof saw short somewhat obscure signposts construction removed course asking trust really great mystery origin earlier proof lot hard work simplifying proof sketched section one clever step required equation intermediate variables activations like clever idea switch using weighted inputs like intermediate variables idea instead continue using activations proof obtain turns slightly complex proof given earlier chapter backpropagation algorithm works 
[improving, way, neural, networks, learn] golf player first learning play golf usually spend time developing basic swing gradually develop shots learning chip draw fade ball building modifying basic swing similar way focused understanding backpropagation algorithm basic swing foundation learning work neural networks chapter explain suite techniques used improve vanilla implementation backpropagation improve way networks learn techniques develop chapter include better choice cost function known use improve results obtained handwriting classification problem studied chapter course covering many many techniques developed use neural nets philosophy best entree plethora available techniques depth study important mastering impor tant techniques useful right also deepen understanding problems arise use neural networks leave well prepared quickly pick techniques need improving way neural networks learn 
[improving, way, neural, networks, learn, cross-entropy, cost, function] find unpleasant wrong soon beginning learn piano gave first performance audience nervous began playing piece octave low got confused continue someone pointed error embarrassed yet unpleasant also learn quickly decisively wrong bet next time played audience played correct octave contrast learn slowly errors less well defined ideally hope expect neural networks learn fast errors happens practice answer question let look toy example example involves neuron one input train neuron something ridiculously easy take input output course trivial task could easily figure appropriate weight bias hand without using learning algorithm however turns illuminating use gradient descent attempt learn weight bias let take look neuron learns make things definite pick initial weight initial bias generic choices used place begin learning picking special way initial output neuron quite bit learning needed neuron gets near desired output learning rate turns slow enough follow happening fast enough get substantial learning seconds cost quadratic cost function introduced back chapter remind exact form cost function shortly need dig definition see neuron rapidly learns weight bias drives cost gives output neuron quite desired output pretty good suppose however instead choose starting weight starting bias case initial output badly wrong let look neuron learns output case cross entropy cost function although example uses learning rate see learning starts much slowly indeed first learning epochs weights biases change much learning kicks much first example neuron output rapidly moves closer behavior strange contrasted human learning said beginning section often learn fastest badly wrong something seen artificial neuron lot difficulty learning badly wrong far difficulty little wrong turns behavior occurs toy model general networks learning slow find way avoiding slowdown understand origin problem consider neuron learns changing weight bias rate determined partial derivatives cost function saying learning slow really saying partial derivatives small challenge understand small understand let compute partial derivatives recall using quadratic cost function equation given neuron output training input used corresponding desired output write explicitly terms weight bias recall using chain rule differentiate respect weight bias get substituted understand behavior expressions let look closely term right hand side recall shape function improving way neural networks learn sigmoid function see graph neuron output close curve gets flat gets small equations tell get small origin learning slowdown shall see little later learning slowdown occurs essentially reason general neural networks toy example playing 
[improving, way, neural, networks, learn, cross-entropy, cost, function, introducing, cross-entropy, cost, function] address learning slowdown turns solve problem replacing quadratic cost different cost function known cross entropy understand cross entropy let move little away super simple toy model suppose instead trying train neuron several input variables corresponding weights bias output neuron course weighted sum inputs define cross entropy cost function neuron total number items training data sum training inputs corresponding desired output obvious expression fixes learning slowdown problem fact frankly even obvious makes sense call cost function addressing learning slowdown let see sense cross entropy interpreted cost function two properties particular make reasonable interpret cross entropy cost function first non negative see notice individual cross entropy cost function terms sum negative since logarithms numbers range minus sign front sum second neuron actual output close desired output training inputs cross entropy close see suppose example input case neuron good job input see first term expression cost vanishes since second term similar analysis holds contribution cost low provided actual output close desired output summing cross entropy positive tends toward zero neuron gets better computing desired output training inputs properties intuitively expect cost function indeed properties also satisfied quadratic cost good news cross entropy cross entropy cost function benefit unlike quadratic cost avoids problem learning slowing see let compute partial derivative cross entropy cost respect weights substitute apply chain rule twice obtaining putting everything common denominator simplifying becomes using definition sigmoid function little algebra show ask verify exercise let accept given see terms cancel equation simplifies become beautiful expression tells rate weight learns controlled error output larger error faster neuron learn intuitively expect particular avoids learning slowdown caused term analogous equation quadratic cost use cross entropy term gets canceled longer need worry small cancellation special miracle ensured cross entropy cost function actually really miracle see later cross entropy specially chosen property similar way compute partial derivative bias prove need assume desired outputs either usually case solving classification problems example computing boolean functions understand happens make assumption see exercises end section improving way neural networks learn details easily verify avoids learning slowdown caused term analogous equation quadratic cost equation 
[improving, way, neural, networks, learn, exercise] prove identity equation type neuron use networks tanh sigmoid priori answer obvious put mildly however theoretical arguments empirical evidence suggest tanh sometimes performs let briefly give flavor one theoretical arguments tanh neurons suppose using sigmoid neurons activations network positive let consider weights input neuron layer rules backpropagation tell associated gradient activations positive technical caveats statement tanh sigmoid neurons well rectified linear neurons discussed however informally usually fine think neural networks able approximate function arbitrary accuracy see example yann lecun léon bottou genevieve orr klaus robert müller xavier glorot yoshua bengio techniques sign gradient sign means positive weights decrease gradient descent negative weights increase gradient descent words weights neuron must either increase together decrease together problem since weights may need increase others need decrease happen input activations different signs suggests replacing sigmoid activation function tanh allows positive negative activations indeed tanh symmetric zero tanh tanh might even expect roughly speaking activations hidden layers would equally balanced positive negative would help ensure systematic bias weight updates one way seriously take argument argument suggestive heuristic rigorous proof tanh neurons outperform sigmoid neurons perhaps properties sigmoid neuron compensate problem indeed many tasks tanh found empirically provide small improvement performance sigmoid neurons unfortunately yet hard fast rules know neuron types learn fastest give best generalization performance particular application another variation sigmoid neuron rectified linear neuron rectified linear unit output rectified linear unit input weight vector bias given max graphically rectifying function max looks like max obviously neurons quite different sigmoid tanh neurons however like sigmoid tanh neurons rectified linear units used compute function trained using ideas backpropagation stochastic gradient descent use rectified linear units instead sigmoid tanh neurons improving way neural networks learn recent work image found considerable benefit using rectified linear units much network however tanh neurons yet really deep understanding exactly rectified linear units preferable give flavor issues recall sigmoid neurons stop learning saturate output near either seen repeatedly chapter problem terms reduce gradient slows learning tanh neurons suffer similar problem saturate contrast increasing weighted input rectified linear unit never cause saturate corresponding learning slowdown hand weighted input rectified linear unit negative gradient vanishes neuron stops learning entirely two many issues make non trivial understand rectified linear units perform better sigmoid tanh neurons painted picture uncertainty stressing yet solid theory activation functions chosen indeed problem harder even described infinitely many possible activation functions best given problem result network learns fastest give highest test accuracies surprised little really deep systematic investigation done questions ideally theory tells detail choose perhaps modify fly activation functions hand let lack full theory stop powerful tools already hand make lot progress tools remainder book continue use sigmoid neurons neuron since powerful provide concrete illustrations core ideas neural nets keep back mind ideas applied types neuron sometimes advantages 
[improving, way, neural, networks, learn, exercises] monotonicity softmax show positive negative consequence increasing guaranteed increase corresponding output activation decrease output activations already saw empirically sliders rigorous proof non locality softmax nice thing sigmoid layers output function corresponding weighted input explain case softmax layer particular output activation depends weighted inputs 
[improving, way, neural, networks, learn, problems] modify code implement regularization use regularization classify mnist digits using hidden neuron network find regularization parameter enables better running unregularized take look network cost_derivative method network method written quadratic cost would rewrite method cross entropy cost think problem might arise cross entropy version network eliminated network cost_derivative method entirely instead incorporating functionality crossentropycost delta method solve problem identified 
[improving, way, neural, networks, learn, using, cross-entropy, classify, mnist, digits] cross entropy easy implement part program learns using gradient descent backpropagation later chapter developing improved version earlier program classifying mnist handwritten digits network new program called network incorporates cross entropy also several techniques developed let look well new program classifies mnist digits case chapter use network hidden neurons use mini batch size set learning rate train epochs interface network slightly different network still clear going way get documentation network interface using commands help network network sgd python shell import mnist_loader training_data validation_data test_data mnist_loader load_data_wrapper import network net network network cost network crossentropycost net large_weight_initializer net sgd training_data evaluation_data test_data monitor_evaluation_accuracy true code available chapter used quadratic cost learning rate discussed possible say precisely means use learning rate cost function changed cost functions experimented find learning rate provides near optimal performance given hyper parameter choices incidentally rough general heuristic relating learning rate cross entropy quadratic cost saw earlier gradient terms quadratic cost extra term suppose average values see roughly quadratic cost learns average times slower learning rate suggests reasonable starting point divide learning rate quadratic cost course argument far rigorous taken seriously still sometimes useful starting point improving way neural networks learn note way net large_weight_initializer command used initial ize weights biases way described chapter need run command later chapter change default weight initialization networks result running sequence commands network percent accuracy pretty close result obtained chapter percent using quadratic cost let look also case use hidden neurons cross entropy otherwise keep parameters case obtain accuracy percent substantial improvement results chapter obtained classification accuracy percent using quadratic cost may look like small change consider error rate dropped percent percent eliminated one fourteen original errors quite handy improvement encouraging cross entropy cost gives similar better results quadratic cost however results conclusively prove cross entropy better choice reason put little effort choosing hyper parameters learning rate mini batch size improvement really convincing need thorough job optimizing hyper parameters still results encouraging reinforce earlier theoretical argument cross entropy better choice quadratic cost way part general pattern see chapter indeed much rest book develop new technique try get improved results course nice see improvements interpretation improvements always problematic truly convincing see improvement putting tremendous effort optimizing hyper parameters great deal work requiring lots computing power usually going exhaustive investigation instead proceed basis informal tests like done still keep mind tests fall short definitive proof remain alert signs arguments breaking discussed cross entropy great length much effort gives small improvement mnist results later chapter see techniques notably regularization give much bigger improvements much focus cross entropy part reason cross entropy widely used cost function worth understanding well important reason neuron saturation important problem neural nets problem return repeatedly throughout book discussed cross entropy length good laboratory begin understanding neuron saturation may addressed 
[improving, way, neural, networks, learn, cross-entropy, mean, come, from] discussion cross entropy focused algebraic analysis practical implemen tation useful leaves unanswered broader conceptual questions like cross entropy mean intuitive way thinking cross entropy could dreamed cross entropy first place let begin last questions could motivated think cross entropy first place suppose discovered learning slowdown described cross entropy cost function earlier understood origin terms equations staring equations bit might wonder possible choose cost function term disappeared case cost single training example would satisfy could choose cost function make equations true would capture simple way intuition greater initial error faster neuron learns also eliminate problem learning slowdown fact starting equations show possible derive form cross entropy simply following mathematical noses see note chain rule using last equation becomes comparing equation obtain integrating expression respect gives constant constant integration contribution cost single training example get full cost function must average training examples obtaining constant constant average individual constants training exam ple see equations uniquely determine form cross entropy overall constant term cross entropy something miraculously pulled thin air rather something could discovered simple natural way intuitive meaning cross entropy think explaining depth would take afield want however worth mentioning standard way interpreting cross entropy comes field information theory roughly speaking idea cross entropy measure improving way neural networks learn surprise particular neuron trying compute function instead computes function suppose think neuron estimated probability estimated probability right value cross entropy measures surprised average learn true value get low surprise output expect high surprise output unexpected course said exactly surprise means perhaps seems like empty verbiage fact precise information theoretic way saying meant surprise unfortunately know good short self contained discussion subject available online want dig deeper wikipedia contains 
[improving, way, neural, networks, learn, problem] add momentum based stochastic gradient descent network approaches minimizing cost function many approaches minimizing cost function developed universal agreement best approach deeper neural networks worth digging techniques understanding work strengths weaknesses apply practice paper mentioned introduces compares several techniques including conjugate gradient descent bfgs method see also closely related limited memory bfgs method known bfgs another technique recently shown promising nesterov accelerated gradient technique improves momentum technique however many problems plain stochastic gradient descent works well especially momentum used stick stochastic gradient descent remainder book 
[improving, way, neural, networks, learn, softmax] chapter mostly use cross entropy cost address problem learning slowdown however want briefly describe another approach problem based called softmax layers neurons actually going use softmax layers remainder chapter great hurry skip next section however softmax still worth understanding part intrinsically interesting part use softmax layers chapter discussion deep neural networks idea softmax define new type output layer neural networks begins way sigmoid layer forming weighted however apply sigmoid function get output instead softmax layer apply called softmax function according function activation output neuron denominator sum output neurons familiar softmax function equation may look pretty opaque certainly obvious want use function also obvious help address learning slowdown problem better understand equation suppose network four output neurons four corresponding weighted inputs denote figure shows graph corresponding output activations different increase see increase describing softmax make frequent use notation introduced last chapter may wish revisit chapter need refresh memory meaning notation paragraph adaptation animation online version book cross entropy cost function figure equation different fixed values variable index avoided clarity corresponding output activation decrease output activations similarly decrease decrease output activations increase fact look closely see cases total change activations exactly compensates change reason output activations guaranteed always sum prove using equation little algebra result increases output activations must decrease total amount ensure sum activations remains course similar statements hold activations equation also implies output activations positive since ponential function positive combining observation last paragraph see output softmax layer set positive numbers sum words output softmax layer thought probability distribution fact softmax layer outputs probability distribution rather pleasing many problems convenient able interpret output activation network estimate probability correct output instance mnist classification problem interpret network estimated probability correct digit classification contrast output layer sigmoid layer certainly assume activations formed probability distribution explicitly prove plausible activations sigmoid layer general form probability distribution sigmoid output layer simple interpretation output activations 
[improving, way, neural, networks, learn, overfitting, regularization] nobel prize winning physicist enrico fermi asked opinion mathematical model colleagues proposed solution important unsolved physics problem model gave excellent agreement experiment fermi skeptical asked many free parameters could set model four answer fermi improving way neural networks learn remember friend johnny von neumann used say four parameters fit elephant five make wiggle trunk point course models large number free parameters describe amazingly wide range phenomena even model agrees well available data make good model may mean enough freedom model describe almost data set given size without capturing genuine insights underlying phenomenon happens model work well existing data fail generalize new situations true test model ability make predictions situations exposed fermi von neumann suspicious models four parameters hidden neuron network classifying mnist digits nearly parameters lot parameters hidden neuron network nearly parameters state art deep neural nets sometimes contain millions even billions parameters trust results let sharpen problem constructing situation network bad job generalizing new situations use hidden neuron network parameters train network using mnist training images instead use first training images using restricted set make problem generalization much evident train similar way using cross entropy cost function learning rate mini batch size however train epochs somewhat larger number using many training examples let use network look way cost function changes import mnist_loader training_data validation_data test_data mnist_loader load_data_wrapper import network net network network cost network crossentropycost net large_weight_initializer net sgd training_data evaluation_data test_data monitor_evaluation_accuracy true monitor_training_cost true using results plot way cost changes network quote comes charming article one people proposed flawed model four parameter elephant may found next four graphs generated program overfitting regularization looks encouraging showing smooth decrease cost expect note shown training epochs gives nice close view later stages learning see turns interesting action let look classification accuracy test data changes time zoomed quite bit first epochs shown accuracy rises percent learning gradually slows finally around epoch classification accuracy pretty much stops improving later epochs merely see small stochastic fluctuations near value accuracy epoch contrast earlier graph cost associated training data continues smoothly drop look cost appears model still getting better test accuracy results show improvement illusion like model fermi disliked network learns epoch longer generalizes test data useful learning say network overfitting overtraining beyond epoch might wonder problem looking cost training data opposed classification accuracy test data words maybe problem making apples oranges comparison would happen compared cost training data cost test data comparing similar measures perhaps could compare classification accuracy training data test data fact essentially phenomenon shows improving way neural networks learn matter comparison details change however instance let look cost test data see cost test data improves around epoch actually starts get worse even though cost training data continuing get better another sign model overfitting poses puzzle though whether regard epoch epoch point overfitting coming dominate learning practical point view really care improving classification accuracy test data cost test data proxy classification accuracy makes sense regard epoch point beyond overfitting dominating learning neural network another sign overfitting may seen classification accuracy training data accuracy rises way percent network correctly classifies training images meanwhile test accuracy tops percent network really learning peculiarities training set recognizing digits general almost though network merely memorizing training set without understanding digits well enough generalize test set overfitting major problem neural networks especially true modern networks often large numbers weights biases train effectively overfitting regularization need way detecting overfitting going overtrain like techniques reducing effects overfitting obvious way detect overfitting use approach keeping track accuracy test data network trains see accuracy test data longer improving stop training course strictly speaking necessarily sign overfitting might accuracy test data training data stop improving time still adopting strategy prevent overfitting fact use variation strategy recall load mnist data load three data sets import mnist_loader training_data validation_data test_data mnist_loader load_data_wrapper using training_data test_data ignoring val idation_data validation_data contains images digits images different images mnist training set images mnist test set instead using test_data prevent overfitting use validation_data use much strategy described test_data compute classification accuracy validation_data end epoch classification accuracy validation_data saturated stop training strategy called early stopping course practice immediately know accuracy saturated instead continue training confident accuracy use validation_data prevent overfitting rather test_data fact part general strategy use validation_data evaluate different trial choices hyper parameters number epochs train learning rate best network architecture use evaluations find set good values hyper parameters indeed although mentioned part arrived hyper parameter choices made earlier book later course way answer question using valida tion_data prevent overfitting rather test_data instead replaces general question using validation_data rather test_data set good hyper parameters understand consider setting hyper parameters likely try many different choices hyper parameters set hyper parameters based evaluations test_data possible end overfitting hyper parameters test_data may end finding hyper parameters fit particular peculiarities test_data performance network generalize data sets guard figuring hyper parameters using validation_data got hyper parameters want final evaluation accuracy using test_data gives confidence results test_data true measure well neural network gener alizes put another way think validation data type training data requires judgment determine stop earlier graphs identified epoch place accuracy saturated possible pessimistic neural networks sometimes plateau training continuing improve surprised learning could occurred even epoch although magnitude improvement would likely small possible adopt less aggressive strategies early stopping improving way neural networks learn helps learn good hyper parameters approach finding good hyper parameters sometimes known hold method since validation_data kept apart held training_data practice even evaluating performance test_data may change minds want try another approach perhaps different network architecture involve finding new set hyper parameters danger end overfitting test_data well need potentially infinite regress data sets confident results generalize addressing concern fully deep difficult problem practical purposes going worry much question instead plunge ahead using basic hold method based training_data validation_data test_data described looking far overfitting using training images happens use full training set images keep parameters hidden neurons learning rate mini batch size train using images epochs graph showing results classification accuracy training data test data note used test data rather validation data order make results directly comparable earlier graphs see accuracy test training data remain much closer together using training examples particular best classification accuracy percent training data percent higher percent test data compared percent gap earlier overfitting still going greatly reduced network generalizing much better training data test data general one best ways reducing overfitting increase size training data enough training data difficult even large network overfit unfortunately training data expensive difficult acquire always practical option 
[improving, way, neural, networks, learn, overfitting, regularization, regularization] increasing amount training data one way reducing overfitting ways reduce extent overfitting occurs one possible approach overfitting regularization reduce size network however large networks potential powerful small networks option adopt reluctantly fortunately techniques reduce overfitting even fixed network fixed training data known regularization techniques section describe one commonly used regularization techniques technique sometimes known weight decay regularization idea regularization add extra term cost function term called regularization term regularized cross entropy first term usual expression cross entropy added second term namely sum squares weights network scaled factor known regularization parameter usual size training set discuss later chosen also worth noting regularization term include biases also come back course possible regularize cost functions quadratic cost done similar way cases write regularized cost function original unregularized cost function intuitively effect regularization make network prefers learn small weights things equal large weights allowed considerably improve first part cost function put another way regularization viewed way compromising finding small weights minimizing original cost function relative importance two elements compromise depends value small prefer minimize original cost function large prefer small weights really obvious making kind compromise help reduce overfitting turns address question helps next section first let work example showing regularization really reduce overfitting construct example first need figure apply stochastic gradient descent learning algorithm regularized neural network particular need know compute partial derivatives weights improving way neural networks learn biases network taking partial derivatives equation gives terms computed using backpropagation described last chapter see easy compute gradient regularized cost function use backpropagation usual add partial derivative weight terms partial derivatives respect biases unchanged gradient descent learning rule biases change usual rule learning rule weights becomes exactly usual gradient descent learning rule except first rescale weight factor rescaling sometimes referred weight decay since makes weights smaller first glance looks though means weights driven unstoppably toward zero right since term may lead weights increase causes decrease unregularized cost function okay gradient descent works stochastic gradient descent well unregularized stochastic gradient descent estimate averaging mini batch training examples thus regularized learning rule stochastic gradient descent becomes equation sum training examples mini batch unregularized cost training example exactly usual rule stochastic gradient descent except weight decay factor finally completeness let state regularized learning rule biases course exactly unregularized case equation sum training examples mini batch let see regularization changes performance neural network use network hidden neurons mini batch size learning rate cross entropy cost function however time use regularization parameter note code use variable name lmbda lambda overfitting regularization reserved word python unrelated meaning also used test_data validation_data strictly speaking use validation_data reasons discussed earlier decided use test_data makes results directly comparable earlier unregularized results easily change code use validation_data instead find gives similar results import mnist_loader training_data validation_data test_data mnist_loader load_data_wrapper import network net network network cost network crossentropycost net large_weight_initializer net sgd training_data evaluation_data test_data lmbda monitor_evaluation_cost true monitor_evaluation_accuracy true monitor_training_cost true monitor_training_accuracy true cost training data decreases whole time much earlier unregularized time accuracy test_data continues increase entire epochs next two graphs produced program overfitting improving way neural networks learn clearly use regularization suppressed overfitting accuracy considerably higher peak classification accuracy percent compared peak percent obtained unregularized case indeed could almost certainly get considerably better results continuing train past epochs seems empirically regularization causing network generalize better considerably reducing effects overfitting happens move artificial environment training images return full image training set course seen already overfitting much less problem full images regularization help let keep hyper parameters epochs learning rate mini batch size however need modify regularization parameter reason size training set changed changes weight decay factor continued use would mean much less weight decay thus much less regularization effect compensate changing okay let train network stopping first initialize weights net large_weight_initializer net sgd training_data evaluation_data test_data lmbda monitor_evaluation_accuracy true monitor_training_accuracy true obtain results lots good news first classification accuracy test data percent running unregularized percent big improvement second see gap results training test data much narrower running percent still significant gap obviously made substantial progress reducing overfitting finally let see test classification accuracy get use hidden neurons regularization parameter detailed analysis overfitting purely fun see high accuracy get use new tricks cross entropy cost function regularization overfitting regularization net network network cost network crossentropycost net large_weight_initializer net sgd training_data lmbda evaluation_data validation_data monitor_evaluation_accuracy true final result classification accuracy percent validation data big jump hidden neuron case fact tuning little run epochs break percent barrier achieving percent classification accuracy validation data bad turns lines code described regularization way reduce overfitting increase classification accuracies fact benefit empirically multiple runs mnist networks different random weight initializations found unregularized runs occasionally get stuck apparently caught local minima cost function result different runs sometimes provide quite different results contrast regularized runs provided much easily replicable results going heuristically cost function unregularized length weight vector likely grow things equal time lead weight vector large indeed cause weight vector get stuck pointing less direction since changes due gradient descent make tiny changes direction length long believe phenomenon making hard learning algorithm properly explore weight space consequently harder find good minima cost function 
[improving, way, neural, networks, learn, overfitting, regularization, regularization, help, reduce, overfitting] seen empirically regularization helps reduce overfitting encouraging unfortunately obvious regularization helps standard story people tell explain going along following lines smaller weights sense lower complexity provide simpler powerful explanation data thus preferred pretty terse story though contains several elements perhaps seem dubious mystifying let unpack story examine critically let suppose simple data set wish build model improving way neural networks learn implicitly studying real world phenomenon representing real world data goal build model lets predict function could try using neural networks build model going something even simpler try model polynomial instead using neural nets using polynomials make things particularly transparent understood polynomial case translate neural networks ten points graph means find unique order polynomial fits data exactly graph provides exact fit also get good fit using linear model better model likely true model likely generalize well examples underlying real world phenomenon difficult questions fact determine certainty answer questions without much information underlying real world phenomenon let consider two possibilities order polynomial fact model truly describes real world phenomenon model therefore generalize perfectly correct model little additional noise due say measurement error model exact fit show coefficients explicitly although easy find using routine numpy polyfit view exact form polynomial curious function defined starting line program produces graph overfitting regularization priori possible say two possibilities correct indeed third possibility holds logically either could true trivial difference true data provided small difference two models suppose want predict value corresponding large value much larger shown graph try dramatic difference predictions two models order polynomial model comes dominated term linear model remains well linear one point view say science simpler explanation unless compelled find simple model seems explain many data points tempted shout eureka seems unlikely simple explanation occur merely coincidence rather suspect model must expressing underlying truth phenomenon case hand model noise seems much simpler would surprising simplicity occurred chance suspect noise expresses underlying truth point view order model really learning effects local noise order model works perfectly particular data points model fail generalize data points noisy linear model greater predictive power let see point view means neural networks suppose network mostly small weights tend happen regularized network smallness weights means behaviour network change much change random inputs makes difficult regularized network learn effects local noise data think way making single pieces evidence matter much output network instead regularized network learns respond types evidence seen often across training set contrast network large weights may change behaviour quite bit response small changes input unregularized network use large weights learn complex model carries lot information noise training data nutshell regularized networks constrained build relatively simple models based patterns seen often training data resistant learning peculiarities noise training data hope force networks real learning phenomenon hand generalize better learn said idea preferring simpler explanation make nervous people sometimes refer idea occam razor zealously apply though status general scientific principle course general scientific principle priori logical reason prefer simple explanations complex explanations indeed sometimes complex explanation turns correct let describe two examples complex explanations turned correct physicist marcel schein announced discovery new particle nature company worked general electric ecstatic publicized discovery widely physicist hans bethe skeptical bethe visited schein looked plates showing tracks schein new particle schein showed bethe plate plate plate bethe identified problem suggested data discarded finally schein showed bethe plate looked good bethe said might statistical fluke schein yes chance would statistics even according formula one five bethe already looked five plates finally schein said plates one good plates one improving way neural networks learn good pictures explain different theory whereas one hypothesis explains plates new particle bethe replied sole difference explanations wrong mine right single explanation wrong multiple explanations right subsequent work confirmed nature agreed bethe schein particle second example astronomer urbain verrier observed orbit planet mercury quite shape newton theory gravitation says tiny tiny deviation newton theory several explanations proferred time boiled saying newton theory less right needed tiny alteration einstein showed deviation could explained well using general theory relativity theory radically different newtonian gravitation based much complex mathematics despite additional complexity today accepted einstein explanation correct newtonian gravity even modified forms wrong part know einstein theory explains many phenomena newton theory difficulty furthermore even impressively einstein theory accurately predicts several phenomena predicted newtonian gravity impressive qualities entirely obvious early days one judged merely grounds simplicity modified form newton theory would arguably attractive three morals draw stories first quite subtle business deciding two explanations truly simpler second even make judgment simplicity guide must used great caution third true test model simplicity rather well predicting new phenomena new regimes behaviour said keeping need caution mind empirical fact regularized neural networks usually generalize better unregularized networks remainder book make frequent use regularization included stories merely help convey one yet developed entirely convincing theoretical explanation regularization helps networks generalize indeed researchers continue write papers try different approaches regularization compare see works better attempt understand different approaches work better worse view regularization something kludge often helps entirely satisfactory systematic understanding going merely incomplete heuristics rules thumb deeper set issues issues heart science question generalize regularization may give computational magic wand helps networks generalize better give principled understanding generalization works best approach particularly galling everyday life humans generalize phenomenally well shown images elephant child quickly learn recognize elephants course may occasionally make mistakes perhaps confusing rhinoceros elephant general process works remarkably accurately story related physicist richard feynman historian charles weiner issues back famously discussed scottish philosopher david hume problem induction given modern machine learning form david wolpert william macready overfitting regularization system human brain huge number free parameters shown one training images system learns generalize images brains sense regularizing amazingly well point know expect years come develop powerful techniques regularization artificial neural networks techniques ultimately enable neural nets generalize well even small data sets fact networks already generalize better one might priori expect network hidden neurons nearly parameters images training data like trying fit degree polynomial data points rights network overfit terribly yet saw earlier network actually pretty good job generalizing case well understood dynamics gradient descent learning multilayer nets self regularization effect exceptionally fortunate also somewhat disquieting understand case meantime adopt pragmatic approach use regularization whenever neural networks better let conclude section returning detail left unexplained earlier fact regularization constrain biases course would easy modify regularization procedure regularize biases empirically often change results much extent merely convention whether regularize biases however worth noting large bias make neuron sensitive inputs way large weights need worry large biases enabling network learn noise training data time allowing large biases gives networks flexibility behaviour particular large biases make easier neurons saturate sometimes desirable reasons usually include bias terms regularizing 
[improving, way, neural, networks, learn, overfitting, regularization, techniques, regularization] many regularization techniques regularization fact many techniques developed possibly summarize section briefly describe three approaches reducing overfitting regularization dropout artificially increasing training set size nearly much depth studying techniques earlier instead purpose get familiar main ideas appreciate something diversity regularization techniques available regularization approach modify unregularized cost function adding sum absolute values weights intuitively similar regularization penalizing large weights tending make network prefer small weights course regularization term regularization term expect get exactly behaviour let yann lecun léon bottou yoshua bengio patrick haffner improving way neural networks learn try understand behaviour network trained using regularization differs network trained using regularization look partial derivatives cost function differentiating obtain sgn sgn sign positive negative using expression easily modify backpropagation stochastic gradient descent using regularization resulting update rule regularized network sgn per usual estimate using mini batch average wish compare update rule regularization equation expressions effect regularization shrink weights accords intuition kinds regularization penalize large weights way weights shrink different regularization weights shrink constant amount toward regularization weights shrink amount proportional particular weight large magnitude regularization shrinks weight much less regularization contrast small regularization shrinks weight much regularization net result regularization tends concentrate weight network relatively small number high importance connections weights driven toward zero glossed issue discussion partial derivative defined reason function sharp corner differentiable point okay though apply usual unregularized rule stochastic gradient descent okay intuitively effect regularization shrink weights obviously shrink weight already put precisely use equations convention sgn gives nice compact rule stochastic gradient descent regularization dropout dropout radically different technique regularization unlike regularization dropout rely modifying cost function instead dropout modify network let describe basic mechanics dropout works getting works results suppose trying train network overfitting regularization particular suppose training input corresponding desired output ordi narily train forward propagating network backpropagating determine contribution gradient dropout process modified start randomly temporarily deleting half hidden neurons network leaving input output neurons untouched end network along following lines note dropout neurons neurons temporarily deleted still ghosted forward propagate input modified network backpropagate result also modified network mini batch examples update appropriate weights biases repeat process first restoring dropout neurons choosing new random subset hidden neurons delete estimating gradient different mini batch updating weights biases network repeating process network learn set weights biases course weights biases learnt conditions half hidden neurons dropped actually run full network means twice many hidden neurons active compensate halve weights outgoing hidden neurons improving way neural networks learn dropout procedure may seem strange hoc would expect help regularization explain going like briefly stop thinking dropout instead imagine training neural networks standard way dropout particular imagine train several different neural networks using training data course networks may start identical result training may sometimes give different results happens could use kind averaging voting scheme decide output accept instance trained five networks three classifying digit probably really two networks probably making mistake kind averaging scheme often found powerful though expensive way reducing overfitting reason different networks may overfit different ways averaging may help eliminate kind overfitting got dropout heuristically dropout different sets neurons rather like training different neural networks dropout procedure like averaging effects large number different networks different networks overfit different ways hopefully net effect dropout reduce overfitting related heuristic explanation dropout given one earliest papers use technique reduces complex adaptations neurons since neuron cannot rely presence particular neurons therefore forced learn robust features useful conjunction many different random subsets neurons words think network model making predictions think dropout way making sure model robust loss individual piece evidence somewhat similar regularization tend reduce weights thus make network robust losing individual connection network course true measure dropout successful improving performance neural networks original introducing technique applied many different tasks particular interest applied dropout mnist digit classification using vanilla feedforward neural network along lines similar considering paper noted best result anyone achieved point using architecture percent classification accuracy test set improved percent accuracy using combination dropout modified form regularization similarly impressive results obtained many tasks including problems image speech recognition natural language processing dropout especially useful training large deep networks problem overfitting often acute artificially expanding training data saw earlier mnist classification accuracy dropped percentages mid used training images surprising case since less training data means network exposed fewer variations way human beings write digits let try training hidden neuron network variety different training data set sizes see performance varies train using mini batch size learning rate alex krizhevsky ilya sutskever geoffrey hinton geoffrey hinton nitish srivastava alex krizhevsky ilya sutskever ruslan salakhutdinov note paper discusses number subtleties glossed brief introduction overfitting regularization regularization parameter cross entropy cost function train epochs full training data set used scale number epochs proportionally smaller training sets used ensure weight decay factor remains across training sets use regularization parameter full training data set used scale proportionally smaller training sets see classification accuracies improve considerably use training data presumably improvement would continue still data available course looking graph appear getting near saturation suppose however redo graph training set size plotted logarithmically seems clear graph still going toward end suggests used vastly training data say millions even billions handwriting samples instead likely get considerably better performance even small network obtaining training data great idea unfortunately expensive always possible practice however another idea work nearly well artificially expand training data suppose example take mnist training image five next two graph produced program more_data improving way neural networks learn rotate small amount let say degrees still recognizably digit yet pixel level quite different image currently mnist training data conceivable adding image training data might help network learn classify digits obviously limited adding one image expand training data making many small rotations mnist training images using expanded training data improve network performance idea powerful widely used let look results applied several variations idea mnist one neural network architectures considered along similar lines using feedforward network hidden neurons using cross entropy cost function running network standard mnist training data achieved classification accuracy percent test set expanded training data using rotations described also translating skewing images training expanded data set increased network accuracy percent also experimented called elastic distortions special type image distortion intended emulate random oscillations found hand muscles using elastic distortions expand data achieved even higher accuracy percent effectively broadening experience network exposing sort variations found real handwriting variations idea used improve performance many learning tasks handwriting recognition general principle expand training data applying operations reflect real world variation difficult think ways suppose example building neural network speech recognition humans recognize speech even presence distortions background noise expand data adding background noise also recognize speech sped slowed another way expand training data techniques always used instance instead expanding training data adding noise may well efficient clean input network first applying noise reduction filter still worth keeping idea expanding training data mind looking opportunities apply 
[improving, way, neural, networks, learn, weight, initialization] create neural networks make choices initial weights biases choosing according prescription discussed briefly back chapter remind prescription choose weights biases using independent gaussian random variables normalized mean standard deviation approach worked well quite hoc worth revisiting see find better way setting initial weights biases perhaps help neural networks learn faster turns quite bit better initializing normalized gaussians see suppose working network large number say input neurons let suppose used normalized gaussians initialize weights striking examples may found michele banko eric brill weight initialization connecting first hidden layer going concentrate specifically weights connecting input neurons first neuron hidden layer ignore rest network suppose simplicity trying train using training input half input neurons set half input neurons set argument follows applies generally get gist special case let consider weighted sum inputs hidden neuron terms sum vanish corresponding input zero sum total normalized gaussian random variables accounting weight terms extra bias term thus distributed gaussian mean zero standard deviation broad gaussian distribution sharply peaked particular see graph quite likely pretty large either case output hidden neuron close either means hidden neuron saturated happens know making small changes weights make absolutely miniscule changes activation hidden neuron miniscule change activation hidden neuron turn barely affect rest neurons network see correspondingly miniscule change cost function result weights learn slowly use gradient descent similar problem discussed earlier chapter output neurons saturated wrong value caused learning slow addressed discussed detail chapter used equations backpropagation show weights input saturated neurons learned slowly improving way neural networks learn earlier problem clever choice cost function unfortunately helped saturated output neurons nothing problem saturated hidden neurons talking weights input first hidden layer course similar arguments apply also later hidden layers weights later hidden layers initialized using normalized gaussians activations often close learning proceed slowly way choose better initializations weights biases get kind saturation avoid learning slowdown suppose neuron input weights shall initialize weights gaussian random variables mean standard deviation squash gaussians making less likely neuron saturate continue choose bias gaussian mean standard deviation reasons return moment choices weighted sum gaussian random variable mean much sharply peaked suppose earlier inputs zero easy show see exercise gaussian distribution mean standard deviation much sharply peaked much even graph understates situation since rescale vertical axis compared earlier graph neuron much less likely saturate correspondingly much less likely problems learning slowdown 
[improving, way, neural, networks, learn, handwriting, recognition, revisited, code] let implement ideas discussed chapter develop new program network improved version program network developed chapter looked network may find helpful spend minutes quickly reading earlier discussion lines code easily understood yoshua bengio handwriting recognition revisited code case network star network network class use represent neural networks initialize instance network list sizes respective layers network choice cost use defaulting cross entropy class network object def __init__ self sizes cost crossentropycost self num_layers len sizes self sizes sizes self default_weight_initializer self cost cost first couple lines __init__ method network pretty self explanatory next two lines new need understand detail let start examining default_weight_initializer method makes use new improved approach weight initialization seen approach weights input neuron initialized gaussian random variables mean standard deviation divided square root number connections input neuron also method initialize biases using gaussian random variables mean standard deviation code def default_weight_initializer self self biases random randn self sizes self weights random randn sqrt zip self sizes self sizes understand code may help recall numpy library linear algebra import numpy beginning program also notice initialize biases first layer neurons avoid first layer input layer biases would used exactly thing network complementing default_weight_initializer also include large_weight_initializer method method initializes weights biases using old approach chapter weights biases initialized gaussian random variables mean standard deviation code course tiny bit different default_weight_initializer def large_weight_initializer self self biases random randn self sizes self weights random randn zip self sizes self sizes included large_weight_initializer method mostly convenience make easier compare results chapter chapter think many practical situations would recommend using second new thing network __init__ method initialize cost attribute understand works let look class use represent cross entropy class crossentropycost object familiar python static methods ignore staticmethod decorators treat delta ordinary methods curious details staticmethod tell python interpreter method follows depend object way self passed parameter delta methods improving way neural networks learn staticmethod def return sum nan_to_num log log staticmethod def delta return let break first thing observe even though cross entropy mathematically speaking function implemented python class python function made choice reason cost plays two different roles network obvious role measure well output activation matches desired output role captured crossentropycost method note way nan_to_num call inside crossentropycost ensures numpy deals correctly log numbers close zero also second way cost function enters network recall chapter running backpropagation algorithm need compute network output error form output error depends choice cost function different cost function different form output error cross entropy output error saw equation reason define second method crossentropycost delta whose purpose tell network compute output error bundle two methods single class containing everything networks need know cost function similar way network also contains class represent quadratic cost function included comparison results chapter since going forward mostly use cross entropy code quadraticcost method straightforward computation quadratic cost associated actual output desired output value returned quadraticcost delta based expression output error quadratic cost derived back chapter class quadraticcost object staticmethod def return linalg norm staticmethod def delta return sigmoid_prime understood main differences network network pretty simple stuff number smaller changes discuss including implementation regularization getting let look complete code network need read code detail worth understanding broad structure particular reading documentation strings understand piece program course also welcome delve deeply wish get lost may wish continue reading prose return code later anyway code network handwriting recognition revisited code improved version network implementing stochastic gradient descent learning algorithm feedforward neural network improvements include addition cross entropy cost function regularization better initialization network weights note focused making code simple easily readable easily modifiable optimized omits many desirable features libraries standard library import json import random import sys third party libraries import numpy define quadratic cross entropy cost functions class quadraticcost object staticmethod def return cost associated output desired output return linalg norm staticmethod def delta return error delta output layer return sigmoid_prime class crossentropycost object staticmethod def return cost associated output desired output note nan_to_num used ensure numerical stability particular slot expression log returns nan nan_to_num ensures converted correct value return sum nan_to_num log log staticmethod def delta return error delta output layer note parameter used method included method parameters order make interface consistent delta method cost classes return main network class class network object def __init__ self sizes cost crossentropycost list sizes contains number neurons respective improving way neural networks learn layers network example list would three layer network first layer containing neurons second layer neurons third layer neuron biases weights network initialized randomly using self default_weight_initializer see docstring method self num_layers len sizes self sizes sizes self default_weight_initializer self cost cost def default_weight_initializer self initialize weight using gaussian distribution mean standard deviation square root number weights connecting neuron initialize biases using gaussian distribution mean standard deviation note first layer assumed input layer convention set biases neurons since biases ever used computing outputs later layers self biases random randn self sizes self weights random randn sqrt zip self sizes self sizes def large_weight_initializer self initialize weights using gaussian distribution mean standard deviation initialize biases using gaussian distribution mean standard deviation note first layer assumed input layer convention set biases neurons since biases ever used computing outputs later layers weight bias initializer uses approach chapter included purposes comparison usually better use default weight initializer instead self biases random randn self sizes self weights random randn zip self sizes self sizes def feedforward self return output network input zip self biases self weights sigmoid dot return def sgd self training_data epochs mini_batch_size eta lmbda evaluation_data none monitor_evaluation_cost false monitor_evaluation_accuracy false monitor_training_cost false monitor_training_accuracy false handwriting recognition revisited code train neural network using mini batch stochastic gradient descent training_data list tuples representing training inputs desired outputs non optional parameters self explanatory regularization parameter lmbda method also accepts evaluation_data usually either validation test data monitor cost accuracy either evaluation data training data setting appropriate flags method returns tuple containing four lists per epoch costs evaluation data accuracies evaluation data costs training data accuracies training data values evaluated end training epoch example train epochs first element tuple element list containing cost evaluation data end epoch note lists empty corresponding flag set evaluation_data n_data len evaluation_data len training_data evaluation_cost evaluation_accuracy training_cost training_accuracy xrange epochs random shuffle training_data mini_batches training_data mini_batch_size xrange mini_batch_size mini_batch mini_batches self update_mini_batch mini_batch eta lmbda len training_data print epoch training complete monitor_training_cost cost self total_cost training_data lmbda training_cost append cost print cost training data format cost monitor_training_accuracy accuracy self accuracy training_data convert true training_accuracy append accuracy print accuracy training data format accuracy monitor_evaluation_cost cost self total_cost evaluation_data lmbda convert true evaluation_cost append cost print cost evaluation data format cost monitor_evaluation_accuracy accuracy self accuracy evaluation_data evaluation_accuracy append accuracy print accuracy evaluation data format self accuracy evaluation_data n_data print return evaluation_cost evaluation_accuracy training_cost training_accuracy def update_mini_batch self mini_batch eta lmbda update network weights biases applying gradient descent using backpropagation single mini batch mini_batch list tuples eta learning rate lmbda regularization parameter total size training data set improving way neural networks learn nabla_b zeros shape self biases nabla_w zeros shape self weights mini_batch delta_nabla_b delta_nabla_w self backprop nabla_b dnb dnb zip nabla_b delta_nabla_b nabla_w dnw dnw zip nabla_w delta_nabla_w self weights eta lmbda eta len mini_batch zip self weights nabla_w self biases eta len mini_batch zip self biases nabla_b def backprop self return tuple nabla_b nabla_w representing gradient cost function c_x nabla_b nabla_w layer layer lists numpy arrays similar self biases self weights nabla_b zeros shape self biases nabla_w zeros shape self weights feedforward activation activations list store activations layer layer list store vectors layer layer zip self biases self weights dot activation append activation sigmoid activations append activation backward pass delta self cost delta activations nabla_b delta nabla_w dot delta activations transpose note variable loop used little differently notation chapter book means last layer neurons second last layer renumbering scheme book used take advantage fact python use negative indices lists xrange self num_layers sigmoid_prime delta dot self weights transpose delta nabla_b delta nabla_w dot delta activations transpose return nabla_b nabla_w def accuracy self data convert false return number inputs data neural network outputs correct result neural network output assumed index whichever neuron final layer highest activation flag convert set false data set validation test data usual case true data set training data need flag arises due differences way results represented different data sets particular flags whether need convert different representations may seem strange use different representations different data sets use representation three data sets done efficiency reasons program usually evaluates cost handwriting recognition revisited code training data accuracy data sets different types computations using different representations speeds things details representations found mnist_loader load_data_wrapper convert results argmax self feedforward argmax data else results argmax self feedforward data return sum int results def total_cost self data lmbda convert false return total cost data set data flag convert set false data set training data usual case true data set validation test data see comments similar reversed convention accuracy method cost data self feedforward convert vectorized_result cost self cost len data cost lmbda len data sum linalg norm self weights return cost def save self filename save neural network file filename data sizes self sizes weights tolist self weights biases tolist self biases cost str self cost __name__ open filename json dump data close loading network def load filename load neural network file filename returns instance network open filename data json load close cost getattr sys modules __name__ data cost net network data sizes cost cost net weights array data weights net biases array data biases return net miscellaneous functions def vectorized_result return dimensional unit vector position zeroes elsewhere used convert digit corresponding desired output neural network improving way neural networks learn zeros return def sigmoid sigmoid function return exp def sigmoid_prime derivative sigmoid function return sigmoid sigmoid one interesting changes code include regularization although major conceptual change trivial implement easy miss code part involves passing parameter lmbda various methods notably network sgd method real work done single line program fourth last line network update_mini_batch method modify gradient descent update rule include weight decay although modification tiny big impact results way common implementing new techniques neural networks spent thousands words discussing regularization conceptually quite subtle difficult understand yet trivial add program occurs surprisingly often sophisticated techniques implemented small changes code another small important change code addition several optional flags stochastic gradient descent method network sgd flags make possible monitor cost accuracy either training_data set evaluation_data passed network sgd used flags often earlier chapter let give example works remind import mnist_loader training_data validation_data test_data mnist_loader load_data_wrapper import network net network network cost network crossentropycost net sgd training_data lmbda evaluation_data validation_data monitor_evaluation_accuracy true monitor_evaluation_cost true monitor_training_accuracy true monitor_training_cost true setting evaluation_data validation_data could also monitored performance test_data data set also four flags telling monitor cost accuracy evaluation_data training_data flags false default turned order monitor network performance furthermore network network sgd method returns four element tuple representing results monitoring use follows evaluation_cost evaluation_accuracy training_cost training_accuracy net sgd training_data lmbda evaluation_data validation_data monitor_evaluation_accuracy true monitor_evaluation_cost true monitor_training_accuracy true monitor_training_cost true example evaluation_cost element list containing cost evaluation data end epoch sort information extremely useful choose neural network hyper parameters understanding network behaviour example used draw graphs showing network learns time indeed exactly constructed graphs earlier chapter note however monitoring flags set corresponding element tuple empty list additions code include network save method save network objects disk function load back later note saving loading done using json python pickle cpickle modules usual way save load objects disk python using json requires code pickle cpickle would understand used json imagine time future decided change network class allow neurons sigmoid neurons implement change likely change attributes defined network __init__ method simply pickled objects would cause load function fail using json serialization explicitly makes easy ensure old networks still load many minor changes code network simple variations network net result expand line program far capable lines 
[improving, way, neural, networks, learn, choose, neural, network’s, hyper-parameters] explained choosing values hyper parameters learning rate regularization parameter supplying values work pretty well practice using neural nets attack problem difficult find good hyper parameters imagine example introduced mnist problem begun working knowing nothing hyper parameters use let suppose good fortune first experiments choose many hyper parameters way done earlier chapter hidden neurons mini batch size training epochs using cross entropy choose learning rate regularization parameter saw one run import mnist_loader training_data validation_data test_data mnist_loader load_data_wrapper import network net network network net sgd training_data lmbda improving way neural networks learn evaluation_data validation_data monitor_evaluation_accuracy true epoch training complete accuracy evaluation data epoch training complete accuracy evaluation data epoch training complete accuracy evaluation data epoch training complete accuracy evaluation data epoch training complete accuracy evaluation data epoch training complete accuracy evaluation data classification accuracies better chance network acting random noise generator well easy fix might say decrease learning rate regularization hyper parameters unfortunately priori know hyper parameters need adjust maybe real problem hidden neuron network never work well matter hyper parameters chosen maybe really need least hidden neurons hidden neurons multiple hidden layers different approach encoding output maybe network learning need train epochs maybe mini batches small maybe better switching back quadratic cost function maybe need try different approach weight initialization easy feel lost hyper parameter space particularly frustrating network large uses lot training data since may train hours days weeks get result situation persists damages confidence maybe neural networks wrong approach problem maybe quit job take beekeeping section explain heuristics used set hyper parameters neural network goal help develop workflow enables pretty good job setting hyper parameters course cover everything hyper parameter optimization huge subject case problem ever completely solved universal agreement amongst practitioners right strategies use always one trick try eke bit performance network heuristics section get started broad strategy using neural networks attack new problem first challenge get non trivial learning network achieve results better chance surprisingly difficult especially confronting new class problem let look strategies use kind trouble suppose example attacking mnist first time start enthusiastic little discouraged first network fails completely example way strip problem get rid training choose neural network hyper parameters validation images except images try train network distinguish inherently easier problem distinguishing ten digits also reduces amount training data percent speeding training factor enables much rapid experimentation gives rapid insight build good network speed experimentation stripping network simplest network likely meaningful learning believe network likely better chance classification mnist digits begin experimentation network much faster training network build back latter get another speed experimentation increasing frequency moni toring network monitor performance end training epoch images per epoch means waiting little ten seconds per epoch laptop training network getting feedback well network learning course ten seconds long want trial dozens hyper parameter choices annoying want trial hundreds thousands choices starts get debilitating get feedback quickly monitoring validation accuracy often say every training images furthermore instead using full image validation set monitor performance get much faster estimate using validation images matters network sees enough images real learning get pretty good rough estimate performance course program network currently kind monitoring kludge achieve similar effect purposes illustration strip training data first mnist training images let try see happens keep code simple implemented idea using images course done little work net network network net sgd training_data lmbda evaluation_data validation_data monitor_evaluation_accuracy true epoch training complete accuracy evaluation data epoch training complete accuracy evaluation data epoch training complete accuracy evaluation data still getting pure noise big win getting feedback fraction second rather every ten seconds means quickly experiment choices hyper parameter even conduct experiments trialling many different choices hyper parameter nearly simultaneously example left used earlier since changed number training examples really change keep weight decay means changing happens net network network net sgd training_data lmbda evaluation_data validation_data improving way neural networks learn monitor_evaluation_accuracy true epoch training complete accuracy evaluation data epoch training complete accuracy evaluation data epoch training complete accuracy evaluation data epoch training complete accuracy evaluation data ahah signal terribly good signal signal nonetheless something build modifying hyper parameters try get improvement maybe guess learning rate needs higher perhaps realize silly guess reasons discuss shortly please bear test guess try dialing net network network net sgd training_data lmbda evaluation_data validation_data monitor_evaluation_accuracy true epoch training complete accuracy evaluation data epoch training complete accuracy evaluation data epoch training complete accuracy evaluation data epoch training complete accuracy evaluation data good suggests guess wrong problem learning rate low instead try dialing net network network net sgd training_data lmbda evaluation_data validation_data monitor_evaluation_accuracy true epoch training complete accuracy evaluation data epoch training complete accuracy evaluation data epoch training complete accuracy evaluation data epoch training complete accuracy evaluation data choose neural network hyper parameters better continue individually adjusting hyper parameter gradually improving performance explored find improved value move find good value experiment complex architecture say network hidden neurons adjust values increase hidden neurons adjust hyper parameters stage evaluating performance using held validation data using evaluations find better better hyper parameters typically takes longer witness impact due modifications hyper parameters gradually decrease frequency monitoring looks promising broad strategy however want return initial stage finding hyper parameters enable network learn anything fact even discussion conveys positive outlook immensely frustrating work network learning nothing tweak hyper parameters days still get meaningful response like emphasize early stages make sure get quick feedback experiments intuitively may seem though simplifying problem architecture merely slow fact speeds things since much quickly find network meaningful signal got signal often get rapid improvements tweaking hyper parameters many things life getting started hardest thing okay broad strategy let look specific recommendations setting hyper parameters focus learning rate regularization parameter mini batch size however many remarks apply also hyper parameters including associated network architecture forms regularization hyper parameters meet later book momentum efficient learning rate suppose run three mnist networks three different learning rates respectively set hyper parameters experiments earlier sections running epochs mini batch size also return using full training images graph showing behaviour training cost graph generated multiple_eta improving way neural networks learn cost decreases smoothly final epoch cost initially decreases epochs near saturation thereafter changes merely small apparently random oscillations finally cost makes large oscillations right start understand reason oscillations recall stochastic gradient descent supposed step gradually valley cost function however large steps large may actually overshoot minimum causing algorithm climb valley instead causing cost oscillate choose initial steps take toward minimum cost function get near minimum start suffer overshooting problem choose suffer problem first epochs course choosing small creates another problem namely slows stochastic gradient descent even better approach would start train epochs switch discuss variable learning rate schedules later though let stick figuring find single good value learning rate picture mind set follows first estimate threshold value cost training data immediately begins decreasing instead oscillating increasing estimate need accurate estimate order magnitude starting cost decreases first epochs successively try find value cost oscillates increases first epochs alternately cost oscillates increases first epochs try find value cost decreases first epochs following procedure give order magnitude estimate threshold value may optionally refine estimate pick largest value cost decreases first epochs say need super accurate gives estimate threshold value obviously actual value use larger threshold value fact value remain usable many epochs likely want picture helpful intended intuition building illustration may complete exhaustive explanation briefly complete explanation follows gradient descent uses first order approximation cost function guide decrease cost large higher order terms cost function become important may dominate behaviour causing gradient descent break especially likely approach minima quasi minima cost function since near points gradient becomes small making easier higher order terms dominate behaviour choose neural network hyper parameters use value smaller say factor two threshold choice typically allow train many epochs without causing much slowdown learning case mnist data following strategy leads estimate order magnitude threshold value refinement obtain threshold value following prescription suggests using value learning rate fact found using worked well enough epochs part worry using lower value seems quite straightforward however using training cost pick appears contradict said earlier section namely pick hyper parameters evaluating performance using held validation data fact use validation accuracy pick regularization hyper parameter mini batch size network parameters number layers hidden neurons things differently learning rate frankly choice personal aesthetic preference perhaps somewhat idiosyncratic reasoning hyper parameters intended improve final classification accuracy test set makes sense select basis validation accuracy however learning rate incidentally meant impact final classification accuracy primary purpose really control step size gradient descent monitoring training cost best way detect step size big said personal aesthetic preference early learning training cost usually decreases validation accuracy improves practice unlikely make much difference criterion use use early stopping determine number training epochs discussed earlier chapter early stopping means end epoch compute classification accuracy validation data stops improving terminate makes setting number epochs simple particular means need worry explicitly figuring number epochs depends hyper parameters instead taken care automatically furthermore early stopping also automatically prevents overfitting course good thing although early stages experimentation helpful turn early stopping see signs overfitting use inform approach regularization implement early stopping need say precisely means classification accuracy stopped improving seen accuracy jump around quite bit even overall trend improve stop first time accuracy decreases almost certainly stop improvements better rule terminate best classification accuracy improve quite time suppose example mnist might elect terminate classification accuracy improved last ten epochs ensures stop soon response bad luck training also waiting around forever improvement never comes improvement ten rule good initial exploration mnist however networks sometimes plateau near particular classification accuracy quite time begin improving trying get really good performance improvement ten rule may aggressive stopping case suggest using improvement ten rule initial experimentation gradually adopting lenient rules better understand way network trains improvement twenty improvement fifty course introduces new hyper parameter improving way neural networks learn optimize practice however usually easy set hyper parameter get pretty good results similarly problems mnist improvement ten rule may much aggressive nearly aggressive enough depending details problem however little experimentation usually easy find pretty good strategy early stopping used early stopping mnist experiments date reason lot comparisons different approaches learning comparisons helpful use number epochs case however well worth modifying network implement early stopping 
[improving, way, neural, networks, learn, techniques] technique developed chapter valuable know right reason explained larger point familiarize problems occur neural networks style analysis help overcome problems sense learning think neural nets remainder chapter briefly sketch handful techniques sketches less depth earlier discussions convey feeling diversity techniques available use neural networks 
[improving, way, neural, networks, learn, techniques, variations, stochastic, gradient, descent] stochastic gradient descent backpropagation served well attacking mnist digit classification problem however many approaches optimizing cost function sometimes approaches offer performance superior mini batch stochastic gradient descent section sketch two approaches hessian momentum techniques hessian technique begin discussion helps put neural networks aside bit instead going consider abstract problem minimizing cost function function many variables taylor theorem cost function approximated near point rewrite compactly usual gradient vector matrix known hessian matrix whose entry suppose approximate discarding higher order terms represented techniques using calculus show expression right hand side choosing provided good approximate expression cost function expect moving point significantly decrease cost function suggests possible algorithm minimizing cost choose starting point update new point hessian computed update new point hessian computed practice approximation better take smaller steps repeatedly changing amount known learning rate approach minimizing cost function known hessian technique hessian optimization theoretical empirical results showing hessian methods converge minimum fewer steps standard gradient descent particular incorporating information second order changes cost function possible hessian approach avoid many pathologies occur gradient descent furthermore versions backpropagation algorithm used compute hessian hessian optimization great using neural networks unfortunately many desirable properties one undesirable property difficult apply practice part problem sheer size hessian matrix suppose neural network weights biases corresponding hessian matrix contain entries lot entries makes computing extremely difficult practice however mean useful understand fact many variations gradient descent inspired hessian optimization avoid problem overly large matrices let take look one technique momentum based gradient descent momentum based gradient descent intuitively advantage hessian optimization incorporates information gradient also information gradient changing momentum based gradient descent based similar intuition avoids large matrices second derivatives understand momentum technique think back original picture gradient considered ball rolling valley time observed gradient descent despite name loosely similar ball falling bottom valley momentum technique modifies gradient descent two ways make similar physical picture first introduces notion velocity parameters trying optimize gradient acts change velocity directly position much way physical forces change velocity indirectly affect position second strictly speaking minimum merely extremum need assume hessian matrix positive definite intuitively means function looks like valley locally mountain saddle improving way neural networks learn momentum method introduces kind friction term tends gradually reduce velocity let give precise mathematical description introduce velocity variables one corresponding replace gradient descent update rule equations hyper parameter controls amount damping friction system understand meaning equations helpful first consider case corresponds friction case inspection equations shows force modifying velocity velocity controlling rate change intuitively build velocity repeatedly adding gradient terms means gradient roughly direction several rounds learning build quite bit steam moving direction think example happens moving straight slope step velocity gets larger slope move quickly bottom valley enable momentum technique work much faster standard gradient descent course problem reach bottom valley overshoot gradient change rapidly could find moving wrong direction reason hyper parameter said earlier controls amount friction system little precise think amount friction system seen friction velocity completely driven gradient contrast lot friction velocity build equations reduce usual equation gradient descent practice using value intermediate give much benefit able build speed without causing overshooting choose value using held validation data much way select avoided naming hyper parameter reason standard name badly chosen called momentum efficient potentially neural net variables would course include weights biases techniques confusing since notion momentum physics rather much closely related friction however term momentum efficient widely used continue use nice thing momentum technique takes almost work mod ify implementation gradient descent incorporate momentum still use backpropagation compute gradients use ideas sampling stochastically chosen mini batches way get advantages hessian technique using information gradient changing done without disadvantages minor modifications code practice momentum technique commonly used often speeds learning 
[improving, way, neural, networks, learn, problem, models, artificial, neuron] built neural networks using sigmoid neurons principle network built sigmoid neurons compute function practice however networks built using model neurons sometimes outperform sigmoid networks depending application networks based alternate models may learn faster generalize better test data perhaps let mention couple alternate model neurons give flavor variations common use perhaps simplest variation tanh pronounced tanch neuron replaces sigmoid function hyperbolic tangent function output tanh neuron input weight vector bias given tanh tanh course hyperbolic tangent function turns closely yann lecun léon bottou genevieve orr klaus robert müller see example ilya sutskever james martens george dahl geoffrey hinton improving way neural networks learn related sigmoid neuron see recall tanh function defined tanh little algebra easily verified tanh tanh rescaled version sigmoid function also see graphically tanh function shape sigmoid function tanh function one difference tanh neurons sigmoid neurons output tanh neurons ranges means going build network based tanh neurons may need normalize outputs depending details application possibly inputs little differently sigmoid networks similar sigmoid neurons network tanh neurons principle compute mapping inputs range furthermore ideas backpropagation stochastic gradient descent easily applied network tanh neurons network sigmoid neurons 
[improving, way, neural, networks, learn, exercise, stories, neural, networks] question approach utilizing researching machine learning techniques supported almost entirely empirically opposed mathematically also situations noticed techniques fail answer realize theoretical tools weak sometimes good mathematical intuitions particular technique work sometimes intuition ends wrong questions become well method work particular problem large set problems works well question answer neural networks researcher yann lecun attending conference foundations quantum mechanics noticed seemed curious verbal habit talks finished questions audience see example kevin jarrett koray kavukcuoglu marc aurelio ranzato yann lecun xavier glorot antoine bordes yoshua bengio alex krizhevsky ilya sutskever geoffrey hinton note papers fill important details set output layer cost function regularization networks using rectified linear units glossed details brief account papers also discuss detail benefits drawbacks using rectified linear units another informative paper vinod nair geoffrey hinton demonstrates benefits using rectified linear units somewhat different approach neural networks techniques often began sympathetic point view quantum foundations usual field noticed style questioning scientific conferences rarely never heard questioner express sympathy point view speaker time thought prevalence question suggested little genuine progress made quantum foundations people merely spinning wheels later realized assessment harsh speakers wrestling hardest problems human minds ever confronted course progress slow still value hearing updates people thinking even always unarguable new progress report may noticed verbal tic similar sympathetic current book explain seeing often fallen back saying heuristically roughly speaking following story explain phenomenon stories plausible empirical evidence presented often pretty thin look research literature see stories similar style appear many research papers neural nets often thin supporting evidence think stories many parts science especially parts deal simple phenomena possible obtain solid reliable evidence quite general hypotheses neural networks large numbers parameters hyper parameters extremely complex interactions extraordinarily complex systems exceedingly difficult establish reliable general statements understanding neural networks full generality problem like quantum foundations tests limits human mind instead often make evidence specific instances general statement result statements sometimes later need modified abandoned new evidence comes light one way viewing situation heuristic story neural networks carries implied challenge example consider statement quoted earlier explaining dropout technique reduces complex adaptations neurons since neuron cannot rely presence particular neurons therefore forced learn robust features useful conjunction many different random subsets neurons rich provocative statement one could build fruitful research program entirely around unpacking statement figuring true false needs variation refinement indeed small industry researchers investigating dropout many variations trying understand works limits goes many heuristics discussed heuristic potential explanation also challenge investigate understand detail course time single person investigate heuristic explanations depth going take decades longer community neural networks researchers develop really powerful evidence based theory neural networks learn mean reject heuristic explanations unrigorous sufficiently evidence based fact need heuristics inspire guide thinking like great age exploration early explorers sometimes explored made new discoveries basis beliefs wrong important ways later mistakes corrected filled knowledge geography alex krizhevsky ilya sutskever geoffrey hinton improving way neural networks learn understand something poorly explorers understood geography understand neural nets today important explore boldly rigorously correct every step thinking view stories useful guide think neural nets retaining healthy awareness limitations stories carefully keeping track strong evidence given line reasoning put another way need good stories help motivate inspire rigorous depth investigation order uncover real facts matter 
[visual, proof, neural, nets, compute, function] one striking facts neural networks compute function suppose someone hands complicated wiggly function matter function guaranteed neural network every possible input value close approximation output network visual proof neural nets compute function result holds even function many inputs many outputs instance network computing function inputs outputs result tells neural networks kind universality matter function want compute know neural network job universality theorem holds even restrict networks single layer intermediate input output neurons called single hidden layer even simple network architectures extremely powerful universality theorem well known people use neural networks true widely understood explanations available quite technical instance one original papers proving using hahn banach theorem riesz representation theorem fourier analysis mathematician argument difficult follow easy people pity since underlying reasons universality simple beautiful chapter give simple mostly visual explanation universality theorem step step underlying ideas understand true neural networks compute function understand limitations result understand result relates deep neural networks follow material chapter need read earlier chapters book instead chapter structured enjoyable self contained essay provided little basic familiarity neural networks able follow explanation however provide occasional links earlier material help fill gaps knowledge george cybenko result much air time several groups proved closely related results cybenko paper contains useful discussion much work another important early paper kurt hornik maxwell stinchcombe halbert white paper uses stone weierstrass theorem arrive similar results two caveats universality theorems commonplace computer science much sometimes forget astonishing worth reminding ability compute arbitrary function truly remarkable almost process imagine thought function computation consider problem naming piece music based short sample piece thought computing function consider problem translating chinese text english thought computing consider problem taking movie file generating description plot movie discussion quality acting thought kind function universality means principle neural networks things many course know neural network exists say translate chinese text english mean good techniques constructing even recognizing network limitation applies also traditional universality theorems models boolean circuits seen earlier book neural networks powerful algorithms learning functions combination learning algorithms universality attractive mix book focused learning algorithms chapter focus universality means 
[visual, proof, neural, nets, compute, function, two, caveats] explaining universality theorem true want mention two caveats informal statement neural network compute function first mean network used exactly compute function rather get approximation good want increasing number hidden neurons improve approximation instance earlier see illustrated network computing function using three hidden neurons functions low quality approximation possible using three hidden neurons increasing number hidden neurons say five typically get better approximation still better increasing number hidden neurons actually computing one many functions since often many acceptable translations given piece text ditto remark translation many possible functions visual proof neural nets compute function make statement precise suppose given function like compute within desired accuracy guarantee using enough hidden neurons always find neural network whose output satisfies inputs words approximation good within desired accuracy every possible input second caveat class functions approximated way described continuous functions function discontinuous makes sudden sharp jumps general possible approximate using neural net surprising since neural networks compute continuous functions input however even function really like compute discontinuous often case continuous approximation good enough use neural network practice usually important limitation summing precise statement universality theorem neural networks single hidden layer used approximate continuous function desired precision chapter actually prove slightly weaker version result using two hidden layers instead one problems briefly outline explanation tweaks adapted give proof uses single hidden layer 
[visual, proof, neural, nets, compute, function, universality, one, input, one, output] understand universality theorem true let start understanding construct neural network approximates function one input one output turns core problem universality understood special case actually pretty easy extend functions many inputs many outputs build insight construct network compute let start network containing single hidden layer two hidden neurons output layer containing single output neuron universality one input one output get feel components network work let focus top hidden neuron diagram click weight drag mouse little ways right increase immediately see function computed top hidden neuron changes output neuron output neuron learnt earlier book computed hidden neuron sigmoid function made frequent use algebraic form proof universality obtain insight ignoring algebra entirely instead manipulating observing shape shown graph visual proof neural nets compute function give better feel going also give universality applies activation functions sigmoid function simplify analysis quite bit increasing weight much output really step function good approximation plotted output top hidden neuron weight output top hidden neuron actually quite bit easier work step functions general sigmoid functions reason output layer add contributions hidden neurons easy analyze sum bunch step functions rather difficult reason happens add bunch sigmoid shaped curves makes things much easier assume hidden neurons outputting step functions concretely fixing weight large value setting position step modifying bias course treating output step function approximation good approximation treat exact come back later discuss impact deviations approximation value step occur put another way position step depend upon weight bias answer question try modifying weight bias diagram may need scroll back bit figure position step depends little work able convince position step proportional inversely proportional fact step position see modifying weight bias following diagram strictly speaking visual approach taking traditionally thought proof believe visual approach gives insight result true traditional proof course kind insight real purpose behind proof occasionally small gaps reasoning present places make visual argument plausible quite rigorous bothers consider challenge fill missing steps lose sight real purpose understand universality theorem true universality one input one output output top hidden neuron greatly simplify lives describe hidden neurons using single parameter step position try modifying following diagram order get used new parameterization output top hidden neuron noted implicitly set weight input large value big enough step function good approximation easily convert neuron parameterized way back conventional model choosing bias focusing output top hidden neuron let take look behavior entire network particular suppose hidden neurons computing step functions parameterized step points top neuron bottom neuron respective output weights network weighted output hidden layer plotted right weighted output hidden layer visual proof neural nets compute function outputs top bottom hidden neurons respectivel outputs denoted often known neurons activations try increasing decreasing step point top hidden neuron get feel changes weighted output hidden layer particularly worth understanding happens goes past see graph changes shape happens since moved situation top hidden neuron first activated situation bottom hidden neuron first activated similarly try manipulating step point bottom hidden neuron get feel changes combined output hidden neurons try increasing decreasing output weights notice rescales contribution respective hidden neurons happens one weights zero finally try setting get bump function starts point ends point height instance weighted output might look like weighted output hidden layer course rescale bump height let use single parameter denote height reduce clutter also remove notations weighted output hidden layer try changing value see height bump changes try changing height negative observe happens try changing step points see changes shape bump note way output whole network bias output neuron obviously weighted output hidden layer plotting going focus weighted output hidden layer right later think relates output whole network universality one input one output notice way using neurons way thought graphical terms conventional programming terms kind else statement input step point add weighted output else add weighted output part going stick graphical point view follows may sometimes find helpful switch points view think things terms else use bump making trick get two bumps gluing two pairs hidden neurons together network weighted output hidden layer suppressed weights simply writing values pair hidden neurons try increasing decreasing values observe changes graph move bumps around changing step points generally use idea get many peaks want height particular divide interval large number subintervals use pairs hidden neurons set peaks desired height let see works quite neurons going pack things bit apologies complexity diagram could hide complexity abstracting away think worth putting little complexity sake getting concrete feel networks work visual proof neural nets compute function weighted output see five pairs hidden neurons step points respective pairs neurons values fixed make get five evenly spaced bumps graph pair neurons value associated remember connections output neurons weights marked click one values drag mouse right left change value watch function change changing output weights actually designing function contrariwise try clicking graph dragging change height bump functions change heights see corresponding change values although shown also change corresponding output weights words directly manipulate function appearing graph right see reflected values left fun thing hold mouse button drag mouse one side graph draw function get watch parameters neural network adapt time challenge let think back function plotted beginning chapter universality one input one output say time plotted actually function sin cos plotted axis taking values obviously trivial function going figure compute using neural network networks analyzing weighted combination output hidden neurons know get lot control quantity noted earlier quantity output network output network bias output neuron way achieve control actual output network solution design neural network whose hidden layer weighted output given inverse function want weighted output hidden layer output network whole good approximation challenge design neural network approximate goal function shown learn much possible want solve problem twice first time please click graph directly adjusting heights different bump note set bias output neuron visual proof neural nets compute function functions find fairly easy get good match goal function well measured average deviation goal function function network actually computing challenge drive average deviation low possible complete challenge drive average deviation weighted output figured elements necessary network approximately compute function coarse approximation could easily much better merely increasing number pairs hidden neurons allowing bumps particular easy convert data found back standard parametrization used neural networks let recap quickly works first layer weights large constant value say biases hidden neurons instance second hidden neuron becomes final layer weights determined values instance value chosen first means output weights top two hidden neurons respectively entire layer output weights finally bias output neuron everything complete description neural network pretty good job computing original goal function understand improve quality approximation improving number hidden neurons nothing special original goal function sin cos could used procedure continuous function essence using single layer neural networks paragraph refers interactive element available online graph shows final result manual minimization average deviation many input variables build lookup table function able build idea provide general proof universality 
[visual, proof, neural, nets, compute, function, many, input, variables] let extend results case many input variables sounds complicated ideas need understood case two inputs let address two input case start considering happens two inputs neuron inputs corresponding weights bias neuron let set weight play around first weight bias see affect output neuron output output output output output output visual proof neural nets compute function see input makes difference output neuron though input given think happens increase weight remaining immediately see answer ponder question bit see figure happens try see right shown happens following movie earlier discussion input weight gets larger output approaches step function difference step function three dimensions also move location step point around modifying bias actual location step point let redo using position step parameter output output output assume weight input large value used weight number neuron step point little number reminds step direction course also possible get step function direction making weight input large say weight equal output output output number neuron step point case little number reminds step direction could explicitly marked many input variables weights inputs decided since would make diagram rather cluttered keep mind little marker implicitly tells weight large weight use step functions constructed compute three dimensional bump function use two neurons computing step function direction combine step functions weight respectively desired height bump illustrated following diagram output output output try changing value height observe relates weights network see changes height bump function right also try changing step point associated top hidden neuron witness changes shape bump happens move past step point associated bottom hidden neuron figured make bump function direction course easily make bump function direction using two step functions direction recall making weight large input weight input result output output output looks nearly identical earlier network thing explicitly shown changing little markers hidden neurons reminds visual proof neural nets compute function producing step functions step functions weight large input zero input vice versa decided show explicitly order avoid clutter let consider happens add two bump functions one direction direction height output output simplify diagram dropped connections zero weight left little markers hidden neurons remind directions bump functions computed drop even markers later since implied input variable try varying parameter see causes output weights change also heights bump functions built looks little like tower function output tower function could build tower functions could use approximate arbitrary functions adding many towers different heights different locations many input variables output many towers course yet figured build tower function constructed looks like central tower height surrounding plateau height make tower function remember earlier saw neurons used implement type inlineif else statement input threshold output else output neuron single input want apply similar idea combined output hidden neurons combined output hidden neurons threshold output else output choose threshold appropriately say value sandwiched height plateau height central tower could squash plateau zero leave tower standing see try experimenting following network figure note plotting output entire network weighted output hidden layer means add bias term weighted output hidden layer apply sigma function find values produce tower bit tricky think remain stuck two hints get output neuron show right kind else behaviour need input weights large value determines scale else threshold visual proof neural nets compute function output output output initial parameters output looks like flattened version earlier diagram tower plateau get desired behaviour increase parameter becomes large gives else thresholding behaviour second get threshold right choose try see works looks like use output output output output even relatively modest value get pretty good tower function course make good want increasing still keeping bias let try gluing two networks together order compute two different tower functions make respective roles two sub networks clear put separate boxes box computes tower function using technique described graph right shows weighted output second hidden layer weighted combination tower functions output output output many input variables particular see modifying weights final layer change height output towers idea used compute many towers like also make thin like whatever height like result ensure weighted output second hidden layer approximates desired function two variables output many towers particular making weighted output second hidden layer good approxi mation ensure output network good approximation desired function functions two variables let try three variables following network used compute tower function four dimensions denote inputs network step points neurons weights first layer large biases set give step points weights second layer alternate large number output bias network computes function provided three conditions met network visual proof neural nets compute function everywhere else kind tower little region input space everywhere else gluing together many networks get many towers want approximate arbitrary function three variables exactly idea works dimensions change needed make output bias order get right kind sandwiching behavior level plateau okay know use neural networks approximate real valued function many variables vector valued functions course function regarded separate real valued functions create network approximating another network simply glue networks together also easy cope 
[visual, proof, neural, nets, compute, function, problem] seen use networks two hidden layers approximate arbitrary function find proof showing possible single hidden layer hint try working case two input variables showing possible get step functions directions arbitrary direction adding many constructions part possible approximate tower function circular shape rather rectangular using circular towers possible approximate arbitrary function part may help use ideas bit later chapter 
[visual, proof, neural, nets, compute, function, extension, beyond, sigmoid, neurons] proved networks made sigmoid neurons compute function recall sigmoid neuron inputs result output weights bias sigmoid function consider different type neuron one using activation function extension beyond sigmoid neurons assume neurons inputs weights bias output use activation function get step function sigmoid try ramping weight following say output neuron sigmoid causes activation function contract ultimately becomes good approximation step function try changing bias see set position step wherever choose use tricks compute desired function properties need satisfy order work need assume well defined two limits two values taken step function also need assume limits different one another step simply flat graph provided activation function satisfies properties neurons based activation function universal computation 
[visual, proof, neural, nets, compute, function, problems] earlier book met another type neuron known rectified linear unit explain neurons satisfy conditions given universality find proof universality showing rectified linear units universal computation suppose consider linear neurons neurons activation function explain linear neurons satisfy conditions given universality show neurons used universal computation visual proof neural nets compute function 
[visual, proof, neural, nets, compute, function, fixing, step, functions] assuming neurons produce step functions exactly pretty good approximation approximation fact narrow window failure illustrated following graph function behaves differently step function windows failure explanation given universality fail terrible failure making weights input neurons big enough make windows failure small like certainly make window much narrower shown narrower indeed eye could see perhaps might worry much problem nonetheless nice way addressing problem fact problem turns easy fix let look fix neural networks computing functions one input one output ideas work also address problem inputs outputs particular suppose want network compute function trying design network weighted output hidden layer neurons fixing step functions using technique described earlier use hidden neurons produce sequence bump functions exaggerated size windows failure order make easier see pretty clear add bump functions end reasonable approximation except within windows failure suppose instead using approximation described use set hidden neurons compute approximation half original goal function course looks like scaled version last graph suppose use another set hidden neurons compute approximation bases bumps shifted half width bump two different approximations add two approxi mations get overall approximation overall approximation visual proof neural nets compute function still failures small windows problem much less reason points failure window one approximation failure window approximation factor roughly better windows could even better adding large number overlapping approximations function provided windows failure narrow enough point ever one window failure provided using large enough number overlapping approximations result excellent overall approximation 
[visual, proof, neural, nets, compute, function, conclusion] explanation universality discussed certainly practical prescription compute using neural networks much like proofs universality nand gates like reason focused mostly trying make construction clear easy follow optimizing details construction however may find fun instructive exercise see improve construction although result directly useful constructing networks important takes table question whether particular function computable using neural network answer question always yes right question ask whether particular function computable rather good way compute function universality construction developed uses two hidden layers compute arbitrary function furthermore discussed possible get result single hidden layer given might wonder would ever interested deep networks networks many hidden layers simply replace networks shallow single hidden layer networks principle possible good practical reasons use deep networks argued deep networks hierarchical structure makes particularly well adapted learn hierarchies knowledge seem useful solving real world problems put concretely attacking problems image recognition helps use system understands individual pixels also increasingly complex concepts edges simple geometric shapes way complex multi object scenes later chapters see evidence suggesting deep networks better job shallow networks learning hierarchies knowledge sum universality tells neural networks compute function empirical evidence suggests deep networks networks best adapted learn functions useful solving many real world problems chapter acknowledgments thanks many discussions univer sality neural networks thanks particular chris suggesting use lookup table prove universality interactive visual form chapter inspired work people 
[deep, neural, networks, hard, train] imagine engineer asked design computer scratch one day working away office designing logical circuits setting gates gates boss walks bad news customer added surprising design requirement circuit entire computer must two layers deep dumbfounded tell boss customer crazy boss replies think crazy customer wants get fact limited sense customer crazy suppose allowed use special logical gate lets together many inputs want also allowed many input nand gate gate multiple inputs negate output special gates turns possible compute function using circuit two layers deep something possible make good idea practice solving circuit design problems kind algorithmic problem usually start deep neural networks hard train figuring solve sub problems gradually integrate solutions words build solution multiple layers abstraction instance suppose designing logical circuit multiply two numbers chances want build sub circuits operations like adding two numbers sub circuits adding two numbers turn built sub sub circuits adding two bits roughly speaking circuit look like final circuit contains least three layers circuit elements fact probably contain three layers break sub tasks smaller units described get general idea deep circuits make process design easier helpful design fact mathematical proofs showing functions shallow circuits require exponentially circuit elements compute deep circuits instance famous series papers early showed computing parity set bits requires exponentially many gates done shallow circuit hand use deeper circuits easy compute parity using small circuit compute parity pairs bits use results compute parity pairs pairs bits building quickly overall parity deep circuits thus intrinsically much powerful shallow circuits book approached neural networks like crazy customer almost networks worked single hidden layer neurons plus input output layers history somewhat complex give detailed references see johan håstad paper account early history references simple networks remarkably useful earlier chapters used networks like classify handwritten digits better percent accuracy nonetheless intuitively expect networks many hidden layers powerful networks could use intermediate layers build multiple layers abstraction boolean circuits instance visual pattern recognition neurons first layer might learn recognize edges neurons second layer could learn recognize complex shapes say triangle rectangles built edges third layer would recognize still complex shapes multiple layers abstraction seem likely give deep networks compelling advantage learning solve complex pattern recognition problems moreover case circuits theoretical results suggesting deep networks intrinsically powerful shallow certain problems network architectures proved razvan pascanu guido montúfar yoshua bengio see also informal discussion section yoshua bengio deep neural networks hard train train deep networks chapter try training deep networks using workhorse learning algorithm stochastic gradient descent backpropagation run trouble deep networks performing much better shallow networks failure seems surprising light discussion rather give deep networks dig try understand making deep networks hard train look closely discover different layers deep network learning vastly different speeds particular later layers network learning well early layers often get stuck training learning almost nothing stuckness simply due bad luck rather discover fundamental reasons learning slowdown occurs connected use gradient based learning techniques delve problem deeply learn opposite phenomenon also occur early layers may learning well later layers become stuck fact find intrinsic instability associated learning gradient descent deep many layer neural networks instability tends result either early later layers getting stuck training sounds like bad news delving difficulties begin gain insight required train deep networks effectively investigations good preparation next chapter use deep learning attack image recognition problems 
[deep, neural, networks, hard, train, vanishing, gradient, problem] goes wrong try train deep network answer question let first revisit case network single hidden layer per usual use mnist digit classification problem playground learning wish follow along training networks computer also course fine read along wish follow live need python numpy copy code get cloning relevant repository command line git clone https github com mnielsen neural networks deep learning git use git download data code need change src subdirectory python shell load mnist data import mnist_loader training_data validation_data test_data mnist_loader load_data_wrapper set network import network net network network introduced mnist problem data vanishing gradient problem network neurons input layer corresponding pixels input image use hidden neurons well output neurons corresponding possible classifications mnist digits let try training network complete epochs using mini batches training examples time learning rate regularization parameter train monitor classification accuracy net sgd training_data lmbda evaluation_data validation_data monitor_evaluation_accuracy true get classification accuracy percent thereabouts vary bit run run comparable earlier results similar configuration let add another hidden layer also neurons try training hyper parameters net network network net sgd training_data lmbda evaluation_data validation_data monitor_evaluation_accuracy true gives improved classification accuracy percent encouraging little depth helping let add another neuron hidden layer net network network net sgd training_data lmbda evaluation_data validation_data monitor_evaluation_accuracy true help fact result drops back percent close original shallow network suppose insert one hidden layer net network network net sgd training_data lmbda evaluation_data validation_data monitor_evaluation_accuracy true classification accuracy drops percent probably statistically significant drop encouraging either behaviour seems strange intuitively extra hidden layers ought make network able learn complex classification functions thus better job classifying certainly things get worse since extra layers worst case simply going going let assume extra hidden layers really could help principle problem learning algorithm finding right weights biases like figure going wrong learning algorithm better get insight going wrong let visualize network learns plotted part network network two hidden layers containing hidden neurons neuron diagram little bar note networks likely take minutes train depending speed machine running code may wish continue reading return later wait code finish executing see later understand build hidden layer nothing deep neural networks hard train representing quickly neuron changing network learns big bar means neuron weights bias changing rapidly small bar means weights bias changing slowly precisely bars denote gradient neuron rate change cost respect neuron bias back chapter saw gradient quantity controlled rapidly bias changes learning also rapidly weights input neuron change worry recall details thing keep mind simply bars show quickly neuron weights bias changing network learns keep diagram simple shown top six neurons two hidden layers omitted input neurons since got weights biases learn also omitted output neurons since layer wise comparisons makes sense compare layers number neurons results plotted beginning training immediately network initialized network initialized randomly surprising lot variation rapidly neurons learn still one thing jumps bars second hidden layer mostly much larger bars first hidden layer result neurons second hidden layer learn quite bit faster neurons first hidden layer merely coincidence neurons second hidden layer likely learn faster neurons first hidden layer general determine whether case helps global way comparing speed learning first second hidden layers let denote gradient gradient neuron think gradient vector whose entries determine quickly first hidden layer learns vector whose entries determine quickly second hidden layer learns data plotted generated using program generate_gradient program also used generate results quoted later section back chapter referred error adopt informal term gradient say informal course explicitly include partial derivatives cost respect weights vanishing gradient problem use lengths vectors rough global measures speed layers learning instance length measures speed first hidden layer learning length measures speed second hidden layer learning definitions configuration plotted find confirms earlier suspicion neurons second hidden layer really learning much faster neurons first hidden layer happens add hidden layers three hidden layers network respective speeds learning turn earlier hidden layers learning much slower later hidden layers suppose add yet another layer hidden neurons case respective speeds learning pattern holds early layers learn slower later layers looking speed learning start training networks initialized speed learning change train networks let return look network two hidden layers speed learning changes follows generate results used batch gradient descent training images trained epochs bit different way usually train used mini batches training images rather full image training set trying anything sneaky pull wool eyes turns using mini batch stochastic gradient descent gives much noisier albeit similar average away noise results using parameters chosen easy way smoothing results see going case see two layers start learning different speeds already know speed layers drops quickly rebounding first hidden layer learns much slowly second hidden layer complex networks results similar experiment time three hidden layers network deep neural networks hard train early hidden layers learn much slowly later hidden layers finally let add fourth hidden layer network see happens train early hidden layers learn much slowly later hidden layers case first hidden layer learning roughly times slower final hidden layer wonder trouble training networks earlier important observation least deep neural networks gradient tends get smaller move backward hidden layers means neurons earlier layers learn much slowly neurons later layers seen single network fundamental reasons happens many neural networks phenomenon known vanishing gradient causing vanishing gradient problem unstable gradients deep neural nets vanishing gradient problem occur ways avoid deal training deep neural networks fact learn shortly inevitable although alternative attractive either sometimes gradient gets much larger earlier layers exploding gradient problem much better news vanishing gradient problem generally turns gradient deep neural networks unstable tending either explode vanish earlier layers instability fundamental problem gradient based learning deep neural networks something need understand possible take steps address one response vanishing unstable gradients wonder really problem momentarily stepping away neural nets imagine trying merically minimize function single variable good news derivative small mean already near extremum similar way might small gradient early layers deep network mean need much adjustment weights biases course case recall randomly initialized weight biases network extremely unlikely initial weights biases good job whatever want network concrete consider first layer weights network mnist problem random initialization means first layer throws away information input image even later layers extensively trained still find extremely difficult identify input image simply enough information possibly case much learning needs done first layer going train deep networks need figure address vanishing gradient problem 
[deep, neural, networks, hard, train, what’s, causing, vanishing, gradient, problem, unstable, gradients, deep, neural, nets] get insight vanishing gradient problem occurs let consider simplest deep neural network one single neuron layer network three hidden layers weights biases cost function remind works output neuron usual sigmoid activation function weighted input neuron drawn cost end emphasize cost function network output actual output network close desired output cost low far away cost high see sepp hochreiter yoshua bengio paolo frasconi jürgen schmidhuber paper studied recurrent neural nets essential phenomenon feedforward networks studying see also sepp hochreiter earlier diploma thesis german deep neural networks hard train going study gradient associated first hidden neuron figure expression studying expression understand vanishing gradient problem occurs start simply showing expression looks forbidding actually got simple structure describe moment expression ignore network note derivative function structure expression follows term product neuron network weight term weight network final term corresponding cost function end notice placed term expression corresponding part network network mnemonic expression welcome take expression granted skip discussion relates vanishing gradient problem harm since expression special case earlier discussion backpropagation also simple explanation expression true fun perhaps enlightening take look explanation imagine make small change bias set cascading series changes rest network first causes change output first hidden neuron turn cause change weighted input second hidden neuron change output second hidden neuron way change cost output suggests figure expression gradient carefully tracking effect step cascade let think causes output first hidden neuron change term look familiar first term claimed expression gradient intuitively term converts change bias change output activation change turn causes change weighted input second hidden neuron combining expressions see change bias propagates causing vanishing gradient problem unstable gradients deep neural nets along network affect look familiar got first two terms claimed expression gradient keep going fashion tracking way changes propagate rest network neuron pick term weight pick term end result expression relating final change cost initial change bias dividing indeed get desired expression gradient vanishing gradient problem occurs understand vanishing gradi ent problem occurs let explicitly write entire expression gradient excepting last term expression product terms form understand terms behave let look plot function derivative sigmoid function derivative reaches maximum use standard approach initializing weights network choose weights using gaussian mean standard deviation weights usually satisfy putting observations together see terms usually satisfy take product many terms product tend exponentially decrease terms smaller product starting smell like possible explanation vanishing gradient problem make bit explicit let compare expression expression gradient respect later bias say course deep neural networks hard train explicitly worked expression follows pattern described comparison two expressions two expressions share many terms gradient includes two extra terms form seen terms typically less magnitude gradient usually factor smaller essential origin vanishing gradient problem course informal argument rigorous proof vanishing gradient problem occur several possible escape clauses particular might wonder whether weights could grow training possible terms product longer satisfy indeed terms get large enough greater longer vanishing gradient problem instead gradient actually grow exponentially move backward layers instead vanishing gradient problem exploding gradient problem exploding gradient problem let look explicit example exploding gradients occur example somewhat contrived going fix parameters network right way ensure get exploding gradient even though example contrived virtue firmly establishing exploding gradients merely hypothetical possibility really happen two steps getting exploding gradient first choose weights network large say second choose biases terms small actually pretty easy need choose biases ensure weighted input neuron instance want achieve setting use idea select biases see terms equal choices get exploding gradient unstable gradient problem fundamental problem much vanishing gradient problem exploding gradient problem gradient early layers product terms later layers many layers intrinsically unstable situation way layers learn close speed products terms come close balancing without mechanism underlying reason balancing occur highly unlikely happen simply chance short real problem neural networks suffer unstable gradient problem result use standard gradient based learning techniques different layers network tend learn wildly different speeds 
[deep, neural, networks, hard, train, exercise] unstable gradients complex networks discussion vanishing gradient problem made use fact suppose used different activation function one whose derivative could much larger would help avoid unstable gradient problem prevalence vanishing gradient problem seen gradient either vanish explode early layers deep network fact using sigmoid neurons gradient usually vanish see consider expression avoid vanishing gradient problem need might think could happen easily large however difficult looks reason term also depends input activation make large need careful simultaneously making small turns considerable constraint reason make large tend make large looking graph see puts wings function takes small values way avoid input activation falls within fairly narrow range values qualitative explanation made quantitative first problem sometimes chance happen often though happen generic case vanishing gradients 
[deep, neural, networks, hard, train, problems] consider product suppose argue ever occur supposing consider set input activations show set satisfying constraint range interval greater width show numerically expression bounding width range greatest takes value even given everything lines perfectly still fairly narrow range input activations avoid vanishing gradient problem identity neuron consider neuron single input corresponding weight bias weight output show choosing weights bias appropriately ensure neuron thus used kind identity neuron neuron whose output rescaling weight factor input hint helps rewrite assume small use taylor series expansion 
[deep, neural, networks, hard, train, unstable, gradients, complex, networks] studying toy networks one neuron hidden layer complex deep networks many neurons hidden layer deep neural networks hard train fact much behaviour occurs networks earlier chapter back propagation saw gradient layer layer network given diagonal matrix whose entries values weighted inputs layer weight matrices different layers vector partial derivatives respect output activations much complicated expression single neuron case still look closely essential form similar lots pairs form matrices small entries diagonal none larger provided weight matrices large additional term tends make gradient vector smaller leading vanishing gradient generally large number terms product tends lead unstable gradient earlier example practice empirically typically found sigmoid networks gradients vanish exponentially quickly earlier layers result learning slows layers slowdown merely accident inconvenience fundamental consequence approach taking learning 
[deep, neural, networks, hard, train, obstacles, deep, learning] chapter focused vanishing gradients generally unstable gra dients obstacle deep learning fact unstable gradients one obstacle deep learning albeit important fundamental obstacle much ongoing research aims better understand challenges occur training deep networks comprehensively summarize work want briefly mention couple papers give flavor questions people asking first example glorot found evidence suggesting use xavier glorot yoshua bengio see also earlier discussion use sigmoids yann lecun léon bottou genevieve orr klaus robert müller obstacles deep learning sigmoid activation functions cause problems training deep networks particular found evidence use sigmoids cause activations final hidden layer saturate near early training substantially slowing learning suggested alternative activation functions appear suffer much saturation problem second example sutskever martens dahl studied impact deep learning random weight initialization momentum schedule momentum based stochastic gradient descent cases making good choices made substantial difference ability train deep networks examples suggest makes deep networks hard train complex question chapter focused instabilities associated gradient based learning deep networks results last two paragraphs suggest also role played choice activation function way weights initialized even details learning gradient descent implemented course choice network architecture hyper parameters also important thus many factors play role making deep networks hard train understanding factors still subject ongoing research seems rather downbeat pessimism inducing good news next chapter turn around develop several approaches deep learning extent manage overcome route around challenges ilya sutskever james martens george dahl geoffrey hinton deep neural networks hard train 
[deep, learning] last chapter learned deep neural networks often much harder train shallow neural networks unfortunate since good reason believe could train deep nets much powerful shallow nets news last chapter discouraging let stop chapter develop techniques used train deep networks apply practice also look broader picture briefly reviewing recent progress using deep nets image recognition speech recognition applications take brief speculative look future may hold neural nets artificial intelligence chapter long one help navigate let take tour sections loosely coupled provided basic familiarity neural nets jump whatever interests main part chapter introduction one widely used types deep network work detailed example code using convolutional nets solve problem classifying handwritten digits mnist data set start account convolutional networks shallow networks used attack problem earlier book many iterations build powerful networks explore many powerful techniques convolutions pooling use gpus far training shallow networks algorithmic expansion training data reduce overfitting use dropout technique also reduce overfitting use ensembles networks others result deep learning system offers near human performance mnist test images images seen training system classify correctly peek images misclassified note correct classification top right program classification bottom right many tough even human classify consider example third image top row looks like official classification network also thinks kind error least understandable perhaps even commendable conclude discussion image recognition survey spectacular recent progress using networks particularly convolutional nets image recognition remainder chapter discusses deep learning broader less detailed perspective briefly survey models neural networks recurrent neural nets long short term memory units models applied problems speech recognition natural language processing areas speculate future neural networks deep learning ranging ideas like intention driven user interfaces role deep learning artificial intelligence chapter builds earlier chapters book making use integrating ideas backpropagation regularization softmax function however read chapter need worked detail earlier chapters however help read chapter basics neural networks use concepts chapters provide links familiarize necessary worth noting chapter tutorial latest greatest neural networks libraries going training deep networks dozens layers solve problems leading edge rather focus understanding core principles behind deep neural networks applying simple easy understand context mnist problem put another way chapter going bring right frontier rather intent earlier chapters focus fundamentals prepare understand wide range current work introducing convolutional networks 
[deep, learning, introducing, convolutional, networks] earlier chapters taught neural networks pretty good job recognizing images handwritten digits using networks adjacent network layers fully connected one another every neuron network connected every neuron adjacent layers particular pixel input image encoded pixel intensity value corresponding neuron input layer pixel images using means network input neurons trained network weights biases network output would hope correctly identify input image earlier networks work pretty well obtained classification accuracy better percent using training test data mnist handwritten digit data set upon reflection strange use networks fully connected layers classify images reason network architecture take account spatial structure images instance treats input pixels far apart close together exactly footing concepts spatial structure must instead inferred training data instead starting network architecture tabula rasa used architecture tries take advantage spatial structure deep learning section describe convolutional neural networks use special architecture particularly well adapted classify images using architecture makes convolutional networks fast train turn helps train deep many layer networks good classifying images today deep convolutional networks close variant used neural networks image recognition convolutional neural networks use three basic ideas local receptive fields shared weights pooling let look ideas turn local receptive fields fully connected layers shown earlier inputs depicted vertical line neurons convolutional net help think instead inputs square neurons whose values correspond pixel intensities using inputs per usual connect input pixels layer hidden neurons connect every input pixel every hidden neuron instead make connections small localized regions input image precise neuron first hidden layer connected small region input neurons say example region corresponding input pixels particular hidden neuron might connections look like region input image called local receptive field hidden neuron little window input pixels connection learns weight hidden neuron origins convolutional neural networks back seminal paper establishing modern subject convolutional networks paper yann lecun léon bottou yoshua bengio patrick haffner lecun since made interesting terminology convolutional nets biological neural inspiration models like convolutional nets tenuous call convolutional nets convolutional neural nets call nodes units neurons despite remark convolutional nets use many ideas neural networks studied ideas backpropagation gradient descent regularization non linear activation functions follow common practice consider type neural network use terms convolutional neural network convolutional net work interchangeably also use terms artificial neuron unit interchangeably introducing convolutional networks learns overall bias well think particular hidden neuron learning analyze particular local receptive field slide local receptive field across entire input image local receptive field different hidden neuron first hidden layer illustrate concretely let start local receptive field top left corner slide local receptive field one pixel right one neuron connect second hidden neuron building first hidden layer note input image local receptive fields neurons hidden layer move local receptive field neurons across neurons colliding right hand side bottom input image shown local receptive field moved one pixel time fact sometimes different stride length used instance might move local receptive field pixels right case say stride length used chapter mostly stick stride length worth knowing people sometimes experiment different stride shared weights biases said hidden neuron bias weights connected local receptive field yet mention going use weights bias hidden neurons words hidden neuron output done earlier chapters interested trying different stride lengths use validation data pick stride length gives best performance details see earlier discussion choose hyper parameters neural network approach may also used choose size local receptive field course nothing special using local receptive field general larger local receptive fields tend helpful input images significantly larger pixel mnist images deep learning neural activation function perhaps sigmoid function used earlier chapters shared value bias array shared weights finally use denote input activation position means neurons first hidden layer detect exactly different locations input image see makes sense suppose weights bias hidden neuron pick say vertical edge particular local receptive field ability also likely useful places image useful apply feature detector everywhere image put slightly abstract terms convolutional networks well adapted translation invariance images move picture cat say little ways still image reason sometimes call map input layer hidden layer feature map call weights defining feature map shared weights call bias defining feature map way shared bias shared weights bias often said define kernel filter literature people sometimes use terms slightly different ways reason going precise rather moment look concrete examples network structure described far detect single kind localized feature image recognition need one feature map complete convolutional layer consists several different feature maps example shown feature maps feature map defined set shared weights single shared bias result network detect different kinds features feature detectable across entire image shown feature maps keep diagram simple however practice convolutional networks may use perhaps many feature maps one early convolutional networks lenet used feature maps associated local receptive field recognize mnist digits example illustrated actually pretty close lenet examples develop later chapter use convolutional layers feature maps let take quick peek features learned precisely defined notion feature informally think feature detected hidden neuron kind input pattern cause neuron activate might edge image instance maybe type shape fact mnist digit classification problem studying images centered size normalized mnist less translation invariance images found wild speak still features like edges corners likely useful across much input space introducing convolutional networks images correspond different feature maps filters kernels map represented block image corresponding weights local receptive field whiter blocks mean smaller typically negative weight feature map responds less corresponding input pixels darker blocks mean larger weight feature map responds corresponding input pixels roughly speaking images show type features convolutional layer responds conclude feature maps clear spatial structure beyond expect random many features clear sub regions light dark shows network really learning things related spatial structure however beyond difficult see feature detectors learning certainly learning say gabor filters used many traditional approaches image recognition fact lot work better understanding features learnt convolutional networks interested following work suggest starting paper matthew zeiler rob fergus big advantage sharing weights biases greatly reduces number parameters involved convolutional network feature map need shared weights plus single shared bias feature map requires parameters feature maps total parameters defining convolutional layer comparison suppose fully connected first layer input neurons relatively modest hidden neurons used many examples earlier book total weights plus extra biases total parameters words fully connected layer would times many parameters convolutional layer course really direct comparison number parameters since two models different essential ways intuitively seems likely use translation invariance convolutional layer reduce number parameters needs get performance fully connected model turn result faster training convolutional model ultimately help build deep networks using convolutional layers incidentally name convolutional comes fact operation equation sometimes known convolution little precisely people sometimes write equation denotes set output activations one feature map set input activations called convolution operation deep learning going make deep use mathematics convolutions need worry much connection worth least knowing name comes pooling layers addition convolutional layers described convolutional neural networks also contain pooling layers pooling layers usually used immediately convolutional layers pooling layers simplify information output convolutional layer detail pooling layer takes feature output convolutional layer prepares condensed feature map instance unit pooling layer may summarize region say neurons previous layer concrete example one common procedure pooling known max pooling max pooling pooling unit simply outputs maximum activation input region illustrated following diagram note since neurons output convolutional layer pooling neurons mentioned convolutional layer usually involves single feature map apply max pooling feature map separately three feature maps combined convolutional max pooling layers would look like think max pooling way network ask whether given feature found anywhere region image throws away exact positional information intuition feature found exact location important rough location relative features big benefit many fewer pooled features helps reduce number parameters needed later layers max pooling technique used pooling another common approach known pooling instead taking maximum activation region nomenclature used loosely particular using feature map mean function computed convolutional layer rather activation hidden neurons output layer kind mild abuse nomenclature pretty common research literature introducing convolutional networks neurons take square root sum squares activations region details different intuition similar max pooling pooling way condensing information convolutional layer practice techniques widely used sometimes people use types pooling operation really trying optimize performance may use validation data compare several different approaches pooling choose approach works best going worry kind detailed optimization putting together put ideas together form complete convolutional neural network similar architecture looking addition layer output neurons corresponding possible values mnist digits etc network begins input neurons used encode pixel intensities mnist image followed convolutional layer using local receptive field feature maps result layer hidden feature neurons next step max pooling layer applied regions across feature maps result layer hidden feature neurons final layer connections network fully connected layer layer connects every neuron max pooled layer every one output neurons fully connected architecture used earlier chapters note however diagram used single arrow simplicity rather showing connections course easily imagine connections convolutional architecture quite different architectures used earlier chapters overall picture similar network made many simple units whose behaviors determined weights biases overall goal still use training data train network weights biases network good job classifying input digits particular earlier book train network using stochastic gradient descent backpropagation mostly proceeds exactly way earlier chapters however need make modifications backpropagation procedure reason earlier derivation backpropagation networks fully connected layers fortunately straightforward modify derivation convolutional max pooling layers like understand details invite work following problem warned problem take time work unless really internalized earlier derivation backpropagation case easy 
[deep, learning, problem] idea convolutional layers behave invariant way across images may seem surprising network learn done translate input data explain actually quite reasonable inserting extra fully connected layer even better one possibility use exactly procedure expand size fully connected layer tried neurons obtaining results percent respectively interesting really convincing win earlier result percent patrice simard dave steinkraus john platt deep learning adding extra fully connected layer let try inserting extra fully connected layer two hidden neuron fully connected layers net network convpoollayer image_shape mini_batch_size filter_shape poolsize activation_fn relu convpoollayer image_shape mini_batch_size filter_shape poolsize activation_fn relu fullyconnectedlayer n_in n_out activation_fn relu fullyconnectedlayer n_in n_out activation_fn relu softmaxlayer n_in n_out mini_batch_size net sgd expanded_training_data mini_batch_size validation_data test_data lmbda obtained test accuracy percent expanded net helping much running similar experiments fully connected layers containing neurons yields results percent encouraging still falls short really decisive win going expanded extra fully connected layers really help mnist might network capacity better going learning wrong way instance maybe could use stronger regularization techniques reduce tendency overfit one possibility dropout technique introduced back chapter recall basic idea dropout remove individual activations random training network makes model robust loss individual pieces evidence thus less likely rely particular idiosyncracies training data let try applying dropout final fully connected layers net network convpoollayer image_shape mini_batch_size filter_shape poolsize activation_fn relu convpoollayer image_shape mini_batch_size filter_shape poolsize activation_fn relu fullyconnectedlayer n_in n_out activation_fn relu p_dropout fullyconnectedlayer n_in n_out activation_fn relu p_dropout softmaxlayer n_in n_out p_dropout mini_batch_size net sgd expanded_training_data mini_batch_size validation_data test_data using obtain accuracy percent substantial improvement earlier results especially main benchmark network hidden neurons achieved percent two changes worth noting convolutional neural networks practice first reduced number training epochs dropout reduced overfitting learned faster second fully connected hidden layers neurons used earlier course dropout effectively omits many neurons training expansion expected fact tried experiments hidden neurons obtained slightly better validation performance hidden neurons using ensemble networks easy way improve performance still create several neural networks get vote determine best classification suppose example trained different neural networks using prescription achieving accuracies near percent even though networks would similar accuracies might well make different errors due different random initializations plausible taking vote amongst networks might yield classification better individual network sounds good true kind ensembling common trick neural networks machine learning techniques fact yield improvements end percent accuracy words ensemble networks classifies test images correctly remaining errors test set shown label top right correct classification according mnist data bottom right label output ensemble nets worth looking detail first two digits genuine errors ensemble however also understandable errors kind human could plausibly make really look lot like looks lot like third image supposedly actually looks like siding network ensemble think done better job whoever originally drew digit hand fourth image really seem classified badly networks cases networks choices seem least plausible cases done better job classifying original person writing digit overall networks offer exceptional performance especially consider correctly classified images shown context clear errors seem quite understandable even careful human makes occasional mistake expect deep learning extremely careful methodical human would much better network getting near human performance applied dropout fully connected layers look carefully code notice applied dropout fully connected section network convolutional layers principle could apply similar procedure convolutional layers fact need convolutional layers considerable inbuilt resistance overfitting reason shared weights mean convolutional filters forced learn across entire image makes less likely pick local idiosyncracies training data less need apply regularizers dropout going possible improve performance mnist still rodrigo benenson compiled informative summary page showing progress years links papers many papers use deep convolutional networks along lines similar networks using dig papers find many interesting techniques may enjoy implementing wise start implementation simple network trained quickly help rapidly understand going part try survey recent work resist making one exception paper cire san meier gambardella like paper simple network many layer neural network using fully connected layers convolutions successful network hidden layers containing neurons respectively used ideas similar simard expand training data apart used tricks including convolutional layers plain vanilla network kind enough patience could trained mnist data set existed given enough computing power achieved classification accuracy percent less key use large deep network use gpu speed training let train many epochs also took advantage long training times gradually decrease learning rate fun exercise try match results using architecture like able train saw last chapter fundamental obstructions training deep many layer neural networks particular saw gradient tends quite unstable move output layer earlier layers gradient tends either vanish vanishing gradient problem explode exploding gradient problem since gradient signal use train causes problems avoided results course answer avoided results instead done things help proceed anyway particular using convolutional layers greatly reduces number parameters layers making learning problem much easier using powerful regularization techniques notably dropout convolutional layers reduce overfitting otherwise problem complex networks using rectified linear units instead sigmoid neurons speed training empirically often factor using gpus willing train long period time particular final experiments trained epochs using data set times larger raw mnist training data earlier book mostly dan claudiu cire san ueli meier luca maria gambardella jürgen schmidhuber code convolutional networks trained epochs using raw training data combining factors though trained factor perhaps times longer response may train deep networks fuss course used ideas making use sufficiently large data sets help avoid overfitting using right cost function avoid learning slowdown using good weight initializations also avoid learning slowdown due neuron saturation algorithmically expanding training data discussed ideas earlier chapters part able reuse ideas little comment chapter said really rather simple set ideas simple powerful used concert getting started deep learning turned pretty easy deep networks anyway counting convolutional pooling layers single layers final architecture hidden layers network really deserve called deep network course hidden layers many shallow networks studied earlier networks single hidden layer occasionally hidden layers hand state art deep networks sometimes dozens hidden layers occasionally heard people adopt deeper thou attitude holding keeping joneses terms number hidden layers really deep learning sympathetic attitude part makes definition deep learning something depends upon result moment real breakthrough deep learning realize practical beyond shallow hidden layer networks dominated work mid really significant breakthrough opening exploration much expressive models beyond number layers primary fundamental interest rather use deeper networks tool use help achieve goals like better classification accuracies word procedure section smoothly moved single hidden layer shallow networks many layer convolutional networks seemed easy make change part get improvement start experimenting guarantee things always smooth reason presented cleaned narrative omitting many experiments including many failed experiments cleaned narrative hopefully help get clear basic ideas also runs risk conveying incomplete impression getting good working network involve lot trial error occasional frustration practice expect engage quite bit experimentation speed process may find helpful revisit chapter discussion choose neural network hyper parameters perhaps also look reading suggested section 
[deep, learning, convolutional, neural, networks, practice] seen core ideas behind convolutional neural networks let look work practice implementing convolutional networks applying mnist digit classification problem program use called network improved version programs network network developed earlier note work code network next section section use network library build convolutional networks programs network network implemented using python matrix library numpy programs worked first principles got right details backpropagation stochastic gradient descent understand details network going use machine learning library known using theano makes easy implement backpropagation convolutional neural networks since automatically computes mappings involved theano also quite bit faster earlier code written easy understand fast makes practical train complex networks particular one great feature theano run code either cpu available gpu running gpu provides substantial speedup helps make practical train complex networks wish follow along need get theano running system install theano follow instructions project homepage examples follow run using theano run mac yosemite gpu run ubuntu nvidia gpu experiments run get network running need set gpu flag either true false appropriate network source beyond get theano running gpu may find instructions helpful also tutorials web easily found using google help get things working gpu available locally may wish look amazon web services spot instances note even gpu code take time execute many experiments take minutes hours run cpu may take days run complex experiments earlier chapters suggest setting things running continuing read occasionally coming back check output code note also network incorporates ideas theano library documentation convo lutional neural nets notably implementation lenet misha denil implementation dropout chris olah see james bergstra olivier breuleux frederic bastien pascal lamblin ravzan pascanu guillaume desjardins joseph turian david warde farley yoshua bengio theano also basis popular neural networks libraries popular neural nets libraries time writing include release chapter current version theano changed version actually rerun examples theano get extremely similar results reported text convolutional neural networks practice using cpu may wish reduce number training epochs complex experiments perhaps omit entirely get baseline start shallow architecture using single hidden layer containing hidden neurons train epochs using learning rate mini batch size regularization import network network import network network import convpoollayer fullyconnectedlayer softmaxlayer training_data validation_data test_data network load_data_shared mini_batch_size net network fullyconnectedlayer n_in n_out softmaxlayer n_in n_out mini_batch_size net sgd training_data mini_batch_size validation_data test_data obtained best classification accuracy percent classification accuracy test_data evaluated training epoch get best classification accuracy validation_data using validation data decide evaluate test accuracy helps avoid overfitting test data see earlier discussion use validation data follow practice results may vary slightly since network weights biases randomly percent accuracy close percent accuracy obtained back chapter using similar network architecture learning hyper parameters particular examples used shallow network single hidden layer containing hidden neurons also trained epochs used mini batch size learning rate however two differences earlier network first regularized earlier network help reduce effects overfitting regularizing current network improve accuracies gain small hold worrying regularization later second final layer earlier network used sigmoid activations cross entropy cost function current network uses softmax final layer log likelihood cost function explained chapter big change made switch particularly deep reason mostly done softmax plus log likelihood cost common modern image classification networks better results using deeper network architecture let begin inserting convolutional layer right beginning network use local receptive fields stride length feature maps also insert max pooling layer combines features using pooling windows overall network architecture looks much like architecture discussed last section extra fully connected layer code experiments section may found script note code script simply duplicates parallels discussion section note also throughout section explicitly specified number training epochs done clarity training practice worth using early stopping tracking accuracy validation set stopping training confident validation accuracy stopped improving fact experiment actually three separate runs training network architecture reported test accuracy corresponded best validation accuracy three runs using multiple runs helps reduce variation results useful comparing many architectures followed procedure except noted practice made little difference results obtained deep learning architecture think convolutional pooling layers learning local spatial structure input training image later fully connected layer learns abstract level integrating global information across entire image common pattern convolutional neural networks let train network see net network convpoollayer image_shape mini_batch_size filter_shape poolsize fullyconnectedlayer n_in n_out softmaxlayer n_in n_out mini_batch_size net sgd training_data mini_batch_size validation_data test_data gets percent accuracy considerable improvement previous results indeed reduced error rate better third great improvement specifying network structure treated convolutional pooling layers single layer whether regarded separate layers single layer extent matter taste network treats single layer makes code network little compact however easy modify network layers specified separately desired 
[deep, learning, exercise] classification accuracy get omit fully connected layer use convolutional pooling layer softmax layer inclusion fully connected layer help improve percent classification accuracy let try inserting second convolutional pooling layer make insertion existing convolutional pooling layer fully connected hidden layer use local receptive field pool regions let see happens train using similar hyper parameters net network convpoollayer image_shape mini_batch_size filter_shape poolsize convpoollayer continued use mini batch size fact discussed earlier may possible speed training using larger mini batches continued use mini batch size mostly consistency experiments earlier chapters convolutional neural networks practice image_shape mini_batch_size filter_shape poolsize fullyconnectedlayer n_in n_out softmaxlayer n_in n_out mini_batch_size net sgd training_data mini_batch_size validation_data test_data get improvement percent classification accuracy two natural questions ask point first question even mean apply second convolutional pooling layer fact think second convolutional pooling layer input images whose pixels represent presence absence particular localized features original input image think layer input version original input image version abstracted condensed still lot spatial structure makes sense use second convolutional pooling layer satisfying point view gives rise second question output previous layer involves separate feature maps inputs second convolutional pooling layer though got separate images input convolutional pooling layer single image case first convolutional pooling layer neurons second convolutional pooling layer respond multiple input images fact allow neuron layer learn input neurons local receptive field informally feature detectors second convolutional pooling layer access features previous layer within particular local receptive 
[deep, learning, code, convolutional, networks] alright let take look code program network structurally similar network program developed chapter although details differ due use theano start looking fullyconnectedlayer class deep learning similar layers studied earlier book code discussion class fullyconnectedlayer object def __init__ self n_in n_out activation_fn sigmoid p_dropout self n_in n_in self n_out n_out self activation_fn activation_fn self p_dropout p_dropout initialize weights biases self theano shared asarray random normal loc scale sqrt n_out size n_in n_out dtype theano config floatx name borrow true self theano shared asarray random normal loc scale size n_out dtype theano config floatx name borrow true self params self self def set_inpt self inpt inpt_dropout mini_batch_size self inpt inpt reshape mini_batch_size self n_in self output self activation_fn self p_dropout dot self inpt self self self y_out argmax self output axis self inpt_dropout dropout_layer inpt_dropout reshape mini_batch_size self n_in self p_dropout self output_dropout self activation_fn dot self inpt_dropout self self def accuracy self return accuracy mini batch return mean self y_out much __init__ method self explanatory remarks may help clarify code per usual randomly initialize weights biases normal random variables suitable standard deviations lines look little forbidding however complication loading weights biases theano calls shared variables ensures variables processed gpu one available get much details interested dig theano documentation note also weight bias initialization designed sigmoid activation function discussed earlier ideally initialize weights biases somewhat differently activation functions tanh rectified linear function discussed problems __init__ method finishes self params self self handy way bundle learnable parameters associated layer later network sgd method use params attributes figure variables network instance learn set_inpt method used set input layer compute corre sponding output use name inpt rather input input built function note added november several readers noted line initializing self set scale sqrt n_out arguments chapter suggest better initialization may scale sqrt n_in simply mistake part ideal world rerun examples chapter correct code still moved projects going let error code convolutional networks python messing built ins tends cause unpredictable behavior difficult diagnose bugs note actually set input two separate ways self inpt self inpt_dropout done training may want use dropout case want remove fraction self p_dropout neurons function dropout_layer second last line set_inpt method ing self inpt_dropout self output_dropout used training self inpt self output used purposes evaluating accuracy validation test data convpoollayer softmaxlayer class definitions similar fullyconnectedlayer indeed close excerpt code interested look full listing network later section however couple minor differences detail worth mentioning obviously convpoollayer softmaxlayer compute output activations way appropriate layer type fortunately theano makes easy providing built operations compute convolutions max pooling softmax function less obviously introduced softmax layer never discussed initialize weights biases elsewhere argued sigmoid layers initialize weights using suitably parameterized normal random variables heuristic argument specific sigmoid neurons amendment tanh neurons however particular reason argument apply softmax layers priori reason apply initialization rather shall initialize weights biases rather hoc procedure works well enough practice okay looked layer classes network class let start looking __init__ method class network object def __init__ self layers mini_batch_size takes list layers describing network architecture value mini_batch_size used training stochastic gradient descent self layers layers self mini_batch_size mini_batch_size self params param layer self layers param layer params self matrix self ivector init_layer self layers init_layer set_inpt self self self mini_batch_size xrange len self layers prev_layer layer self layers self layers layer set_inpt prev_layer output prev_layer output_dropout self mini_batch_size self output self layers output self output_dropout self layers output_dropout self explanatory nearly line self params param layer bundles parameters layer single list anticipated network sgd method use self params figure variables network learn lines self matrix self ivector define theano symbolic variables named used represent input deep learning desired output network theano tutorial get deeply means symbolic rough idea represent mathematical variables explicit values usual things one would variables add subtract multiply apply functions indeed theano provides many ways manipulating symbolic variables things like convolutions max pooling big win ability fast symbolic differentiation using general form backpropagation algorithm extremely useful applying stochastic gradient descent wide variety network architectures particular next lines code define symbolic outputs network start setting input initial layer line init_layer set_inpt self self self mini_batch_size note inputs set one mini batch time mini batch size note also pass input self twice may use network two different ways without dropout loop propagates symbolic variable self forward layers network allows define final output output_dropout attributes symbolically represent output network understood network initialized let look trained using sgd method code looks lengthy structure actually rather simple explanatory comments code def sgd self training_data epochs mini_batch_size eta validation_data test_data lmbda train network using mini batch stochastic gradient descent training_x training_y training_data validation_x validation_y validation_data test_x test_y test_data compute number minibatches training validation testing num_training_batches size training_data mini_batch_size num_validation_batches size validation_data mini_batch_size num_test_batches size test_data mini_batch_size define regularized cost function symbolic gradients updates l_norm_squared sum layer sum layer self layers cost self layers cost self lmbda l_norm_squared num_training_batches grads grad cost self params updates param param eta grad param grad zip self params grads define functions train mini batch compute accuracy validation test mini batches lscalar mini batch index train_mb theano function cost updates updates givens self training_x self mini_batch_size self mini_batch_size provides good introduction theano get stuck may find helpful look one tutorials available online instance covers many basics code convolutional networks self training_y self mini_batch_size self mini_batch_size validate_mb_accuracy theano function self layers accuracy self givens self validation_x self mini_batch_size self mini_batch_size self validation_y self mini_batch_size self mini_batch_size test_mb_accuracy theano function self layers accuracy self givens self test_x self mini_batch_size self mini_batch_size self test_y self mini_batch_size self mini_batch_size self test_mb_predictions theano function self layers y_out givens self test_x self mini_batch_size self mini_batch_size actual training best_validation_accuracy epoch xrange epochs minibatch_index xrange num_training_batches iteration num_training_batches epoch minibatch_index iteration print training mini batch number format iteration cost_ij train_mb minibatch_index iteration num_training_batches validation_accuracy mean validate_mb_accuracy xrange num_validation_batches print epoch validation accuracy format epoch validation_accuracy validation_accuracy best_validation_accuracy print best validation accuracy date best_validation_accuracy validation_accuracy best_iteration iteration test_data test_accuracy mean test_mb_accuracy xrange num_test_batches print corresponding test accuracy format test_accuracy print finished training network print best validation accuracy obtained iteration format best_validation_accuracy best_iteration print corresponding test accuracy format test_accuracy first lines straightforward separating datasets components computing number mini batches used dataset next lines interesting show makes theano fun work let explicitly excerpt lines define regularized cost function symbolic gradients updates l_norm_squared sum layer sum layer self layers cost self layers cost self deep learning lmbda l_norm_squared num_training_batches grads grad cost self params updates param param eta grad param grad zip self params grads lines symbolically set regularized log likelihood cost function compute corresponding derivatives gradient function well corresponding parameter updates theano lets achieve lines thing hidden computing cost involves call cost method output layer code elsewhere network code short simple anyway things defined stage set define train_mb function theano symbolic function uses updates update network parameters given mini batch index similarly validate_mb_accuracy test_mb_accuracy compute accuracy network given mini batch validation test data averaging functions able compute accuracies entire validation test data sets remainder sgd method self explanatory simply iterate epochs repeatedly training network mini batches training data computing validation test accuracies okay understood important pieces code network let take brief look entire program need read detail may enjoy glancing perhaps diving pieces strike fancy best way really understand course modifying adding extra features refactoring anything think could done elegantly code problems contain starter suggestions things network theano based program training running simple neural networks supports several layer types fully connected convolutional max pooling softmax activation functions sigmoid tanh rectified linear units easily added run cpu program much faster network network however unlike network network also run gpu makes faster still code based theano code different many ways network network however possible tried maintain consistency earlier programs particular api similar network note focused making code simple easily readable easily modifiable optimized omits many desirable features program incorporates ideas theano documentation convolutional neural nets notably http deeplearning net tutorial lenet html misha denil implementation dropout https github com mdenil dropout chris olah http colah github using theano gpu little tricky particular easy make mistake pulling data gpu slow things lot tried avoid said code certainly sped quite bit careful optimization theano configuration see theano documentation details code convolutional networks written theano needs changes recent versions theano libraries standard library import cpickle import gzip third party libraries import numpy import theano import theano tensor theano tensor nnet import conv theano tensor nnet import softmax theano tensor import shared_randomstreams theano tensor signal import downsample activation functions neurons def linear return def relu return maximum theano tensor nnet import sigmoid theano tensor import tanh constants gpu true gpu print trying run gpu desired modify network nto set gpu flag false try theano config device gpu except pass already set theano config floatx float else print running cpu desired modify network set nthe gpu flag true load mnist data def load_data_shared filename data mnist pkl gzip open filename training_data validation_data test_data cpickle load close def shared data place data shared variables allows theano copy data gpu one available shared_x theano shared asarray data dtype theano config floatx borrow true shared_y theano shared asarray data dtype theano config floatx borrow true return shared_x cast shared_y int return shared training_data shared validation_data shared test_data main class used construct train networks class network object def __init__ self layers mini_batch_size takes list layers describing network architecture value mini_batch_size used training deep learning stochastic gradient descent self layers layers self mini_batch_size mini_batch_size self params param layer self layers param layer params self matrix self ivector init_layer self layers init_layer set_inpt self self self mini_batch_size xrange len self layers prev_layer layer self layers self layers layer set_inpt prev_layer output prev_layer output_dropout self mini_batch_size self output self layers output self output_dropout self layers output_dropout def sgd self training_data epochs mini_batch_size eta validation_data test_data lmbda train network using mini batch stochastic gradient descent training_x training_y training_data validation_x validation_y validation_data test_x test_y test_data compute number minibatches training validation testing num_training_batches size training_data mini_batch_size num_validation_batches size validation_data mini_batch_size num_test_batches size test_data mini_batch_size define regularized cost function symbolic gradients updates l_norm_squared sum layer sum layer self layers cost self layers cost self lmbda l_norm_squared num_training_batches grads grad cost self params updates param param eta grad param grad zip self params grads define functions train mini batch compute accuracy validation test mini batches lscalar mini batch index train_mb theano function cost updates updates givens self training_x self mini_batch_size self mini_batch_size self training_y self mini_batch_size self mini_batch_size validate_mb_accuracy theano function self layers accuracy self givens self validation_x self mini_batch_size self mini_batch_size self validation_y self mini_batch_size self mini_batch_size test_mb_accuracy theano function self layers accuracy self givens self test_x self mini_batch_size self mini_batch_size code convolutional networks self test_y self mini_batch_size self mini_batch_size self test_mb_predictions theano function self layers y_out givens self test_x self mini_batch_size self mini_batch_size actual training best_validation_accuracy epoch xrange epochs minibatch_index xrange num_training_batches iteration num_training_batches epoch minibatch_index iteration print training mini batch number format iteration cost_ij train_mb minibatch_index iteration num_training_batches validation_accuracy mean validate_mb_accuracy xrange num_validation_batches print epoch validation accuracy format epoch validation_accuracy validation_accuracy best_validation_accuracy print best validation accuracy date best_validation_accuracy validation_accuracy best_iteration iteration test_data test_accuracy mean test_mb_accuracy xrange num_test_batches print corresponding test accuracy format test_accuracy print finished training network print best validation accuracy obtained iteration format best_validation_accuracy best_iteration print corresponding test accuracy format test_accuracy define layer types class convpoollayer object used create combination convolutional max pooling layer sophisticated implementation would separate two purposes always use together simplifies code makes sense combine def __init__ self filter_shape image_shape poolsize activation_fn sigmoid filter_shape tuple length whose entries number filters number input feature maps filter height filter width image_shape tuple length whose entries mini batch size number input feature maps image height image width poolsize tuple length whose entries pooling sizes self filter_shape filter_shape deep learning self image_shape image_shape self poolsize poolsize self activation_fn activation_fn initialize weights biases n_out filter_shape prod filter_shape prod poolsize self theano shared asarray random normal loc scale sqrt n_out size filter_shape dtype theano config floatx borrow true self theano shared asarray random normal loc scale size filter_shape dtype theano config floatx borrow true self params self self def set_inpt self inpt inpt_dropout mini_batch_size self inpt inpt reshape self image_shape conv_out conv convd input self inpt filters self filter_shape self filter_shape image_shape self image_shape pooled_out downsample max_pool_d input conv_out self poolsize ignore_border true self output self activation_fn pooled_out self dimshuffle self output_dropout self output dropout convolutional layers class fullyconnectedlayer object def __init__ self n_in n_out activation_fn sigmoid p_dropout self n_in n_in self n_out n_out self activation_fn activation_fn self p_dropout p_dropout initialize weights biases self theano shared asarray random normal loc scale sqrt n_out size n_in n_out dtype theano config floatx name borrow true self theano shared asarray random normal loc scale size n_out dtype theano config floatx name borrow true self params self self def set_inpt self inpt inpt_dropout mini_batch_size self inpt inpt reshape mini_batch_size self n_in self output self activation_fn self p_dropout dot self inpt self self self y_out argmax self output axis self inpt_dropout dropout_layer inpt_dropout reshape mini_batch_size self n_in self p_dropout self output_dropout self activation_fn dot self inpt_dropout self self def accuracy self return accuracy mini batch return mean self y_out code convolutional networks class softmaxlayer object def __init__ self n_in n_out p_dropout self n_in n_in self n_out n_out self p_dropout p_dropout initialize weights biases self theano shared zeros n_in n_out dtype theano config floatx name borrow true self theano shared zeros n_out dtype theano config floatx name borrow true self params self self def set_inpt self inpt inpt_dropout mini_batch_size self inpt inpt reshape mini_batch_size self n_in self output softmax self p_dropout dot self inpt self self self y_out argmax self output axis self inpt_dropout dropout_layer inpt_dropout reshape mini_batch_size self n_in self p_dropout self output_dropout softmax dot self inpt_dropout self self def cost self net return log likelihood cost return mean log self output_dropout arange net shape net def accuracy self return accuracy mini batch return mean self y_out miscellanea def size data return size dataset data return data get_value borrow true shape def dropout_layer layer p_dropout srng shared_randomstreams randomstreams random randomstate randint mask srng binomial p_dropout size layer shape return layer cast mask theano config floatx 
[deep, learning, problems] present sgd method requires user manually choose number epochs train earlier book discussed automated way selecting number epochs train known early stopping modify network implement early stopping add network method return accuracy arbitrary data set modify sgd method allow learning rate function epoch number hint working problem may find useful see discussion earlier chapter described technique expanding training data apply ing small rotations skewing translation modify network incorporate techniques note unless tremendous amount memory practical explicitly generate entire expanded data set consider deep learning alternate approaches add ability load save networks network shortcoming current code provides diagnostic tools think diagnostics add would make easier understand extent network overfitting add used initialization procedure rectified linear units sigmoid tanh neurons argument initialization specific sigmoid function consider network made entirely rectified linear units including outputs show rescaling weights network constant factor simply rescales outputs factor number layers change final layer softmax think using sigmoid initialization procedure rectified linear units think better initialization procedure note open ended problem something simple self contained answer still considering problem help better understand networks containing rectified linear units analysis unstable gradient problem sigmoid neurons analysis change networks made rectified linear units think good way modifying network suffer unstable gradient problem note word good second part makes problem research problem actually easy think ways making modifications investigated enough depth know really good technique 
[deep, learning, recent, progress, image, recognition] year mnist introduced took weeks train state art workstation achieve accuracies substantially worse achieve using gpu less hour training thus mnist longer problem pushes limits available technique rather speed training means problem good teaching learning purposes meanwhile focus research moved modern work involves much challenging image recognition problems section briefly describe recent work image recognition using neural networks section different book book focused ideas likely lasting interest ideas backpropagation regularization convolutional networks tried avoid results fashionable write whose long term value unknown science results often ephemera fade little lasting impact given skeptic might say well surely recent progress image recognition example ephemera another two three years things moved surely results interest specialists want compete absolute frontier bother discussing skeptic right finer details recent papers gradually diminish perceived importance said past years seen extraordinary improvements using deep nets attack extremely difficult image recognition tasks imagine historian science writing computer vision year identify years probably years beyond time huge breakthroughs driven deep convolutional nets mean deep convolutional nets still used much less detailed ideas dropout rectified linear units mean important transition taking place right history ideas recent progress image recognition bit like watching discovery atom invention antibiotics invention discovery historic scale dig deep details worth getting idea exciting discoveries currently made lrmd paper let start group researchers stanford google refer paper lrmd last names first four authors lrmd used neural network classify images imagenet challenging image recognition problem imagenet data used included million full color images thousand categories images crawled open net classified workers amazon mechanical turk service imagenet respectively categories beading plane brown root rot fungus scalded milk common roundworm looking challenge encourage visit imagenet list distinguishes beading planes block planes chamfer planes dozen types plane amongst categories know cannot confidently distinguish tool types obviously much challenging image recognition task mnist lrmd network obtained respectable percent accuracy correctly classifying imagenet images may sound impressive huge improvement previous best result percent accuracy jump suggested neural networks might offer powerful approach challenging image recognition tasks imagenet ksh paper work lrmd followed paper krizhevsky sutskever hinton ksh trained tested deep convolutional neural network using restricted subset imagenet data subset used came popular machine learning competition imagenet large scale visual recognition challenge ilsvrc using competition dataset gave good way comparing approach leading techniques ilsvrc training set contained million imagenet images drawn categories validation test sets contained images respectively drawn categories one difficulty running ilsvrc competition many imagenet images contain multiple objects suppose image shows labrador retriever chasing soccer ball called correct imagenet classification image might labrador retriever quoc marc aurelio ranzato rajat monga matthieu devin kai chen greg corrado jeff dean andrew note detailed architecture network used paper differed many details deep convolutional networks studying broadly speaking however lrmd based many similar ideas dataset somewhat changed qualitatively however dataset extremely similar details imagenet available original imagenet paper jia deng wei dong richard socher jia kai fei fei alex krizhevsky ilya sutskever geoffrey hinton deep learning algorithm penalized labels image soccer ball ambiguity algorithm considered correct actual imagenet classification among classifications algorithm considered likely top criterion ksh deep convolutional network achieved accuracy percent vastly better next best contest entry achieved accuracy percent using restrictive metric getting label exactly right ksh network achieved accuracy percent worth briefly describing ksh network since inspired much subsequent work also shall see closely related networks trained earlier chapter albeit elaborate ksh used deep convolutional neural network trained two gpus used two gpus particular type gpu using nvidia geforce gtx enough chip memory store entire network split network two parts partitioned across two gpus ksh network layers hidden neurons first hidden layers convolu tional layers max pooling next layers fully connected layers output layer unit softmax layer corresponding image classes sketch network taken ksh details explained note many layers split parts corresponding gpus input layer contains neurons representing rgb values image recall mentioned earlier imagenet contains images varying resolution poses problem since neural network input layer usually fixed size ksh dealt rescaling image shorter side length cropped area center rescaled image finally ksh extracted random subimages horizontal reflections images random cropping way expanding training data thus reducing overfitting particularly helpful large network ksh images used inputs network cases cropped image still contains main object uncropped image moving hidden layers ksh network first hidden layer convolutional layer max pooling step uses local receptive fields size stride length pixels total feature maps feature maps split two groups first feature maps residing one gpu second feature maps residing gpu max pooling later layers done regions pooling regions allowed overlap pixels apart thanks ilya sutskever recent progress image recognition second hidden layer also convolutional layer max pooling step uses local receptive fields total feature maps split gpu note feature maps use input channels full output previous layer would usually case single feature map uses inputs gpu sense network departs convolutional architecture described earlier chapter though obviously basic idea still third fourth fifth hidden layers convolutional layers unlike previous layers involve max pooling respectives parameters feature maps local receptive fields input channels feature maps local receptive fields input channels feature maps local receptive fields input channels note third layer involves inter gpu communication depicted figure order feature maps use input channels sixth seventh hidden layers fully connected layers neurons layer output layer unit softmax layer ksh network takes advantage many techniques instead using sigmoid tanh activation functions ksh use rectified linear units sped training significantly ksh network roughly million learned parameters thus even large training set susceptible overfitting overcome expanded training set using random cropping strategy discussed also addressed overfitting using variant mini batch stochastic gradient descent overview many core ideas ksh paper omitted details look paper also look alex krizhevsky successors contains code implementing many ideas theano based implementation also code recognizably along similar lines developed chapter although use multiple gpus complicates things somewhat caffe neural nets framework also includes version ksh network see details ilsvrc competition since rapid progress continues made consider ilsvrc competition involved training set million images categories figure merit whether top predictions included correct category winning team based primarily used deep convolutional network layers neurons called network googlenet homage lenet googlenet achieved top accuracy percent giant improvement winner percent winner ksh percent good googlenet percent accuracy team researchers wrote survey paper ilsvrc one questions address well humans perform ilsvrc built system lets humans weiguang ding ruoyan wang fei mao graham taylor christian szegedy wei liu yangqing jia pierre sermanet scott reed dragomir anguelov dumitru erhan vincent vanhoucke andrew rabinovich olga russakovsky jia deng hao jonathan krause sanjeev satheesh sean zhiheng huang andrej karpathy aditya khosla michael bernstein alexander berg fei fei deep learning classify ilsvrc images one authors andrej karpathy explains informative lot trouble get humans googlenet performance task labeling images categories quickly turned extremely challenging even friends lab working ilsvrc classes first thought would put amazon mechanical turk thought could recruit paid undergrads organized labeling party intense labeling effort among expert labelers lab developed modified interface used googlenet predictions prune number categories still hard people kept missing categories getting ranges error rates end realized get anywhere competitively close googlenet efficient sat went painfully long training process subsequent careful annotation process labeling happened rate per minute decreased time images easily recognized images fine grained breeds dogs birds monkeys require multiple minutes concentrated effort became good identifying breeds dogs based sample images worked googlenet classification error turned error end turned approximately better words expert human working painstakingly great effort able narrowly beat deep neural network fact karpathy reports second human expert trained smaller sample images able attain percent top error rate significantly googlenet performance half errors due expert failing spot consider ground truth label option astonishing results indeed since work several teams reported systems whose top error rate actually better sometimes reported media systems better human vision results genuinely exciting many caveats make misleading think systems better human vision ilsvrc challenge many ways rather limited problem crawl open web necessarily representative images found applications course top criterion quite artificial still long way solving problem image recognition broadly computer vision still extremely encouraging see much progress made challenging problem years activity focused imagenet considerable amount activity using neural nets image recognition let briefly describe interesting recent results give flavour current work one encouraging practical set results comes team google applied deep convolutional networks problem recognizing street numbers google street view paper report detecting automatically transcribing nearly million street numbers accuracy similar human operator system fast system transcribed street view images street numbers france less ian goodfellow yaroslav bulatov julian ibarz sacha arnoud vinay shet recent progress image recognition hour say new dataset significantly increased geocoding quality google maps several countries especially ones already sources good geocoding make broader claim believe model solved optical character recognition short sequences characters many applications perhaps given impression parade encouraging results course interesting work reports fundamental things yet understand instance showed deep networks may suffer effectively blind spots consider lines images left imagenet image classified correctly network right slightly perturbed image perturbation middle classified incorrectly network authors found adversarial images every sample image special ones disturbing result paper used network based code ksh network type network increasingly widely used neural networks compute functions principle continuous results like suggest practice likely compute functions nearly discontinuous worse discontinuous ways violate intuition reasonable behavior concerning furthermore yet well understood causing discontinuity something loss function activation functions used architecture network something else yet know results quite bad sound although adversarial images common also unlikely practice paper notes existence adversarial negatives appears contradiction network ability achieve high generalization performance indeed network generalize well confused adversarial negatives indistinguishable regular examples explanation set adversarial negatives extremely low probability thus never rarely observed christian szegedy wojciech zaremba ilya sutskever joan bruna dumitru erhan ian goodfellow rob fergus deep learning test set yet dense much like rational numbers found near virtually every test case nonetheless distressing understand neural nets poorly kind result recent discovery course major benefit results stimulated much followup work example one recent shows given trained network possible generate images look human like white noise network classifies known category high degree confidence another demonstration long way understanding neural networks use image recognition despite results like overall picture encouraging seeing rapid progress extremely difficult benchmarks like imagenet also seeing rapid progress solution real world problems like recognizing street numbers streetview encouraging enough see improvements benchmarks even real world applications fundamental phenomena still understand poorly existence adversarial images fundamental problems still discovered never mind solved premature say near solving problem image recognition time problems exciting stimulus work 
[deep, learning, approaches, deep, neural, nets] book concentrated single problem classifying mnist digits juicy problem forced understand many powerful ideas stochastic gradient descent backpropagation convolutional nets regularization also narrow problem read neural networks literature run many ideas discussed recurrent neural networks boltzmann machines generative models transfer learning reinforcement learning neural networks vast field however many important ideas variations ideas already discussed understood little effort section provide glimpse yet unseen vistas discussion detailed comprehensive would greatly expand book rather impressionistic attempt evoke conceptual richness field relate riches already seen section provide links sources entrees learn course many links soon superseded may wish search recent literature point notwithstanding expect many underlying ideas lasting interest recurrent neural networks rnns feedforward nets using single input completely determines activations neurons remaining layers static picture everything network fixed frozen crystalline quality suppose allow elements network keep changing dynamic way instance behaviour hidden neurons might determined activations previous hidden layers also activations earlier times indeed neuron activation might determined part activation earlier time certainly happens feedforward network perhaps activations hidden output neurons determined current input network also earlier inputs anh nguyen jason yosinski jeff clune approaches deep neural nets neural networks kind time varying behaviour known recurrent neural networks rnns many different ways mathematically formalizing informal description recurrent nets given last paragraph get flavour mathematical models glancing wikipedia article rnns write page lists fewer different models mathematical details aside broad idea rnns neural networks notion dynamic change time surprisingly particularly useful analysing data processes change time data processes arise naturally problems speech natural language example one way rnns currently used connect neural networks closely traditional ways thinking algorithms ways thinking based concepts turing machines conventional programming languages also used rnns starting point develop called neural turing machine ntm universal computer whose entire structure trained using gradient descent trained ntm infer algorithms several simple problems sorting copying stands extremely simple toy models learning execute python pro gram print make network full fledged python interpreter clear much possible push ideas still results intriguing historically neural networks done well pattern recognition problems conventional algorithmic approaches trouble vice versa conventional algorith mic approaches good solving problems neural nets good one today implements web server database program using neural network great develop unified models integrate strengths neural networks traditional approaches algorithms rnns ideas inspired rnns may help rnns also used recent years attack many problems particularly useful speech recognition approaches based rnns example better language models help disambiguate utterances otherwise sound alike good language model example tell infinity beyond much likely two infinity beyond despite fact phrases sound identical rnns used set new records certain language benchmarks work incidentally part broader use deep neural nets types rnns speech recognition example approach based deep nets achieved said little rnns much work perhaps surprise learn many ideas used feedforward networks also used rnns particular train rnns using straightforward modifications gradient descent backpropagation many ideas used feedforward nets ranging regularization techniques convolutions activation cost functions used also useful recurrent nets many techniques developed book deep learning adapted use rnns long short term memory units lstms one challenge affecting rnns early models turned difficult train harder even deep feedforward networks reason unstable gradient problem discussed chapter recall usual manifestation problem gradient gets smaller smaller propagated back layers makes learning early layers extremely slow problem actually gets worse rnns since gradients propagated backward layers propagated backward time network runs long time make gradient extremely unstable hard learn fortunately possible incorporate idea known long short term memory units lstms rnns units introduced explicit purpose helping address unstable gradient problem lstms make much easier get good results training rnns many recent papers including many linked make use lstms related ideas deep belief nets generative models boltzmann machines modern interest deep learning began papers explaining train type neural network known deep belief network dbns influential several years since lessened popularity models feedforward networks recurrent neural nets become fashionable despite dbns several properties make interesting one reason dbns interesting example called generative model feedforward network specify input activations determine activations feature neurons later network generative model like dbn used similar way also possible specify values feature neurons run network backward generating values input activations concretely dbn trained images handwritten digits potentially care also used generate images look like handwritten digits words dbn would sense learning write generative model much like human brain read digits also write geoffrey hinton memorable phrase second reason dbns interesting unsupervised semi supervised learning instance trained image data dbns learn useful features understanding images even training images unlabelled ability unsupervised learning extremely interesting fundamental scientific reasons made work well enough practical applications given attractive features dbns lessened popularity models deep learning part reason models feedforward recurrent nets achieved many spectacular results breakthroughs image speech recognition benchmarks surprising quite right lots attention paid models unfortunate corollary however marketplace ideas often functions winner take fashion nearly attention going current fashion moment given area become extremely difficult people work momentarily unfashionable ideas even ideas obviously real long term interest personal opinion dbns generative models likely deserve attention currently receiving surprised dbns see geoffrey hinton simon osindero yee whye teh well related work geoffrey hinton ruslan salakhutdinov future neural networks related model one day surpass currently fashionable models introduction dbns see helpful primarily deep belief nets per contain much useful information restricted boltzmann machines key component dbns ideas else going neural networks deep learning well huge amount fascinating work active areas research include using neural networks course many areas many cases read book able begin following recent work although course need fill gaps presumed background knowledge let finish section mentioning particularly fun paper combines deep convolutional networks technique known reinforcement learning order learn idea use convolutional network simplify pixel data game screen turning simpler set features used decide action take left fire particularly interesting single network learned play seven different classic video games pretty well outperforming human experts three games sounds like stunt doubt paper well marketed title playing atari reinforcement learning looking past surface gloss consider system taking raw pixel data even know game rules data learning high quality decision making several different adversarial environments complex set rules pretty neat 
[deep, learning, future, neural, networks] intention driven user interfaces old joke impatient professor tells confused student listen say listen mean historically computers often like confused student dark users mean changing still remember surprise first time misspelled google search query google say mean corrected query offer corresponding search results google ceo larry page vision intention driven user interface vision instead responding users literal queries search use machine learning take vague user input discern precisely meant take action basis insights idea intention driven interfaces applied far broadly search next decades thousands companies build products use machine learning make user interfaces tolerate imprecision discerning acting user true intent already seeing early examples intention driven interfaces apple siri wolfram alpha ibm watson systems much products fail inspired user interface design hard expect many companies take powerful machine learning technology use build insipid user interfaces best machine learning world help user interface concept stinks residue products succeed time cause profound change relate computers long ago let say deep learning users took granted needed precision interactions computers indeed computer literacy great extent meant internalizing idea computers extremely literal single misplaced semi colon may completely change nature interaction computer next decades expect develop many successful intention driven user interfaces dramatically change expect interacting computers machine learning data science virtuous circle innovation course machine learning used build intention driven interfaces another notable application data science machine learning used find known unknowns hidden data already fashionable area much written say much want mention one consequence fashion often remarked long run possible biggest breakthrough machine learning single conceptual breakthrough rather biggest breakthrough machine learning research becomes profitable applications data science areas company invest dollar machine learning research get dollar cents back reasonably rapidly lot money end machine learning research put another way machine learning engine driving creation several major new markets areas growth technology result large teams people deep subject expertise access extraordinary resources propel machine learning forward creating markets opportunities virtuous circle innovation role neural networks deep learning talking broadly machine learning creator new opportunities technology specific role neural networks deep learning answer question helps look history back great deal excitement optimism neural networks especially backpropagation became widely known excitement faded machine learning baton passed techniques support vector machines today neural networks riding high setting sorts records defeating comers many problems say tomorrow new approach developed sweeps neural networks away perhaps progress neural networks stagnate nothing immediately arise take place reason much easier think broadly future machine learning neural networks specifically part problem understand neural networks poorly neural networks generalize well avoid overfitting well given large number parameters learn stochastic gradient descent works well well neural networks perform data sets scaled instance imagenet expanded factor would neural networks performance improve less machine learning techniques simple fundamental questions present understand answers questions poorly case difficult say role neural networks play future machine learning make one prediction believe deep learning stay ability learn hierarchies concepts building multiple layers abstraction seems fundamental making sense world mean tomorrow deep learners radically different today could see major changes constituent units used architectures learning algorithms changes may dramatic enough future neural networks longer think resulting systems neural networks still deep learning neural networks deep learning soon lead artificial intelligence book focused using neural nets specific tasks classifying images let broaden ambitions ask general purpose thinking computers neural networks deep learning help solve problem general artificial intelligence given rapid recent progress deep learning expect general time soon addressing questions comprehensively would take separate book instead let offer one observation based idea known organization designs system inevitably produce design whose structure copy organization communication structure example conway law suggests design boeing aircraft mirror extended organizational structure boeing contractors time designed simple specific example consider company building complex software application application dashboard supposed integrated machine learning algorithm person building dashboard better talking company machine learning expert conway law merely observation writ large upon first hearing conway law many people respond either well banal obvious wrong let start objection wrong instance objection consider question boeing accounting department show design janitorial department internal catering answer parts organization probably show explicitly anywhere understand conway law referring parts organization concerned explicitly design engineering objection conway law banal obvious may per haps true think organizations often act disregard conway law teams building new products often bloated legacy hires contrariwise lack person crucial expertise think products useless complicating features think products obvious major deficiencies terrible user interface problems classes often caused mismatch team needed produce good product team actually assembled conway law may obvious mean people routinely ignore conway law applies design engineering systems start pretty good understanding likely constituent parts build applied directly development artificial intelligence yet problem know constituent parts indeed even sure basic questions asking others words point problem science engineering imagine beginning design without knowing jet engines principles aerodynamics know kinds experts hire organization wernher von braun put basic research know version conway law applies problems science engineering gain insight question consider history medicine early days medicine domain practitioners like galen hippocrates studied entire body knowledge grew people forced specialize discovered deep learning many deep new think things like germ theory disease instance understanding antibodies work understanding heart lungs veins arteries form complete cardiovascular system deep insights formed basis subfields epidemiology immunology cluster inter linked fields around cardiovascular system structure knowledge shaped social structure medicine particularly striking case immunology realizing immune system exists system worthy study extremely non trivial insight entire field medicine specialists conferences even prizes organized around something invisible arguably distinct thing common pattern repeated many well established sciences medicine physics mathematics chemistry others fields start monolithic deep ideas early experts master ideas time passes monolithic character changes discover many deep new ideas many one person really master result social structure field organizes divides around ideas instead monolith fields within fields within fields complex recursive self referential social structure whose organization mirrors connections deepest insights structure knowledge shapes social organization science social shape turn constrains helps determine discover scientific analogue conway law got deep learning well since early days arguments one side hey going hard got super special weapon side countered super special weapon enough deep learning latest super special weapon heard used earlier versions argument used logic prolog expert systems whatever powerful technique day problem arguments give good way saying powerful given candidate super special weapon course spent chapter reviewing evidence deep learning solve extremely challenging problems certainly looks exciting promising also true systems like prolog eurisko expert systems day mere fact set ideas looks promising mean much tell deep learning truly different earlier ideas way measuring powerful promising set ideas conway law suggests rough heuristic proxy metric evaluate complexity social structure associated ideas two questions ask first powerful set ideas associated deep learning according metric social complexity second powerful theory need order able build general artificial intelligence first question look deep learning today exciting fast paced also relatively monolithic field deep ideas main conferences substantial overlap several conferences paper paper leveraging basic set ideas using stochastic gradient descent close variation optimize cost function fantastic ideas successful apologies overloading deep define deep ideas precisely loosely mean kind idea basis rich field enquiry backpropagation algorithm germ theory disease good examples interestingly often leading experts deep learning quite restrained see example yann lecun difference many earlier incarnations argument future neural networks yet see lots well developed subfields exploring sets deep ideas pushing deep learning many directions according metric social complexity deep learning forgive play words still rather shallow field still possible one person master deepest ideas field second question complex powerful set ideas needed obtain course answer question one knows sure appendix examine existing evidence question conclude even rather optimistically going take many many deep ideas build conway law suggests get point necessarily see emergence many interrelating disciplines complex surprising structure mirroring structure deepest insights yet see rich social structure use neural networks deep learning believe several decades least using deep learning develop general gone lot trouble construct argument tentative perhaps seems rather obvious indefinite conclusion doubt frustrate people crave certainty reading around online see many people loudly assert definite strongly held opinions often basis flimsy reasoning non existent evidence frank opinion early say old joke goes ask scientist far away discovery say years mean got idea like controlled fusion technologies years away plus years flipside definitely deep learning powerful technique whose limits yet found many wide open fundamental problems exciting creative opportunity deep learning 
[simple, algorithm, intelligence] book focused nuts bolts neural networks work used solve pattern recognition problems material many immediate practical applications course one reason interest neural nets hope one day far beyond basic pattern recognition problems perhaps approach based digital computers eventually used build thinking machines machines match surpass human intelligence notion far exceeds material discussed book anyone world knows fun speculate much debate whether even possible computers match human intelligence going engage question despite ongoing dispute believe serious doubt intelligent computer possible although may extremely complicated perhaps far beyond current technology current naysayers one day seem much like vitalists rather question explore whether simple set principles used explain intelligence particular concretely simple algorithm intelligence idea truly simple algorithm intelligence bold idea perhaps sounds optimistic true many people strong intuitive sense intelligence considerable irreducible complexity impressed amazing variety flexibility human thought conclude simple algorithm intelligence must impossible despite intuition think wise rush judgement history science filled instances phenomenon initially appeared extremely complex later explained simple powerful set ideas consider example early days astronomy humans known since ancient times menagerie objects sky sun moon planets comets stars objects behave different ways stars move stately simple algorithm intelligence regular way across sky example comets appear nowhere streak across sky disappear century foolish optimist could imagined objects motions could explained simple set principles century newton formulated theory universal gravitation explained motions also explained terrestrial phenomena tides behaviour earth bound projecticles century foolish optimist seems retrospect like pessimist asking little course science contains many examples consider myriad chemical substances making world beautifully explained mendeleev periodic table turn explained simple rules may obtained quantum mechanics puzzle much complexity diversity biological world whose origin turns lie principle evolution natural selection many examples suggest would wise rule simple explanation intelligence merely grounds brains currently best examples intelligence appears contrariwise despite optimistic examples also logically possible intelligence explained large number fundamentally distinct mechanisms case brains many mechanisms may perhaps evolved response many different selection pressures species evolutionary history point view correct intelligence involves considerable irreducible complexity simple algorithm intelligence possible two points view correct get insight question let ask closely related question whether simple explanation human brains work particular let look ways quantifying complexity brain first approach view brain connectomics raw wiring many neurons brain many glial cells many connections neurons probably heard numbers brain contains order billion neurons billion glial cells trillion connections neurons numbers staggering also intimidating need understand details connections mention neurons glial cells order understand brain works certainly going end simple algorithm intelligence second optimistic point view view brain molecular biology idea ask much genetic information needed describe brain architecture get handle question start considering genetic differences humans chimpanzees probably heard sound bite human beings percent chimpanzee saying sometimes varied popular variations also give number percent variations occur numbers originally estimated comparing samples human chimp genomes entire genomes however entire chimpanzee genome know human chimp dna differ roughly million dna base pairs total roughly billion dna base pairs genome right say human beings percent chimpanzee like percent chimpanzee appendix assume computer considered intelligent capabilities must match exceed human thinking ability regard question simple algorithm intelligence equivalent simple algorithm think along essentially lines human brain worth noting however may well forms intelligence subsume human thought nonetheless beyond interesting ways much information million base pairs base pair labelled one four possibilities letters genetic code bases adenine cytosine guanine thymine base pair described using two bits information enough information specify one four labels million base pairs equivalent million bits information genetic difference humans chimps course million bits accounts genetic differences humans chimps interested difference associated brain unfortunately one knows fraction total genetic difference needed explain difference brains let assume sake argument half million bits accounts brain differences total million bits million bits impressively large number let get sense large translating human terms particular much would equivalent amount english text information content english text bit per letter sounds low alphabet letters tremendous amount redundancy english text course might argue genomes redundant two bits per base pair overestimate ignore since worst means overestimating brain genetic complexity assumptions see genetic difference brains chimp brains equivalent million letters million english words times much king james bible lot information incomprehensibly large human scale maybe single human could ever understand written code group people could perhaps understand collectively appropriate specialization although lot information minuscule compared information required describe billion neurons billion glial cells trillion connections brains even use simple coarse description say floating point numbers characterize connection would require quadrillion bits means genetic description factor half billion less complex full connectome human brain learn genome cannot possibly contain detailed description neural connections rather must specify broad architecture basic principles underlying brain architecture principles seem enough guarantee humans grow intelligent course caveats growing children need healthy stimulating environment good nutrition achieve intellectual potential provided grow reasonable environment healthy human remarkable intelligence sense information genes contains essence think furthermore principles contained genetic information seem likely within ability collectively grasp numbers rough estimates possible million bits tremendous overestimate much compact set core principles underlying human thought maybe million bits fine tuning relatively minor details maybe overly conservative computed numbers obviously great true current purposes key point architecture brain complicated nearly complicated might think based number connections brain view brain molecular biology suggests humans ought one day able understand basic principles simple algorithm intelligence behind brain architecture last paragraphs ignored fact million bits merely quantifies genetic difference human chimp brains brain function due million bits chimps remarkable thinkers right maybe key intelligence lies mostly mental abilities genetic information chimps humans common correct human brains might minor upgrade chimpanzee brains least terms complexity underlying principles despite conventional human chauvinism unique capabilities inconceivable chimpanzee human genetic lines diverged million years ago blink evolutionary timescales however absence compelling argument sympathetic conventional human chauvinism guess interesting principles underlying human thought lie million bits part genome share chimpanzees adopting view brain molecular biology gave reduction roughly nine orders magnitude complexity description encouraging tell whether truly simple algorithm intelligence possible get reductions complexity point settle question whether simple algorithm intelligence possible unfortunately yet evidence strong enough decisively settle ques tion let describe available evidence caveat brief incomplete overview meant convey flavour recent work comprehensively survey known among evidence suggesting may simple algorithm intelligence experiment april journal nature team scientists led mriganka sur rewired brains newborn ferrets usually signal ferret eyes transmitted part brain known visual cortex ferrets scientists took signal eyes rerouted instead went auditory cortex brain region usually used hearing understand happened need know bit visual cortex visual cortex contains many orientation columns little slabs neurons responds visual stimuli particular direction think orientation columns tiny directional sensors someone shines bright light particular direction corresponding orientation column activated light moved different orientation column activated one important high level structures visual cortex orientation map charts orientation columns laid scientists found visual signal ferrets eyes rerouted auditory cortex auditory cortex changed orientation columns orientation map began emerge auditory cortex disorderly orientation map usually found visual cortex unmistakably similar furthermore scientists simple tests ferrets responded visual stimuli training respond differently lights flashed different directions tests suggested ferrets could still learn see least rudimentary fashion using auditory cortex astonishing result suggests common principles underlying different parts brain learn respond sensory data commonality pro vides least support idea set simple principles underlying intelligence however kid good ferrets vision experiments behavioural tests tested gross aspects vision course ask ferrets learned see experiments prove rewired auditory cortex giving ferrets high fidelity visual experience provide limited evidence favour idea common principles underlie different parts brain learn evidence idea simple algorithm intelligence evidence comes fields evolutionary psychology neuroanatomy since evolutionary psychologists discovered wide range human universals complex behaviours common humans across cultures upbringing human universals include incest taboo mother son use music dance well much complex linguistic structure use swear words taboo words pronouns even structures basic verb complementing results great deal evidence neuroanatomy shows many human behaviours controlled particular localized areas brain areas seem similar people taken together findings suggest many specialized behaviours hardwired particular parts brains people conclude results separate explanations must required many brain functions consequence irreducible complexity brain function complexity makes simple explanation brain operation perhaps simple algorithm intelligence impossible example one well known artificial intelligence researcher point view marvin minsky minsky developed society mind theory based idea human intelligence result large society individually simple different computational processes minsky calls agents minsky sums sees power point view magical trick makes intelligent trick trick power intelligence stems vast diversity single perfect principle response reviews book minsky elaborated motivation society mind giving argument similar stated based neuroanatomy evolutionary psychology know brain composed hundreds different regions nuclei significantly different architectural elements arrangements many involved demonstrably different aspects mental activities modern mass knowledge shows many phenomena traditionally described commonsense terms like intelligence understanding actually involve complex assemblies machinery minsky course person hold point view along lines merely giving example supporter line argument find argument interesting believe evidence compelling true brain composed large number different regions different functions therefore follow simple explanation brain function impossible perhaps architectural differences arise common underlying principles much motion comets planets sun stars arise single gravitational force neither minsky anyone else argued convincingly underlying principles edited william clancey stephen smoliar mark stefik mit press simple algorithm intelligence prejudice favour simple algorithm intelligence main reason like idea beyond inconclusive arguments optimistic idea comes research unjustified optimism often productive seemingly better justified pessimism optimist courage set try new things path discovery even discovered perhaps originally hoped pessimist may correct narrow sense discover less optimist point view stark contrast way usually judge ideas attempting figure whether right wrong sensible strategy dealing routine minutiae day day research wrong way judging big bold idea sort idea defines entire research program sometimes weak evidence whether idea correct meekly refuse follow idea instead spending time squinting available evidence trying discern true accept one yet knows instead work hard developing big bold idea understanding guarantee success thus understanding advances said optimistic form believe ever find simple algorithm intelligence concrete believe ever find really short python lisp whatever program let say anywhere thousand lines code implements artificial intelligence think ever find really easily described neural network implement artificial intelligence believe worth acting though could find program network path insight pursuing path may one day understand enough write longer program build sophisticated network exhibit intelligence worth acting though extremely simple algorithm intelligence exists eminent mathematician computer scientist jack schwartz invited debate artificial intelligence proponents artificial intelligence skeptics debate became unruly proponents making top claims amazing things round corner skeptics doubling pessimism claiming artificial intelligence outright impossible schwartz outsider debate remained silent discussion heated lull asked speak state thoughts issues discussion said well developments may lie one hundred nobel prizes away page seems perfect response key artificial intelligence simple powerful ideas search optimistically ideas going need many ideas still got long way 
