[introduction, machine, learning, why] drowning information starving knowledge john naisbitt entering era big data example trillion web pages one hour video uploaded youtube every second amounting years content every day genomes people length base pairs sequenced various labs walmart handles transactions per hour databases containing petabytes information cukier deluge data calls automated methods data analysis machine learning provides particular deﬁne machine learning set methods automatically detect patterns data use uncovered patterns predict future data perform kinds decision making uncertainty planning collect data books adopts view best way solve problems use tools probability theory probability theory applied problem involving uncertainty machine learning uncertainty comes many forms best prediction future given past data best model explain data measurement perform next etc probabilistic approach machine learning closely related ﬁeld statistics differs slightly terms emphasis terminology describe wide variety probabilistic models suitable wide variety data tasks also describe wide variety algorithms learning using models goal develop cook book hoc techiques instead present uniﬁed view ﬁeld lens probabilistic modeling inference although pay attention computational efficiency details scale methods truly massive datasets better described books rajaraman ullman bekkerman http googleblog blogspot com knew web big html source http www youtube com press_statistics rob tibshirani statistician stanford university created amusing comparison machine learning statistics available http www stat stanford edu tibs stata glossary pdf chapter introduction noted however even one apparently massive data set effective number data points certain cases interest might quite small fact data across variety domains exhibits property known long tail means things words common things quite rare see section details example google searches day never seen means core statistical issues discuss book concerning generalizing relatively small samples sizes still relevant even big data era 
[introduction, machine, learning, why, types, machine, learning] machine learning usually divided two main types predictive supervised learning approach goal learn mapping inputs outputs given labeled set input output pairs called training set number training examples simplest setting training input dimensional vector numbers rep resenting say height weight person called features attributes covariates general however could complex structured object image sentence email message time series molecular shape graph etc similarly form output response variable principle anything methods assume categorical nominal variable ﬁnite set male female real valued scalar income level categorical problem known classiﬁcation pattern recognition real valued problem known regression another variant known ordinal regression occurs label space natural ordering grades second main type machine learning descriptive unsupervised learning approach given inputs goal ﬁnd interesting patterns data sometimes called knowledge discovery much less well deﬁned problem since told kinds patterns look obvious error metric use unlike supervised learning compare prediction given observed value third type machine learning known reinforcement learning somewhat less commonly used useful learning act behave given occasional reward punishment signals example consider baby learns walk unfortunately beyond scope book although discuss decision theory section basis see kaelbling sutton barto russell norvig szepesvari wiering van otterlo information http certifiedknowledge org blog search queries becoming even unique statistic google supervised learning figure left labeled training examples colored shapes along unlabeled test cases right representing training data design matrix row represents feature vector last column label based ﬁgure leslie kaelbling 
[introduction, supervised, learning] begin investigation machine learning discussing supervised learning form widely used practice 
[introduction, supervised, learning, classiﬁcation] section discuss classiﬁcation goal learn mapping inputs outputs number classes called binary classiﬁcation case often assume called multiclass classiﬁcation class labels mutually exclusive somebody may classiﬁed tall strong call multi label classiﬁcation best viewed predicting multiple related binary class labels called multiple output model use term classiﬁcation mean multiclass classiﬁcation single output unless state otherwise one way formalize problem function approximation assume unknown function goal learning estimate function given labeled training set make predictions using use hat symbol denote estimate main goal make predictions novel inputs meaning ones seen called generalization since predicting response training set easy look answer example simple toy example classiﬁcation consider problem illustrated figure two classes object correspond labels inputs colored shapes described set features attributes stored design matrix shown figure input features discrete continuous combination two addition inputs vector training labels figure test cases blue crescent yellow circle blue arrow none seen thus required generalize beyond training set chapter introduction reasonable guess blue crescent since blue shapes labeled training set yellow circle harder classify since yellow things labeled labeled circles labeled consequently clear right label case yellow circle similarly correct label blue arrow unclear need probabilistic predictions handle ambiguous cases yellow circle desirable return probability reader assumed already familiarity basic concepts probability please consult chapter refresher necessary denote probability distribution possible labels given input vector training set general represents vector length two classes sufficient return single number since notation make explicit probability conditional test input well training set putting terms right hand side conditioning bar also implicitly conditioning form model use make predictions choosing different models make assumption explicit writing denotes model however model clear context drop notation brevity given probabilistic output always compute best guess true label using argmax corresponds probable class label called mode distribution also known map estimate map stands maximum posteriori using probable label makes intuitive sense give formal justiﬁcation procedure section consider case yellow circle far case conﬁdent answer might better say know instead returning answer really trust particularly important domains medicine ﬁnance may risk averse explain section another application important assess risk playing game shows jeopardy game contestants solve various word puzzles answer variety trivia questions answer incorrectly lose money ibm unveiled computer system called watson beat top human jeopardy champion watson uses variety interesting techniques ferrucci pertinent one present purposes contains module estimates conﬁdent answer system chooses buzz answer sufficiently conﬁdent correct similarly google system known smartass selection system predicts probability click based search history user speciﬁc features metz probability known click rate ctr used maximize expected proﬁt discuss basic principles behind systems smartass later book supervised learning words documents figure subset size newsgroups data show rows clarity row document represented bag words bit vector column word red lines separate classes descending order comp rec sci talk titles usenet groups see subsets words whose presence absence indicative class data available http nyu edu roweis data html figure generated newsgroupsvisualize real world applications classiﬁcation probably widely used form machine learning used solve many interesting often difficult real world problems already mentioned important applciations give examples document classiﬁcation email spam ﬁltering document classiﬁcation goal classify document web page email message one classes compute represen tation text special case email spam ﬁltering classes spam ham classiﬁers assume input vector ﬁxed size common way represent variable length documents feature vector format use bag words representation explained detail section basic idea deﬁne iff word occurs document apply transformation every document data set get binary document word occurrence matrix see figure example essentially document classiﬁcation problem reduced one looks subtle changes pattern bits example may notice spam messages high probability containing words buy cheap viagra etc exercise exercise get hands experience applying various classiﬁcation techniques spam ﬁltering problem chapter introduction figure three types iris ﬂowers setosa versicolor virginica source http www statlab heidelberg data iris used kind permission dennis kramb signa sepal length sepal length sepal width petal length petal width sepal width petal length petal width figure visualization iris data pairwise scatter plot diagonal plots marginal histograms features diagonals contain scatterplots possible pairs features red circle setosa green diamond versicolor blue star virginica figure generated fisheririsdemo classifying ﬂowers figure gives another example classiﬁcation due statistician ronald fisher goal learn distinguish three different kinds iris ﬂower called setosa versicolor virginica fortunately rather working directly images botanist already extracted useful features characteristics sepal length width petal length width feature extraction important difficult task machine learning methods use features chosen human later discuss methods learn good features data make scatter plot iris data figure see easy distinguish setosas red circles two classes checking petal length supervised learning true class true class true class true class true class true class true class true class true class true class true class true class true class true class true class true class true class true class figure first test mnist gray scale images features permuted randomly classiﬁcation performance identical versions data assuming training data permuted identical way figure generated shuffleddigitsdemo width threshold however distinguishing versicolor virginica slightly harder decision need based least two features always good idea perform exploratory data analysis plotting data applying machine learning method image classiﬁcation handwriting recognition consider harder problem classifying images directly human pre processed data might want classify image whole indoors outdoors scene horizontal vertical photo contain dog called image classiﬁcation special case images consist isolated handwritten letters digits example postal zip code letter use classiﬁcation perform handwriting recognition standard dataset used area known mnist stands modiﬁed national institute standards term modiﬁed used images preprocessed ensure digits mostly center image dataset contains training images test images digits written various people images size grayscale values range see figure example images many generic classiﬁcation methods ignore structure input features spatial layout consequently also easily handle data looks like figure data except randomly permuted order features verify exercise ﬂexibility blessing since methods general purpose curse since methods ignore obviously useful source information discuss methods exploiting structure input features later book available http yann lecun com exdb mnist chapter introduction figure example face detection input image murphy family photo taken august used kind permission bernard diedrich sherwood studios output classiﬁer detected faces different poses produced using online demo http demo pittpatt com classiﬁer trained manually labeled images faces non faces applied dense set overlapping patches test image patches whose probability containing face sufficiently high returned used kind permission pittpatt com face detection recognition harder problem ﬁnd objects within image called object detection object localization important special case face detection one approach problem divide image many small overlapping patches different locations scales orientations classify patch based whether contains face like texture called sliding window detector system returns locations probability face sufficiently high see figure example face detection systems built modern digital cameras locations detected faces used determine center auto focus another application automatically blurring faces google streetview system found faces one proceed perform face recognition means estimating identity person see figure case number class labels might large also features one use likely different face detection problem recognition subtle differences faces hairstyle may important determining identity detection important invariant details focus differences faces non faces information visual object detection see szeliski 
[introduction, supervised, learning, regression] regression like classiﬁcation except response variable continuous figure shows simple example single real valued input single real valued response consider ﬁtting two models data straight line quadratic function explain models various extensions basic problem arise high dimensional inputs outliers non smooth responses etc discuss ways handle problems later book unsupervised learning degree degree figure linear regression data data polynomial regression degree figure generated linregpolyvsdegree examples real world regression problems predict tomorrow stock market price given current market conditions possible side information predict age viewer watching given video youtube predict location space robot arm end effector given control signals torques sent various motors predict amount prostate speciﬁc antigen psa body function number different clinical measurements predict temperature location inside building using weather data time door sensors etc 
[introduction, unsupervised, learning] consider unsupervised learning given output data without inputs goal discover interesting structure data sometimes called knowledge discovery unlike supervised learning told desired output input instead formalize task one density estimation want build models form two differences supervised case first written instead supervised learning conditional density estimation whereas unsupervised learning unconditional density estimation second vector features need create multivariate probability models contrast supervised learning usually single variable trying predict means supervised learning problems use univariate probability models input dependent parameters signiﬁcantly simpliﬁes problem discuss multi output classiﬁcation chapter see also involves multivariate probability models unsupervised learning arguably typical human animal learning also widely applicable supervised learning since require human expert chapter introduction height weight height weight figure height weight people possible clustering using clusters figure generated kmeansheightweight manually label data labeled data expensive acquire also contains relatively little information certainly enough reliably estimate parameters complex models geoff hinton famous professor university toronto said learning see nobody telling right answers look every often mother says dog little information lucky got bits information even one bit per second way brain visual system neural connections live seconds use learning one bit per second need like bits per second one place get much information input geoffrey hinton quoted gorder describe canonical examples unsupervised learning 
[introduction, unsupervised, learning, discovering, clusters] canonical example unsupervised learning consider problem clustering data groups example figure plots data representing height weight group people seems might various clusters subgroups although clear many let denote number clusters ﬁrst goal estimate distribution number clusters tells subpopulations within data simplicity often approximate distribution mode arg max supervised case told two classes male female unsupervised case free choose many clusters like picking model right complexity called model selection discussed detail second goal estimate cluster point belongs let represent cluster data point assigned example hidden advent crowd sourcing web sites mechanical turk https www mturk com mturk welcome outsource data processing tasks humans world reduced cost labeling data nevertheless amount unlabeled data still orders magnitude larger amount labeled data unsupervised learning figure set points live linear subspace embedded solid red line ﬁrst principal component direction dotted black line second direction representation data figure generated pcademod latent variable since never observed training set infer cluster data point belongs computing argmax illustrated figure use different colors indicate assignments assuming book focus model based clustering means probabilistic model data rather running hoc algorithm advantages model based approach one compare different kinds models objective way terms likelihood assign data combine together larger systems etc real world applications clustering astronomy autoclass system cheeseman discovered new type star based clustering astrophysical measurements commerce common cluster users groups based purchasing web surﬁng behavior send customized targeted advertising group see berkhin biology common cluster ﬂow cytometry data groups discover different sub populations cells see 
[introduction, unsupervised, learning, discovering, latent, factors] dealing high dimensional data often useful reduce dimensionality projecting data lower dimensional subspace captures essence data called dimensionality reduction simple example shown figure project data plane approximation quite good since points lie close subspace reducing would involve projecting points onto red line figure would rather poor approximation make notion precise chapter motivation behind technique although data may appear high dimensional may small number degrees variability corresponding latent factors example modeling appearance face images may underlying latent factors describe variability lighting pose identity etc illustrated figure chapter introduction figure randomly chosen pixel images olivetti face database mean ﬁrst three principal component basis vectors eigenfaces figure generated pcaimagedemo used input statistical models low dimensional representations often result better predictive accuracy focus essence object ﬁltering inessential features also low dimensional representations useful enabling fast nearest neighbor searches two dimensional projections useful visualizing high dimensional data common approach dimensionality reduction called principal components analysis pca thought unsupervised version multi output linear regression observe high dimensional response low dimensional cause thus model form invert arrow infer latent low dimensional observed high dimensional see section details dimensionality reduction pca particular applied many different areas examples include following biology common use pca interpret gene microarray data account fact measurement usually result many genes correlated behavior fact belong different biological pathways natural language processing common use variant pca called latent semantic analysis document retrieval see section signal processing acoustic neural signals common use ica variant pca separate signals different sources see section computer graphics common project motion capture data low dimensional space use create animations see section one way tackle problems unsupervised learning figure sparse undirected gaussian graphical model learned using graphical lasso section applied ﬂow cytometry data sachs measures phosphorylation status proteins figure generated ggmlassodemo 
[introduction, unsupervised, learning, discovering, graph, structure] sometimes measure set correlated variables would like discover ones correlated others represented graph nodes represent variables edges represent direct dependence variables make precise chapter discuss graphical models learn graph structure data compute argmax unsupervised learning general two main applications learning sparse graphs discover new knowledge get better joint probability density estimators give somes example much motivation learning sparse graphical models comes systems biology community example suppose measure phosphorylation status proteins cell sachs figure gives example graph structure learned data using methods discussed section another example smith showed one recover neural wiring diagram certain kind bird time series eeg data recovered structure closely matched known functional connectivity part bird brain cases interested interpreting graph structure want use model correlations make predictions one example ﬁnancial portfolio management accurate models covariance large numbers different stocks important carvalho west show learning sparse graph using basis trading strategy possible outperform make money methods exploit sparse graphs another example predicting traffic jams freeway horvitz describe deployed system called jambayes predicting traffic ﬂow seattle area predictions made using graphical model whose structure learned data chapter introduction figure noisy image occluder estimate underlying pixel intensities based pairwise mrf model source figure felzenszwalb huttenlocher used kind permission pedro felzenszwalb 
[introduction, unsupervised, learning, matrix, completion] sometimes missing data variables whose values unknown example might conducted survey people might answered certain questions might various sensors fail corresponding design matrix holes missing entries often represented nan stands number goal imputation infer plausible values missing entries sometimes called matrix completion give example applications image inpainting interesting example imputation like task known image inpainting goal ﬁll holes due scratches occlusions image realistic texture illustrated figure denoise image well impute pixels hidden behind occlusion tackled building joint probability model pixels given set clean images inferring unknown variables pixels given known variables pixels somewhat like masket basket analysis except data real valued spatially structured kinds probability models use quite different see sections possible choices collaborative ﬁltering another interesting example imputation like task known collaborative ﬁltering common example concerns predicting movies people want watch based people rated movies already seen key idea prediction based features movie user although could merely ratings matrix precisely matrix rating unsupervised learning xxvhuv prylhv figure example movie rating data training data red test data denoted empty cells unknown say integer dislike like user movie note entries missing unknown since users rated movies hence observe tiny subset matrix want predict different subset particular given user might want predict unrated movies likely want watch order encourage research area dvd rental company netﬂix created com petition launched usd prize see http netflixprize com particular provided large matrix ratings scale movies created users full matrix would entries entries observed matrix extremely sparse subset used training rest testing shown figure goal competition predict accurately netﬂix existing system september prize awarded team researchers known bellkor pragmatic chaos section discusses methodology details teams methods found http www netflixprize com community viewtopic php market basket analysis commercial data mining much interest task called market basket analysis data consists typically large sparse binary matrix column represents item product row represents transaction set item purchased transaction many items purchased together bread butter correlations amongst bits given new partially observed bit vector representing subset items consumer bought goal predict bits likely turn representing items consumer might likely buy unlike collaborative ﬁltering often assume missing data training data since know past shopping behavior customer task arises domains besides modeling purchasing patterns example similar techniques used model dependencies ﬁles complex software systems case task predict given subset ﬁles changed ones need updated ensure consistency see common solve tasks using frequent itemset mining create association rules see hastie sec details alternatively adopt probabilistic approach joint density model bit vectors see chapter introduction figure illustration nearest neighbors classiﬁer nearest neighbors test point labels predict nearest neighbors test point labels predict illustration voronoi tesselation induced based figure duda figure generated knnvoronoi models often better predictive acccuracy association rules although may less interpretible typical difference data mining machine learning data mining emphasis interpretable models whereas machine learning emphasis accurate models 
[introduction, basic, concepts, machine, learning] section provide introduction key ideas machine learning expand concepts later book introduce brieﬂy give ﬂavor things come 
[introduction, basic, concepts, machine, learning, parametric, non-parametric, models] book focussing probabilistic models form depending whether interested supervised unsupervised learning respectively many ways deﬁne models important distinction model ﬁxed number parameters number parameters grow amount training data former called parametric model latter called non parametric model parametric models advantage often faster use disadvantage making stronger assumptions nature data distributions non parametric models ﬂexible often computationally intractable large datasets give examples kinds models sections focus supervised learning simplicity although much discussion also applies unsupervised learning 
[introduction, basic, concepts, machine, learning, -nearest, neighbors] simple example non parametric classiﬁer nearest neighbor knn classiﬁer simply looks points training set nearest test input basic concepts machine learning train data data predicted label figure synthetic class training data probability class knn probability class map estimate class label figure generated knnclassifydemo counts many members class set returns empirical fraction estimate illustrated figure formally indices nearest points indicator function deﬁned follows true false method example memory based learning instance based learning derived probabilistic framework explained section common chapter introduction fraction data neighborhood edge length cube figure illustration curse dimensionality embed small cube side inside larger unit cube plot edge length cube needed cover given volume unit cube function number dimensions based figure hastie figure generated cursedimensionality distance metric use euclidean distance limits applicability technique data real valued although metrics used figure gives example method action input two dimensional three classes discuss effect panel plots training data panel plots evaluated grid points panel plots need plot since probabilities sum one panel plots map estimate argmax knn classiﬁer induces voronoi tessellation points see figure partition space associates region point way points closer point within cell predicted label label corresponding training point 
[introduction, basic, concepts, machine, learning, curse, dimensionality] knn classiﬁer simple work quite well provided given good distance metric enough labeled training data fact shown knn classiﬁer come within factor best possible performance cover hart however main problem knn classiﬁers work well high dimensional inputs poor performance high dimensional settings due curse dimensionality explain curse give examples hastie consider applying knn classiﬁer data inputs uniformly distributed dimensional unit cube suppose estimate density class labels around test point growing hyper cube around contains desired fraction data points expected edge length cube want base estimate basic concepts machine learning pdf figure gaussian pdf mean variance figure generated gaussplotdemo visualization conditional density model density falls exponentially fast move away regression line figure generated linregwedgedemo data need extend cube along dimension around even use data ﬁnd see figure since entire range data along dimension see method longer local despite name nearest neighbor trouble looking neighbors far away may good predictors behavior input output function given point 
[introduction, basic, concepts, machine, learning, parametric, models, classiﬁcation, regression] main way combat curse dimensionality make assumptions nature data distribution either supervised problem unsupervised problem assumptions known inductive bias often embodied form parametric model statistical model ﬁxed number parameters brieﬂy describe two widely used examples revisit models much greater depth later book 
[introduction, basic, concepts, machine, learning, linear, regression] one widely used models regression known linear regression asserts response linear function inputs written follows represents inner scalar product input vector model weight vector residual error linear predictions true response statistics common denote regression weights chapter introduction degree degree figure polynomial degrees least squares data points figure generated linregpolyvsdegree often assume gaussian normal distribution denote mean variance see chapter details plot distribution get well known bell curve shown figure make connection linear regression gaussians explicit rewrite model following form makes clear model conditional probability density simplest case assume linear function noise ﬁxed case parameters model example suppose input dimensional represent expected response follows intercept bias term slope deﬁned vector prepending constant term input vector common notational trick allows combine intercept term terms model positive means expect output increase input increases illustrated figure conventional plot mean response shown figure linear regression made model non linear relationships replacing non linear function inputs use known basis function expansion example figure illustrates case known polynomial regression consider kinds basis functions later book fact many popular machine learning methods support vector machines neural networks classiﬁcation regression trees etc seen different ways estimating basis functions data discuss chapters carl friedrich gauss german mathematician physicist basic concepts machine learning figure sigmoid logistic function sigm sigm sigm figure generated sigmoidplot logistic regression sat scores solid black dots data open red circles predicted probabilities green crosses denote two students sat score hence input representation different training labels one student passed failed hence data perfectly separable using sat feature figure generated logregsatdemo 
[introduction, basic, concepts, machine, learning, logistic, regression] generalize linear regression binary classiﬁcation setting making two changes first replace gaussian distribution bernoulli distribution appropriate case response binary use ber second compute linear combination inputs pass function ensures deﬁning sigm sigm refers sigmoid function also known logistic logit function deﬁned sigm exp term sigmoid means shaped see figure plot also known squashing function since maps whole real line necessary output interpreted probability putting two steps together get ber sigm called logistic regression due similarity linear regression although form classiﬁcation regression daniel bernoulli dutch swiss mathematician physicist chapter introduction simple example logistic regression shown figure plot sigm sat score student whether passed failed class solid black dots show training data red circles plot parameters estimated training data discuss compute estimates section threshold output probability induce decision rule form looking figure see sigm imagine drawing vertical line known decision boundary everything left line classiﬁed everything right line classiﬁed notice decision rule non zero error rate even training set data linearly separable straight line draw separate create models non linear decision boundaries using basis function expansion non linear regression see many examples later book 
[introduction, basic, concepts, machine, learning, overﬁtting] highly ﬂexible models need careful overﬁt data avoid trying model every minor variation input since likely noise true signal illustrated figure see using high degree polynomial results curve wiggly unlikely true function extreme oscillations thus using model might result accurate predictions future outputs another example consider knn classiﬁer value large effect behavior model method makes errors training set since return labels original training points resulting prediction surface wiggly see figure therefore method may work well predicting future data figure see using results smoother prediction surface averaging larger neighborhood increases predictions becomes smoother limit end predicting majority label whole data set discuss pick right value 
[introduction, basic, concepts, machine, learning, model, selection] variety models different complexity linear logistic regression models different degree polynomials knn classiﬁers different values pick right one natural approach compute misclassiﬁcation rate sat stands scholastic aptitude test standardized test college admissions used united states data example johnson albert basic concepts machine learning predicted label predicted label figure prediction surface knn data figure figure generated knnclassifydemo training set method deﬁned follows err classiﬁer figure plot error rate knn classiﬁer dotted blue line see increasing increases error rate training set smoothing said get minimal error training set using since model memorizing data however care generalization error expected value misclassiﬁcation rate averaged future data see section details approximated computing misclassiﬁcation rate large independent test set used model training plot test error figure solid red upper curve see shaped curve complex models small method overﬁts simple models big method underﬁts therefore obvious way pick pick value minimum error test set example value ﬁne unfortunately training model access test set assumption cannot use test set pick model right complexity however create test set partitioning training set two part used training model second part called validation set used selecting model complexity models training set evaluate performance validation set pick best picked best reﬁt available data separate test set evaluate performance order estimate accuracy method discuss detail section often use data training set validation set number training cases small technique runs problems model academic settings usually access test set use model ﬁtting model selection otherwise get unrealistically optimistic estimate performance method one golden rules machine learning research chapter introduction misclassification rate train test figure misclassiﬁcation rate nearest neighbor classiﬁer left small model complex hence overﬁt right large model simple underﬁt dotted blue line training set size solid red line test set size schematic fold cross validation figure generated knnclassifydemo enough data train enough data make reliable estimate future performance simple popular solution use cross validation idea simple split training data folds fold train folds test round robin fashion sketched figure compute error averaged folds use proxy test error note point gets predicted although used training times common use called fold set get method called leave one cross validation loocv since fold train data cases except test exercise asks compute fold estimate test error compare empirical test error figure choosing knn classiﬁer special case general problem known model selection choose models different degrees ﬂexibility cross validation widely used solving problems although discuss approaches later book 
[introduction, basic, concepts, machine, learning, free, lunch, theorem] models wrong models useful george box box draper much machine learning concerned devising different models different algorithms use methods cross validation empirically choose best method particular problem however universally best model sometimes called free lunch theorem wolpert reason set assumptions works well one domain may work poorly another george box retired statistics professor university wisconsin basic concepts machine learning consequence free lunch theorem need develop many different types models cover wide variety data occurs real world model may many different algorithms use train model make different speed accuracy complexity tradeoffs combination data models algorithms studying subsequent chapters 
[introduction, exercises] exercise knn classiﬁer shuffled mnist data run mnistnndemo verify misclassiﬁcation rate ﬁrst test cases mnist classiﬁer run test cases error rate modify code ﬁrst randomly permute features columns training test design matrices shuffleddigitsdemo apply classiﬁer verify error rate changed exercise approximate knn classiﬁers use matlab code http people ubc mariusm index php flann flann per form approximate nearest neighbor search combine mnistnndemo classify mnist data set much speedup get drop accuracy exercise knn use knnclassifydemo plot estimate misclassiﬁcation rate test set compare figure discuss similarities differences test error rate 
[introduction, machine, learning, why] drowning information starving knowledge john naisbitt entering era big data example trillion web pages one hour video uploaded youtube every second amounting years content every day genomes people length base pairs sequenced various labs walmart handles transactions per hour databases containing petabytes information cukier deluge data calls automated methods data analysis machine learning provides particular deﬁne machine learning set methods automatically detect patterns data use uncovered patterns predict future data perform kinds decision making uncertainty planning collect data books adopts view best way solve problems use tools probability theory probability theory applied problem involving uncertainty machine learning uncertainty comes many forms best prediction future given past data best model explain data measurement perform next etc probabilistic approach machine learning closely related ﬁeld statistics differs slightly terms emphasis terminology describe wide variety probabilistic models suitable wide variety data tasks also describe wide variety algorithms learning using models goal develop cook book hoc techiques instead present uniﬁed view ﬁeld lens probabilistic modeling inference although pay attention computational efficiency details scale methods truly massive datasets better described books rajaraman ullman bekkerman http googleblog blogspot com knew web big html source http www youtube com press_statistics rob tibshirani statistician stanford university created amusing comparison machine learning statistics available http www stat stanford edu tibs stata glossary pdf chapter introduction noted however even one apparently massive data set effective number data points certain cases interest might quite small fact data across variety domains exhibits property known long tail means things words common things quite rare see section details example google searches day never seen means core statistical issues discuss book concerning generalizing relatively small samples sizes still relevant even big data era 
[introduction, machine, learning, why, types, machine, learning] machine learning usually divided two main types predictive supervised learning approach goal learn mapping inputs outputs given labeled set input output pairs called training set number training examples simplest setting training input dimensional vector numbers rep resenting say height weight person called features attributes covariates general however could complex structured object image sentence email message time series molecular shape graph etc similarly form output response variable principle anything methods assume categorical nominal variable ﬁnite set male female real valued scalar income level categorical problem known classiﬁcation pattern recognition real valued problem known regression another variant known ordinal regression occurs label space natural ordering grades second main type machine learning descriptive unsupervised learning approach given inputs goal ﬁnd interesting patterns data sometimes called knowledge discovery much less well deﬁned problem since told kinds patterns look obvious error metric use unlike supervised learning compare prediction given observed value third type machine learning known reinforcement learning somewhat less commonly used useful learning act behave given occasional reward punishment signals example consider baby learns walk unfortunately beyond scope book although discuss decision theory section basis see kaelbling sutton barto russell norvig szepesvari wiering van otterlo information http certifiedknowledge org blog search queries becoming even unique statistic google supervised learning figure left labeled training examples colored shapes along unlabeled test cases right representing training data design matrix row represents feature vector last column label based ﬁgure leslie kaelbling 
[introduction, supervised, learning] begin investigation machine learning discussing supervised learning form widely used practice 
[introduction, supervised, learning, classiﬁcation] section discuss classiﬁcation goal learn mapping inputs outputs number classes called binary classiﬁcation case often assume called multiclass classiﬁcation class labels mutually exclusive somebody may classiﬁed tall strong call multi label classiﬁcation best viewed predicting multiple related binary class labels called multiple output model use term classiﬁcation mean multiclass classiﬁcation single output unless state otherwise one way formalize problem function approximation assume unknown function goal learning estimate function given labeled training set make predictions using use hat symbol denote estimate main goal make predictions novel inputs meaning ones seen called generalization since predicting response training set easy look answer example simple toy example classiﬁcation consider problem illustrated figure two classes object correspond labels inputs colored shapes described set features attributes stored design matrix shown figure input features discrete continuous combination two addition inputs vector training labels figure test cases blue crescent yellow circle blue arrow none seen thus required generalize beyond training set chapter introduction reasonable guess blue crescent since blue shapes labeled training set yellow circle harder classify since yellow things labeled labeled circles labeled consequently clear right label case yellow circle similarly correct label blue arrow unclear need probabilistic predictions handle ambiguous cases yellow circle desirable return probability reader assumed already familiarity basic concepts probability please consult chapter refresher necessary denote probability distribution possible labels given input vector training set general represents vector length two classes sufficient return single number since notation make explicit probability conditional test input well training set putting terms right hand side conditioning bar also implicitly conditioning form model use make predictions choosing different models make assumption explicit writing denotes model however model clear context drop notation brevity given probabilistic output always compute best guess true label using argmax corresponds probable class label called mode distribution also known map estimate map stands maximum posteriori using probable label makes intuitive sense give formal justiﬁcation procedure section consider case yellow circle far case conﬁdent answer might better say know instead returning answer really trust particularly important domains medicine ﬁnance may risk averse explain section another application important assess risk playing game shows jeopardy game contestants solve various word puzzles answer variety trivia questions answer incorrectly lose money ibm unveiled computer system called watson beat top human jeopardy champion watson uses variety interesting techniques ferrucci pertinent one present purposes contains module estimates conﬁdent answer system chooses buzz answer sufficiently conﬁdent correct similarly google system known smartass selection system predicts probability click based search history user speciﬁc features metz probability known click rate ctr used maximize expected proﬁt discuss basic principles behind systems smartass later book supervised learning words documents figure subset size newsgroups data show rows clarity row document represented bag words bit vector column word red lines separate classes descending order comp rec sci talk titles usenet groups see subsets words whose presence absence indicative class data available http nyu edu roweis data html figure generated newsgroupsvisualize real world applications classiﬁcation probably widely used form machine learning used solve many interesting often difficult real world problems already mentioned important applciations give examples document classiﬁcation email spam ﬁltering document classiﬁcation goal classify document web page email message one classes compute represen tation text special case email spam ﬁltering classes spam ham classiﬁers assume input vector ﬁxed size common way represent variable length documents feature vector format use bag words representation explained detail section basic idea deﬁne iff word occurs document apply transformation every document data set get binary document word occurrence matrix see figure example essentially document classiﬁcation problem reduced one looks subtle changes pattern bits example may notice spam messages high probability containing words buy cheap viagra etc exercise exercise get hands experience applying various classiﬁcation techniques spam ﬁltering problem chapter introduction figure three types iris ﬂowers setosa versicolor virginica source http www statlab heidelberg data iris used kind permission dennis kramb signa sepal length sepal length sepal width petal length petal width sepal width petal length petal width figure visualization iris data pairwise scatter plot diagonal plots marginal histograms features diagonals contain scatterplots possible pairs features red circle setosa green diamond versicolor blue star virginica figure generated fisheririsdemo classifying ﬂowers figure gives another example classiﬁcation due statistician ronald fisher goal learn distinguish three different kinds iris ﬂower called setosa versicolor virginica fortunately rather working directly images botanist already extracted useful features characteristics sepal length width petal length width feature extraction important difficult task machine learning methods use features chosen human later discuss methods learn good features data make scatter plot iris data figure see easy distinguish setosas red circles two classes checking petal length supervised learning true class true class true class true class true class true class true class true class true class true class true class true class true class true class true class true class true class true class figure first test mnist gray scale images features permuted randomly classiﬁcation performance identical versions data assuming training data permuted identical way figure generated shuffleddigitsdemo width threshold however distinguishing versicolor virginica slightly harder decision need based least two features always good idea perform exploratory data analysis plotting data applying machine learning method image classiﬁcation handwriting recognition consider harder problem classifying images directly human pre processed data might want classify image whole indoors outdoors scene horizontal vertical photo contain dog called image classiﬁcation special case images consist isolated handwritten letters digits example postal zip code letter use classiﬁcation perform handwriting recognition standard dataset used area known mnist stands modiﬁed national institute standards term modiﬁed used images preprocessed ensure digits mostly center image dataset contains training images test images digits written various people images size grayscale values range see figure example images many generic classiﬁcation methods ignore structure input features spatial layout consequently also easily handle data looks like figure data except randomly permuted order features verify exercise ﬂexibility blessing since methods general purpose curse since methods ignore obviously useful source information discuss methods exploiting structure input features later book available http yann lecun com exdb mnist chapter introduction figure example face detection input image murphy family photo taken august used kind permission bernard diedrich sherwood studios output classiﬁer detected faces different poses produced using online demo http demo pittpatt com classiﬁer trained manually labeled images faces non faces applied dense set overlapping patches test image patches whose probability containing face sufficiently high returned used kind permission pittpatt com face detection recognition harder problem ﬁnd objects within image called object detection object localization important special case face detection one approach problem divide image many small overlapping patches different locations scales orientations classify patch based whether contains face like texture called sliding window detector system returns locations probability face sufficiently high see figure example face detection systems built modern digital cameras locations detected faces used determine center auto focus another application automatically blurring faces google streetview system found faces one proceed perform face recognition means estimating identity person see figure case number class labels might large also features one use likely different face detection problem recognition subtle differences faces hairstyle may important determining identity detection important invariant details focus differences faces non faces information visual object detection see szeliski 
[introduction, supervised, learning, regression] regression like classiﬁcation except response variable continuous figure shows simple example single real valued input single real valued response consider ﬁtting two models data straight line quadratic function explain models various extensions basic problem arise high dimensional inputs outliers non smooth responses etc discuss ways handle problems later book unsupervised learning degree degree figure linear regression data data polynomial regression degree figure generated linregpolyvsdegree examples real world regression problems predict tomorrow stock market price given current market conditions possible side information predict age viewer watching given video youtube predict location space robot arm end effector given control signals torques sent various motors predict amount prostate speciﬁc antigen psa body function number different clinical measurements predict temperature location inside building using weather data time door sensors etc 
[introduction, unsupervised, learning] consider unsupervised learning given output data without inputs goal discover interesting structure data sometimes called knowledge discovery unlike supervised learning told desired output input instead formalize task one density estimation want build models form two differences supervised case first written instead supervised learning conditional density estimation whereas unsupervised learning unconditional density estimation second vector features need create multivariate probability models contrast supervised learning usually single variable trying predict means supervised learning problems use univariate probability models input dependent parameters signiﬁcantly simpliﬁes problem discuss multi output classiﬁcation chapter see also involves multivariate probability models unsupervised learning arguably typical human animal learning also widely applicable supervised learning since require human expert chapter introduction height weight height weight figure height weight people possible clustering using clusters figure generated kmeansheightweight manually label data labeled data expensive acquire also contains relatively little information certainly enough reliably estimate parameters complex models geoff hinton famous professor university toronto said learning see nobody telling right answers look every often mother says dog little information lucky got bits information even one bit per second way brain visual system neural connections live seconds use learning one bit per second need like bits per second one place get much information input geoffrey hinton quoted gorder describe canonical examples unsupervised learning 
[introduction, unsupervised, learning, discovering, clusters] canonical example unsupervised learning consider problem clustering data groups example figure plots data representing height weight group people seems might various clusters subgroups although clear many let denote number clusters ﬁrst goal estimate distribution number clusters tells subpopulations within data simplicity often approximate distribution mode arg max supervised case told two classes male female unsupervised case free choose many clusters like picking model right complexity called model selection discussed detail second goal estimate cluster point belongs let represent cluster data point assigned example hidden advent crowd sourcing web sites mechanical turk https www mturk com mturk welcome outsource data processing tasks humans world reduced cost labeling data nevertheless amount unlabeled data still orders magnitude larger amount labeled data unsupervised learning figure set points live linear subspace embedded solid red line ﬁrst principal component direction dotted black line second direction representation data figure generated pcademod latent variable since never observed training set infer cluster data point belongs computing argmax illustrated figure use different colors indicate assignments assuming book focus model based clustering means probabilistic model data rather running hoc algorithm advantages model based approach one compare different kinds models objective way terms likelihood assign data combine together larger systems etc real world applications clustering astronomy autoclass system cheeseman discovered new type star based clustering astrophysical measurements commerce common cluster users groups based purchasing web surﬁng behavior send customized targeted advertising group see berkhin biology common cluster ﬂow cytometry data groups discover different sub populations cells see 
[introduction, unsupervised, learning, discovering, latent, factors] dealing high dimensional data often useful reduce dimensionality projecting data lower dimensional subspace captures essence data called dimensionality reduction simple example shown figure project data plane approximation quite good since points lie close subspace reducing would involve projecting points onto red line figure would rather poor approximation make notion precise chapter motivation behind technique although data may appear high dimensional may small number degrees variability corresponding latent factors example modeling appearance face images may underlying latent factors describe variability lighting pose identity etc illustrated figure chapter introduction figure randomly chosen pixel images olivetti face database mean ﬁrst three principal component basis vectors eigenfaces figure generated pcaimagedemo used input statistical models low dimensional representations often result better predictive accuracy focus essence object ﬁltering inessential features also low dimensional representations useful enabling fast nearest neighbor searches two dimensional projections useful visualizing high dimensional data common approach dimensionality reduction called principal components analysis pca thought unsupervised version multi output linear regression observe high dimensional response low dimensional cause thus model form invert arrow infer latent low dimensional observed high dimensional see section details dimensionality reduction pca particular applied many different areas examples include following biology common use pca interpret gene microarray data account fact measurement usually result many genes correlated behavior fact belong different biological pathways natural language processing common use variant pca called latent semantic analysis document retrieval see section signal processing acoustic neural signals common use ica variant pca separate signals different sources see section computer graphics common project motion capture data low dimensional space use create animations see section one way tackle problems unsupervised learning figure sparse undirected gaussian graphical model learned using graphical lasso section applied ﬂow cytometry data sachs measures phosphorylation status proteins figure generated ggmlassodemo 
[introduction, unsupervised, learning, discovering, graph, structure] sometimes measure set correlated variables would like discover ones correlated others represented graph nodes represent variables edges represent direct dependence variables make precise chapter discuss graphical models learn graph structure data compute argmax unsupervised learning general two main applications learning sparse graphs discover new knowledge get better joint probability density estimators give somes example much motivation learning sparse graphical models comes systems biology community example suppose measure phosphorylation status proteins cell sachs figure gives example graph structure learned data using methods discussed section another example smith showed one recover neural wiring diagram certain kind bird time series eeg data recovered structure closely matched known functional connectivity part bird brain cases interested interpreting graph structure want use model correlations make predictions one example ﬁnancial portfolio management accurate models covariance large numbers different stocks important carvalho west show learning sparse graph using basis trading strategy possible outperform make money methods exploit sparse graphs another example predicting traffic jams freeway horvitz describe deployed system called jambayes predicting traffic ﬂow seattle area predictions made using graphical model whose structure learned data chapter introduction figure noisy image occluder estimate underlying pixel intensities based pairwise mrf model source figure felzenszwalb huttenlocher used kind permission pedro felzenszwalb 
[introduction, unsupervised, learning, matrix, completion] sometimes missing data variables whose values unknown example might conducted survey people might answered certain questions might various sensors fail corresponding design matrix holes missing entries often represented nan stands number goal imputation infer plausible values missing entries sometimes called matrix completion give example applications image inpainting interesting example imputation like task known image inpainting goal ﬁll holes due scratches occlusions image realistic texture illustrated figure denoise image well impute pixels hidden behind occlusion tackled building joint probability model pixels given set clean images inferring unknown variables pixels given known variables pixels somewhat like masket basket analysis except data real valued spatially structured kinds probability models use quite different see sections possible choices collaborative ﬁltering another interesting example imputation like task known collaborative ﬁltering common example concerns predicting movies people want watch based people rated movies already seen key idea prediction based features movie user although could merely ratings matrix precisely matrix rating unsupervised learning xxvhuv prylhv figure example movie rating data training data red test data denoted empty cells unknown say integer dislike like user movie note entries missing unknown since users rated movies hence observe tiny subset matrix want predict different subset particular given user might want predict unrated movies likely want watch order encourage research area dvd rental company netﬂix created com petition launched usd prize see http netflixprize com particular provided large matrix ratings scale movies created users full matrix would entries entries observed matrix extremely sparse subset used training rest testing shown figure goal competition predict accurately netﬂix existing system september prize awarded team researchers known bellkor pragmatic chaos section discusses methodology details teams methods found http www netflixprize com community viewtopic php market basket analysis commercial data mining much interest task called market basket analysis data consists typically large sparse binary matrix column represents item product row represents transaction set item purchased transaction many items purchased together bread butter correlations amongst bits given new partially observed bit vector representing subset items consumer bought goal predict bits likely turn representing items consumer might likely buy unlike collaborative ﬁltering often assume missing data training data since know past shopping behavior customer task arises domains besides modeling purchasing patterns example similar techniques used model dependencies ﬁles complex software systems case task predict given subset ﬁles changed ones need updated ensure consistency see common solve tasks using frequent itemset mining create association rules see hastie sec details alternatively adopt probabilistic approach joint density model bit vectors see chapter introduction figure illustration nearest neighbors classiﬁer nearest neighbors test point labels predict nearest neighbors test point labels predict illustration voronoi tesselation induced based figure duda figure generated knnvoronoi models often better predictive acccuracy association rules although may less interpretible typical difference data mining machine learning data mining emphasis interpretable models whereas machine learning emphasis accurate models 
[introduction, basic, concepts, machine, learning] section provide introduction key ideas machine learning expand concepts later book introduce brieﬂy give ﬂavor things come 
[introduction, basic, concepts, machine, learning, parametric, non-parametric, models] book focussing probabilistic models form depending whether interested supervised unsupervised learning respectively many ways deﬁne models important distinction model ﬁxed number parameters number parameters grow amount training data former called parametric model latter called non parametric model parametric models advantage often faster use disadvantage making stronger assumptions nature data distributions non parametric models ﬂexible often computationally intractable large datasets give examples kinds models sections focus supervised learning simplicity although much discussion also applies unsupervised learning 
[introduction, basic, concepts, machine, learning, -nearest, neighbors] simple example non parametric classiﬁer nearest neighbor knn classiﬁer simply looks points training set nearest test input basic concepts machine learning train data data predicted label figure synthetic class training data probability class knn probability class map estimate class label figure generated knnclassifydemo counts many members class set returns empirical fraction estimate illustrated figure formally indices nearest points indicator function deﬁned follows true false method example memory based learning instance based learning derived probabilistic framework explained section common chapter introduction fraction data neighborhood edge length cube figure illustration curse dimensionality embed small cube side inside larger unit cube plot edge length cube needed cover given volume unit cube function number dimensions based figure hastie figure generated cursedimensionality distance metric use euclidean distance limits applicability technique data real valued although metrics used figure gives example method action input two dimensional three classes discuss effect panel plots training data panel plots evaluated grid points panel plots need plot since probabilities sum one panel plots map estimate argmax knn classiﬁer induces voronoi tessellation points see figure partition space associates region point way points closer point within cell predicted label label corresponding training point 
[introduction, basic, concepts, machine, learning, curse, dimensionality] knn classiﬁer simple work quite well provided given good distance metric enough labeled training data fact shown knn classiﬁer come within factor best possible performance cover hart however main problem knn classiﬁers work well high dimensional inputs poor performance high dimensional settings due curse dimensionality explain curse give examples hastie consider applying knn classiﬁer data inputs uniformly distributed dimensional unit cube suppose estimate density class labels around test point growing hyper cube around contains desired fraction data points expected edge length cube want base estimate basic concepts machine learning pdf figure gaussian pdf mean variance figure generated gaussplotdemo visualization conditional density model density falls exponentially fast move away regression line figure generated linregwedgedemo data need extend cube along dimension around even use data ﬁnd see figure since entire range data along dimension see method longer local despite name nearest neighbor trouble looking neighbors far away may good predictors behavior input output function given point 
[introduction, basic, concepts, machine, learning, parametric, models, classiﬁcation, regression] main way combat curse dimensionality make assumptions nature data distribution either supervised problem unsupervised problem assumptions known inductive bias often embodied form parametric model statistical model ﬁxed number parameters brieﬂy describe two widely used examples revisit models much greater depth later book 
[introduction, basic, concepts, machine, learning, linear, regression] one widely used models regression known linear regression asserts response linear function inputs written follows represents inner scalar product input vector model weight vector residual error linear predictions true response statistics common denote regression weights chapter introduction degree degree figure polynomial degrees least squares data points figure generated linregpolyvsdegree often assume gaussian normal distribution denote mean variance see chapter details plot distribution get well known bell curve shown figure make connection linear regression gaussians explicit rewrite model following form makes clear model conditional probability density simplest case assume linear function noise ﬁxed case parameters model example suppose input dimensional represent expected response follows intercept bias term slope deﬁned vector prepending constant term input vector common notational trick allows combine intercept term terms model positive means expect output increase input increases illustrated figure conventional plot mean response shown figure linear regression made model non linear relationships replacing non linear function inputs use known basis function expansion example figure illustrates case known polynomial regression consider kinds basis functions later book fact many popular machine learning methods support vector machines neural networks classiﬁcation regression trees etc seen different ways estimating basis functions data discuss chapters carl friedrich gauss german mathematician physicist basic concepts machine learning figure sigmoid logistic function sigm sigm sigm figure generated sigmoidplot logistic regression sat scores solid black dots data open red circles predicted probabilities green crosses denote two students sat score hence input representation different training labels one student passed failed hence data perfectly separable using sat feature figure generated logregsatdemo 
[introduction, basic, concepts, machine, learning, logistic, regression] generalize linear regression binary classiﬁcation setting making two changes first replace gaussian distribution bernoulli distribution appropriate case response binary use ber second compute linear combination inputs pass function ensures deﬁning sigm sigm refers sigmoid function also known logistic logit function deﬁned sigm exp term sigmoid means shaped see figure plot also known squashing function since maps whole real line necessary output interpreted probability putting two steps together get ber sigm called logistic regression due similarity linear regression although form classiﬁcation regression daniel bernoulli dutch swiss mathematician physicist chapter introduction simple example logistic regression shown figure plot sigm sat score student whether passed failed class solid black dots show training data red circles plot parameters estimated training data discuss compute estimates section threshold output probability induce decision rule form looking figure see sigm imagine drawing vertical line known decision boundary everything left line classiﬁed everything right line classiﬁed notice decision rule non zero error rate even training set data linearly separable straight line draw separate create models non linear decision boundaries using basis function expansion non linear regression see many examples later book 
[introduction, basic, concepts, machine, learning, overﬁtting] highly ﬂexible models need careful overﬁt data avoid trying model every minor variation input since likely noise true signal illustrated figure see using high degree polynomial results curve wiggly unlikely true function extreme oscillations thus using model might result accurate predictions future outputs another example consider knn classiﬁer value large effect behavior model method makes errors training set since return labels original training points resulting prediction surface wiggly see figure therefore method may work well predicting future data figure see using results smoother prediction surface averaging larger neighborhood increases predictions becomes smoother limit end predicting majority label whole data set discuss pick right value 
[introduction, basic, concepts, machine, learning, model, selection] variety models different complexity linear logistic regression models different degree polynomials knn classiﬁers different values pick right one natural approach compute misclassiﬁcation rate sat stands scholastic aptitude test standardized test college admissions used united states data example johnson albert basic concepts machine learning predicted label predicted label figure prediction surface knn data figure figure generated knnclassifydemo training set method deﬁned follows err classiﬁer figure plot error rate knn classiﬁer dotted blue line see increasing increases error rate training set smoothing said get minimal error training set using since model memorizing data however care generalization error expected value misclassiﬁcation rate averaged future data see section details approximated computing misclassiﬁcation rate large independent test set used model training plot test error figure solid red upper curve see shaped curve complex models small method overﬁts simple models big method underﬁts therefore obvious way pick pick value minimum error test set example value ﬁne unfortunately training model access test set assumption cannot use test set pick model right complexity however create test set partitioning training set two part used training model second part called validation set used selecting model complexity models training set evaluate performance validation set pick best picked best reﬁt available data separate test set evaluate performance order estimate accuracy method discuss detail section often use data training set validation set number training cases small technique runs problems model academic settings usually access test set use model ﬁtting model selection otherwise get unrealistically optimistic estimate performance method one golden rules machine learning research chapter introduction misclassification rate train test figure misclassiﬁcation rate nearest neighbor classiﬁer left small model complex hence overﬁt right large model simple underﬁt dotted blue line training set size solid red line test set size schematic fold cross validation figure generated knnclassifydemo enough data train enough data make reliable estimate future performance simple popular solution use cross validation idea simple split training data folds fold train folds test round robin fashion sketched figure compute error averaged folds use proxy test error note point gets predicted although used training times common use called fold set get method called leave one cross validation loocv since fold train data cases except test exercise asks compute fold estimate test error compare empirical test error figure choosing knn classiﬁer special case general problem known model selection choose models different degrees ﬂexibility cross validation widely used solving problems although discuss approaches later book 
[introduction, basic, concepts, machine, learning, free, lunch, theorem] models wrong models useful george box box draper much machine learning concerned devising different models different algorithms use methods cross validation empirically choose best method particular problem however universally best model sometimes called free lunch theorem wolpert reason set assumptions works well one domain may work poorly another george box retired statistics professor university wisconsin basic concepts machine learning consequence free lunch theorem need develop many different types models cover wide variety data occurs real world model may many different algorithms use train model make different speed accuracy complexity tradeoffs combination data models algorithms studying subsequent chapters 
[introduction, exercises] exercise knn classiﬁer shuffled mnist data run mnistnndemo verify misclassiﬁcation rate ﬁrst test cases mnist classiﬁer run test cases error rate modify code ﬁrst randomly permute features columns training test design matrices shuffleddigitsdemo apply classiﬁer verify error rate changed exercise approximate knn classiﬁers use matlab code http people ubc mariusm index php flann flann per form approximate nearest neighbor search combine mnistnndemo classify mnist data set much speedup get drop accuracy exercise knn use knnclassifydemo plot estimate misclassiﬁcation rate test set compare figure discuss similarities differences test error rate 
[probability, introduction] probability theory nothing common sense reduced calculation pierre laplace previous chapter saw probability play useful role machine learning chapter discuss probability theory detail space great detail better consulting excellent textbooks available topic jaynes bertsekas tsitsiklis wasserman brieﬂy review many key ideas need later chapters start technical material let pause ask probability familiar phrase probability coin land heads mean actually least two different interpretations probability one called frequentist interpretation view probabilities represent long run frequencies events example statement means ﬂip coin many times expect land heads half time interpretation called bayesian interpretation probability view probability used quantify uncertainty something hence fundamentally related information rather repeated trials jaynes bayesian view statement means believe coin equally likely land heads tails next toss one big advantage bayesian interpretation used model uncer tainty events long term frequencies example might want compute probability polar ice cap melt event happen zero one times cannot happen repeatedly nevertheless ought able quantify uncertainty event based probable think event hopefully take appropriate actions see section discussion optimal decision making uncertainty give machine learning oriented examples might received speciﬁc email message want compute probability spam might observed blip radar screen want compute probability distribution location corresponding target bird plane missile cases idea repeated trials make sense bayesian interpretation valid indeed actually stanford statistician former professional magician persi diaconis shown coin likely land facing way started due physics problem diaconis chapter probability figure uniform distribution degenerate distribution figure generated discreteprobdistfig quite natural shall therefore adopt bayesian interpretation book fortunately basic rules probability theory matter interpretation adopted 
[probability, brief, review, probability, theory] section brief review basics probability theory merely meant refresher readers may rusty readers already familiar basics may safely skip section 
[probability, brief, review, probability, theory, discrete, random, variables] expression denotes probability event true example might logical expression rain tomorrow require means event deﬁnitely happen means event deﬁnitely happen write denote probability event deﬁned often write mean event true mean event false extend notion binary events deﬁning discrete random variable take value ﬁnite countably inﬁnite set denote probability event short called probability mass function pmf satisﬁes properties figure shows two pmf deﬁned ﬁnite state space left uniform distribution right degenerate distribution binary indicator function distribution represents fact always equal value words constant 
[probability, brief, review, probability, theory, fundamental, rules] section review basic rules probability brief review probability theory probability union two events given two events deﬁne probability follows mutually exclusive joint probabilities deﬁne probability joint event follows sometimes called product rule given joint distribution two events deﬁne marginal distribution follows summing possible states deﬁne similarly sometimes called sum rule rule total probability product rule applied multiple times yield chain rule probability introduce matlab like notation denote set conditional probability deﬁne conditional probability event given event true follows 
[probability, brief, review, probability, theory, bayes, rule] combining deﬁnition conditional probability product sum rules yields bayes rule also called bayes theorem example medical diagnosis example use rule consider following medical diagonsis problem suppose woman decide medical test breast cancer called mammogram test positive probability cancer obviously depends reliable test suppose told test sensitivity thomas bayes english mathematician presbyterian minister chapter probability means cancer test positive probability words event mammogram positive event breast cancer many people conclude therefore likely cancer false ignores prior probability breast cancer fortunately quite low ignoring prior called base rate fallacy also need take account fact test may false positive false alarm unfortunately false positives quite likely current screening technology combining three terms using bayes rule compute correct answer follows words test positive chance actually breast cancer example generative classiﬁers generalize medical diagonosis example classify feature vectors arbitrary type follows called generative classiﬁer since speciﬁes generate data using class conditional density class prior discuss models detail chapters alternative approach directly class posterior known discriminative classiﬁer discuss pros cons two approaches section 
[probability, brief, review, probability, theory, independence, conditional, independence] say unconditionally independent marginally independent denoted represent joint product two marginals see figure brief review probability theory figure computing discrete random variables possible states values possible states general joint distribution two variables would require parameters deﬁne subtract sum one constraint assuming unconditional independence need parameters deﬁne general say set variables mutually independent joint written product marginals unfortunately unconditional independence rare variables inﬂuence variables however usually inﬂuence mediated via variables rather direct therefore say conditionally independent given iff conditional joint written product conditional marginals discuss graphical models chapter see write assumption graph captures intuition dependencies mediated via example probability rain tomorrow event independent whether ground wet today event given knowledge whether raining today event intuitively causes know need know order predict vice versa shall expand concept chapter another characterization theorem iff exist function numbers mcgrayne based analysis government decided recommend annual mammogram screening women number false alarms would cause needless worry stress amongst women result unnecesssary expensive potentially harmful followup tests see section optimal way trade risk reverse reward face uncertainty chapter probability see exercise proof assumptions allow build large probabilistic models small pieces see many examples throughout book particular section discuss naive bayes classiﬁers section discuss markov models chapter discuss graphical models models heavily exploit properties 
[probability, brief, review, probability, theory, continuous, random, variables] far considered reasoning uncertain discrete quantities show following jaynes extend probability reason uncertain continuous quantities suppose uncertain continuous quantity probability lies interval computed follows deﬁne events since mutually exclusive sum rules gives hence deﬁne function called cumulative distribution function cdf obviously monotonically increasing function see figure example using notation deﬁne assume derivative exists called probability density function pdf see figure example given pdf compute probability continuous variable ﬁnite interval follows size interval gets smaller write require possible given long density integrates example consider uniform distribution unif unif set brief review probability theory cdf figure plot cdf standard normal corresponding pdf shaded regions contain probability mass therefore nonshaded region contains probability mass distribution gaussian leftmost cutoff point cdf gaussian symmetry rightost cutoff point central interval left cutoff right figure generated quantiledemo 
[probability, brief, review, probability, theory, quantiles] since cdf monotonically increasing function inverse let denote cdf value called quantile value median distribution half probability mass left half right values lower upper quartiles also use inverse cdf compute tail area probabilities example cdf gaussian distribution points left contain probability mass illustrated figure symmetry points right also contain mass hence central interval contains mass set central interval covered range distribution interval becomes sometimes approximated writing 
[probability, brief, review, probability, theory, mean, variance] familiar property distribution mean expected value denoted discrete deﬁned continuous deﬁned integral ﬁnite mean deﬁned see examples later variance measure spread distribution denoted deﬁned chapter probability follows var derive useful result standard deviation deﬁned std var useful since units 
[probability, common, discrete, distributions] section review commonly used parametric distributions deﬁned discrete state spaces ﬁnite countably inﬁnite 
[probability, common, discrete, distributions, binomial, bernoulli, distributions] suppose toss coin times let number heads probability heads say binomial distribution written bin pmf given bin number ways choose items known binomial coefficient pronounced choose see figure examples binomial distribution distribution following mean variance mean var suppose toss coin let binary random variable probability success heads say bernoulli distribution written ber pmf deﬁned ber words ber obviously special case binomial distribution common discrete distributions figure illustration binomial distribution figure generated binomdistplot 
[probability, common, discrete, distributions, multinomial, multinoulli, distributions] binomial distribution used model outcomes coin tosses model outcomes tossing sided die use multinomial distribution deﬁned follows let random vector number times side die occurs following pmf probability side shows multinomial coefficient number ways divide set size subsets sizes suppose like rolling sided dice vector bit vector one bit turned speciﬁcally dice shows face bit case think scalar categorical random variable states values dummy encoding example encode states also called one hot encoding since imagine one wires hot case pmf becomes see figure example common special case known categorical discrete distribution gustavo lacerda suggested call multinoulli distribution analogy binomial bernoulli distinction term shall adopt book chapter probability name multinomial multinoulli encoding binomial bernoulli table summary multinomial related distributions sequence position bits figure aligned dna sequences corresponding sequence logo figure generated seqlogodemo use following notation case cat otherwords cat see table summary application dna sequence motifs interesting application multinomial models arises biosequence analysis suppose set aligned dna sequences figure rows sequences columns locations along genome see several locations con served evolution part gene coding region since corresponding columns tend pure example column one way visually summarize data using sequence logo see figure plot letters fontsize proportional empirical probability probable letter top empirical probability distribution location gotten normalizing vector counts see equation distribution known motif also compute probable letter location called consensus sequence common discrete distributions poi poi figure illustration poisson distributions truncated axis clarity support distribution non negative integers figure generated poissonplotdemo 
[probability, common, discrete, distributions, poisson, distribution] say poisson distribution parameter written poi pmf poi ﬁrst term normalization constant required ensure distribution sums poisson distribution often used model counts rare events like radioactive decay traffic accidents see figure plots 
[probability, common, discrete, distributions, empirical, distribution] given set data deﬁne empirical distribution also called empirical measure follows emp dirac measure deﬁned general associate weights sample require think histogram spikes data points determines height spike distribution assigns probability point data set chapter probability 
[probability, common, continuous, distributions] section present commonly used univariate one dimensional continuous prob ability distributions 
[probability, common, continuous, distributions, gaussian, normal, distribution] widely used distribution statistics machine learning gaussian normal distribution pdf given mean mode var variance normalization constant needed ensure density integrates see exercise write denote say follows standard normal distribution see figure plot pdf sometimes called bell curve often talk precision gaussian mean inverse variance high precision means narrow distribution low variance centered note since pdf see consider evaluating density center cumulative distribution function cdf gaussian deﬁned see figure plot cdf integral closed form expression built software packages particular compute terms error function erf erf erf gaussian distribution widely used distribution statistics several reasons first two parameters easy interpret capture basic properties distribution namely mean variance second central limit theorem section tells sums independent random variables approximately gaussian distribution making good choice modeling residual errors noise third gaussian distribution makes least number assumptions symbol many different meanings book order consistent rest literature intended meaning clear context common continuous distributions maximum entropy subject constraint speciﬁed mean variance show section makes good default choice many cases finally simple mathematical form results easy implement often highly effective methods see see jaynes extensive discussion gaussians widely used 
[probability, common, continuous, distributions, degenerate, pdf] limit gaussian becomes inﬁnitely tall inﬁnitely thin spike centered lim called dirac delta function deﬁned useful property delta functions sifting property selects single term sum integral since integrand non zero one problem gaussian distribution sensitive outliers since log probability decays quadratically distance center robust distribution student distribution pdf follows mean scale parameter called degrees freedom see figure plots later reference note distribution following properties mean mode var distribution colourful etymology ﬁrst published william sealy gosset worked guinness brewery dublin since employer would allow use name called student distribution origin term seems arisen context tables student distribution used fisher developing basis classical statistical inference see http jeff tripod com html historical details chapter probability gauss student laplace gauss student laplace figure pdf lap mean variance gaussian laplace mean variance student undeﬁned log pdf note student distribution log concave parameter value unlike laplace distribution always log concave log convex nevertheless unimodal figure generated studentlaplacepdfplot gaussian student laplace gaussian student laplace figure illustration effect outliers ﬁtting gaussian student laplace distributions outliers gaussian student curves top outliers see gaussian affected outliers student laplace distributions based figure bishop figure generated robustdemo variance deﬁned mean deﬁned illustration robustness student distribution consider figure left show gaussian student data outliers right add outliers see gaussian affected lot whereas student distribution hardly changes student heavier tails least small see figure distribution known cauchy lorentz distribution notable heavy tails integral deﬁnes mean converge ensure ﬁnite variance require common use gives good performance range problems lange student distribution rapidly approaches gaussian distribution loses robustness properties common continuous distributions gamma distributions figure distributions mode otherwise increase rate reduce horizontal scale thus squeezing everything leftwards upwards figure generated gammaplotdemo empirical pdf rainfall data ﬁtted gamma distribution superimposed figure generated gammarainfalldemo 
[probability, common, continuous, distributions, laplace, distribution] another distribution heavy tails laplace distribution also known double sided exponential distribution following pdf lap exp location parameter scale parameter see figure plot distribution following properties mean mode var robustness outliers illustrated figure also put mores probability density gaussian property useful way encourage sparsity model see section 
[probability, common, continuous, distributions, gamma, distribution] gamma distribution ﬂexible distribution positive real valued deﬁned terms two parameters called shape rate shape rate pierre simon laplace french mathematician played key role creating ﬁeld bayesian statistics alternative parameterization use scale parameter instead rate version one used matlab gampdf although book use rate parameterization unless otherwise speciﬁed chapter probability gamma function see figure plots later reference note distribution following properties mean mode var several distributions special cases gamma discuss exponential distribution deﬁned expon rate parameter distribution describes times events poisson process process events occur continuously independently constant average rate erlang distribution gamma distribution integer common yielding one parameter erlang distribution erlang rate parameter chi squared distribution deﬁned distribution sum squared gaussian random variables precisely another useful result following one show exercise inverse gamma distribution deﬁned shape scale distribution properties mean mode var mean exists variance exists see applications distributions later 
[probability, common, continuous, distributions, beta, distribution] beta distribution support interval deﬁned follows beta beta function see figure plots beta distributions require ensure distribution integrable ensure exists get uniform distirbution common continuous distributions beta distributions figure beta distributions figure generated betaplotdemo less get bimodal distribution spikes greater distribution unimodal later reference note distribution following properties exercise mean mode var 
[probability, common, continuous, distributions, pareto, distribution] pareto distribution used model distribution quantities exhibit long tails also called heavy tails example observed frequent word english occurs approximately twice often second frequent word occurs twice often fourth frequent word etc plot frequency words rank get power law known zipf law wealth similarly skewed distribution especially plutocracies usa pareto pdf deﬁned follow pareto density asserts must greater constant much greater controls much distribution approaches see figure plots plot distibution log log scale forms straight line form log log constants see figure illustration known power law distribution following properties mean mode var usa americans wealth half americans combined source http www politifact com wisconsin statements mar michael moore michael moore ays americans wealth see hacker pierson political analysis extreme distribution income arisen democratic country chapter probability pareto distribution pareto log scale figure pareto distribution pareto pdf log log scale figure generated paretoplot 
[probability, joint, probability, distributions] far mostly focusing modeling univariate probability distributions section start discussion challenging problem building joint probability distributions multiple related random variables central topic book joint probability distribution form set variables models stochastic relationships variables variables discrete represent joint distribution big multi dimensional array one variable per dimension however number parameters needed deﬁne model number states variable deﬁne high dimensional joint distributions using fewer parameters making con ditional independence assumptions explain chapter case continuous distributions alternative approach restrict form pdf certain functional forms examine 
[probability, joint, probability, distributions, covariance, correlation] covariance two measures degree linearly related covariance deﬁned cov joint probability distributions figure several sets points correlation coefficient set note correlation reﬂects noisiness direction linear relationship top row slope relationship middle many aspects nonlinear relationships bottom ﬁgure center slope case correlation coefficient undeﬁned variance zero source http wikipedia org wiki file correlation_examples png dimensional random vector covariance matrix deﬁned following symmetric positive deﬁnite matrix cov var cov cov cov var cov cov cov var covariances inﬁnity sometimes convenient work normalized measure ﬁnite upper bound pearson correlation coefficient deﬁned corr cov var var correlation matrix form corr corr corr corr corr corr one show exercise corr hence correlation matrix entry diagonal entries one also show corr parameters linear relationship see exercise intuitively one chapter probability might expect correlation coefficient related slope regression line coefficient expression however show equation later regression coefficient fact given cov var better way think correlation coefficient degree linearity see figure independent meaning see section cov hence corr uncorrelated however con verse true uncorrelated imply independent example let clearly dependent fact uniquely determined yet one show exercise corr striking examples fact shown figure shows several data sets clear dependendence yet correlation coefficient general measure dependence random variables mutual information discussed section zero variables truly independent 
[probability, joint, probability, distributions, multivariate, gaussian] multivariate gaussian multivariate normal mvn widely used joint prob ability density function continuous variables discuss mvns detail chapter give deﬁnitions plots pdf mvn dimensions deﬁned following exp mean vector cov covariance matrix sometimes work terms precision matrix concentration matrix instead inverse covariance matrix normalization constant ensures pdf integrates see exercise figure plots mvn densities three different kinds covariance matrices full covariance matrix parameters divide since symmetric diagonal covariance matrix parameters diagonal terms spherical isotropic covariance one free parameter 
[probability, joint, probability, distributions, distribution] robust alternative mvn multivariate student distribution whose pdf given called scale matrix since exactly covariance matrix fatter tails gaussian smaller fatter tails joint probability distributions full diagonal spherical spherical figure show level sets gaussians full covariance matrix elliptical contours diagonal covariance matrix axis aligned ellipse spherical covariance matrix circular shape surface plot spherical gaussian figure generated gaussplotddemo distribution tends towards gaussian distribution following properties mean mode cov 
[probability, joint, probability, distributions, dirichlet, distribution] multivariate generalization beta distribution dirichlet distribution support probability simplex deﬁned pdf deﬁned follows dir johann dirichlet german mathematician chapter probability figure dirichlet distribution deﬁnes distribution simplex represented triangular surface points surface satisfy plot dirichlet density figure generated visdirichletgui jonathan huang comb like structure edges plotting artifact figure generated dirichletdplot samples dir alpha samples dir alpha figure samples dimensional symmetric dirichlet distribution different parameter values results sparse distributions many results uniform dense distributions figure generated dirichlethistogramdemo transformations random variables natural generalization beta function variables figure shows plots dirichlet figure sampled probability vectors see controls strength distribution peaked control peak occurs example dir uniform distribution dir broad distribution centered dir narrow distribution centered get spikes corner simplex future reference distribution properties mode var often use symmetric dirichlet prior form case mean becomes variance becomes var increasing increases precision decreases variance distribution 
[probability, transformations, random, variables] random variable distribution question address section 
[probability, transformations, random, variables, linear, transformations] suppose linear function case easily derive mean covariance follows first mean called linearity expectation scalar valued function corresponding result covariance cov cov aσa cov leave proof exercise scalar valued result becomes var var chapter probability use results extensively later chapters note however mean covariance completely deﬁne distribution gaussian general must use techniques described derive full distribution opposed ﬁrst two moments 
[probability, transformations, random, variables, general, transformations] discrete derive pmf simply summing probability mass example even otherwise uniform set similarly note example many one function continuous cannot use equation since density pmf cannot sum densities instead work cdf write derive pdf differentiating cdf case monotonic hence invertible functions write taking derivatives get think measure volume space similarly measures volume space thus measures change volume since sign change important take absolute value get general expression called change variables formula understand result intuitively follows observations falling range get transformed hence example suppose see also exercise multivariate change variables extend previous results multivariate distributions follows let function maps let jacobian matrix given transformations random variables det measures much unit cube changes volume apply invertible mapping deﬁne pdf transformed variables using jacobian inverse mapping det det exercise use formula derive normalization constant multivariate gaussian simple example consider transforming density cartesian coordinates polar coordinates cos sin cos sin sin cos det cos sin hence det cos sin see geometrically notice area shaded patch figure given drdθ limit equal density center patch times size patch hence drdθ cos sin 
[probability, transformations, random, variables, central, limit, theorem] consider random variables pdf necessarily gaussian mean variance assume variable independent identically distributed iid short let sum simple widely used transformation one show increases distribution sum approaches exp hence distribution quantity converges standard normal sample mean called central limit theorem see jaynes rice proof figure give example compute mean drawn beta distribution see sampling distribution mean value rapidly converges gaussian distribution chapter probability figure change variables polar cartesian area shaded patch based rice figure figure central limit theorem pictures plot histogram beta distribution tends towards gaussian based figure bishop figure generated centrallimitdemo 
[probability, monte, carlo, approximation] general computing distribution function using change variables formula difficult one simple powerful alternative follows first generate samples distribution call many ways generate samples one popular method high dimensional distributions called markov chain monte carlo mcmc explained chapter given samples approximate distribution using empirical distribution called monte carlo approximation named city europe known plush gambling casinos monte carlo techniques ﬁrst developed area statistical physics particular development atomic bomb widely used statistics machine learning well use monte carlo approximate expected value function random monte carlo approximation figure computing distribution uniform left analytic result shown middle monte carlo approximation shown right figure generated changeofvarsdemod variable simply draw samples compute arithmetic mean function applied samples written follows called monte carlo integration advantage numerical integration based evaluating function ﬁxed grid points function evaluated places non negligible probability varying function approximate many quantities interest var median median give examples see many later chapters 
[probability, monte, carlo, approximation, example, change, variables, way] section discussed analytically compute distribution function random variable much simpler approach use monte carlo approximation example suppose unif approximate drawing many samples squaring computing resulting empirical distribution see figure illustration use technique extensively later chapters see also figure chapter probability figure estimating monte carlo integration blue points inside circle red crosses outside figure generated mcestimatepi 
[probability, monte, carlo, approximation, monte, carlo, integration] approximation used many applications statistical ones suppose want estimate know area circle radius also equal following deﬁnite integral dxdy hence let approximate monte carlo integration let indicator function points inside circle outside let uniform distributions dxdy dxdy ﬁnd standard error see section discussion standard errors plot points accepted rejected figure 
[probability, monte, carlo, approximation, accuracy, monte, carlo, approximation] accuracy approximation increases sample size illustrated fig ure top line plot histogram samples gaussian distribution bottom line plot smoothed version samples created using kernel density estimate section smoothed distribution evaluated dense grid points monte carlo approximation samples samples samples samples figure samples gaussian distribution solid red line true pdf top line histogram samples bottom line kernel density estimate derived samples dotted blue solid red line true pdf based figure hoff figure generated mcaccuracydemo plotted note smoothing purposes plotting used monte carlo estimate denote exact mean approximation one show independent samples var consequence central limit theorem course unknown expression also estimated chapter probability term called numerical empirical standard error estimate uncertainty estimate see section discussion standard errors want report answer accurate within probability least need use number samples satisﬁes approximate factor yielding 
[probability, information, theory] information theory concerned representing data compact fashion task known data compression source coding well transmitting storing way robust errors task known error correction channel coding ﬁrst seems far removed concerns probability theory machine learning fact intimate connection see note compactly representing data requires allocating short codewords highly probable bit strings reserving longer codewords less probable bit strings similar situation natural language common words generally much shorter rare words also decoding messages sent noisy channels requires good probability model kinds messages people tend send cases need model predict kinds data likely unlikely also central problem machine learning see mackay details connection information theory machine learning obviously cannot details information theory see cover thomas interested learn however introduce basic concepts need later book 
[probability, information, theory, entropy] entropy random variable distribution denoted sometimes measure uncertainty particular discrete variable states deﬁned log usually use log base case units called bits short binary digits use log base units called nats example histogram distribution ﬁnd discrete distribution maximum entropy uniform distribution see section proof hence ary random variable entropy maximized case log conversely distribution minimum entropy zero delta function puts mass one state distribution uncertainty figure plotted dna sequence logo height bar deﬁned entropy distribution maximum possible entropy thus bar height corresponds uniform distribution whereas bar height corresponds deterministic distribution information theory figure entropy bernoulli random variable function maximum entropy log figure generated bernoullientropyfig special case binary random variables write hence entropy becomes log log log log called binary entropy function also written plot figure see maximum value occurs distribution uniform 
[probability, information, theory, divergence] one way measure dissimilarity two probability distributions known kullback leibler divergence divergence relative entropy deﬁned follows log sum gets replaced integral pdfs rewrite log log called cross entropy log one show cover thomas cross entropy average number bits needed encode data coming source distribution use model divergence distance since asymmetric one symmetric version divergence jensen shannon divergence deﬁned chapter probability deﬁne codebook hence regular entropy deﬁned section expected number bits use true model divergence difference words divergence average number extra bits needed encode data due fact used distribution encode data instead true distribution extra number bits interpretation make clear equal zero iff give proof important result theorem information inequality equality iff proof prove theorem need use jensen inequality states convex function clearly true deﬁnition convexity proved induction let prove main theorem following cover thomas let support log log log log log log ﬁrst inequality follows jensen since log strictly concave function equality equation iff equality equation iff implies hence iff one important consequence result discrete distribution maximum entropy uniform distribution precisely log number states equality iff uniform see let log log log log formulation laplace principle insufficient reason argues favor using uniform distributions reasons favor one distribution another see section discussion create distributions satisfy certain constraints otherwise least commital possible example gaussian satisﬁes ﬁrst second moment constraints otherwise maximum entropy information theory 
[probability, information, theory, mutual, information] consider two random variables suppose want know much knowing one variable tells could compute correlation coefficient deﬁned real valued random variables furthermore limited measure dependence saw figure general approach determine similar joint distribution factored distribution called mutual information deﬁned follows log equality iff zero iff variables independent gain insight meaning helps express terms joint conditional entropies one show exercise expression equivalent following conditional entropy deﬁned thus interpret reduction uncertainty observing symmetry reduction uncertainty observing encounter several applications later book see also exercises connection correlation coefficients quantity closely related pointwise mutual information pmi two events random variables deﬁned pmi log log log measures discrepancy events occuring together compared would expected chance clearly expected value pmi interestingly rewrite pmi follows pmi log log amount learn updating prior posterior equiva lently updating prior posterior mutual information continuous random variables formula deﬁned discrete random variables continuous random variables common ﬁrst discretize quantize dividing ranges variable bins computing many values fall histogram bin scott easily compute using formula see mutualinfoallpairsmixed code mimixeddemo demo unfortunately number bins used location bin boundaries signiﬁcant effect results one way around try estimate directly chapter probability xamp amp amp amp xlt amp figure left correlation coefficient maximal information criterion mic pairwise relation ships data right scatter plots certain pairs variables red lines non parametric smoothing regressions section separately trend source figure reshed used kind permission david reshef american association advancement science without ﬁrst performing density estimation learned miller another approach try many different bin sizes locations compute maximum achieved statistic appropriately normalized known maximal information coefficient mic reshed precisely deﬁne max log min set grids size represents discretization variables onto grid maximization bin locations performed efficiently using dynamic programming reshed deﬁne mic mic max xylt sample size dependent bound number bins use still reliably estimate distribution reshed suggest shown mic lies range represents relationship variables represents noise free relationship form linear figure gives example statistic action data consists variables measuring variety social economic health political indicators collected world health organization left ﬁgure see correlation coefficient plotted mic variable pairs right ﬁgure see scatter plots particular pairs variables discuss point marked low low mic corresponding scatter plot makes information theory clear relationship two variables percentage lives lost injury density dentists population points marked high absolute value high mic represent nearly linear relationships points marked low high mic correspond non linear sometimes case non functional one many relationships variables summary see statistics mic based mutual information used discover interesting relationships variables way simpler measures correlation coefficients cannot reason mic called correlation century speed 
[probability, exercises] exercise probabilities sensitive form question used generate answer source minka neighbor two children assuming gender child like coin ﬂip likely priori neighbor one boy one girl probability possibilities two boys two girls probabilities suppose ask whether boys says yes probability one child girl suppose instead happen see one children run boy probability child girl exercise legal reasoning source peter lee suppose crime committed blood found scene innocent explanation type present population prosecutor claims chance defendant would crime blood type innocent thus chance guilty known prosecutor fallacy wrong argument defender claims crime occurred city people blood type would found approximately people evidence provided probability defendant guilty thus relevance known defender fallacy wrong argument exercise variance sum show variance sum var var var cov cov covariance exercise bayes rule medical diagnosis source koller yearly checkup doctor bad news good news bad news tested positive serious disease test accurate probability testing positive given disease probability tetsing negative given disease good news rare disease striking one people chances actually disease show calculations well giving ﬁnal result chapter probability exercise monty hall problem source mackay game show contestant told rules follows three doors labelled single prize hidden behind one get select one door initially chosen door opened instead gameshow host open one two doors way reveal prize example ﬁrst choose door open one doors guaranteed choose one open prize revealed point given fresh choice door either stick ﬁrst choice switch closed door doors opened receive whatever behind ﬁnal choice door imagine contestant chooses door ﬁrst gameshow host opens door revealing nothing behind door promised contestant stick door switch door make difference may assume initially prize equally likely behind doors hint use bayes rule exercise conditional independence source koller let discrete random variable let observed values two random variables suppose wish calculate vector following sets numbers sufficient calculation iii suppose assume conditionally independent given sets sufficent show calculations well giving ﬁnal result hint use bayes rule exercise pairwise independence imply mutual independence say two random variables pairwise independent hence say random variables mutually independent hence show pairwise independence pairs variables necessarily imply mutual inde pendence suffices give counter example information theory exercise conditional independence iff joint factorizes text said iff prove following alternative deﬁnition iff exist function exercise conditional independence source koller following properties true prove disprove note restricting attention distributions represented graphical model true false true false exercise deriving inverse gamma density let let show shape scale hint use change variables formula exercise normalization constant gaussian normalization constant zero mean gaussian given exp compute consider square exp dxdy let change variables cartesian polar using cos sin since dxdy rdrdθ cos sin exp drdθ evaluate integral hence show hint separate integral product two terms ﬁrst involving constant easy hint second integral also easy since chapter probability exercise expressing mutual information terms entropies show exercise mutual information correlated normals source cover thomas find mutual information bivariate normal distribution evaluate comment hint differential entropy dimensional gaussian log det case becomes log πeσ hint log exercise measure correlation normalized mutual information source cover thomas let discrete random variables identically distributed necessarily independent deﬁne show show exercise mle minimizes divergence empirical distribution let emp empirical distribution let model show argmin emp obtained mle hint use non negativity divergence exercise mean mode variance beta distribution suppose beta derive mean mode variance exercise expected value minimum suppose two points sampled independently uniformly random interval expected location left point 
[generative, models, discrete, data, introduction] section discussed classify feature vector applying bayes rule generative classiﬁer form key using models specifying suitable form class conditional density deﬁnes kind data expect see class chapter focus case observed data discrete symbols also discuss infer unknown parameters models 
[generative, models, discrete, data, bayesian, concept, learning] consider child learns understand meaning word dog presumably child parents point positive examples concept saying things look cute dog mind doggy etc however unlikely provide negative examples saying look non dog certainly negative examples may obtained active learning process child says look dog parent says cat dear dog psychological research shown people learn concepts positive examples alone tenenbaum think learning meaning word equivalent concept learning turn equivalent binary classiﬁcation see deﬁne example concept otherwise goal learn indicator function deﬁnes elements set allowing uncertainty deﬁnition equivalently elements emulate fuzzy set theory using standard probability calculus note standard binary classiﬁcation techniques require positive negative examples contrast devise way learn positive examples alone pedagogical purposes consider simple example concept learning called number game based part josh tenenbaum phd thesis tenenbaum game proceeds follows choose simple arithmetical concept prime number number give series randomly chosen positive examples drawn ask whether new test case belongs ask classify chapter generative models discrete data examples figure empirical predictive distribution averaged humans number game first two rows seeing illustrates diffuse similarity third row seeing illustrates rule like behavior powers bottom row seeing illustrates focussed similarity numbers near source figure tenenbaum used kind permission josh tenenbaum suppose simplicity numbers integers suppose tell positive example concept numbers think positive hard tell one example predictions quite vague presumably numbers similar sense likely similar way similar close similar digit common similar also even power seem similar thus numbers likely others represent probability distribution probability given data called posterior predictive distribution figure top shows predictive distribution people derived lab experiment see people predict numbers similar variety kinds similarity suppose tell also positive examples may guess hidden concept powers two example induction given hypothesis predictive distribution quite speciﬁc puts mass powers shown figure third row instead tell data get different kind generalization gradient shown figure bottom explain behavior emulate machine classic approach induction suppose hypothesis space concepts odd numbers even numbers numbers powers two numbers ending bayesian concept learning etc subset consistent data called version space see examples version space shrinks become increasingly certain concept mitchell however version space whole story seeing many consistent rules combine predict also seeing choose rule powers two say even numbers powers two except equally consistent evidence provide bayesian explanation 
[generative, models, discrete, data, bayesian, concept, learning, likelihood] must explain chose two powers two say even even numbers seeing given hypotheses consistent evidence key intuition want avoid suspicious coincidences true concept even numbers come saw numbers happened powers two formalize let assume examples sampled uniformly random extension concept extension concept set numbers belong extension even extension numbers ending tenenbaum calls strong sampling assumption given assumption probability independently sampling items replacement given size crucial equation embodies tenenbaum calls size principle means model favors simplest smallest hypothesis consistent data commonly known occam razor see works let two since powers two less even since even numbers likelihood two higher even examples likelihood two whereas likelihood even likelihood ratio almost favor two quantiﬁes earlier intuition would suspicious coincidence generated even 
[generative, models, discrete, data, bayesian, concept, learning, prior] suppose given data concept powers two except likely powers two since need explain coincidence missing set examples however hypothesis powers two except seems conceptually unnatural capture intution assigning low prior probability unnatural concepts course prior might different mine subjective aspect bayesian reasoning source much controversy since means example child math professor william occam also spelt ockham english monk philosopher chapter generative models discrete data reach different answers fact presumably different priors also different hypothesis spaces however ﬁnesse deﬁning hypothesis space child math professor setting child prior weight zero certain advanced concepts thus sharp distinction prior hypothesis space although subjectivity prior controversial actually quite useful told numbers arithmetic rule given may think likely unlikely told numbers examples healthy cholesterol levels would probably think unlikely likely thus see prior mechanism background knowledge brought bear problem without rapid learning small samples sizes impossible prior use illustration purposes let use simple prior puts uniform probability simple arithmetical concepts even numbers odd numbers prime numbers numbers ending etc make things interesting make concepts even odd likely apriori also include two unnatural concepts namely powers plus powers except give low prior weight see figure plot prior consider slightly sophisticated prior later 
[generative, models, discrete, data, bayesian, concept, learning, posterior] posterior simply likelihood times prior normalized context iff iff data extension hypothesis figure plots prior likelihood posterior seeing see posterior combination prior likelihood case concepts prior uniform posterior proportional likelihood however unnatural concepts powers plus powers except low posterior support despite high likelihood due low prior conversely concept odd numbers low posterior support despite high prior due low likelihood figure plots prior likelihood posterior seeing likelihood much peaked powers two concept dominates posterior essentially learner aha moment ﬁgures true concept see need low prior unnatural concepts otherwise would overﬁt data picked powers except general enough data posterior becomes peaked single concept namely map estimate map argmax posterior mode dirac measure deﬁned bayesian concept learning powers powers powers powers powers powers powers powers powers powers powers ends ends ends ends ends ends ends ends ends mult mult mult mult mult mult mult mult squares odd even prior lik data post figure prior likelihood posterior based tenenbaum figure generated numbersgame note map estimate written argmax argmax log log since likelihood term depends exponentially prior stays constant get data map estimate converges towards maximum likelihood estimate mle mle argmax argmax log words enough data see data overwhelms prior chapter generative models discrete data powers powers powers powers powers powers powers powers powers powers powers ends ends ends ends ends ends ends ends ends mult mult mult mult mult mult mult mult squares odd even prior lik data post figure prior likelihood posterior based tenenbaum figure generated numbersgame case map estimate converges towards mle true hypothesis hypothesis space map estimate converge upon hypothesis thus say bayesian inference estimation consistent estimators see section details also say hypothesis space identiﬁable limit meaning recover truth limit inﬁnite data hypothesis class rich enough represent truth usually case converge hypothesis close possible truth however formalizing notion closeness beyond scope chapter bayesian concept learning powers powers mult mult even squares ends powers powers figure posterior hypotheses corresponding predictive distribution seeing one example dot means number consistent hypothesis graph right weight given hypothesis taking weighed sum dots get top based figure tenenbaum figure generated numbersgame 
[generative, models, discrete, data, bayesian, concept, learning, posterior, predictive, distribution] posterior internal belief state world way test beliefs justiﬁed use predict objectively observable quantities basis scientiﬁc method speciﬁcally posterior predictive distribution context given weighted average predictions individual hypothesis called bayes model averaging hoeting illustrated figure dots bottom show predictions hypothesis vertical curve right shows weight associated hypothesis multiply row weight add get distribution top small ambiguous dataset posterior vague induces broad predictive distribution however ﬁgured things posterior becomes delta function centered map estimate case predictive distribution chapter generative models discrete data becomes called plug approximation predictive density widely used due simplicity however general represents uncertainty predictions smooth using bma see examples later book although map learning simple cannot explain gradual shift similarity based reasoning uncertain posteriors rule based reasoning certain posteriors example suppose observe use simple prior minimal consistent hypothesis powers get non zero probability predicted course example overﬁtting given map hypothesis powers two thus plug predictive distribution gets broader stays see data starts narrow forced broaden seems data contrast bayesian approach start broad narrow learn makes intuitive sense particular given many hypotheses non negligible posterior support predictive distribution broad however see posterior concentrates mass one hypothesis predictive distribution becomes narrower predictions made plug approach bayesian approach quite different small sample regime although converge answer see data 
[generative, models, discrete, data, bayesian, concept, learning, complex, prior] model human behavior tenenbaum used slightly sophisticated prior rived analysing experimental data people measure similarity numbers see tenenbaum details result set arithmetical concepts similar mentioned plus intervals note hypotheses mutually exclusive thus prior mixture two priors one arithmetical rules one intervals rules interval free parameter model relative weight given two parts prior results sensitive value long reﬂecting fact people likely think concepts deﬁned rules predictive distribution model using larger hypothesis space shown figure strikingly similar human predictive distribution shown figure even though human data modulo choice hypothesis space 
[generative, models, discrete, data, beta-binomial, model] number game involved inferring distribution discrete variable drawn ﬁnite hypothesis space given series discrete observations made computations particularly simple needed sum multiply divide however many applications unknown parameters continuous hypothesis space subset beta binomial model examples figure predictive distributions model using full hypothesis space compare figure predictions bayesian model plotted values human data available top line looks sparser figure source figure tenenbaum used kind permission josh tenenbaum number parameters complicates mathematics since replace sums integrals however basic ideas illustrate considering problem inferring probability coin shows heads given series observed coin tosses although might seem trivial turns model forms basis many methods consider later book including naive bayes classiﬁers markov models etc historically important since example analyzed bayes original paper bayes analysis subsequently generalized pierre simon laplace creating call bayes rule see stigler historical details follow familiar recipe specifying likelihood prior deriving posterior posterior predictive 
[generative, models, discrete, data, beta-binomial, model, likelihood] suppose ber represents heads represents tails rate parameter probability heads data iid likelihood form chapter generative models discrete data heads tails two counts called sufficient statistics data since need know infer alternative set sufficient statistics formally say sufficient statistic data data use uniform prior equivalent saying consequently two datasets sufficient statistics infer value suppose data consists count number heads observed ﬁxed number trials case bin bin represents binomial distribution following pmf bin since constant independent likelihood binomial sampling model likelihood bernoulli model inferences make whether observe counts sequence trials 
[generative, models, discrete, data, beta-binomial, model, prior] need prior support interval make math easier would convenient prior form likelihood prior looked like prior parameters case could easily evaluate posterior simply adding exponents prior posterior form say prior conjugate prior corresponding likelihood conjugate priors widely used simplify computation easy interpret see case bernoulli conjugate prior beta distribution encountered section beta parameters prior called hyper parameters set order encode prior beliefs example encode beliefs mean standard deviation set exercise encode beliefs mean think lives interval probability ﬁnd exercise know nothing except lies interval use uni form prior kind uninformative prior see section details uniform distribution represented beta distribution beta binomial model prior lik post prior lik post figure updating beta prior binomial likelihood sufficient statistics yield beta posterior updating beta prior binomial likeli hood sufficient statistics yield beta posterior figure generated binomialbetaposteriordemo 
[generative, models, discrete, data, beta-binomial, model, posterior] multiply likelihood beta prior get following posterior following equa tion bin beta beta particular posterior obtained adding prior hyper parameters empirical counts reason hyper parameters known pseudo counts strength prior also known effective sample size prior sum pseudo counts plays role analogous data set size figure gives example update weak beta prior peaked likelihood function corresponding large sample size see posterior essentially identical likelihood since data overwhelmed prior figure gives example update strong beta prior peaked likelihood function see posterior compromise prior likelihood note updating posterior sequentially equivalent updating single batch see suppose two data sets sufficient statistics let sufficient statistics combined datasets batch mode bin beta beta sequential mode bin beta beta makes bayesian inference particularly well suited online learning see later chapter generative models discrete data posterior mean mode equation map estimate given use uniform prior map estimate reduces mle empirical fraction heads makes intuitive sense also derived applying elementary calculus maximize likelihood function equation exercise contrast posterior mean given difference mode mean prove important later show posterior mean convex combination prior mean mle captures notion posterior compromise previously believed data telling let equivalent sample size prior controls strength let prior mean posterior mean given ratio prior posterior equivalent sample size weaker prior smaller hence closer posterior mean mle one show similarly posterior mode convex combination prior mode mle converges mle posterior variance mean mode point estimates useful know much trust variance posterior one way measure variance beta posterior given var simplify formidable expression case get var mle hence error bar estimate posterior standard deviation given var beta binomial model see uncertainty goes rate note however uncertainty variance maximized minimized close means easier sure coin biased sure fair 
[generative, models, discrete, data, beta-binomial, model, posterior, predictive, distribution] far focusing inference unknown parameter let turn attention prediction future observable data consider predicting probability heads single future trial beta poste rior beta thus see mean posterior predictive distribution equivalent case plugging posterior mean parameters ber overﬁtting black swan paradox suppose instead plug mle use ber unfortunately approximation perform quite poorly sample size small example suppose seen tails row mle since makes observed data probable possible however using estimate predict heads impossible called zero count problem sparse data problem frequently occurs estimating counts small amounts data one might think era big data concerns irrelevant note partition data based certain criteria number times speciﬁc person engaged speciﬁc activity sample sizes become much smaller problem arises example trying perform personalized recommendation web pages thus bayesian methods still useful even big data regime jordan zero count problem analogous problem philosophy called black swan paradox based ancient western conception swans white context black swan metaphor something could exist black swans discovered australia european explorers century term black swan paradox ﬁrst coined famous philosopher science karl popper term also used title recent popular book taleb paradox used illustrate problem induction problem draw general conclusions future speciﬁc observations past let derive simple bayesian solution problem use uniform prior case plugging posterior mean gives laplace rule succession justiﬁes common practice adding empirical counts normalizing plugging technique known add one smoothing note plugging map chapter generative models discrete data parameters would smoothing effect since mode form becomes mle predicting outcome multiple future trials suppose interested predicting number heads future trials given bin beta recognize integral normalization constant beta distribution hence thus ﬁnd posterior predictive given following known compound beta binomial distribution distribution following mean variance var hence see mean becomes consistent equation process illustrated figure start beta prior plot posterior predictive density seeing heads tails figure plots plug approximation using map estimate see bayesian prediction longer tails spreading probablity mass widely therefore less prone overﬁtting blackswan type paradoxes 
[generative, models, discrete, data, dirichlet-multinomial, model] previous section discussed infer probability coin comes heads section generalize results infer probability dice sides comes face might seem like another toy exercise methods study widely used analyse text data biosequence data etc see later dirichlet multinomial model posterior predictive plugin predictive figure posterior predictive distributions seeing plugin approximation figure generated betabinompostpreddemo 
[generative, models, discrete, data, dirichlet-multinomial, model, likelihood] suppose observe dice rolls assume data iid likelihood form number times event occured sufficient statistics model likelihood multinomial model form irrelevant constant factor 
[generative, models, discrete, data, dirichlet-multinomial, model, prior] since parameter vector lives dimensional probability simplex need prior support simplex ideally would also conjugate fortunately dirichlet distribution section satisﬁes criteria use following prior dir 
[generative, models, discrete, data, dirichlet-multinomial, model, posterior] multiplying likelihood prior ﬁnd posterior also dirichlet dir chapter generative models discrete data see posterior obtained adding prior hyper parameters pseudo counts empirical counts derive mode posterior map estimate using calculus however must enforce constraint using lagrange multiplier constrained objective function lagrangian given log likelihood plus log prior plus constraint log log simplify notation deﬁne taking derivatives respect yields original constraint taking derivatives respect yields solve using sum one constraint equivalent sample size prior thus map estimate given consistent equation use uniform prior recover mle empirical fraction times face shows need explicitly enforce constraint since gradient objective form negative values would reduce objective rather maximize course preclude setting indeed optimal solution dirichlet multinomial model 
[generative, models, discrete, data, dirichlet-multinomial, model, posterior, predictive] posterior predictive distribution single multinoulli trial given following expression components except see also exercise expression avoids zero count problem saw section fact form bayesian smoothing even important multinomial case binary case since likelihood data sparsity increases start partitioning data many categories worked example language models using bag words one application bayesian smoothing using dirichlet multinomial model language modeling means predicting words might occur next sequence take simple minded approach assume word sampled independently words using cat distribution called bag words model given past sequence words predict one likely come next example suppose observe following sequence part children nursery rhyme mary little lamb little lamb little lamb mary little lamb fleece white snow furthermore suppose vocabulary consists following words mary lamb little big fleece white black snow rain unk unk stands unknown represents words appear elsewhere list encode line nursery rhyme ﬁrst strip punctuation remove stop words etc also perform stemming means reducing words base form stripping ﬁnal plural words ing verbs running becomes run example words need stemming finally replace word index vocabulary get ignore word order count often word occurred resulting histogram word counts chapter generative models discrete data token word mary lamb little big ﬂeece white black snow rain unk count denote counts use dir prior posterior predictive set get modes predictive distribution lamb unk note words big black rain predicted occur non zero probability future even though never seen later see sophisticated language models 
[generative, models, discrete, data, naive, bayes, classiﬁers] section discuss classify vectors discrete valued features number values feature number features use generative approach requires specify class conditional distribution simplest approach assume features conditionally independent given class label allows write class conditional density product one dimensional densities resulting model called naive bayes classiﬁer nbc model called naive since expect features independent even conditional class label however even naive bayes assumption true often results classiﬁers work well domingos pazzani one reason model quite simple parameters classes features hence relatively immune overﬁtting form class conditional density depends type feature give possibilities case real valued features use gaussian distribution mean feature objects class variance case binary features use bernoulli distribution ber probability feature occurs class sometimes called multivariate bernoulli naive bayes model see application naive bayes classiﬁers case categorical features model use multinoulli distribution cat histogram possible values class obviously handle kinds features use different distributional assumptions also easy mix match features different types 
[generative, models, discrete, data, naive, bayes, classiﬁers, model, ﬁtting] discuss train naive bayes classiﬁer usually means computing mle map estimate parameters however also discuss compute full posterior mle nbc probability single data case given hence log likelihood given log log log see expression decomposes series terms one concerning terms containing hence optimize parameters separately equation mle class prior given number examples class mle likelihood depends type distribution choose use feature simplicity let suppose features binary ber case mle becomes extremely simple implement model ﬁtting procedure see algorithm pseudo code naivebayesfit matlab code algorithm obviously takes time method easily generalized handle features mixed type simplicity one reason method widely used figure gives example classes binary features representing presence absence words bag words model plot visualizes vectors two classes big spike index corresponds word subject occurs classes probability section discuss ﬁlter uninformative features chapter generative models discrete data algorithm fitting naive bayes classiﬁer binary features class label example figure class conditional densities two document classes corresponding windows windows figure generated naivebayesbowdemo bayesian naive bayes trouble maximum likelihood overﬁt example consider example figure feature corresponding word subject call feature always occurs classes estimate happen encounter new email word algorithm crash burn since ﬁnd classes another manifestation black swan paradox discussed section simple solution overﬁtting bayesian simplicity use factored prior use dir prior beta prior often take corresponding add one laplace smoothing naive bayes classiﬁers combining factored likelihood equation factored prior gives following factored posterior dir beta words compute posterior update prior counts empirical counts likelihood straightforward modify algorithm handle version model ﬁtting 
[generative, models, discrete, data, naive, bayes, classiﬁers, using, model, prediction] test time goal compute correct bayesian procedure integrate unknown parameters cat ber fortunately easy least posterior dirichlet particular equa tion know posterior predictive density obtained simply plugging posterior mean parameters hence approximated posterior single point may map estimate posterior predictive density obtained simply plugging parameters yield virtually identical rule chapter generative models discrete data difference replaced posterior mean posterior mode mle however small difference important practice since posterior mean result less overﬁtting see section 
[generative, models, discrete, data, naive, bayes, classiﬁers, log-sum-exp, trick] discuss one important practical detail arises using generative classiﬁers kind compute posterior class labels using equation using appropriate class conditional density plug approximation unfortunately naive implementation equation fail due numerical underﬂow problem often small number especially high dimensional vector require probability observing particular high dimensional vector small obvious solution take logs applying bayes rule follows log log log log however requires evaluating following expression log log log add log domain fortunately factor largest term represent remaining numbers relative example log log log general log log log max called log sum exp trick widely used see function logsumexp implementation trick used algorithm gives pseudo code using nbc compute see naivebayespredict matlab code note need log sum exp trick want compute since maximize unnormalized quantity log log 
[generative, models, discrete, data, naive, bayes, classiﬁers, feature, selection, using, mutual, information] since nbc ﬁtting joint distribution potentially many features suffer overﬁtting addition run time cost may high applications one common approach tackling problems perform feature selection remove irrelevant features help much classiﬁcation problem simplest approach feature selection evaluate relevance feature separately naive bayes classiﬁers algorithm predicting naive bayes classiﬁer binary features log log else log exp logsumexp argmax take top chosen based tradeoff accuracy complexity approach known variable ranking ﬁltering screening one way measure relevance use mutual information section feature class label log mutual information thought reduction entropy label distribution observe value feature features binary easy show exercise computed follows log log quantities computed product ﬁtting naive bayes classiﬁer figure illustrates happens apply binary bag words dataset used figure see words highest mutual information much discriminative words probable example probable word classes subject always occurs newsgroup data always subject line obviously discriminative words highest class label decreasing order windows microsoft dos motif makes sense since classes correspond microsoft windows windows 
[generative, models, discrete, data, naive, bayes, classiﬁers, classifying, documents, using, bag, words] document classiﬁcation problem classifying text documents different categories one simple approach represent document binary vector records whether word present iff word occurs document otherwise use following class conditional density ber chapter generative models discrete data class prob class prob highest subject subject windows windows microsoft dos motif window table list likely words class windows class windows also show words highest mutual information class label produced naivebayesbowdemo called bernoulli product model binary independence model however ignoring number times word occurs document loses formation mccallum nigam accurate representation counts number occurrences word speciﬁcally let vector counts document number terms document class conditional densities use multinomial distribution implicitly assumed document length independent class probability generating word documents class parameters satisfy constraint class although multinomial classiﬁer easy train easy use test time work particularly well document classiﬁcation one reason take account burstiness word usage refers phenomenon words never appear given document appear likely appear words occur bursts multinomial model cannot capture burstiness phenomenon see note equation form since rare words becomes increasingly unlikely generate many frequent words decay rate fast see intuitively note frequent words function words speciﬁc class chance word occuring pretty much matter many time previously occurred modulo document length independence assumption reasonable common words however since rare words ones matter classiﬁcation purposes ones want model carefully various hoc heuristics proposed improve performance multinomial document classiﬁer rennie present alternative class conditional density performs well hoc methods yet probabilistically sound madsen since equation models word independently model often called naive bayes classiﬁer although technically features independent constraint naive bayes classiﬁers suppose simply replace multinomial class conditional density dirichlet compound multinomial dcm density deﬁned follows dir equation derived equation surprisingly simple change needed capture burstiness phenomenon intuitive reason follows seeing one occurence word say word posterior counts gets updated making another occurence word likely contrast ﬁxed occurences word independent multinomial model corresponds drawing ball urn colors ball recording color replacing contrast dcm model corresponds drawing ball recording color replacing one additional copy called polya urn using dcm class conditional density gives much better results using multinomial performance comparable state art methods described madsen disadvantage ﬁtting dcm model complex see minka elkan details 
[generative, models, discrete, data, exercises] exercise mle bernoulli binomial model derive equation optimizing log likelihood equation exercise marginal likelihood beta bernoulli model equation showed marginal likelihood ratio normalizing constants derive alternative derivation fact chain rule probability section showed posterior predictive distribution data seen far suppose show reduces equation using fact integers chapter generative models discrete data exercise posterior predictive beta binomial model recall equation posterior predictive beta binomial given prove reduces hence show hint use fact exercise beta updating censored likelihood source gelman suppose toss coin times let number heads observe fewer heads know exactly many let prior probability heads beta compute posterior normalization constants derive expression proportional hint answer mixture distribution exercise uninformative prior log odds ratio let logit log show beta hint use change variables formula exercise mle poisson distribution poisson pmf deﬁned poi rate parameter derive mle exercise bayesian analysis poisson distribution exercise deﬁned poisson distribution rate derived mle perform conjugate bayesian analysis derive posterior assuming conjugate prior hint posterior also gamma distribution posterior mean tend recall mean distribution exercise mle uniform distribution source kaelbling consider uniform distribution centered width density function given naive bayes classiﬁers given data set maximum likelihood estimate call probability would model assign new data point using see problem approach brieﬂy suggest words better approach exercise bayesian analysis uniform distribution consider uniform distribution unif maximum likelihood estimate max saw exercise unsuitable predicting future data since puts zero probability mass outside training data exercise perform bayesian analysis uniform distribution following minka conjugate prior pareto distribution pareto deﬁned section given pareto prior joint distribution max let max evidence probability samples came uniform distribution derive posterior show expressed pareto distribution exercise taxicab tramcar problem suppose arrive new city see taxi numbered many taxis city let assume taxis numbered sequentially integers starting unknown upper bound number taxis simplicity also count without changing analysis hence likelihood function uniform distribution goal estimate use bayesian analysis exercise suppose see one taxi numbered using improper non informative prior form posterior compute posterior mean mode median number taxis city quantities exist rather trying compute point estimate number taxis compute predictive density next taxicab number using hyper parameters updated hyper parameters consider case using equation write expression use non informative prior use predictive density formula compute probability next taxi see say next day number compute brieﬂy describe sentences ways might make model accurate prediction chapter generative models discrete data exercise bayesian analysis exponential distribution lifetime machine modeled exponential distribution unknown parameter likelihood show mle suppose observe lifetimes years different iid machines mle given data assume expert believes prior distribution also exponential expon choose prior parameter call hint recall gamma distribution form mean posterior exponential prior conjugate exponential likelihood posterior mean explain mle posterior mean differ reasonable example exercise map estimation bernoulli non conjugate priors source jaakkola book discussed bayesian inference bernoulli rate parameter prior beta know prior map estimate given number heads number tails total number trials consider following prior believes coin fair slightly biased towards tails otherwise derive map estimate prior function suppose true parameter prior leads better estimate small prior leads better estimate large exercise posterior predictive distribution batch data dirichlet multinomial model equation gave posterior predictive distribution single multinomial trial using dirichlet prior consider predicting batch new data consisting single multinomial trials think predicting next words sentence assuming drawn iid derive expression naive bayes classiﬁers answer function old new counts sufficient statistics deﬁned old new hint recall vector counts marginal likelihood evidence given exercise posterior predictive dirichlet multinomial source koller suppose compute empirical distribution letters roman alphabet plus space character distribution values samples suppose see letter times assume dir suppose samples saw times times times assume dir show work exercise setting beta hyper parameters suppose believe var using equation solve terms values get exercise setting beta hyper parameters source draper suppose believe write program solve terms hint write function pdf one unknown write probability mass contained interval integral minimize squared discrepancy values get equivalent sample size prior exercise marginal likelihood beta binomial uniform prior suppose toss coin times observe heads let bin beta show marginal likelihood hint integer exercise bayes factor coin tossing suppose toss coin times observe heads let null hypothesis coin fair alternative coin bias unif derive bayes factor favor biased coin hypothesis hint see exercise exercise irrelevant features naive bayes source jaakkola let word occurs document otherwise let estimated probability word occurs documents class log likelihood document chapter generative models discrete data belongs class log log log log log log number words vocabulary write succintly log bit vector log log log see linear classiﬁer since class conditional density linear function inner product parameters assuming write expression log posterior odds ratio log terms features parameters intuitively words occur classes discriminative therefore affect beliefs class label consider particular word state conditions equivalently conditions presence absence test document effect class posterior word ignored classiﬁer hint using previous result ﬁgure posterior odds ratio posterior mean estimate using beta prior given sum documents class consider particular word suppose always occurs every document regardless class let documents class number documents class since get much non spam spam example class imbalance use estimate word ignored classiﬁer explain ways think encourage irrelevant words ignored exercise class conditional densities binary data consider generative classiﬁer classes class conditional density uniform class prior suppose features binary assume features conditionally independent naive bayes assumption write ber requires parameters naive bayes classiﬁers consider different model call full model features fully dependent make factorization assumptions might represent case many parameters needed represent assume number features ﬁxed let training cases sample size small model naive bayes full likely give lower test set error sample size large model naive bayes full likely give lower test set error computational complexity ﬁtting full naive bayes models function use big notation fitting model means computing mle map parameter estimates may assume convert bit vector array index time computational complexity applying full naive bayes models test time single test case suppose test case missing data let visible features size hidden missing features size computational complexity computing full naive bayes models function exercise mutual information naive bayes classiﬁers binary features derive equation exercise fitting naive bayes spam ﬁlter hand source daphne koller consider naive bayes model multivariate bernoulli version spam classiﬁca tion vocabulary secret offer low price valued customer today dollar million sports for play healthy pizza following example spam messages million dollar offer secret offer today secret secret normal messages low price valued customer play secret sports today sports healthy low price pizza give mles following parameters spam secret spam secret non spam sports non spam dollar spam 
[gaussian, models, introduction] chapter discuss multivariate gaussian multivariate normal mvn widely used joint probability density function continuous variables form basis many models encounter later chapters unfortunately level mathematics chapter higher many chapters particular rely heavily linear algebra matrix calculus price one must pay order deal high dimensional data beginners may choose skip sections marked addition since many equations chapter put box around particularly important 
[gaussian, models, introduction, notation] let brieﬂy say words notation denote vectors boldface lower case letters denote matrices boldface upper case letters denote entries matrix non bold upper case letters vectors assumed column vectors unless noted otherwise use denote column vector created stacking scalars similarly write left hand side tall column vector mean stack along rows usually written rather ugly write left hand side matrix mean stack along columns creating matrix 
[gaussian, models, introduction, basics] recall section pdf mvn dimensions deﬁned following exp chapter gaussian models figure visualization dimensional gaussian density major minor axes ellipse deﬁned ﬁrst two eigenvectors covariance matrix namely based figure bishop expression inside exponent mahalanobis distance data vector mean vector gain better understanding quantity performing eigendecomposition write uλu orthonormal matrix eigenvectors satsifying diagonal matrix eigenvalues using eigendecomposition column containing eigenvector hence rewrite mahalanobis distance follows recall equation ellipse hence see contours equal probability density gaussian lie along ellipses illustrated figure eigenvectors determine orientation ellipse eigenvalues determine elogonated general see mahalanobis distance corresponds euclidean distance transformed coordinate system shift rotate introduction 
[gaussian, models, introduction, mle, mvn] describe one way estimate parameters mvn using mle later sections discuss bayesian inference parameters mitigate overﬁtting provide measure conﬁdence estimates theorem mle gaussian iid samples mle parameters given mle mle mle empirical mean empirical covariance univariate case get following familiar results proof prove result need several results matrix algebra summarize equations vectors matrices also notation refers trace matrix sum diagonals log abc cab bca last equation called cyclic permutation property trace operator using derive widely used trace trick reorders scalar inner product follows axx chapter gaussian models proof begin proof log likelihood log log precision matrix using substitution chain rule calculus hence mle empirical mean use trace trick rewrite log likelihood follows log log scatter matrix centered taking derivatives expression respect yields empirical covariance matrix centered plug mle since parameters must simultaneously optimized get standard equation mle covariance matrix gaussian discriminant analysis 
[gaussian, models, introduction, maximum, entropy, derivation, gaussian] section show multivariate gaussian distribution maximum entropy subject speciﬁed mean covariance see also section one reason gaussian widely used ﬁrst two moments usually reliably estimate data want distribution captures properties otherwise makes addtional assumptions possible simplify notation assume mean zero pdf form exp deﬁne see form equation differential entropy distribution using log base given show mvn maximum entropy amongst distributions speciﬁed variance theorem let density satisfying let proof cover thomas log log log key step equation marked follows since yield moments quadratic form encoded log 
[gaussian, models, gaussian, discriminant, analysis] one important application mvns deﬁne class conditional densities generative classiﬁer resulting technique called gaussian discriminant analysis gda even though generative discriminative classiﬁer see section distinction diagonal equivalent naive bayes chapter gaussian models height weight red female blue male height weight red female blue male figure height weight data visualization gaussians class probability mass inside ellipse figure generated gaussheightweight classify feature vector using following decision rule derived equation argmax log log compute probability class conditional density measuring distance center class using mahalanobis distance thought nearest centroids classiﬁer example figure shows two gaussian class conditional densities representing height weight men women see features correlated expected tall people tend weigh ellipses class contain probability mass uniform prior classes classify new test vector follows argmin 
[gaussian, models, gaussian, discriminant, analysis, quadratic, discriminant, analysis, qda] posterior class labels given equation gain insight model plugging deﬁnition gaussian density follows exp exp thresholding results quadratic function result known quadratic discriminant analysis qda figure gives examples decision boundaries look like gaussian discriminant analysis parabolic boundary linear quadratic figure quadratic decision boundaries class case figure generated discrimanalysisdboundariesdemo figure softmax distribution different temperatures temperature high left distribution uniform whereas temperature low right distribution spiky mass largest element figure generated softmaxdemo 
[gaussian, models, gaussian, discriminant, analysis, linear, discriminant, analysis, lda] consider special case covariance matrices tied shared across classes case simplify equation follows exp exp log exp since quadratic term independent cancel numerator denominator deﬁne log chapter gaussian models write softmax function deﬁned follows softmax function called since acts bit like max function see let divide constant called temperature ﬁnd argmax otherwise words low temperatures distribution spends essentially time probable state whereas high temperatures visits states uniformly see figure illustration note terminology comes area statistical physics common use boltzmann distribution form softmax function interesting property equation take logs end linear function reason linear cancels numerator denominator thus decision boundary two classes say straight line hence technique called linear discriminant analysis lda derive form line follows see figure examples alternative ﬁtting lda model deriving class posterior directly cat weight matrix called multi class logistic regression multinomial logistic regression discuss model detail section difference two approaches explained section 
[gaussian, models, gaussian, discriminant, analysis, two-class, lda] gain insight meaning equations let consider binary case case posterior given sigm abbreviation lda could either stand linear discriminant analysis latent dirichlet allocation sec tion hope meaning clear text language modeling community model called maximum entropy model reasons explained section gaussian discriminant analysis linear boundary linear boundaries figure linear decision boundaries class case figure generated discrimanalysisdboundariesdemo figure geometry lda class case sigm refers sigmoid function equation log log deﬁne log chapter gaussian models hence sigm closely related logistic regression discuss section ﬁnal decision rule follows shift project onto line see result positive negative direction classify point based whether projection closer illustrated figure furthemore half way means make gets closer line belongs class priori conversely boundary shifts right thus see class prior changes decision threshold overall geometry claimed similar argument applies multi class case magnitude determines steepness logistic function depends well separated means relative variance psychology signal detection theory common deﬁne discriminability signal background noise using quantity called prime mean signal mean noise standard deviation noise large signal easier discriminate noise 
[gaussian, models, gaussian, discriminant, analysis, mle, discriminant, analysis] discuss discriminant analysis model simplest way use maximum likelihood log likelihood function follows log log log see factorizes term terms hence estimate parameters separately class prior naive bayes class conditional densities partition data based class label compute mle gaussian see discrimanalysisfit matlab implementation model make predictions using discrimanalysispredict uses plug approximation 
[gaussian, models, gaussian, discriminant, analysis, strategies, preventing, overﬁtting] speed simplicity mle method one greatest appeals however mle badly overﬁt high dimensions particular mle full covariance matrix singular even mle ill conditioned meaning close singular several possible solutions problem gaussian discriminant analysis use diagonal covariance matrix class assumes features conditionally independent equivalent using naive bayes classiﬁer section use full covariance matrix force classes example parameter tying parameter sharing equivalent lda section use diagonal covariance matrix forced shared called diagonal covariance lda discussed section use full covariance matrix impose prior integrate use conjugate prior done closed form using results section analogous bayesian naive bayes method section see minka details fit full diagonal covariance matrix map estimation discuss two different kinds prior project data low dimensional subspace gaussians see sec tion way ﬁnd best discriminative linear projection discuss options 
[gaussian, models, gaussian, discriminant, analysis, regularized, lda] suppose tie covariance matrices lda furthermore perform map estimation using inverse wishart prior form diag mle see section diag mle mle controls amount regularization related strength prior see section details technique known regularized discriminant analysis rda hastie evaluate class conditional densities need compute hence mle impossible compute however use svd section get around show note trick cannot applied qda nonlinear function let udv svd design matrix orthogonal matrix diagonal matrix size furthermore deﬁne matrix like design matrix lower dimensional space since assume also deﬁne mean data reduced space recover original mean using since deﬁnitions chapter gaussian models rewrite mle follows mle empirical covariance hence rewrite map estimate map diag note however never need actually compute matrix map equation tells classify using lda need compute exp log compute crucial term rda without inverting matrix follows map mean matrix data belonging class see rdafit code 
[gaussian, models, gaussian, discriminant, analysis, diagonal, lda] simple alternative rda tie covariance matrices lda use diagonal covariance matrix class called diagonal lda model equivalent rda corresponding discriminant function follows compare equation log log typically set pooled empirical variance feature pooled across classes deﬁned high dimensional settings model work much better lda rda bickel levina gaussian discriminant analysis number genes misclassification error test train figure error versus amount shrinkage nearest shrunken centroid classiﬁer applied srbct gene expression data based figure hastie figure generated shrunkencentroidssrbctdemo 
[gaussian, models, gaussian, discriminant, analysis, nearest, shrunken, centroids, classiﬁer] one drawback diagonal lda depends features high dimensional problems might prefer method depends subset features reasons accuracy interpretability one approach use screening method perhaps based mutual information section discuss another approach problem known nearest shrunken centroids classiﬁer hastie basic idea perform map estimation diagonal lda sparsity promoting laplace prior see section precisely deﬁne class speciﬁc feature mean terms class independent feature mean class speciﬁc offset thus put prior terms encourage strictly zero compute map estimate feature ﬁnd feature play role classiﬁcation decision since independent thus features discriminative automatically ignored details found hastie greenshtein park see shrunkencentroidsfit code let give example method action based hastie consider problem classifying gene expression dataset genes classes training samples test samples using diagonal lda classiﬁer produces errors test set using nearest shrunken centroids classiﬁer produced errors test set range values see figure importantly model sparse hence interpretable figure plots unpenalized estimate difference gray well shrunken estimates blue estimates computed using value estimated see genes used original consider even harder problem genes training set patients test set patients different types cancer ramaswamy hastie hastie report nearest shrunken centroids produced errors test chapter gaussian models class class class class figure proﬁle shrunken centroids corresponding optimal fig ure selects genes based figure hastie figure generated shrunkencentroidssrbctdemo set using genes rda section produced errors test set using genes pmtk function cancerhighdimclassifdemo used reproduce numbers 
[gaussian, models, inference, jointly, gaussian, distributions] given joint distribution useful able compute marginals conditionals discuss give applications operations take time worst case see section faster methods inference jointly gaussian distributions 
[gaussian, models, inference, jointly, gaussian, distributions, statement, result] theorem marginals conditionals mvn suppose jointly gaussian parameters marginals given posterior conditional given equation crucial importance book put box around easily ﬁnd proof see section see marginal conditional distributions gaussian marginals extract rows columns corresponding conditional bit work however complicated conditional mean linear function conditional covariance constant matrix independent give three different equivalent expressions posterior mean two different equivalent expressions posterior covariance one useful different circumstances 
[gaussian, models, inference, jointly, gaussian, distributions, examples] give examples equations action make seem intuitive marginals conditionals gaussian let consider example covariance matrix marginal gaussian obtained projecting joint distribution onto line chapter gaussian models figure joint gaussian distribution correlation coefficient plot contour principal axes unconditional marginal conditional obtained slicing height figure generated gaussconditionddemo suppose observe conditional obtained slicing joint distribution line see figure get figure show example see makes sense since means believe increases beyond mean increases also see var also makes sense uncertainty gone since learned something indirectly observing get since conveys information uncorrelated hence independent interpolating noise free data suppose want estimate function deﬁned interval observed points assume data noise free want interpolate function goes exactly data see section noisy data case question function behave observed data points often reasonable assume unknown function smooth chapter shall see encode priors functions update prior observed values get posterior functions section take simpler approach adequate map estimation functions deﬁned inputs follow presentation calvetti somersalo start discretizing problem first divide support function equal subintervals deﬁne inference jointly gaussian distributions figure interpolating noise free data using gaussian prior precision see also figure based figure calvetti somersalo figure generated gaussinterpdemo encode smoothness prior assuming average neighbors plus gaussian noise precision term controls much think function vary large corresponds belief function smooth small corresponds belief function quite wiggly vector form equation written follows second order ﬁnite difference matrix corresponding prior form exp henceforth assume scaled ignore term write precision matrix note although dimensional precision matrix rank thus improper prior known intrinsic gaussian random ﬁeld see section chapter gaussian models information however providing observe data points posterior proper let noise free observations function unknown function values without loss generality assume unknown variables ordered ﬁrst known variables partition matrix follows also partition precision matrix joint distribution using equation write conditional distribution follows note compute mean solving following system linear equations efficient since tridiagonal figure gives illustration equations see posterior mean equals observed data speciﬁed points smoothly interpolates desired also interesting plot pointwise marginal credibility intervals shown grey see variance goes move away data also see variance goes decrease precision prior terestingly effect posterior mean since cancels multiplying contrast consider noisy data section see prior precision affects smoothness posterior mean estimate marginal credibility intervals capture fact neighboring locations correlated represent drawing complete functions vectors posterior plotting shown thin lines figure quite smooth posterior mean prior penalizes ﬁrst order differences see section discussion point data imputation suppose missing entries design matrix columns correlated use observed entries predict missing entries figure shows simple example sampled data dimensional gaussian deliberately hid data row inferred missing entries given observed entries using true generating model precisely row compute indices hidden visible entries case compute marginal distribution missing variable plot mean distribution represents best guess true value entry inference jointly gaussian distributions observed imputed truth figure illustration data imputation left column visualization three rows data matrix missing entries middle column mean posterior predictive based partially observed data row true model parameters right column true values figure generated gaussimputationdemo sense minimizes expected squared error see section details figure shows estimates quite close truth course expected value equal observed value use var measure conﬁdence guess although shown alternatively could draw multiple samples called multiple imputation addition imputing missing entries may interested computing like lihood partially observed row table computed using equation useful detecting outliers atypical observations 
[gaussian, models, inference, jointly, gaussian, distributions, information, form] suppose one show mean vector cov covariance matrix called moment parameters distribution however sometimes useful use canonical parameters natural parameters deﬁned convert back moment parameters using using canonical parameters write mvn information form exponential family form deﬁned section exp use notation distinguish moment parameterization also possible derive marginalization conditioning formulas information form ﬁnd chapter gaussian models thus see marginalization easier moment form conditioning easier information form another operation signiﬁcantly easier information form multiplying two gaussians one show however moment form things much messier 
[gaussian, models, inference, jointly, gaussian, distributions, proof, result] prove theorem readers intimidated heavy matrix algebra safely skip section ﬁrst derive results need elsewhere book return proof end inverse partitioned matrix using schur complements key tool need way invert partitioned matrix done using following result theorem inverse partitioned matrix consider general partitioned matrix assume invertible say schur complement wrt equation called partitioned inverse formula proof could block diagonalize would easier invert zero top right block pre multiply follows inference jointly gaussian distributions similarly zero bottom left post multiply follows putting together get taking inverse sides yields hence substituting deﬁnitions get alternatively could decomposed matrix terms yielding matrix inversion lemma derive useful corollaries result corollary matrix inversion lemma consider general partitioned matrix assume invertible chapter gaussian models ﬁrst two equations known matrix inversion lemma sherman morrison woodbury formula third equation known matrix determinant lemma typical application machine learning statistics following let diagonal matrix let size let lhs takes time compute rhs takes time compute another application concerns computing rank one update inverse matrix let scalar column vector row vector useful incrementally add data vector design matrix want update sufficient statistics one derive analogous formula removing data vector proof prove equation simply equate top left block equation equa tion prove equation simple equate top right blocks equations proof equation left exercise proof gaussian conditioning formulas return original goal derive equation let factor joint follows exp xamp using equation exponent becomes exp exp exp form exp quadratic form exp quadratic form linear gaussian systems hence successfully factorized joint parameters conditional distribution read equations using also use fact check normalization constants correct dim dim leave proof forms result equation exercise 
[gaussian, models, linear, gaussian, systems] suppose two variables let hidden variable noisy observation let assume following prior likelihood matrix size example linear gaussian system represent schematically meaning generates section show invert arrow infer state result give several examples ﬁnally derive result see many applications results later chapters 
[gaussian, models, linear, gaussian, systems, statement, result] theorem bayes rule linear gaussian systems given linear gaussian system equation posterior given following chapter gaussian models addition normalization constant given proof see section 
[gaussian, models, linear, gaussian, systems, examples] section give example applications result inferring unknown scalar noisy measurements suppose make noisy measurements underlying quantity let assume measurement noise ﬁxed precision likelihood let use gaussian prior value unknown source want compute convert form lets apply bayes rule gaussians deﬁning row vector diag get equations quite intuitive posterior precision prior precision plus units measurement precision also posterior mean convex combination mle prior mean makes clear posterior mean compromise mle prior prior weak relative signal strength small relative put weight mle prior strong relative signal strength large relative put weight prior illustrated figure similar analogous results beta binomial model figure note posterior mean written terms measurements precision like one measurement value precision rewrite results terms posterior variance rather posterior precision linear gaussian systems prior variance prior lik post prior variance prior lik post figure inference given noisy observation strong prior posterior mean shrunk towards prior mean weak prior posterior mean similar mle figure generated gaussinferparamsmeand follows prior variance posterior variance also compute posterior sequentially updating observation rewrite posterior seeing single observation follows deﬁne variances likelihood prior posterior rewrite posterior mean different ways chapter gaussian models ﬁrst equation convex combination prior data second equation prior mean adjusted towards data third equation data adjusted towards prior mean called shrinkage equivalent ways expressing tradeoff likelihood prior small relative corresponding strong prior amount shrinkage large see figure whereas large relative corresponding weak prior amount shrinkage small see figure another way quantify amount shrinkage terms signal noise ratio deﬁned follows snr true signal observed signal noise term inferring unknown vector noisy measurements consider vector valued observations gaussian prior setting using effective observation precision see figure example think representing true unknown location object space missile airplane noisy observations radar blips receive blips better able localize source section see extend example track moving objects using famous kalman ﬁlter algorithm suppose multiple measuring devices want combine together known sensor fusion multiple observations different covariances cor responding sensors different reliabilities posterior appropriate weighted average data consider example figure use uninformative prior namely get noisy observations compute figure set sensors equally reliable case posterior mean half way two observations figure set sensor reliable sensor case posterior mean closer figure set sensor reliable component vertical direction sensor reliable component horizontal direction case posterior mean uses vertical component horizontal component linear gaussian systems data prior post obs figure illustration bayesian inference mean gaussian data generated assume sensor noise covariance known unknown black cross represents prior show posterior data points observed figure generated gaussinferparamsmeand figure observe red cross green cross infer black cross equally reliable sensors posterior mean estimate two circles sensor reliable estimate shifts towards green circle sensor reliable vertical direction sensor reliable horizontal direction estimate appropriate combination two measurements figure generated sensorfusiond note technique crucially relies modeling uncertainty sensor comput ing unweighted average would give wrong result however assumed sensor precisions known model uncertainty well see section details interpolating noisy data revisit example section time longer assume noise free observations instead let assume obtain noisy observations without loss generality assume correspond model setup linear chapter gaussian models gaussian system observation noise projection matrix selects observed elements example using improper prior easily compute posterior mean variance figure plot posterior mean posterior variance posterior samples see prior precision effects posterior mean well posterior variance particular strong prior large estimate smooth uncertainty low weak prior small estimate wiggly uncertainty away data high posterior mean also computed solving following optimization problem min deﬁned notational simplicity recognize discrete approximation following problem min ﬁrst derivative ﬁrst term measures data second term penalizes functions wiggly example tikhonov regularization popular approach functional data analysis see chapter sophisticated approaches enforce higher order smoothness resulting samples look less jagged 
[gaussian, models, linear, gaussian, systems, proof, result] derive equation basic idea derive joint distribution use results section computing detail proceed follows log joint distribution follows dropping irrelevant constants log clearly joint gaussian distribution since exponential quadratic form expanding quadratic terms involving ignoring linear constant terms digression wishart distribution figure interpolating noisy data noise variance using gaussian prior precision see also figure based figure calvetti somersalo figure generated gaussinterpnoisydemo see also splinebasisdemo precision matrix joint deﬁned equation using fact 
[gaussian, models, digression, wishart, distribution] wishart distribution generalization gamma distribution positive deﬁnite matrices press press said wishart distribution ranks next multi variate normal distribution order importance usefuleness multivariate statistics mostly use model uncertainty covariance matrices inverses pdf wishart deﬁned follows exp called degrees freedom scale matrix shall get intuition parameters shortly normalization constant distribution chapter gaussian models requires integrating symmetric matrices following formidable expression multivariate gamma function hence normalization constant exists hence pdf well deﬁned connection wishart distribution gaussian particular let scatter matrix wishart distribution hence generally one show mean mode given mean mode mode exists wishart reduces gamma distribution 
[gaussian, models, digression, wishart, distribution, inverse, wishart, distribution] recall showed exercise similarly inverse wishart multidimensional generalization inverse gamma deﬁned follows exp one show distribution properties mean mode reduces inverse gamma inferring parameters mvn dof figure visualization wishart distribution left samples wishart distribution right plots marginals gamma approximate sample based marginal correlation coefficient lot uncertainty value correlation coefficient see almost uniform distribution sampled matrices highly variable nearly singular increases sampled matrices concentrated prior figure generated wiplotdemo 
[gaussian, models, digression, wishart, distribution, visualizing, wishart, distribution] since wishart distribution matrices hard plot density function however easily sample case use eigenvectors resulting matrix deﬁne ellipse explained section see figure examples higher dimensional matrices plot marginals distribution diagonals wishart distributed matrix gamma distributions easy plot hard general work distribution diagonal elements sample matrices distribution compute distribution empirically particular convert sampled matrix correlation matrix thus compute monte carlo approximation section expected correlation coefficients converts matrix correlation matrix use kernel density estimation section produce smooth approximation univariate density plotting purposes see figure examples 
[gaussian, models, inferring, parameters, mvn] discuss compute results bit complex prove useful later book feel free skip section ﬁrst reading likelihood likelihood given exp one show hence rewrite likelihood follows exp exp use form prior obvious prior use following unfortunately conjugate likelihood see note appear together non factorized way likelihood hence also coupled together posterior prior sometimes called semi conjugate conditionally conjugate since conditionals individually conjugate create full conjugate prior need use prior dependent use joint distribution form looking form likelihood equation equation see natural conjugate inferring parameters mvn prior form normal inverse wishart niw distribution deﬁned follows niw exp exp exp multivariate gamma function parameters niw interpreted follows prior mean strongly believe prior proportional prior mean strongly believe prior one show minka improper uninformative prior form lim niw practice often better use weakly informative data dependent prior common choice see chipman fraley raftery use diag ensure set small number although prior four parameters really three free parameters since uncertainty mean proportional variance particular believe variance large uncertainty must large makes sense intuitively since data large spread may hard pin mean see also exercise see three free parameters explicitly want separate control conﬁdence must use semi conjugate prior chapter gaussian models posterior posterior shown exercise niw updated parameters niw deﬁned uncentered sum squares matrix easier update incrementally centered version result actually quite intuitive posterior mean convex combination prior mean mle strength posterior scatter matrix prior scatter matrix plus empirical scatter matrix plus extra term due uncertainty mean creates virtual scatter matrix posterior mode mode joint distribution following form argmax set reduces argmax corresponding estimate almost equation differs denominator mode joint mode marginal posterior marginals posterior marginal simply mode mean marginal given map one show posterior marginal multivariate student distribution follows fact student distribution represented scaled mixture gaussians see equation inferring parameters mvn figure niχ distribution prior mean strongly believe prior variance strongly believe notice contour plot underneath surface shaped like squashed egg increase strength belief mean gets narrower increase strength belief variance gets narrower figure generated nixdemo posterior predictive posterior predictive given easily evaluated terms ratio marginal likelihoods turns ratio form multivariate student distribution niw dμd student wider tails gaussian takes account fact unknown however rapidly becomes gaussian like posterior scalar data specialise results case results widely used statistics literature section conventional use normal inverse chapter gaussian models wishart use normal inverse chi squared nix distribution deﬁned exp see figure plots along axis distribution shaped like gaussian along axis distribution shaped like contours joint density squashed egg appearance interestingly see contours peaked small values makes sense since data low variance able estimate mean reliably one show posterior given niχ posterior marginal posterior mean given posterior marginal student distribution follows scale mixture representation student posterior mean given let see results look use following uninformative prior niχ prior posterior form niχ mle sample standard deviation section show unbiased estimate variance hence marginal posterior mean given inferring parameters mvn posterior variance var square root called standard error mean var thus approximate posterior credible interval mean bayesian credible intervals discussed detail section contrasted frequentist conﬁdence intervals section bayesian test suppose want test hypothesis known value often given values called two sided one sample test simple way perform test check sure common scenario want test two paired samples mean precisely suppose want determine using data evaluate quantity follows called one sided paired test similar approach unpaired tests comparing difference binomial proportions see section calculate posterior must specify prior suppose use uninformative prior showed ﬁnd posterior marginal form let deﬁne following statistic denominator standard error mean see cdf standard student distribution complex approach perform bayesian model comparison compute bayes factor described section point null hypothesis alternative hypothesis see gonen rouder details chapter gaussian models connection frequentist statistics use uninformative prior turns bayesian analysis gives result derived using frequentist methods discuss frequentist statistics chapter speciﬁcally results see form sampling distribution mle reason student distribution symmetric ﬁrst two arguments hence statements posterior form statements sampling distribution consequently one sided value deﬁned sec tion returned frequentist test returned bayesian method see bayesttestdemo example despite superﬁcial similarity two results different interpretation bayesian approach unknown ﬁxed whereas frequentist approach unknown ﬁxed equivalences frequentist bayesian inference simple models using uninformative priors found box tiao see also section 
[gaussian, models, inferring, parameters, mvn, sensor, fusion, unknown, precisions] section apply results section problem sensor fusion case precision measurement device unknown generalizes results section measurement model assumed gaussian known precision unknown precision case turns give qualitatively different results yielding potentially multi modal posterior see presentation based minka suppose want pool data multiple sources estimate quantity reliability sources unknown speciﬁcally suppose two different measurement devices different precisions make two independent measurements device turn use non informative prior emulate using inﬁnitely broad gaussian terms known posterior would gaussian inferring parameters mvn number measurements number measurements result follows posterior precision sum measurement precisions posterior mean weighted sum prior mean data means however measurement precisions known initially estimate maximum likelihood log likelihood given log log mle obtained solving following simultaneous equations gives notice mle form posterior mean solve equations ﬁxed point iteration let initialize estimating using get iterate converge plug approximation posterior plotted figure weights sensor according estimated precision since sensor estimated much less reliable sensor effectively ignore sensor adopt bayesian approach integrate unknown precisions rather trying estimate compute use uninformative jeffrey priors chapter gaussian models since terms symmetric focus one key integral exp exploiting fact simpliﬁes exp recognize proportional integral unnormalized gamma density hence integral proportional normalizing constant gamma distribution get posterior becomes exact posterior plotted figure see two modes one near one near correspond beliefs sensor reliable one vice versa weight ﬁrst mode larger since data sensor agree seems slightly likely sensor reliable one obviously cannot reliable since disagree values reporting however bayesian solution keeps open possibility sensor reliable one two measurements cannot tell choosing sensor plug approximation results conﬁdence posterior narrow 
[gaussian, models, exercises] exercise uncorrelated imply independent let clearly dependent fact uniquely determined however show hint var exercise uncorrelated gaussian imply independent unless jointly gaussian let clear independent since function show inferring parameters mvn figure posterior plug approximation exact posterior figure generated sensorfusionunknownprec show cov thus uncorrelated dependent even though gaussian hint use deﬁnition covariance cov rule iterated expectation exercise correlation coefficient prove exercise correlation coefficient linearly related variables show parameters similarly show exercise normalization constant multidimensional gaussian prove normalization constant dimensional gaussian given exp hint diagonalize use fact write joint pdf product one dimensional gaussians transformed coordinate system need change variables formula finally use normalization constant univariate gaussians exercise bivariate gaussian let correlation coefficient show pdf given exp chapter gaussian models raw standarized whitened figure height weight data men standardized whitened exercise conditioning bivariate gaussian consider bivariate gaussian distribution correlation coefficient given simplify answer expressing terms assume exercise whitening standardizing load height weight data using rawdata dlmread heightweightdata txt ﬁrst col umn class label male female second column height third weight extract height weight data corresponding males fit gaussian male data using empirical mean covariance plot gaussian ellipse use gaussplotd superimposing scatter plot look like figure labeled datapoint index turn ﬁgure code standardizing data means ensuring empirical variance along dimension done computing empirical std dimension standardize data replot look like figure use axis equal turn ﬁgure code whitening sphereing data means ensuring empirical covariance matrix proportional data uncorrelated equal variance along dimension done computing data vector eigenvectors eigenvalues whiten data replot look like figure note whitening rotates data people move counter intuitive locations new coordinate system see person moves right hand side left exercise sensor fusion known variances suppose two sensors known different variances unknown mean suppose observe observations ﬁrst sensor observations inferring parameters mvn second sensor example suppose true temperature outside sensor precise low variance digital thermosensing device sensor imprecise high variance mercury thermometer let represent data sensors posterior assuming non informative prior simulate using gaussian precision give explicit expression posterior mean variance exercise derivation information form formulae marginalizing conditioning derive information form results section exercise derivation niw posterior derive equation hint one show matrix generalization operation called completing square derive corresponding result normal wishart model exercise bic gaussians source jaakkola bayesian information criterion bic penalized log likelihood function used model selection see section deﬁned bic log log number free parameters model number samples question see use choose full covariance gaussian gaussian diagonal covariance obviously full covariance gaussian higher likelihood may worth extra parameters improvement diagonal covariance matrix small use bic score choose model following section write log log scatter matrix empirical covariance trace matrix sum diagonals used trace trick derive bic score gaussian dimensions full covariance matrix simplify answer much possible exploiting form mle sure specify number free parameters derive bic score gaussian dimensions diagonal covariance matrix sure specify number free parameters hint digaonal case estimate except diagonal terms zero diag diag scalar case completing square means rewriting chapter gaussian models exercise gaussian posterior credible interval source degroot let unknown prior posterior seeing samples called credible interval bayesian analog conﬁdence interval big ensure interval centered width data hint recall probability mass gaussian within mean exercise map estimation gaussians source jaakkola consider samples gaussian random variable known variance unknown mean assume prior distribution also gaussian mean ﬁxed mean ﬁxed variance thus unknown calculate map estimate map state result without proof alternatively lot work compute derivatives log posterior set zero solve show number samples increase map estimate converges maximum likelihood estimate suppose small ﬁxed map estimator converge increase prior variance suppose small ﬁxed map estimator converge decrease prior variance exercise sequential recursive updating source duda unbiased estimates covariance dimensional gaussian based samples given clear takes time compute data points arrive one time efficient incrementally update estimates recompute scratch show covariance sequentially udpated follows much time take per sequential update use big notation show sequentially update precision matrix using hint notice update consists adding rank one matrix namely use matrix inversion lemma rank one updates equation repeat convenience inferring parameters mvn time complexity per update exercise likelihood ratio gaussians source source alpaydin consider binary classiﬁer class conditional densities mvn bayes rule log log log words log posterior ratio log likelihood ratio plus log prior ratio cases table derive expression log likelihood ratio log simplifying much possible form cov num parameters arbitrary shared shared axis aligned shared spherical exercise lda qda height weight data function discrimanalysisheightweightdemo ﬁts lda qda model height weight data compute misclassiﬁcation rate models training set turn numbers code exercise naive bayes mixed features consider class naive bayes classiﬁer one binary feature one gaussian feature ber let parameter vectors follows compute result vector numbers sums compute compute explain interesting patterns see results hint look parameter vector exercise decision boundary lda semi tied covariances consider generative classiﬁer class conditional densities form lda assume qda arbitrary consider class case gaussian ellipsoids shape one class wider derive expression simplifying much possible give geometric interpretation result possible exercise logistic regression lda qda source jaakkola suppose train following binary classiﬁers via maximum likelihood gaussi generative classiﬁer class conditional densities gaussian covariance matrices set identity matrix assume uniform gaussx gaussi covariance matrices unconstrained chapter gaussian models linlog logistic regression model linear features quadlog logistic regression model using linear quadratic features polynomial basis function expansion degree training compute performance model training set follows log note conditional log likelihood joint log likelihood want compare performance model write model must lower equal log likelihood training set training set words worse least far training set logprob concerned following model pairs state whether whether statement made might sometimes better sometimes worse also question brieﬂy sentences explain gaussi linlog gaussx quadlog linlog quadlog gaussi quadlog suppose measure performance terms average misclassiﬁcation rate training set true general implies explain exercise gaussian decision boundaries source duda let let class priors equal find decision region sketch result hint draw curves ﬁnd intersect find solutions equation hint recall solve quadratic equation use suppose parameters remain case inferring parameters mvn exercise qda classes consider three category classiﬁcation problem let prior probabilites class conditional densities multivariate normal densities parameters classify following points exercise scalar qda note solve exercise hand using computer matlab whatever either case show work consider following training set heights inches gender male female college students fit bayes classiﬁer data using maximum likelihood estimation estimate parameters class conditional likelihoods class prior values show work get partial credit make arithmetic error compute mle parameters called plug prediction would simple way extend technique multiple attributes per person height weight write proposed model equation 
[bayesian, statistics, introduction] seen variety different probability models discussed data discussed compute map parameter estimates argmax using variety different priors also discussed compute full posterior well posterior predictive density certain special cases later chapters discuss algorithms general case using posterior distribution summarize everything know set unknown variables core bayesian statistics chapter discuss approach statistics detail chapter discuss alternative approach statistics known frequentist classical statistics 
[bayesian, statistics, summarizing, posterior, distributions] posterior summarizes everything know unknown quantities section discuss simple quantities derived probability distribution posterior summary statistics often easier understand visualize full joint 
[bayesian, statistics, summarizing, posterior, distributions, map, estimation] easily compute point estimate unknown quantity computing posterior mean median mode section discuss use decision theory choose methods typically posterior mean median appropriate choice real valued quantity vector posterior marginals best choice discrete quantity however posterior mode aka map estimate popular choice reduces optimization problem efficient algorithms often exist futhermore map estimation interpreted non bayesian terms thinking log prior regularizer see section details although approach computationally appealing important point various drawbacks map estimation brieﬂy discuss provide motivation thoroughly bayesian approach study later chapter elsewhere book chapter bayesian statistics figure bimodal distribution mode untypical distribution thin blue vertical line mean arguably better summary distribution since near majority probability mass figure generated bimodaldemo skewed distribution mode quite different mean figure generated gammaplotdemo measure uncertainty obvious drawback map estimation indeed point estimate posterior mean median provide measure uncertainty many applications important know much one trust given estimate derive conﬁdence measures posterior discuss section plugging map estimate result overﬁtting machine learning often care predictive accuracy interpreting parameters models however model uncertainty parameters predictive distribution overconﬁdent saw several examples chapter see examples later overconﬁdence predictions particularly problematic situations may risk averse see section details mode untypical point choosing mode summary posterior distribution often poor choice since mode usually quite untypical distribution unlike mean median illustrated figure continuous space basic problem mode point measure zero whereas mean median take volume space account another example shown figure mode mean non zero skewed distributions often arise inferring variance parameters especially hierarchical models cases map estimate hence mle obviously bad estimate summarize posterior mode good choice answer use decision theory discuss section basic idea specify loss function loss incur truth estimate use loss optimal estimate posterior mode loss means get points make errors otherwise get nothing partial credit summarizing posterior distributions figure example transformation density nonlinear transform note mode transformed distribution transform original mode based exercise bishop figure generated bayeschangeofvar loss function continuous valued quantities often prefer use squared error loss corresponding optimal estimator posterior mean show section use robust loss function gives rise posterior median map estimation invariant reparameterization subtle problem map estimation result get depends rameterize probability distribution changing one representation another equivalent representation changes result desirable since units measurement arbitrary measuring distance use centimetres inches understand problem suppose compute posterior deﬁne distribution given equation repeat convenience term called jacobian measures change size unit volume passed let argmax map estimate general case argmax given example let exp derive distribution using monte carlo simulation see section result shown figure see original gaussian become squashed sigmoid nonlinearity particular see mode transformed distribution equal transform original mode chapter bayesian statistics see problem arises context map estimation consider following example due michael jordan bernoulli distribution typically parameterized mean suppose uniform prior unit interval data map estimate mode prior anywhere show different parameterizations pick different points interval arbitrarily first let new prior new mode arg max let new prior new mode arg max thus map estimate depends parameterization mle suffer since likelihood function probability density bayesian inference suffer problem either since change measure taken account integrating parameter space one solution problem optimize following objective function argmax fisher information matrix associated see section estimate parameterization independent reasons explained jermyn druilhet marin unfortunately optimizing equation often difficult minimizes appeal whole approach 
[bayesian, statistics, summarizing, posterior, distributions, credible, intervals] addition point estimates often want measure conﬁdence standard measure conﬁdence scalar quantity width posterior distribution measured using credible interval contiguous region standing lower upper contains posterior probability mass may many intervals choose one mass tail called central interval summarizing posterior distributions figure central interval hpd region beta posterior hpd based figure hoff figure generated betahpd posterior known functional form compute posterior central interval using cdf posterior example posterior gaussian denotes cdf gaussian illustrated figure justiﬁes common practice quoting credible interval form represents posterior mean represents posterior standard deviation good approximation course posterior always gaussian example coin example use uniform prior observe heads trials posterior beta distribution beta ﬁnd posterior credible interval see betacredibleint one line matlab code used compute know functional form draw samples posterior use monte carlo approximation posterior quantiles simply sort samples ﬁnd one occurs location along sorted list converges true quantile see mcquantiledemo demo people often confuse bayesian credible intervals frequentist conﬁdence intervals ever thing discuss section general credible intervals usually people want compute conﬁdence intervals usually actually compute people taught frequentist statistics bayesian statistics fortu nately mechanics computing credible interval easy computing conﬁdence interval see betacredibleint matlab highest posterior density regions problem central intervals might points outside higher probability density illustrated figure see points outside left boundary higher density inside right boundary motivates alternative quantity known highest posterior density hpd region deﬁned set probable points total constitute chapter bayesian statistics pmin figure central interval hpd region hypothetical multimodal posterior based figure gelman figure generated postdensityintervals probability mass formally ﬁnd threshold pdf deﬁne hpd hpd region sometimes called highest density interval hdi example figure shows hdi beta distribution see narrower even though still contains mass furthermore every point inside higher density every point outside unimodal distribution hdi narrowest interval around mode contain ing mass see imagine water ﬁlling reverse lower level mass revealed submerged gives simple algorithm computing hdis case simply search points interval contains mass minimal width done numerical optimization know inverse cdf distribution search sorted data points bag samples see betahpd demo posterior multimodal hdi may even connected region see figure example however summarizing multimodal posteriors always difficult 
[bayesian, statistics, summarizing, posterior, distributions, inference, difference, proportions] sometimes multiple parameters interested computing posterior distribution function parameters example suppose buy something amazon com two sellers offering price seller positive reviews negative reviews seller positive reviews negative reviews buy example www johndcook com blog bayesian amazon see also lingpipe blog bayesian counterpart fisher exact test contingency tables bayesian model selection data pdf figure exact posteriors monte carlo approximation use kernel density estimation get smooth plot vertical lines enclose central interval figure generated amazonsellerdemo face pick seller cannot conﬁdent seller better since reviews section sketch bayesian analysis problem similar methodology used compare rates proportions across groups variety settings let unknown reliabilities two sellers since know much endow uniform priors beta posteriors beta beta want compute convenience let deﬁne difference rates alternatively might want work terms log odds ratio compute desired quantity using numerical integration beta beta ﬁnd means better buying seller see amazonsellerdemo code also possible solve integral analytically cook simpler way solve problem approximate posterior monte carlo sampling easy since independent posterior beta distributions sampled using standard methods distributions shown figure approximation together hpd shown figure approximation obtained counting fraction samples turns close exact value see amazonsellerdemo code 
[bayesian, statistics, bayesian, model, selection] figure saw using high degree polynomial results overﬁtting using low degree results underﬁtting similarly figure saw using small chapter bayesian statistics regularization parameter results overﬁtting large value results underﬁtting general faced set models families parametric distributions different complexity choose best one called model selection problem one approach use cross validation estimate generalization error candiate models pick model seems best however requires ﬁtting model times number folds efficient approach compute posterior models easily compute map model argmax called bayesian model selection use uniform prior models amounts picking model maximizes quantity called marginal likelihood integrated likelihood evidence model details perform integral discussed section ﬁrst give intuitive interpretation quantity means 
[bayesian, statistics, bayesian, model, selection, bayesian, occam’s, razor] one might think using select models would always favor model parameters true use select models mle map estimate parameters model models parameters data better hence achieve higher likelihood however integrate parameters rather maximizing automatically protected overﬁtting models parameters necessarily higher marginal likelihood called bayesian occam razor effect mackay murray ghahramani named principle known occam razor says one pick simplest model adequately explains data one way understand bayesian occam razor notice marginal likelihood rewritten follows based chain rule probability equation dropped conditioning brevity similar leave one cross validation estimate section likelihood since predict future point given previous ones course order data matter expression model complex overﬁt early examples predict remaining ones poorly another way understand bayesian occam razor effect note probabilities must sum one hence sum possible data sets complex models predict many things must spread probability mass thinly hence obtain large probability given data set simpler models sometimes bayesian model selection figure schematic illustration bayesian occam razor broad green curve corresponds complex model narrow blue curve simple model middle red curve right based figure bishop see also murray ghahramani figure similar plot produced real data called conservation probability mass principle illustrated figure horizontal axis plot possible data sets order increasing complexity measured abstract sense vertical axis plot predictions possible models simple one medium one complex one also indicate actually observed data vertical line model simple assigns low probability model also assigns relatively low probability predict many data sets hence spreads probability quite widely thinly model right predicts observed data reasonable degree conﬁdence predict many things hence model probable model concrete example bayesian occam razor consider data figure plot polynomials degrees data points also shows posterior models use gaussian prior see section details enough data justify complex model map model figure shows happens clear right model data fact generated quadratic another example figure plots log log polynomial ridge regres sion model ranges set values used experiment see maximum evidence occurs roughly point minimum test mse also corresponds point chosen using bayesian approach restricted evaluating evidence ﬁnite grid values instead use numerical optimization ﬁnd argmax technique called empirical bayes type maximum likelihood see section details example shown figure see curve similar shape estimate computed efficiently chapter bayesian statistics logev logev logev method figure plot polynomials degrees data points using empirical bayes solid green curve true function dashed red curve prediction dotted blue lines represent around mean plot posterior models assuming uniform prior based ﬁgure zoubin ghahramani figure generated linregebmodelselvsn 
[bayesian, statistics, bayesian, model, selection, computing, marginal, likelihood, evidence] discussing parameter inference ﬁxed model often wrote thus ignoring normalization constant valid since constant wrt however comparing models need know compute marginal likelihood general quite hard since integrate possible parameter values conjugate prior easy compute show let prior unnormalized distribution normalization constant prior let likelihood contains constant factors likelihood finally let poste bayesian model selection logev logev logev method figure figure except figure generated linregebmodelselvsn rior unnormalized posterior normalization constant posterior assuming relevant normalization constants tractable easy way compute marginal likelihood give examples chapter bayesian statistics beta binomial model let apply result beta binomial model since know beta know normalization constant posterior hence marginal likelihood beta bernoulli model except missing term dirichlet multinoulli model reasoning beta bernoulli case one show marginal likelihood dirichlet multinoulli model given hence rewrite result following form usually presented literature see many applications equation later gaussian gaussian wishart model consider case mvn conjugate niw prior let normalizer prior normalizer posterior let normalizer bayesian model selection likelihood easy see equation prove useful later bic approximation log marginal likelihood general computing integral equation quite difficult one simple popular approximation known bayesian information criterion bic following form schwarz bic log dof log log dof number degrees freedom model mle model see form penalized log likelihood penalty term depends model complexity see section derivation bic score example consider linear regression show section mle given rss rss mle corresponding log likelihood given log log hence bic score follows dropping constant terms bic log log number variables model statistics literature common use alternative deﬁnition bic call bic cost since want minimize bic cost log dof log log context linear regression becomes bic cost log log traditionally bic score deﬁned using estimate independent prior however models mixtures gaussians estimate poorly behaved better evaluate bic score using map estimate fraley raftery chapter bayesian statistics bic method closely related minimum description length mdl principle characterizes score model terms well ﬁts data minus complex model deﬁne see hansen details similar expression bic mdl called akaike information criterion aic deﬁned aic log dof derived frequentist framework cannot interpreted approximation marginal likelihood nevertheless form expression similar bic see penalty aic less bic causes aic pick complex models however result better predictive accuracy see clarke sec discussion information criteria effect prior sometimes clear set prior performing posterior inference details prior may matter much since likelihood often overwhelms prior anyway computing marginal likelihood prior plays much important role since averaging likelihood possible parameter settings weighted prior figures demonstrated model selection linear regression used prior form tuning parameter controls strong prior parameter large effect discuss section intuitively large weights forced small need use complex model many small parameters high degree polynomial data conversely small favor simpler models since parameter allowed vary magnitude lot prior unknown correct bayesian procedure put prior prior put prior hyper parameter well parametrs compute marginal likelihood integrate unknowns compute course requires specifying hyper prior fortunately higher bayesian hierarchy less sensitive results prior settings usually make hyper prior uninformative computational shortcut optimize rather integrating use argmax argmax approach called empirical bayes discussed detail section method used figures bayesian model selection bayes factor interpretation decisive evidence strong evidence moderate evidence weak evidence weak evidence moderate evidence strong evidence decisive evidence table jeffreys scale evidence interpreting bayes factors 
[bayesian, statistics, bayesian, model, selection, bayes, factors] suppose prior models uniform model selection equivalent picking model highest marginal likelihood suppose two models considering call null hypothesis alternative hypothesis deﬁne bayes factor ratio marginal likelihoods like likelihood ratio except integrate parameters allows compare models different complexity prefer model otherwise prefer model course might slightly greater case conﬁdent model better jeffreys proposed scale evidence interpreting magnitude bayes factor shown table bayesian alternative frequentist concept value alternatively convert bayes factor posterior models example testing coin fair suppose observe coin tosses want decide data generated fair coin potentially biased coin could value let denote ﬁrst model second model marginal likelihood simply value deﬁned probability null hypothesis observing test statistic chi squared statistic large larger actually observed pvalue note almost nothing really want know chapter bayesian statistics log bic approximation log figure log marginal likelihood coins example bic approximation figure generated coinsmodelseldemo number coin tosses marginal likelihood using beta prior plot log number heads figure assuming shape curve sensitive long observe heads unbiased coin hypothesis likely since simpler model free parameters would suspicious coincidence coin biased happened produce almost exactly heads tails however counts become extreme favor biased coin hypothesis note plot log bayes factor log exactly shape since log constant see also exercise figure shows bic approximation log biased coin example section see curve approximately shape exact log marginal likelihood matters model selection purposes since absolute scale irrelevant particular favors simpler model unless data overwhelmingly support complex model 
[bayesian, statistics, bayesian, model, selection, jeffreys-lindley, paradox] problems arise use improper priors priors integrate model selection hypothesis testing even though priors may acceptable purposes example consider testing hypotheses deﬁne marginal density use following mixture model priors meaningful proper normalized density functions case posterior given suppose use improper priors integrated marginal likelihood model let hence thus change posterior arbitrarily choosing please note using proper vague priors cause similar problems particular bayes factor always favor simpler model since probability observed data complex model diffuse prior small called jeffreys lindley paradox thus important use proper priors performing model selection note however share prior subset parameters part prior improper since corresponding normalization constant cancel 
[bayesian, statistics, priors] controversial aspect bayesian statistics reliance priors bayesians argue unavoidable since nobody tabula rasa blank slate inference must done conditional certain assumptions world nevertheless one might interested minimizing impact one prior assumptions brieﬂy discuss ways 
[bayesian, statistics, priors, uninformative, priors] strong beliefs common use uninformative non informative prior let data speak issue designing uninformative priors actually somewhat tricky example difficulty consider bernoulli parameter one might think uninformative prior would uniform distribution beta posterior mean case whereas mle hence one could argue prior completely uninformative chapter bayesian statistics clearly decreasing magnitude pseudo counts lessen impact prior argument non informative prior lim beta beta mixture two equal point masses see zhu also called haldane prior note haldane prior improper prior meaning integrate however long see least one head least one tail posterior proper section argue right uninformative prior fact beta clearly difference practice three priors likely negligible general advisable perform kind sensitivity analysis one checks much one conclusions predictions change response change modeling assumptions includes choice prior also choice likelihood kind data pre processing conclusions relatively insensitive modeling assumptions one conﬁdence results 
[bayesian, statistics, priors, jeffreys, priors] harold jeffreys designed general purpose technique creating non informative priors result known jeffreys prior key observation non informative parameterization prior function also non informative change variables formula prior general change however let pick fisher information log measure curvature expected negative log likelihood hence measure stability mle see section log log squaring taking expectations log harold jeffreys english mathematician statistician geophysicist astronomer priors ﬁnd transformed prior examples make clearer example jeffreys prior bernoulli multinoulli suppose ber log likelihood single sample log log log score function gradient log likelihood log observed information second derivative log likelihood log fisher information expected information hence jeffreys prior beta consider multinoulli random variable states one show jeffreys prior given dir note different obvious choices dir dir example jeffreys prior location scale parameters one show jeffreys prior location parameter gaussian mean thus example translation invariant prior satisﬁes property probability mass assigned interval assigned shifted interval width chapter bayesian statistics achieved using approximate using gaussian inﬁnite variance note improper prior since integrate using improper priors ﬁne long posterior proper case provided seen data points since nail location soon seen single data point similarly one show jeffreys prior scale parameter gaussian variance example scale invariant prior satisﬁes property probability mass assigned interval assigned interval scaled size constant factor example change units meters feet want affect inferences achieved using see note log log log log log approximate using degenerate gamma distribution section prior also improper posterior proper soon seen data points since need least two data points estimate variance 
[bayesian, statistics, priors, robust, priors] many cases conﬁdent prior want make sure undue inﬂuence result done using robust priors insua ruggeri typically heavy tails avoids forcing things close prior mean let consider example berger suppose observe want estimate mle course seems reasonable posterior mean uniform prior also suppose know prior median prior quantiles let also assume prior smooth unimodal easy show gaussian prior form satisﬁes prior constraints case posterior mean given seem satisfactory suppose use cauchy prior also satisﬁes prior constraints example time ﬁnd using numerical method integration see robustpriordemo code posterior mean seems much reasonable 
[bayesian, statistics, priors, mixtures, conjugate, priors] robust priors useful computationally expensive use conjugate priors simplify computation often robust ﬂexible enough encode prior knowl priors edge however turns mixture conjugate priors also conjugate exercise approximate kind prior dallal hall diaconis ylvisaker thus priors provide good compromise computational convenience ﬂexibility example suppose modeling coin tosses think coin either fair biased towards heads cannot represented beta distribution however model using mixture two beta distributions example might use beta beta comes ﬁrst distribution coin fair comes second biased towards heads represent mixture introducing latent indicator variable means comes mixture component prior form conjugate called prior mixing weights one show exercise posterior also written mixture conjugate distributions follows posterior mixing weights given quantity marginal likelihood mixture component see sec tion example suppose use mixture prior beta beta observe heads tails posterior becomes beta beta heads tails using equation posterior becomes beta beta see figure illustration chapter bayesian statistics mixture beta distributions prior posterior figure mixture two beta distributions figure generated mixbetademo application finding conserved regions dna protein sequences mentioned dirichlet multinomial models widely used biosequence analysis let give simple example illustrate machinery developed speciﬁcally consider sequence logo discussed section suppose want ﬁnd locations represent coding regions genome locations often letter across sequences evolutionary pressure need ﬁnd columns pure nearly sense mostly mostly mostly mostly one approach look low entropy columns ones whose distribution nearly deterministic pure suppose want associate conﬁdence measure estimates purity useful believe adjacent locations conserved together case let location conserved let otherwise add dependence adjacent variables using markov chain see chapter details case need deﬁne likelihood model vector counts column natural make multinomial distribution parameter since column different distribution want integrate thus compute marginal likelihood prior use use uniform prior dir use column conserved could nearly pure column natural approach use mixture dirichlet priors one tilted towards appropriate corner dimensional simplex dir dir since conjugate easily compute see brown hierarchical bayes application ideas real bio sequence problem 
[bayesian, statistics, hierarchical, bayes] key requirement computing posterior speciﬁcation prior hyper parameters know set cases use uninformative priors discussed bayesian approach put prior priors terms graphical models chapter represent situation follows example hierarchical bayesian model also called multi level model since multiple levels unknown quantities give simple example see many others later book 
[bayesian, statistics, hierarchical, bayes, example, modeling, related, cancer, rates] consider problem predicting cancer rates various cities example johnson albert particular suppose measure number people various cities number people died cancer cities assume bin want estimate cancer rates one approach estimate separately suffer sparse data problem underestimation rate cancer due small another approach assume called parameter tying resulting pooled mle assumption cities rate rather strong one compromise approach assume similar may city speciﬁc variations modeled assuming drawn common distribution say beta full joint distribution written bin beta note crucial infer data clamp constant conditionally independent information ﬂow contrast treating unknown hidden variable allow data poor cities borrow statistical strength data rich ones suppose compute joint posterior get posterior marginals figure plot posterior means blue bars well population level mean shown red line represents average see posterior mean shrunk towards pooled estimate strongly cities small sample sizes example city city observed cancer incidence rate city smaller population rate shrunk towards population level estimate closer horizontal red line city figure shows posterior credible intervals see city large population people small posterior uncertainty consequently city chapter bayesian statistics number people cancer truncated pop city truncated mle red line pooled mle posterior mean red line pop mean credible interval theta median figure results ﬁtting model using data johnson albert first row number cancer incidents cities missouri second row population size largest city number population incidents truncate vertical axes ﬁrst two rows differences cities visible third row mle red line pooled mle fourth row posterior mean red line population level mean posterior credible intervals cancer rates figure generated cancerrateseb largest impact posterior estimate turn impact estimate cancer rates cities cities highest mle also highest posterior uncertainty reﬂecting fact high estimate conﬂict prior estimated cities example one parameter per city modeling probability response making bernoulli rate parameter function covariates sigm model multiple correlated logistic regression tasks called multi task learning discussed detail section 
[bayesian, statistics, empirical, bayes] hierarchical bayesian models need compute posterior multiple levels latent variables example two level model need compute cases analytically marginalize leaves simpler problem computing computational shortcut approximate posterior hyper parameters point estimate argmax since typically much smaller dimensionality less prone overﬁtting safely use uniform prior estimate becomes argmax argmax empirical bayes quantity inside brackets marginal integrated likelihood sometimes called evidence overall approach called empirical bayes type maximum likelihood machine learning sometimes called evidence procedure empirical bayes violates principle prior chosen independently data however view computationally cheap approximation inference hierarchical bayesian model viewed map estimation approximation inference one level model fact construct hierarchy integrals one performs bayesian one becomes method deﬁnition maximum likelihood argmax map estimation argmax empirical bayes argmax argmax map argmax argmax full bayes note shown good frequentist properties see carlin louis efron widely used non bayesians example popular james stein estimator discussed section derived using 
[bayesian, statistics, empirical, bayes, example, beta-binomial, model] let return cancer rates model analytically integrate write marginal likelihood directly follows bin beta various ways maximizing wrt discussed minka estimated plug hyper parameters compute posterior usual way using conjugate analysis net result posterior mean weighted average local mle prior means depends since estimated based data inﬂuenced data 
[bayesian, statistics, empirical, bayes, example, gaussian-gaussian, model] study another example analogous cancer rates example except data real valued use gaussian likelihood gaussian prior allow write solution analytically particular suppose data multiple related groups example could test score student school want estimate mean score school however since sample size may small chapter bayesian statistics schools regularize problem using hierarchical bayesian model assume come common prior joint distribution following form assume known simplicity relax assumption exercise explain estimate estimated compute posteriors simpliﬁes matters rewrite joint distribution following form exploiting fact gaussian measurements values variance equivalent one measurement value variance yields follows results section posteriors given deﬁned quantity controls degree shrinkage towards overall mean data reliable group sample size large small relative hence small put weight estimate however groups small sample sizes get regularized shrunk towards overall mean heavily see example groups posterior mean becomes exactly form james stein estimator discussed section example predicting baseball scores give example shrinkage applied baseball batting averages efron morris observe number hits players ﬁrst games call number hits assume bin true batting average player goal estimate mle course empirical batting average however use approach better apply gaussian shrinkage approach described require likelihood gaussian known drop subscript since assume empirical bayes mle top shrinkage estimates bottom player number mse mse mle mse shrunk true shrunk mle figure mle parameters top corresponding shrunken estimates bottom plot true parameters blue posterior mean estimate green mles red players figure generated shrinkagedemobaseball since already represents average player however example binomial likelihood right mean variance constant var var let apply variance stabilizing transform better match gaussian assump tion arcsin approximately use gaussian shrinkage estimate using equation transform back get sin results shown figure plot mle posterior mean see estimates shrunk towards global mean plot true value mle posterior mean true values estimated large number independent games see average shrunken estimate much closer true parameters mle speciﬁcally mean squared error deﬁned mse three times smaller using shrinkage estimates using mles estimating hyper parameters section give algorithm estimating suppose initially groups case derive estimate closed form show equation suppose var let taylor series expansions gives hence var var variance stabilizing transformation function independent chapter bayesian statistics hence marginal likelihood thus estimate hyper parameters using usual mles gaussian overall mean variance use moment matching equivalent mle gaussian simply equate model variance empirical variance since know must positive common use following revised estimate max hence shrinkage factor case different longer derive solution closed form exercise discusses use algorithm derive estimate exercise discusses perform full bayesian inference hierarchical model 
[bayesian, statistics, bayesian, decision, theory] seen probability theory used represent updates beliefs state world however ultimately goal convert beliefs actions section discuss optimal way formalize given statistical decision problem game nature opposed game strategic players topic game theory see shoham leyton brown details game nature picks state parameter label unknown generates observation get see make decision choose action action space finally incur loss measures compatible action nature hidden state example might use misclassiﬁcation loss squared loss see examples bayesian decision theory goal devise decision procedure policy speciﬁes optimal action possible input optimal mean action minimizes expected loss argmin economics common talk utility function negative loss thus rule becomes argmax called maximum expected utility principle essence mean rational behavior note two different interpretations mean expected bayesian version discuss mean expected value given data seen far frequentist version discuss section mean expected value expect see future bayesian approach decision theory optimal action observed deﬁned action minimizes posterior expected loss continuous want estimate parameter vector replace sum integral hence bayes estimator also called bayes decision rule given arg min 
[bayesian, statistics, bayesian, decision, theory, bayes, estimators, common, loss, functions] section show construct bayes estimators loss functions commonly arising machine learning map estimate minimizes loss loss deﬁned commonly used classiﬁcation problems true class label estimate example two class case write loss matrix follows chapter bayesian statistics figure regions input space class posteriors uncertain may prefer choose class instead may prefer reject option based figure bishop section generalize loss function penalizes two kinds errors diagonal differently posterior expected loss hence action minimizes expected loss posterior mode map estimate arg max reject option classiﬁcation problems uncertain may prefer choose reject action refuse classify example speciﬁed classes instead say know ambiguous cases handled human expert see figure illustration useful risk averse domains medicine ﬁnance formalize reject option follows let choosing correspond picking reject action choosing correspond picking one classes suppose deﬁne loss function otherwise cost reject action cost substitution error exercise show optimal action pick reject action probable class probability otherwise pick probable class bayesian decision theory figure plots figure generated lossfunctionfig posterior mean minimizes quadratic loss continuous parameters appropriate loss function squared error loss quadratic loss deﬁned posterior expected loss given hence optimal estimate posterior mean often called minimum mean squared error estimate mmse estimate linear regression problem case optimal estimate given training data given plug posterior mean parameter estimate note optimal thing matter prior use posterior median minimizes absolute loss loss penalizes deviations truth quadratically thus sensitive outliers robust alternative absolute loss see figure optimal estimate posterior median value see exercise proof supervised learning consider prediction function suppose cost function gives cost predicting truth deﬁne loss incurred chapter bayesian statistics taking action using predictor unknown state nature parameters data generating mechanism follows known generalization error goal minimize posterior expected loss given contrasted frequentist risk deﬁned equation 
[bayesian, statistics, bayesian, decision, theory, false, positive, false, negative, tradeoff] section focus binary decision problems hypothesis testing two class classiﬁcation object event detection etc two types error make false positive aka false alarm arises estimate truth false negative aka missed detection arises estimate truth loss treats two kinds errors equivalently however consider following general loss matrix cost false negative cost false positive posterior expected loss two possible actions given hence pick class iff easy show exercise pick iff see also muller example false negative costs twice much false positive use decision threshold declaring positive discuss roc curves provide way study tradeoff without choose speciﬁc threshold roc curves suppose solving binary decision problem classiﬁcation hypothesis testing object detection etc also assume labeled data set let bayesian decision theory truth estimate table quantities derivable confusion matrix true number positives called number positives true number negatives called number negatives tpr sensitivity recall fpr type fnr miss rate type tnr speciﬁty table estimating confusion matrix abbreviations fnr false negative rate fpr false positive rate tnr true negative rate tpr true positive rate decision rule measure conﬁdence monotonically related need probability threshold parameter given value apply decision rule count number true positives false positives true negatives false negatives occur shown table table errors called confusion matrix table compute true positive rate tpr also known sensitivity recall hit rate using also compute false positive rate fpr also called false alarm rate type error rate using deﬁnitions summarized tables combine errors way choose compute loss function however rather computing tpr fpr ﬁxed threshold run detector set thresholds plot tpr fpr implicit function called receiver operating characteristic roc curve see figure example system achieve point bottom left setting thus classifying everything negative similarly system achieve point top right setting thus classifying everything positive system performing chance level achieve point diagonal line choosing appropriate threshold system perfectly separates positives negatives threshold achieve top left corner varying threshold system hug left axis top axis shown figure quality roc curve often summarized single number using area curve auc higher auc scores better maximum obviously another summary statistic used equal error rate eer also called cross rate deﬁned value satisﬁes since compute eer drawing line top left bottom right seeing intersects roc curve see points figure lower eer scores better minimum obviously chapter bayesian statistics fpr tpr recall precision figure roc curves two hypothetical classiﬁcation systems better plot true positive rate tpr false positive rate fpr vary threshold also indicate equal error rate eer red blue dots area curve auc classiﬁer precision recall curve two hypothetical classiﬁcation systems better figure generated prhand precision ppv fdp npv table estimating confusion matrix abbreviations fdp false discovery probability npv negative predictive value ppv positive predictive value precision recall curves trying detect rare event retrieving relevant document ﬁnding face image number negatives large hence comparing informative since fpr small hence action roc curve occur extreme left cases common plot tpr versus number false positives rather false positive rate however cases notion negative well deﬁned example detecting objects images see section detector works classifying patches number patches examined hence number true negatives parameter algorithm part problem deﬁnition would like use measure talks positives precision deﬁned recall deﬁned precision measures fraction detections actually positive recall measures fraction positives actually detected predicted label true label estimate precision recall using precision recall curve plot precision recall vary threshold see figure hugging top right best one curve summarized single number using mean precision averaging bayesian decision theory class class pooled table illustration difference macro micro averaging true label called label example macro averaged precision micro averaged precision based table manning recall values approximates area curve alternatively one quote precision ﬁxed recall level precision ﬁrst entities recalled called average precision score measure widely used evaluating information retrieval systems scores ﬁxed threshold one compute single precision recall value often combined single statistic called score score harmonic mean precision recall using equation write widely used measure information retrieval systems understand use harmonic mean instead arithmetic mean consider following scenario suppose recall entries precision given prevalence suppose prevalence low say arithmetic mean given contrast harmonic mean strategy multi class case document classiﬁcation problems two ways generalize scores ﬁrst called macro averaged deﬁned score obtained task distinguishing class others called micro averaged deﬁned score pool counts class contingency table table gives worked example illustrates difference see precision class class macro averaged precision therefore whereas micro averaged precision latter much closer precision class precision class since class ﬁve times larger class give equal weight class use macro averaging chapter bayesian statistics false discovery rates suppose trying discover rare phenomenon using kind high throughput measurement device gene expression micro array radio telescope need make many binary decisions form may large called multiple hypothesis testing note difference standard binary classiﬁcation classifying based data based simultaneous classiﬁcation problem might hope better series individual classiﬁcation problems set threshold natural approach try minimize expected number false positives bayesian approach computed follows error discovery belief object exhibits phenomenon question deﬁne posterior expected false discovery rate follows number discovered items given desired fdr tolerance say one adapt achieve called direct posterior probability approach controlling fdr newton muller order control fdr helpful estimate jointly using hierar chical bayesian model section rather independently allows pooling statistical strength thus lower fdr see berry hochberg information 
[bayesian, statistics, bayesian, decision, theory, topics] section brieﬂy mention topics related bayesian decision theory space detail include pointers relevant literature contextual bandits one armed bandit colloquial term slot machine found casinos around world game insert money pull arm wait machine stop lucky win money imagine bank machines choose one use called multi armed bandit modeled using bayesian decision theory possible actions action unknown reward payoff function maintaining belief state one devise optimal policy compiled series gittins indices gittins optimally solves exploration exploitation tradeoff speciﬁes many times one try action deciding winner consider extension arm player associated feature vector call features called contextual bandit see sarkar scott example arms could represent ads news articles want show user features could represent properties ads articles bayesian decision theory bag words well properties user demographics assume linear model reward maintain distribution parameters arm series tuples form speciﬁes arm pulled features resulting outcome user clicked otherwise discuss ways compute linear logistic regression models later chapters given posterior must decide action take one common heuristic known ucb stands upper conﬁdence bound take action maximizes argmax var tuning parameter trades exploration exploitation intuition pick actions believe good large actions uncertain large even simpler method known thompson sampling follows step pick action probability equal probability optimal action max approximate drawing single sample posterior choosing argmax despite simplicity shown work quite well chapelle utility theory suppose doctor trying decide whether operate patient imagine states nature patient cancer patient lung cancer patient breast cancer since action state space discrete represent loss function loss matrix following surgery surgery cancer lung cancer breast cancer numbers reﬂects fact performing surgery patient cancer bad loss depending type cancer since patient might die performing surgery patient cancer incurs loss performing surgery patient cancer wasteful loss performing surgery patient cancer painful necessary natural ask numbers come ultimately represent personal preferences values ﬁctitious doctor somewhat arbitrary people prefer chocolate ice cream others prefer vanilla thing right loss utility function however shown see degroot set consistent preferences converted scalar loss utility function note utility measured arbitrary scale dollars since relative values matter people often squeamish talking human lives monetary terms decision making requires chapter bayesian statistics sequential decision theory far concentrated one shot decision problems make one decision game ends setion generalize multi stage sequential decision problems problems frequently arise many business engineering settings closely related problem reinforcement learning however discussion point beyond scope book 
[bayesian, statistics, exercises] exercise proof mixture conjugate priors indeed conjugate derive equation exercise optimal threshold classiﬁcation probability consider case learned conditional probability distribution suppose two classes let consider loss matrix predicted true label label show decision minimizes expected loss equivalent setting probability threshold predicting function show work show loss matrix threshold show work exercise reject option classiﬁers source duda many classiﬁcation problems one option either assigning class uncertain choosing reject option cost rejects less cost falsely classifying object may optimal action let mean choose action number classes reject action let true unknown state nature deﬁne loss function follows otherwise otherwords incur loss correctly classify incur loss cost choose reject option incur loss cost make substitution error misclassiﬁcation tradeoffs one needs use kind currency compare different courses action insurance companies time ross schachter decision theorist stanford university likes tell story school board rejected study absestos removal schools performed cost beneﬁt analysis considered inhumane put dollar value children health result rejecting report absestos removed surely inhumane medical domains one often measures utility terms qaly quality adjusted life years instead dollars idea course even explicitly specify much value different people lives behavior reveal implicit values preferences preferences converted real valued scale dollars qaly inferring utility function behavior called inverse reinforcement learning bayesian decision theory decision true label predict predict reject show minimum risk obtained decide probable class otherwise decide reject describe qualitatively happens increased relative cost rejection increases exercise reject options many applications classiﬁer allowed reject test example rather classifying one classes consider example case cost misclassiﬁcation cost human manually make decison formulate following loss matrix suppose predicted decision minimizes expected loss suppose decision minimizes expected loss show general loss matrix posterior distribution two thresholds optimal decisionn predict reject predict thresholds exercise newsvendor problem consider following classic problem decision theory economics suppose trying decide much quantity product newspapers buy maximize proﬁts optimal amount depend much demand think product well cost selling price suppose unknown pdf cdf evaluate expected proﬁt considering two cases sell items make proﬁt sell items proﬁt wasted unsold items expected proﬁt buy quantity simplify expression take derivatives wrt show optimal quantity maximizes expected proﬁt satisﬁes exercise bayes factors roc curves let bayes factor favor model suppose plot two roc curves one computed thresholding computed thresholding different explain exercise bayes model averaging helps predictive accuracy let quantity want predict let observed data ﬁnite set models suppose action provide probabilistic prediction loss function chapter bayesian statistics log either perform bayes model averaging predict using bma could predict using single model plugin approximation show models posterior expected loss using bma lower bma expectation respect hint use non negativity divergence exercise mle model selection discrete distribution source jaakkola let denote result coin toss tails heads coin potentially biased heads occurs probability suppose someone else observes coin ﬂip reports outcome person unreliable reports result correctly probability given assume independent write joint probability distribution table terms suppose following dataset mles justify answer hint note likelihood function factorizes denotes parameter model may leave answer fractional form wish consider model parameters representing parameters free vary since must sum one mle denotes parameter model suppose sure model correct compute leave one cross validated log likelihood parameter model parameter model follows log denotes mle computed excluding row model pick hint notice table counts changes omit training case one time bayesian decision theory recall alternative use bic score deﬁned bic log mle dof log dof number free parameters model compute bic scores models use log base model bic prefer exercise posterior median optimal estimate loss prove posterior median optimal estimate loss exercise decision rule trading fps fns show pick iff 
[frequentist, statistics, introduction] approach statistical inference described chapter known bayesian statistics perhaps surprisingly considered controversial people whereas plication bayes rule non statistical problems medical diagnosis section spam ﬁltering section airplane tracking section controversial reason objection misguided distinction parameters statis tical model kinds unknown quantities attempts made devise approaches statistical inference avoid treating parameters like random variables thus avoid use priors bayes rule approaches known frequentist statistics classical statistics orthodox statistics instead based posterior distribution based concept sampling distribution distribution estimator applied multiple data sets sampled true unknown distribution see section details notion variation across repeated trials forms basis modeling uncertainty used frequentist approach contrast bayesian approach ever condition actually observed data notion repeated trials allows bayesian compute probability one events discussed section perhaps importantly bayesian approach avoids certain paradoxes plague frequentist approach see section nevertheless important familiar frequentist statistics especially section since widely used machine learning 
[frequentist, statistics, sampling, distribution, estimator] frequentist statistics parameter estimate computed applying estimator data parameter viewed ﬁxed data random exact opposite bayesian approach uncertainty parameter estimate measured computing sampling distribution estimator understand parameters sometimes considered represent true unknown physical quantities therefore random however seen perfectly reasonable use probability distribution represent one uncertainty unknown constant chapter frequentist statistics boot true mle boot true mle figure bootstrap approximation sampling distribution bernoulli distribution use bootstrap samples datacases generated ber mle mle figure generated bootstrapdemober concept imagine sampling many different data sets true model let true parameter indexes sampled data set size dataset apply estimator get set estimates let distribution induced sampling distribution estimator discuss various ways use sampling distribution later sections ﬁrst sketch two approaches computing sampling distribution 
[frequentist, statistics, sampling, distribution, estimator, bootstrap] bootstrap simple monte carlo technique approximate sampling distribution particularly useful cases estimator complex function true parameters idea simple knew true parameters could generate many say fake datasets size true distribution could compute estimator sample use empirical distribution resulting samples estimate sampling distribution since unknown idea parametric bootstrap generate samples using instead alternative called non parametric bootstrap sample replacement original data compute induced distribution methods speeding bootstrap applied massive data sets discussed kleiner figure shows example compute sampling distribution mle bernoulli using parametric bootstrap results using non parametric bootstrap essentially see sampling distribution asymmetric therefore quite far gaussian distribution looks gaussian theory suggests see natural question connection parameter estimates computed bootstrap parameter values sampled posterior sampling distribution estimator conceptually quite different common case prior strong quite similar example figure shows example compute posterior using uniform beta prior sample see posterior sampling distribution quite similar one think bootstrap distribution poor man posterior see hastie details however perhaps surprisingly bootstrap slower posterior sampling reason bootstrap model times whereas posterior sampling usually model ﬁnd local mode perform local exploration around mode local exploration usually much faster ﬁtting model scratch 
[frequentist, statistics, sampling, distribution, estimator, large, sample, theory, mle] cases sampling distribution estimators computed analytically particular shown certain conditions sample size tends inﬁnity sampling distribution mle becomes gaussian informally requirement result hold parameter model gets see inﬁnite amount data model identiﬁable unfortunately excludes many models interest machine learning nevertheless let assume simple setting theorem holds center gaussian mle variance gaussian intuitively variance estimator inversely related amount curvature likelihood surface peak curvature large peak sharp variance low case estimate well determined contrast curvature small peak nearly ﬂat variance high let formalize intuition deﬁne score function gradient log likelihood evaluated point log deﬁne observed information matrix gradient negative score function equivalently hessian nll log becomes log measure curvature log likelihood function since studying sampling distribution set random variables fisher information matrix deﬁned expected value observed information matrix usual deﬁnition equivalent standard assumptions precisely standard deﬁnition follows give scalar case simplify notation var log variance score function mle easy see log since chapter frequentist statistics expected value function applied data sampled often representing true parameter generated data assumed known write short furthermore easy see log likelihood sample size times steeper log likelihood sample size drop subscript write notation usually used let mle mle shown see rice proof say sampling distribution mle asymptotically normal variance mle used measure conﬁdence mle unfortunately unknown evaluate variance sampling distribution however approximate sampling distribution replacing consequently approximate standard errors given example equation know fisher information binomial sampling model approximate standard error mle compare equation posterior standard deviation uniform prior 
[frequentist, statistics, frequentist, decision, theory] frequentist classical decision theory loss function likelihood prior hence posterior posterior expected loss thus automatic way deriving optimal estimator unlike bayesian case instead frequentist approach free choose estimator decision procedure want gradient must zero maximum variance reduces expected square score function log shown rice log log fisher information reduces expected second derivative nll much intuitive quantity variance score practice frequentist approach usually applied one shot statistical decision problems classiﬁcation regression parameter estimation since non constructive nature makes difficult apply sequential decision problems adapt data online frequentist decision theory chosen estimator deﬁne expected loss risk follows data sampled nature distribution represented parameter words expectation wrt sampling distribution estimator compare bayesian posterior expected loss see bayesian approach averages unknown conditions known whereas frequentist approach averages thus ignoring observed data conditions unknown frequentist deﬁnition unnatural cannot even computed unknown consequently cannot compare different estimators terms frequentist risk discuss various solutions 
[frequentist, statistics, frequentist, decision, theory, bayes, risk] choose amongst estimators need way convert single measure quality depend knowing one approach put prior deﬁne bayes risk integrated risk estimator follows bayes estimator bayes decision rule one minimizes expected risk argmin note integrated risk also called preposterior risk since seen data minimizing useful experiment design prove important theorem connects bayesian frequentist approaches decision theory theorem bayes estimator obtained minimizing posterior expected loss proof switching order integration chapter frequentist statistics 
[frequentist, statistics] figure risk functions two decision procedures since lower worst case risk minimax estimator even though lower risk values thus minimax estimators overly conservative minimize overall expectation minimize term inside decision rule pick argmin hence see picking optimal action case case basis bayesian approach optimal average frequentist approach words bayesian approach provides good way achieving frequentist goals fact one prove following theorem wald every admissable decision rule bayes decision rule respect possibly improper prior distribution theorem shows best way minimize frequentist risk bayesian see bernardo smith discussion point 
[frequentist, statistics, minimax, risk] obviously frequentists dislike using bayes risk since requires choice prior though evaluation estimator necessarily part construction alternative approach follows deﬁne maximum risk estimator max max minimax rule one minimizes maximum risk argmin max frequentist decision theory example figure see lower worst case risk ranging possible values minimax estimator see section explanation compute risk function actual model minimax estimators certain appeal however computing hard furthermore pessimistic fact one show minimax estimators equivalent bayes estimators least favorable prior statistical situations excluding game theoretic ones assuming nature adversary reasonable assumption 
[frequentist, statistics, admissible, estimators] basic problem frequentist decision theory relies knowing true distri bution order evaluate risk however might case estimators worse others regardless value particular say dominates domination said strict inequality strict estimator said admissible strictly dominated estimator example let give example based bernardo smith consider problem estimating mean gaussian assume data sampled use quadratic loss corresponding risk function mse possible decision rules estimators follows sample mean sample median ﬁxed value posterior mean prior consider weak prior stronger prior prior mean ﬁxed value assume known thus inﬁnitely strong prior let derive risk functions analytically since toy example know true parameter section show mse decomposed squared bias plus variance var bias sample mean unbiased risk var chapter frequentist statistics risk functions mle median fixed postmean postmean risk functions mle median fixed postmean postmean figure risk functions estimating mean gaussian using data sampled solid dark blue horizontal line mle solid light blue curved line posterior mean left samples right samples based figure bernardo smith figure generated riskfngauss sample median also unbiased one show variance approximately variance zero finally posterior mean functions plotted figure see general best estimator depends value unknown close predicts best within reasonable range around posterior mean combines prior guess actual data best far mle best none suprising small amount shrinkage using posterior mean weak prior usually desirable assuming prior mean sensible surprising risk decision rule sample median always higher sample mean every value consequently sample median frequentist decision theory inadmissible estimator particular problem data assumed come gaussian practice sample median often better sample mean robust outliers one show minka median bayes estimator squared loss assume data comes laplace distribution heavier tails gaussian generally construct robust estimators using ﬂexible models data mixture models non parametric density estimators section computing posterior mean median stein paradox suppose iid random variables want estimate obvious estimator mle case sets turns inadmissible estimator quadratic loss show suffices construct estimator better james stein estimator one estimator deﬁned follows tuning constant estimate shrinks towards overall mean derive estimator using empirical bayes approach section shown shrinkage estimator lower frequentist risk mse mle sample mean known stein paradox reason called paradox illustrated following example suppose true student test score estimate depend global mean hence student scores one create even paradoxical examples making different dimensions qualitatively different average rainfall vancouver etc solution paradox following goal estimate cannot better using goal estimate whole vector use squared error loss function shrinkage helps see suppose want estimate single sample simple estimate overestimate result since consequently reduce risk pooling information even unrelated sources shrinking towards overall mean section give bayesian explanation see also efron morris admissibility enough seems clear restrict search good estimators class admissible estimators fact easy construct admissible estimators show following example chapter frequentist statistics theorem let consider estimating squared loss let constant independent data admissible estimator proof suppose estimator smaller risk inequality must strict suppose true parameter since hence thus way avoid higher risk speciﬁc point equal hence estimator strictly lower risk admissible 
[frequentist, statistics, desirable, properties, estimators] since frequentist decision theory provide automatic way choose best estimator need come heuristics choosing amongst section discuss properties would like estimators unfortunately see cannot achieve properties time 
[frequentist, statistics, desirable, properties, estimators, consistent, estimators] estimator said consistent eventually recovers true parameters generated data sample size goes inﬁnity arrow denotes convergence probability course concept makes sense data actually comes speciﬁed model parameters usually case real data nevertheless useful theoretical property shown mle consistent estimator intuitive reason maxi mizing likelihood equivalent minimizing true distribution estimate achieve divergence iff 
[frequentist, statistics, desirable, properties, estimators, unbiased, estimators] bias estimator deﬁned bias true parameter value bias zero estimator called unbiased means sampling distribution centered true parameter example mle gaussian mean unbiased bias model unidentiﬁable mle may select set parameters different true parameters induced distribution exact distribution parameters said likelihood equivalent desirable properties estimators however mle gaussian variance unbiased estimator fact one show exercise however following estimator unbiased estimator easily prove follows matlab var returns whereas var returns mle large enough difference negligible although mle may sometimes biased estimator one show asymptotically always unbiased necessary mle consistent estimator although unbiased sounds like desirable property always true see sec tion lindley discussion point 
[frequentist, statistics, desirable, properties, estimators, minimum, variance, estimators] seems intuitively reasonable want estimator unbiased although shall give arguments claim however unbiased enough example suppose want estimate mean gaussian estimator looks ﬁrst data point unbiased estimator generally empirical mean also unbiased variance estimator also important natural question long variance famous result called cramer rao lower bound provides lower bound variance unbiased estimator precisely theorem cramer rao inequality let unbiased estimator various smoothness assumptions var fisher information matrix see section proof found rice shown mle achieves cramer rao lower bound hence smallest asymptotic variance unbiased estimator thus mle said asymptotically optimal chapter frequentist statistics 
[frequentist, statistics, desirable, properties, estimators, bias-variance, tradeoff] although using unbiased estimator seems like good idea always case see suppose use quadratic loss showed corresponding risk mse derive useful decomposition mse expectations variances wrt true distribution drop explicit conditioning notational brevity let denote estimate denote expected value estimate vary var bias words mse variance bias called bias variance tradeoff see geman means might wise use biased estimator long reduces variance assuming goal minimize squared error example estimating gaussian mean let give example based hoff suppose want estimate mean gaussian assume data sampled obvious estimate mle bias variance var could also use map estimate section show map estimate gaussian prior form given controls much trust mle compared prior also posterior mean since mean mode gaussian bias variance given var desirable properties estimators sampling distribution truth prior postmean postmean postmean postmean sample size rela mse mse postmean mse mle postmean postmean postmean postmean figure left sampling distribution map estimate different prior strengths mle corresponds right mse relative mle versus sample size based figure hoff figure generated samplingdistgaussshrinkage although map estimate biased assuming lower variance let assume prior slightly misspeciﬁed use whereas truth figure see sampling distribution map estimate biased away truth lower variance narrower mle figure plot mse mse see map estimate lower mse mle especially small sample size case corresponds mle case corresponds strong prior hurts performance prior mean wrong clearly important tune strength prior topic discuss later example ridge regression another important example bias variance tradeoff arises ridge regression discuss section brief corresponds map estimation linear regression gaussian prior zero mean prior encourages weights small reduces overﬁtting precision term controls strength prior setting results mle using results biased estimate illustrate effect variance consider simple example figure left plots individual ﬁtted curve right plots average ﬁtted curve see increase strength regularizer variance decreases bias increases bias variance tradeoff classiﬁcation use loss instead squared error analysis breaks since frequentist risk longer expressible squared bias plus variance fact one show exercise hastie bias variance combine multiplicatively estimate chapter frequentist statistics figure illustration bias variance tradeoff ridge regression generate data sets true function shown solid green left plot regularized different data sets use linear regression gaussian rbf expansion centers evenly spread interval right plot average ﬁts averaged datasets top row strongly regularized see individual ﬁts similar low variance average far truth high bias bottom row lightly regularized see individual ﬁts quite different high variance average close truth low bias based bishop figure figure generated biasvarmodelcomplexity correct side decision boundary bias negative decreasing variance decrease misclassiﬁcation rate estimate wrong side decision boundary bias positive pays increase variance friedman little known fact illustrates bias variance tradeoff useful classiﬁcation better focus expected loss see directly bias variance approximate expected loss using cross validatinon discuss section 
[frequentist, statistics, empirical, risk, minimization] frequentist decision theory suffers fundamental problem one cannot actually compute risk function since relies knowing true data distribution contrast bayesian posterior expected loss always computed since conditions data rather conditioning however one setting avoids problem task predict observable quantities opposed estimating hidden variables parameters instead looking loss functions form true unknown parameter estimator let look loss empirical risk minimization functions form true unknown response prediction given input case frequentist risk becomes represents nature distribution course distribution unknown simple approach use empirical distribution derived training data approximate emp deﬁne empirical risk follows emp emp case loss becomes misclassiﬁcation rate case squared error loss becomes mean squared error deﬁne task empirical risk minimization erm ﬁnding decision procedure typically classiﬁcation rule minimize empirical risk erm argmin emp unsupervised case eliminate references replace example measures reconstruc tion error deﬁne decision rule using decode encode vector quantization section pca section finally deﬁne empirical risk emp course always trivially minimize risk setting critical encoder decoder via kind bottleneck 
[frequentist, statistics, empirical, risk, minimization, regularized, risk, minimization] note empirical risk equal bayes risk prior nature distribution exactly equal empirical distribution minka emp emp therefore minimizing empirical risk typically result overﬁtting therefore often necessary add complexity penalty objective function emp chapter frequentist statistics measures complexity prediction function controls strength complexity penalty approach known regularized risk minimization rrm note loss function negative log likelihood regularizer negative log prior equivalent map estimation two key issues rrm measure complexity pick linear model deﬁne complexity terms degrees freedom discussed section general models use dimension discussed section pick use methods discussed section 
[frequentist, statistics, empirical, risk, minimization, structural, risk, minimization] regularized risk minimization principle says model given complexity penalty using argmin emp pick cannot using training set since underestimate true risk problem known optimism training error alternative use following rule known structural risk minimization principle vapnik argmin estimate risk two widely used estimates cross validation theoretical upper bounds risk discuss 
[frequentist, statistics, empirical, risk, minimization, estimating, risk, using, cross, validation] estimate risk estimator using validation set separate validation set use cross validation brieﬂy discussed section precisely deﬁned follows let data cases training set denote data test fold data stratiﬁed folds chosen class proportions discrete labels present roughly equal fold let learning algorithm ﬁtting function takes dataset model index could discrete index degree polynomial continuous index strength regularizer returns parameter vector finally let prediction function takes input parameter vector returns prediction thus combined predict cycle denoted empirical risk minimization fold estimate risk deﬁned note call ﬁtting algorithm per fold let function trained data except test data fold rewrite estimate fold used test data words predict using model trained data contain method known leave one cross validation loocv case estimated risk becomes requires ﬁtting model times omit training case fortunately model classes loss functions namely linear models quadratic loss model analytically remove effect training case known generalized cross validation gcv example using pick ridge regression concrete example consider picking strength regularizer penalized linear regression use following rule arg min min max train min max ﬁnite range values search train fold estimate risk using given train train prediction function trained data excluding fold arg min map estimate figure gives example estimate risk log loss function squared error performing classiﬁcation usually use loss case optimize convex upper bound empirical risk estimate optimize estimate risk estimate handle non smooth loss function estimating using brute force search entire one dimensional space one two tuning parameters approach becomes infeasible cases one use empirical bayes allows one optimize large numbers hyper parameters using gradient based optimizers instead brute force search see section details chapter frequentist statistics log lambda mean squared error train mse test mse log lambda mse fold cross validation ntrain figure mean squared error penalized degree polynomial regression log regularizer figures except training points instead stars correspond values used plot functions figure estimate vertical scale truncated clarity blue line corresponds value chosen one standard error rule figure generated linregpolyvsregdemo one standard error rule procedure estimates risk give measure uncertainty standard frequentist measure uncertainty estimate standard error mean deﬁned estimate variance loss note measures intrinsic variability across samples whereas measures uncertainty mean suppose apply set models compute mean estimated risks common heuristic picking model noisy estimates pick value corresponds simplest model whose risk one standard error risk best model called one standard error rule hastie example figure see heuristic choose lowest point curve one slightly right since corresponds heavily regularized model essentially empirical performance empirical risk minimization model selection non probabilistic unsupervised learning performing unsupervised learning must use loss function measures reconstruction error encode decode scheme however discussed section cannot use determine complexity since always get lower loss complex model even evaluated test set complex models compress data less induce less distortion consequently must either use probabilistic models invent heuristics 
[frequentist, statistics, empirical, risk, minimization, upper, bounding, risk, using, statistical, learning, theory] principle problem cross validation slow since model multiple times motivates desire compute analytic approximations bounds generalization error studied ﬁeld statistical learning theory slt precisely slt tries bound risk data distribution hypothesis terms empirical risk emp sample size size hypothesis space let initially consider case hypothesis space ﬁnite size dim words selecting model hypothesis ﬁnite list rather optimizing real valued parameters prove following theorem data distribution dataset size drawn probability estimate error rate wrong worst case upper bounded follows max emp dim proof prove need two useful results first hoeffding inequality states ber second union bound says set events finally notational brevity let true risk emp empirical risk using results max dim chapter frequentist statistics ths bound tells optimism training error increases dim creases expected hypothesis space inﬁnite real valued parameters cannot use dim instead use quantity called vapnik chervonenkis dimen sion hypothesis class see vapnik details stepping back theory key intuition behind statistical learning theory quite simple suppose ﬁnd model low empirical risk hypothesis space big relative data size quite likely got lucky given data set well modeled chosen function chance however mean function low generalization error hypothesis class sufficiently constrained size training set sufficiently large unlikely get lucky way low empirical risk evidence low true risk note optimism training error necessarily increase model complexity increase number different models searched advantage statistical learning theory compared bounds risk quicker compute using disadvantage hard compute dimension many interesting models upper bounds usually loose although see kaariainen langford one extend statistical learning theory taking computational complexity learner account ﬁeld called computational learning theory colt work focuses case binary classiﬁer loss function loss observe low empirical risk hypothesis space suitably small say estimated function probably approximately correct pac hypothesis space said efficiently pac learnable polynomial time algorithm identify function pac see kearns vazirani details 
[frequentist, statistics, empirical, risk, minimization, surrogate, loss, functions] minimizing loss erm rrm framework always easy example might want optimize auc scores simply might want minimize loss common classiﬁcation unfortunately risk non smooth objective hence hard optimize one alternative use maximum likelihood estimation instead since log likelihood smooth convex upper bound risk show see consider binary logistic regression let suppose decision function computes log odds ratio log corresponding probability distribution output label sigm let deﬁne log loss nll log log pathologies frequentist statistics loss hinge logloss figure illustration various loss functions binary classiﬁcation horizontal axis margin vertical axis loss log loss uses log base figure generated hingelossplot clear minimizing average log loss equivalent maximizing likelihood consider computing probable label equivalent using loss function becomes figure plots two loss functions see nll indeed upper bound loss log loss example surrogate loss function another example hinge loss hinge max see figure plot see function looks like door hinge hence name loss function forms basis popular classiﬁcation method known support vector machines svm discuss section surrogate usually chosen convex upper bound since convex functions easy minimize see bartlett information 
[frequentist, statistics, pathologies, frequentist, statistics] believe would difficult persuade intelligent person current frequentist statistical practice sensible would much less difficulty approach via likelihood bayes theorem george box frequentist statistics exhibits various forms weird undesirable behaviors known pathologies give examples order caution reader examples explained detail lindley lindley phillips lindley berger jaynes minka chapter frequentist statistics 
[frequentist, statistics, pathologies, frequentist, statistics, counter-intuitive, behavior, conﬁdence, intervals] conﬁdence interval interval derived sampling distribution estimator whereas bayesian credible interval derived posterior parameter dis cussed section precisely frequentist conﬁdence interval parameter deﬁned following rather natural expression sample hypothetical future data conﬁdence interval parameter lies inside interval percent time let step back moment think going bayesian statistics condition known namely observed data average known namely parameter frequentist statistics exactly opposite condition unknown namely true parameter value average hypothetical future data sets counter intuitive deﬁnition conﬁdence intervals lead bizarre results consider following example berger suppose draw two integers otherwise would expect following outcomes probability let min deﬁne following conﬁdence interval samples yields hence equation clearly since contained intervals however know must yet conﬁdence fact another less contrived example follows suppose want estimate parameter bernoulli distribution let sample mean mle approximate conﬁdence interval bernoulli parameter called wald interval based gaussian approximation binomial distribution compare equation consider single trial mle overﬁts saw section conﬁdence interval also seems even worse argued ﬂaw approximated true sampling distribution gaussian sample size small parameter extreme however wald interval behave badly even large non extreme parameters brown pathologies frequentist statistics 
[frequentist, statistics, pathologies, frequentist, statistics, p-values, considered, harmful] suppose want decide whether accept reject baseline model call null hypothesis need deﬁne decision rule frequentist statistics standard ﬁrst compute quantity called value deﬁned probability null observing test statistic chi squared statistic large larger actually observed pvalue quantity relies computing tail area probability sampling distribution give example given value deﬁne decision rule follows reject null hypothesis iff value less threshold reject say difference observed test statistic expected test statistic statistically signiﬁcant level approach known null hypothesis signiﬁcance testing nhst procedure guarantees expected type false positive error rate sometimes interpreted saying frequentist hypothesis testing conservative since unlikely accidently reject null hypothesis fact opposite case method worries trying reject null never gather evidence favor null matter large sample size values tend overstate evidence null thus trigger happy general huge differences values quantity really care posterior probability null hypothesis given data particular sellke show even value slow posterior probability least often much higher frequentists often claim signiﬁcant evidence effect cannot explained null hypothesis whereas bayesians usually conservative claims example values used prove esp extra sensory perception real wagenmakers even though esp clearly improbable reason values banned certain medical journals matthews another problem values computation depends decisions make stop collecting data even decisions change data actually observed example suppose toss coin times observe successes heads failures tails case ﬁxed hence random relevant sampling model binomial bin let null hypothesis coin fair probability success heads one sided value using test statistic bin reason cannot compute probability observed value test statistic probability zero pdf value deﬁned terms cdf always number chapter frequentist statistics two sided value bin bin either case value larger magical threshold frequentist would reject null hypothesis suppose told actually kept tossing coin observed tails case ﬁxed hence random probability model becomes negative binomial distribution given negbinom note term depends equations posterior would cases however two interpretations data give different values particular negative binomial model get value suddenly seems signiﬁcant evidence bias coin obviously ridiculous data inferences coin could chosen experimental protocol random outcome experiment matters details decided one run although might seem like mathematical curiosity also signiﬁcant practical implications particular fact stopping rule affects computation value means frequentists often terminate experiments early even obvious conclusions lest adversely affect statistical analysis experiments costly harmful people obviously bad idea perhaps surprising food drug administration fda regulates clinical trials new drugs recently become supportive bayesian methods since bayesian methods affected stopping rule 
[frequentist, statistics, pathologies, frequentist, statistics, likelihood, principle] fundamental reason many pathologies frequentist inference violates likelihood principle says inference based likelihood observed data based hypothetical future data observed bayes obviously satisﬁes likelihood principle consequently suffer pathologies compelling argument favor likelihood principle presented birnbaum showed followed automatically two simpler principles ﬁrst sufficiency principle says sufficient statistic contains relevant information see http yamlb wordpress com fda becoming progressively bayes ian pathologies frequentist statistics unknown parameter arguably true deﬁnition second principle known weak conditionality says inferences based events happened might happened motivate consider example berger suppose need analyse substance send either laboratory new york california two labs seem equally good fair coin used decide coin comes heads california lab chosen results come back taken account coin could come tails thus new york lab could used people would argue new york lab irrelevant since tails event happen example weak conditionality given principle one show inferences based observed contrast standard frequentist procedures see berger wolpert details likelihood principle 
[frequentist, statistics, pathologies, frequentist, statistics, isn’t, everyone, bayesian] given fundamental ﬂaws frequentist statistics fact bayesian methods ﬂaws obvious question ask everyone bayesian frequentist statistician bradley efron wrote paper exactly title efron short paper well worth reading anyone interested topic quote opening section title reasonable question ask least two counts first everone used bayesian laplace wholeheatedly endorsed bayes formulation inference problem century scientists followed suit included gauss whose statistical work usually presented frequentist terms second important point cogency bayesian argument modern statisticians following lead savage finetti advanced powerful theoret ical arguments preferring bayesian inference byproduct work disturbing catalogue inconsistencies frequentist point view nevertheless everyone bayesian current era ﬁrst century statistics widely used scientiﬁc reporting fact century statistics mainly non bayesian however lindley predicts change century time tell whether lindley right 
[frequentist, statistics, exercises] exercise pessimism loocv source witten suppose completely random labeled dataset features tell nothing class labels examples class examples class best misclassiﬁcation rate method achieve estimated misclassiﬁcation rate method using loocv exercise james stein estimator gaussian means consider stage model suppose known observe following data points chapter frequentist statistics find estimates find posterior estimates var terms computed similarly give credible interval trust interval assuming gaussian assumption reasonable likely large small right expect would happen estimates much smaller say need compute numerical answer brieﬂy explain would happen qualitatively exercise mle biased show mle biased estimator show hint note independent use fact expectation product independent random variables product expectations exercise estimation known suppose sample known constant derive expression mle case unbiased 
[linear, regression, introduction] linear regression work horse statistics supervised machine learning augmented kernels forms basis function expansion model also non linear relationships gaussian output replaced bernoulli multinoulli distribution used classiﬁcation see pays study model detail 
[linear, regression, model, speciﬁcation] discussed section linear regression model form linear regression made model non linear relationships replacing non linear function inputs use known basis function expansion note model still linear parameters still called linear regression importance become clear simple example polynomial basis functions model form figure illustrates effect changing increasing degree allows create increasingly complex functions also apply linear regression input example consider modeling temperature function location figure plots figure plots 
[linear, regression, maximum, likelihood, estimation, least, squares] common way esitmate parameters statistical model compute mle deﬁned arg max log chapter linear regression figure linear regression applied data vertical axis temperature horizontal axes location within room data collected remote sensing motes intel lab berkeley data courtesy romain thibaux ﬁtted plane form temperature data ﬁtted quadratic form produced surfacefitdemo common assume training examples independent identically distributed commonly abbreviated iid means write log likelihood follows log log instead maximizing log likelihood equivalently minimize negative log likeli hood nll nll log nll formulation sometimes convenient since many optimization software packages designed ﬁnd minima functions rather maxima let apply method mle linear regression setting inserting deﬁnition gaussian ﬁnd log likelihood given log exp rss log rss stands residual sum squares deﬁned rss rss also called sum squared errors sse sse called mean squared error mse also written square norm vector maximum likelihood estimation least squares prediction truth sum squares error contours linear regression figure linear least squares try minimize sum squared distances training point denoted red circle approximation denoted blue cross minimize sum lengths little vertical blue lines red diagonal line represents least squares regression line note residual lines perpendicular least squares line contrast figure figure generated residualsdemo contours rss error surface example red cross represents mle figure generated contoursssedemo residual errors rss see mle one minimizes rss method known least squares method illustrated figure training data shown red circles estimated values shown blue crosses residuals shown vertical blue lines goal ﬁnd setting parameters slope intercept resulting red line minimizes sum squared residuals lengths vertical blue lines figure plot nll surface linear regression example see quadratic bowl unique minimum derive importantly true even use basis function expansion polynomials nll still linear parameters even linear inputs 
[linear, regression, maximum, likelihood, estimation, least, squares, derivation, mle] first rewrite objective form amenable differentiation nll chapter linear regression sum squares matrix using results equation see gradient given equating zero get known normal equation corresponding solution linear system equations called ordinary least squares ols solution given ols 
[linear, regression, maximum, likelihood, estimation, least, squares, geometric, interpretation] equation elegant geometrical intrepretation explain assume examples features columns deﬁne linear subspace dimensionality embedded dimensions let column vector confused represents data case similarly vector example suppose examples dimensions vectors illustrated figure seek vector lies linear subspace close possible want ﬁnd argmin span since span exists weight vector maximum likelihood estimation least squares figure graphical interpretation least squares examples features vectors together deﬁne plane also vector lie plane orthogonal projection onto plane denoted red line residual whose norm want minimize visual clarity vectors converted unit norm figure generated leastsquaresprojection minimize norm residual want residual vector orthogonal every column hence hence projected value given corresponds orthogonal projection onto column space projection matrix called hat matrix since puts hat 
[linear, regression, maximum, likelihood, estimation, least, squares, convexity] discussing least squares noted nll bowl shape unique minimum technical term functions like convex convex functions play important role machine learning let deﬁne concept precisely say set convex chapter linear regression figure illustration convex set illustration nonconvex set figure illustration convex function see chord joining lies function function neither convex concave local minimum global minimum figure generated convexfnhand draw line points line lie inside set see figure illustration convex set figure illustration non convex set function called convex epigraph set points function deﬁnes convex set equivalently function called convex deﬁned convex set log examples scalar concave functions include log intuitively strictly convex function bowl shape hence unique global minimum corresponding bottom bowl hence second derivative must positive everywhere twice continuously differentiable multivariate function convex iff hessian positive deﬁnite machine learning context function often corresponds nll recall hessian matrix second partial derivatives deﬁned also recall matrix positive deﬁnite iff non zero vector robust linear regression linear data noise outliers least squares laplace huber figure illustration robust linear regression figure generated linregrobustdemocombined illustration huber loss functions figure generated huberlossdemo models nll convex desirable since means always ﬁnd globally optimal mle see many examples later book however many models interest concave likelihoods cases discuss ways derive locally optimal parameter estimates 
[linear, regression, robust, linear, regression] common model noise regression models using gaussian distribution zero mean constant variance case maximizing likelihood equivalent minimizing sum squared residuals seen however outliers data result poor illustrated figure outliers points bottom ﬁgure squared error penalizes deviations quadratically points far line affect points near line one way achieve robustness outliers replace gaussian distribution response variable distribution heavy tails distribution assign higher likelihood outliers without perturb straight line explain one possibility use laplace distribution introduced section use observation model regression get following likelihood lap exp robustness arises use instead simplicity assume ﬁxed let residual nll form chapter linear regression likelihood prior name section gaussian uniform least squares gaussian gaussian ridge gaussian laplace lasso laplace uniform robust regression student uniform robust regression exercise table summary various likelihoods priors used linear regression likelihood refers distributional form prior refers distributional form map estimation uniform distribution corresponds mle unfortunately non linear objective function hard optimize fortunately convert nll linear objective subject linear constraints using following split variable trick first deﬁne impose linear inequality constraints constrained objective becomes min example linear program unknowns constraints since convex optimization problem unique solution solve must ﬁrst write standard form follows min current example solved solver see boyd vandenberghe see figure example method action alternative using nll laplace likelihood minimize huber loss function huber deﬁned follows equivalent errors smaller equivalent larger errors see figure advantage loss function everywhere differentiable using fact sign also check function continuous since gradients two parts function match namely consequently optimizing huber loss much faster using laplace likelihood since use standard smooth optimization methods quasi newton instead linear programming figure gives illustration huber loss function results qualitatively similiar probabilistic methods fact turns huber method also probabilistic interpretation although rather unnatural pontil ridge regression lambda lambda figure degree polynomial data points increasing amounts regularization data generated noise variance error bars representing noise variance get wider gets smoother since ascribing data variation noise figure generated linregpolyvsregdemo 
[linear, regression, ridge, regression] one problem estimation result overﬁtting section discuss way ameliorate problem using map estimation gaussian prior simplicity assume gaussian likelihood rather robust likelihood 
[linear, regression, ridge, regression, basic, idea] reason mle overﬁt picking parameter values best modeling training data data noisy parameters often result complex functions simple example suppose degree polynomial data points using least squares resulting curve wiggly shown figure corresponding least squares coefficients excluding follows see many large positive negative numbers balance exactly make curve wiggle right way almost perfectly interpolates data situation unstable changed data little coefficients would change lot encourage parameters small thus resulting smoother curve using zero mean gaussian prior controls strength prior corresponding map estimation problem becomes argmax log log chapter linear regression log lambda mean squared error train mse test mse log lambda negative log marg likelihood estimate mse figure training error dotted blue test error solid red degree polynomial ridge regression plotted log data generated noise variance training set size note models ordered complex small regularizer left simple large regularizer right stars correspond values used plot functions figure estimate performance using training set dotted blue fold cross validation estimate future mse solid black negative log marginal likelihood log curves vertically rescaled make comparable figure generated linregpolyvsregdemo simple exercise show equivalent minimizing following squared two norm ﬁrst term mse nll usual second term complexity penalty corresponding solution given ridge technique known ridge regression penalized least squares general adding gaussian prior parameters model encourage small called regularization weight decay note offset term regularized since affects height function complexity penalizing sum magnitudes weights ensure function simple since corresponds straight line simplest possible function corresponding constant illustrate idea figure see increasing results smoother functions resulting coefficients also become smaller example using ridge regression figure plot mse training test sets log see increase model becomes constrained error training set increases test set see characteristic shaped curve model overﬁts underﬁts common use cross validation pick shown figure section discuss probabilistic approach consider variety different priors book corresponds different form regularization technique widely used prevent overﬁtting 
[linear, regression, ridge, regression, numerically, stable, computation] interestingly ridge regression works better statistically also easier numerically since much better conditioned hence likely invertible least suitable largy nevertheless inverting matrices still best avoided reasons numerical stability indeed write inv matlab give warning describe useful trick ﬁtting ridge regression models hence extension computing vanilla ols estimates numerically robust assume prior form precision matrix case ridge regression avoid penalizing term center data ﬁrst explained exercise first let augment original data virtual data coming prior cholesky decomposition see extra rows represent pseudo data prior show nll expanded data equivalent penalized nll original data hence map estimate given ridge claimed chapter linear regression let decomposition orthonormal meaning upper triangular hence ridge note easy invert since upper triangular gives way compute ridge estimate avoiding invert use technique ﬁnd mle simply computing decomposition unaugmented matrix using original method choice solving least squares problems fact sommon implemented one line matlab using backslash operator note computing decomposition matrix takes time numerically stable ﬁrst perform svd decomposition particular let usv svd diagonal matrix let matrix rewrite ridge estimate thus ridge words replace dimensional vectors dimensional vectors perform penalized transform dimensional solution dimensional solution multiplying geometrically rotating new coordinate system ﬁrst coordinates zero affect solution since spherical gaussian prior rotationally invariant overall time operations 
[linear, regression, ridge, regression, connection, pca] section discuss interesting connection ridge regression pca sec tion gives insight ridge regression works well discussion based hastie let usv svd equation ridge hence ridge predictions training set given ridge usv ridge regression prior mean map estimate estimate figure geometry ridge regression likelihood shown ellipse prior shown circle centered origin based figure bishop figure generated geomridge singular values hence ridge contrast least squares prediction usv small compared direction much effect prediction view deﬁne effective number degrees freedom model follows dof dof dof let try understand behavior desirable section show cov use uniform prior thus directions uncertain determined eigenvectors matrix smallest eigenvalues shown figure furthermore section show squared singular values equal eigenvalues hence small singular values correspond directions high posterior variance directions ridge shrinks chapter linear regression process illustrated figure horizontal parameter well determined data high posterior variance vertical parameter well determined hence map close mle map shifted strongly towards prior mean compare figure illustrated sensor fusion sensors different reliabilities way ill determined parameters reduced size towards called shrinkage related different technique called principal components regression idea ﬁrst use pca reduce dimensionality dimensions use low dimensional features input regression however technique work well ridge terms predictive accuracy hastie reason regression ﬁrst derived dimensions retained remaining dimensions entirely ignored contrast ridge regression uses soft weighting dimensions 
[linear, regression, ridge, regression, regularization, effects, big, data] regularization common way avoid overﬁtting however another effective approach always available use lots data intuitively obvious training data better able learn expect test set error decrease plateau increases illustrated figure plot mean squared error incurred test set achieved polynomial regression models different degrees plot error training set size known learning curve level plateau test error consists two terms irreducible component models incur due intrinsic variability generating process called noise ﬂoor component depends discrepancy generating process truth model called structural error figure truth degree polynomial try ﬁtting polynomials degrees data call models see structural error models zero since able capture true generating process however structural error substantial evident fact plateau occurs high noise ﬂoor model expressive enough capture truth one small structural error test error noise ﬂoor however typically zero faster simpler models since fewer parameters estimate particular ﬁnite training sets discrepancy parameters estimate best parameters could estimate given particular model class called approximation error goes zero goes zero faster simpler models illustrated figure see also exercise domains lots data simple methods work surprisingly well halevy however still reasons study sophisticated learning methods always problems little data example even data rich domain web search soon want start personalizing results amount data available given user starts look small relative complexity problem assumes training data randomly sampled get repetitions examples informatively sampled data help even motivation approach known active learning get choose training data bayesian linear regression size training set mse truth degree model degree train test size training set mse truth degree model degree train test size training set mse truth degree model degree train test size training set mse truth degree model degree train test figure mse training test sets size training set data generated degree polynomial gaussian noise variance polynomial models varying degree data degree degree degree degree note small training set sizes test error degree polynomial higher degree polynomial due overﬁtting difference vanishes enough data note also degree polynomial simple high test error even given large amounts training data figure generated linregpolyvsn cases may want learn multiple related models time known multi task learning allow borrow statistical strength tasks lots data share tasks little data discuss ways later book 
[linear, regression, bayesian, linear, regression, computing, posterior] linear regression likelihood given exp offset term inputs centered mean output equally likely positive negative let put improper prior form integrate get exp empirical mean output notational simplicity shall assume output centered write conjugate prior gaussian likelihood also gaussian denote using bayes rule gaussians equation posterior given posterior mean reduces ridge estimate deﬁne mean mode gaussian gain insight posterior distribution mode let consider example true parameters figure plot prior likelihood posterior samples posterior predictive particular right hand column plots function ranges sample parameter posterior initially sample prior ﬁrst row predictions place since prior uniform see one data point second row posterior becomes constrained corresponding likelihood predictions pass close observed data however see posterior ridge like shape reﬂecting fact many possible solutions different bayesian linear regression prior posterior data space likelihood figure sequential bayesian updating linear regression model row represents prior row represents ﬁrst data point row represents second data point row represents data point left column likelihood function current data point middle column posterior given data far ﬁrst line prior right column samples current prior posterior predictive distribution white cross columns represents true parameter value see mode posterior rapidly samples converges point blue circles column observed data points based figure bishop figure generated bayeslinregdemod slopes intercepts makes sense since cannot uniquely infer two parameters one observation see two data points third row posterior becomes much narrower predictions similar slopes intercepts observe data points last row posterior essentially delta function centered true value indicated white cross estimate converges truth since data generated model bayes consistent estimator see section discussion point 
[linear, regression, bayesian, linear, regression, computing, posterior, predictive] tough make predictions especially future yogi berra chapter linear regression machine learning often care predictions interpreting parame ters using equation easily show posterior predictive distribution test point also gaussian variance prediction depends two terms variance observation noise variance parameters latter translates variance observations way depends close training data illustrated figure see error bars get larger move away training points representing increased uncertainty important applications active learning want model know well contrast plugin approximation constant sized error bars since see figure 
[linear, regression, bayesian, linear, regression, unknown] section apply results section problem computing linear regression model generalizes results section assumed known case use uninformative prior see interesting connections frequentist statistics conjugate prior usual likelihood form analogy section one show natural conjugate prior following form nig exp bayesian linear regression plugin approximation mle prediction training data posterior predictive known variance prediction training data functions sampled plugin approximation posterior functions sampled posterior figure plug approximation predictive density plug mle parameters posterior predictive density obtained integrating parameters black curve posterior mean error bars standard deviations posterior predictive density samples plugin approximation posterior predictive samples posterior predictive figure generated linregpostpreddemo prior likelihood one show posterior following form nig expressions similar case known expression also intuitive since updates counts expression interpreted chapter linear regression follows prior sum squares plus empirical sum squares plus term due error prior posterior marginals follows give worked example using equations section analogy section posterior predictive distribution student distribution particular given new test inputs predictive variance two components due measurement noise due uncertainty latter terms varies depending close test inputs training data common set corresponding uninformative prior set positive value called zellner prior zellner plays role analogous ridge regression however prior covariance proportional rather ensures posterior invariant scaling inputs minka see also exercise see use uninformative prior posterior precision given measurements unit information prior deﬁned contain much information one sample kass wasserman create unit information prior linear regression need use equivalent prior uninformative prior uninformative prior obtained considering uninformative limit conjugate prior corresponds setting equivalent improper nig prior gives alternatively start semi conjugate prior take term uninformative limit individually gives equivalent improper nig prior corresponding posterior given nig mle mle mle bayesian linear regression var sig table posterior mean standard deviation credible intervals linear regression model uninformative prior caterpillar data produced linregbayescaterpillar marginal distribution weights given mle discuss implications equations example bayesian frequentist inference coincide use semi conjugate uninformative prior interesting resulting posterior turns equivalent results frequentist statistics see also section particular equation equivalent sampling distribution mle given following see rice casella berger standard error estimated parameter see section discussion sampling distributions consequently frequentist conﬁdence interval bayesian marginal credible interval parameters case worked example consider caterpillar dataset marin robert details data mean matter present purposes compute chapter linear regression posterior mean standard deviation credible intervals regression coefficients using equation results shown table easy check credible intervals identical conﬁdence intervals computed using standard frequentist methods see linregbayescaterpillar code also use marginal posteriors compute coefficients signiﬁcantly different informal way without using decision theory check excludes table see cis coefficients signiﬁcant measure put little star easy check results produced standard frequentist software packages compute values level although correspondence bayesian frequentist results might seem pealing readers recall section frequentist inference riddled patholo gies also note mle even exist standard frequentist inference theory breaks setting bayesian inference theory still works although requires use proper priors see maruyama george one extension prior case 
[linear, regression, bayesian, linear, regression, linear, regression, evidence, procedure] far assumed prior known section describe empirical bayes procedure picking hyper parameters precisely choose maximize marignal likelihood precision observation noise precision prior known evidence procedure mackay see section algorithmic details evidence procedure provides alternative using cross validation example figure plot log marginal likelihood different values well maximum value found optimizer see example get result shown figure kept ﬁxed methods make comparable principle practical advantage evidence procedure become apparent section generalize prior allowing different every feature used perform feature selection using technique known automatic relevancy determination ard contrast would possible use tune different hyper parameters evidence procedure also useful comparing different kinds models since provides good approximation evidence max important least approximately integrate rather setting arbitrarily reasons discussed section indeed method used evaluate marginal alternatively could integrate analytically shown section optimize buntine weigend however turns less accurate optimizing mackay bayesian linear regression log lambda mse fold cross validation ntrain log alpha log evidence figure estimate test mse produced fold cross validation log smallest value indicated vertical line note vertical scale log units log marginal likelihood log largest value indicated vertical line figure generated linregpolyvsregdemo likelihood polynomial regression models figures bayesian approach model uncertainty rather computing point estimates see section 
[linear, regression, exercises] exercise behavior training set error increasing sample size error test always decrease get training data since model better estimated however shown figure sufficiently complex models error training set increase get training data reach plateau explain exercise multi output linear regression source jaakkola multiple independent outputs linear regression model becomes since likelihood factorizes across dimensions mle thus exercise apply result model dimensional response vector suppose binary input data training data follows chapter linear regression let embed using following basis function model becomes matrix compute mle data exercise centering ridge regression assume input data centered show optimizer exercise mle linear regression show mle error variance linear regression given empirical variance residual errors plug estimate exercise mle offset term linear regression linear regression form common include column design matrix solve offset term term parameters time using normal equations however also possible solve separately show models difference average output average predicted output also show centered input matrix containing along rows centered output vector thus ﬁrst compute centered data estimate using bayesian linear regression exercise mle simple linear regression simple linear regression refers case input scalar show mle case given following equations may familiar basic statistics classes cov var see linregdemo demo exercise sufficient statistics online linear regression source jaakkola consider ﬁtting model using least squares unfortunately keep original data following functions statistics data minimal set statistics need estimate hint see equation minimal set statistics need estimate hint see equation suppose new data point arrives want update sufficient statistics without looking old data stored useful online learning show follows form new estimate old estimate plus correction see size correction diminishes time get samples derive similar expression update show one update recursively using derive similar expression update implement online learning algorithm write function form linregupdatess scalars structure containing sufficient statistics plot coefficients time using dataset linregdemo speciﬁcally use polydatamake sampling thibaux check converge solution given batch offline learner ordinary least squares result look like figure turn derivation code plot exercise bayesian linear regression known source bolstad consider ﬁtting model form data shown chapter linear regression time weights online linear regression batch batch figure regression coefficients time produced exercise compute unbiased estimate using denominator since inputs namely offset term mle assume following prior use improper uniform prior prior show written gaussian prior form compute marginal posterior slope data unbiased estimate computed var show work use matlab like hint posterior variance small number credible interval exercise generative model linear regression linear regression problem estimating using linear function form typically assume conditional distribution given gaussian either estimate conditional gaussian directly discriminative approach gaussian joint distribution derive exercise showed discriminative approach leads equations bayesian linear regression centered input matrix replicates across rows similarly centered output vector replicates across rows ﬁnding maximum likelihood estimates derive equations ﬁtting joint gaussian using formula conditioning gaussian see section show work advantages disadvantages approach compared standard discriminative approach exercise bayesian linear regression using prior show use prior nig posterior following form nig mle mle mle 
[logistic, regression, introduction] one way build probabilistic classiﬁer create joint model form condition thereby deriving called generative approach alternative approach model form directly called discrimi native approach approach adopt chapter particular assume discriminative models linear parameters turn signiﬁcantly sim plify model ﬁtting see section compare generative discriminative approaches later chapters consider non linear non parametric discriminative models 
[logistic, regression, model, speciﬁcation] discussed section logistic regression corresponds following binary classiﬁ cation model ber sigm example shown figure logistic regression easily extended higher dimensional inputs example figure shows plots sigm input different weight vectors threshold probabilities induce linear decision boundary whose normal perpendicular given 
[logistic, regression, model, ﬁtting, mle] negative log likelihood logistic regression given nll log log log also called cross entropy error function see section another way writing follows suppose instead exp exp hence log exp unlike linear regression longer write mle closed form instead need use optimization algorithm compute need derive gradient hessian case logistic regression one show exercise gradient hessian model ﬁtting figure gradient descent simple function starting steps using ﬁxed learning rate step size global minimum figure generated steepestdescentdemo given following diag one also show exercise positive deﬁnite hence nll convex unique global minimum discuss methods ﬁnding minimum 
[logistic, regression, model, ﬁtting, steepest, descent] perhaps simplest algorithm unconstrained optimization gradient descent also known steepest descent written follows step size learning rate main issue gradient descent set step size turns quite tricky use constant learning rate make small convergence slow make large method fail converge illustrated figure plot following convex function arbitrarily decide start figure use ﬁxed step size see moves slowly along valley figure use ﬁxed step size see algorithm starts oscillating sides valley never converges optimum chapter logistic regression exact line searching figure steepest descent function figure starting using line search figure generated steepestdescentdemo illustration fact end line search top picture local gradient function perpendicular search direction based figure press let develop stable method picking step size method guaran teed converge local optimum matter start property called global convergence confused convergence global optimum taylor theorem descent direction chosen small enough since gradient negative want choose step size small move slowly may reach minimum let pick minimize called line minimization line search various methods solving optimization problem see nocedal wright details figure demonstrates line search indeed work simple problem however see steepest descent path exact line searches exhibits characteristic zig zag behavior see note exact line search satisﬁes arg min ηgt necessary condition optimum chain rule gradient end step either means found stationary point means exact search stops point local gradient perpendicular search direction hence consecutive directions orthogonal see figure explains zig zag behavior one simple heuristic reduce effect zig zagging add momentum term follows model ﬁtting controls importance momentum term optimization community known heavy ball method see bertsekas alternative way minimize zig zagging use method conjugate gradients see nocedal wright golub van loan sec method choice quadratic objectives form arise solving linear systems however non linear less popular 
[logistic, regression, model, ﬁtting, newton’s, method] algorithm newton method minimizing strictly convex function initialize convergence evaluate evaluate solve use line search ﬁnd stepsize along one derive faster optimization methods taking curvature space hessian account called second order optimization metods primary example newton algorithm iterative algorithm consists updates form full pseudo code given algorithm algorithm derived follows consider making second order taylor series approximation around quad let rewrite quad minimum quad thus newton step added minimize second order approximation around see figure illustration chapter logistic regression quad quad figure illustration newton method minimizing function solid curve function dotted line quad second order approximation newton step must added get minimum quad based figure vandenberghe figure generated newtonsmethodminquad illustration newton method applied nonconvex function quadratic around current point move stationary point unfortunately local maximum minimum means need careful extent quadratic approximation based figure vandenberghe figure generated newtonsmethodnonconvex simplest form listed newton method requires positive deﬁnite hold function strictly convex objective function convex may positive deﬁnite may descent direction see figure example case one simple strategy revert steepest descent levenberg marquardt algorithm adaptive way blend newton directly solve linear system equations using conjugate gradient positive deﬁnite simply truncate iterations soon negative curvature detected called truncated newton 
[logistic, regression, model, ﬁtting, iteratively, reweighted, least, squares, irls] since hessian exact deﬁned working response model ﬁtting equation example weighted least squares problem minimizer since diagonal matrix rewrite targets component form case algorithm known iteratively reweighted least squares irls short since iteration solve weighted least squares problem weight matrix changes iteration see algorithm pseudocode algorithm iteratively reweighted least squares irls log repeat sigm diag converged 
[logistic, regression, model, ﬁtting, quasi-newton, variable, metric, methods] mother second order optimization algorithm newton algorithm dis cussed section unfortunately may expensive compute explicitly quasi newton methods iteratively build approximation hessian using information gleaned gradient vector step common method called bfgs named inventors broyden fletcher goldfarb shanno updates approximation hessian follows rank two update matrix ensures matrix remains positive deﬁnite certain restrictions step size typically start diagonal approximation thus bfgs thought diagonal plus low rank approximation hessian chapter logistic regression alternatively bfgs iteratively update approximation inverse hessian follows since storing hessian takes space large problems one use limited memory bfgs bfgs approximated diagonal plus low rank matrix particular product obtained performing sequence inner products using recent pairs ignoring older information storage requirements therefore typically suffices good performance see nocedal wright information bfgs often method choice unconstrained smooth optimization problems arise machine learning although see section 
[logistic, regression, model, ﬁtting, regularization] prefer ridge regression linear regression prefer map estimation logistic regression computing mle fact regularization important classiﬁcation setting even lots data see suppose data linearly separable case mle obtained corresponding inﬁnitely steep sigmoid function also known linear threshold unit assigns maximal amount probability mass training data however solution brittle generalize well prevent use regularization ridge regression note new objective gradient hessian following forms nll simple matter pass modiﬁed equations gradient based optimizer 
[logistic, regression, model, ﬁtting, multi-class, logistic, regression] consider multinomial logistic regression sometimes called maximum entropy classiﬁer model form exp exp slight variant known conditional logit model normalizes different set classes data case useful modeling choices users make different sets items offered let introduce notation let vector also let one encoding thus bit vector bit turns iff following krishnapuram let model ﬁtting set ensure identiﬁability deﬁne vec column vector log likelihood written log log log exp deﬁne nll proceed compute gradient hessian expression since block structured notation gets bit heavy ideas simple helps deﬁne kronecker product matrices matrix matrix block matrix returning task hand one show exercise gradient given column vectors length example feature dimensions classes becomes words class derivative weights column form binary logistic regression case namely error term times turns general property distributions exponential family see section chapter logistic regression one also show exercise hessian following block structured matrix diag example features classes becomes words block submatrix given also positive deﬁnite matrix unique mle consider minimizing log log new objective gradient hessian given passed gradient based optimizer ﬁnd map estimate note however hessian size times row columns binary case limited memory bfgs appropriate newton method see logregfit matlab code 
[logistic, regression, bayesian, logistic, regression] natural want compute full posterior parameters logistic regression models useful situation want associate conﬁdence intervals predictions necessary solving contextual bandit problems discussed section unfortunately unlike linear regression case cannot done exactly since convenient conjugate prior logistic regression discuss one simple approximation approaches include mcmc section variational inference section expectation propagation kuss rasmussen etc notational simplicity stick binary logistic regression bayesian logistic regression 
[logistic, regression, bayesian, logistic, regression, laplace, approximation] section discuss make gaussian approximation posterior distribution approximation works follows suppose let called energy function equal negative log unnormal ized log posterior log normalization constant performing taylor series expansion around mode lowest energy state get gradient hessian energy function evaluated mode since mode gradient term zero hence exp last line follows normalization constant multivariate gaussian equation known laplace approximation marginal likelihood therefore equation sometimes called laplace approximation posterior however statistics community term laplace approximation refers sophisticated method see rue details may therefore better use term gaussian approximation refer equation gaussian approximation often reasonable approximation since posteriors often become gaussian like sample size increases reasons analogous central limit theorem physics analogous technique known saddle point approximation 
[logistic, regression, bayesian, logistic, regression, derivation, bic] use gaussian approximation write log marginal likelihood follows dropping irrelevant constants log log log log penalization terms added log sometimes called occam factor measure model complexity uniform prior drop second term replace mle chapter logistic regression focus approximating third term log let approximate ﬁxed matrix log log log log log dim assumed full rank drop log term since independent thus get overwhelmed likelihood putting pieces together recover bic score section log log log 
[logistic, regression, bayesian, logistic, regression, gaussian, approximation, logistic, regression] let apply gaussian approximation logistic regression use gaussian prior form map estimation approximate posterior given arg min log log example consider linearly separable data figure many parameter settings correspond lines perfectly separate training data show examples likelihood surface shown figure see likelihood unbounded move right parameter space along ridge indicated diagonal line reasons maximize likelihood driving inﬁnity subject line since large regression weights make sigmoid function steep turning step function consequently mle well deﬁned data linearly separable regularize problem let use vague spherical prior centered origin multiplying spherical prior likelihood surface results highly skewed posterior shown figure posterior skewed likelihood function chops regions parameter space soft fashion disagree data map estimate shown blue dot unlike mle inﬁnity gaussian approximation posterior shown figure see symmetric distribution therefore great approximation course gets mode correct construction least represents fact uncertainty along southwest northeast direction corresponds uncertainty orientation separating lines perpendicular although crude approximation surely better approximating posterior delta function map estimation 
[logistic, regression, bayesian, logistic, regression, approximating, posterior, predictive] given posterior compute credible intervals perform hypothesis tests etc section case linear regression machine learning interest usually focusses prediction posterior predictive distribution form bayesian logistic regression data log likelihood log unnormalised posterior laplace approximation posterior figure two class data log likelihood logistic regression model line drawn origin direction mle inﬁnity numbers correspond points parameter space corresponding lines unnormalized log posterior assuming vague spherical prior laplace approximation posterior based ﬁgure mark girolami figure generated logreglaplacegirolamidemo unfortunately integral intractable simplest approximation plug approximation binary case takes form posterior mean context called bayes point course plug estimate underestimates uncertainty discuss better approximations chapter logistic regression wmap decision boundary sampled approx numerical approx figure posterior predictive distribution logistic regression model top left contours map top right samples posterior predictive distribution bottom left averaging samples bottom right moderated output probit approximation based ﬁgure mark girolami figure generated logreglaplacegirolamidemo monte carlo approximation better approach use monte carlo approximation follows sigm samples posterior technique trivially extended multi class case approximated posterior using monte carlo reuse samples prediction made gaussian approximation posterior draw independent samples gaussian using standard methods figure shows samples posteiror predictive example figure bayesian logistic regression sigmoid probit figure posterior predictive density sat data red circle denotes posterior mean blue cross posterior median blue lines denote percentiles predictive distribution figure generated logregsatdemobayes logistic sigmoid function sigm solid red rescaled probit function dotted blue superimposed chosen derivatives two curves match based figure bishop figure generated probitplot figure generated probitregdemo shows average samples averaging multiple predictions see uncertainty decision boundary splays move training data although decision boundary linear posterior predictive density linear note also posterior mean decision boundary roughly equally far classes bayesian analog large margin principle discussed section figure shows example red dots denote mean posterior predictive evaluated training data vertical blue lines denote credible intervals posterior predictive small blue star median see bayesian approach able model uncertainty probability student pass exam based sat score rather getting point estimate probit approximation moderated output gaussian approximation posterior also compute deterministic approximation posterior predictive distribution least binary case proceed follows sigm sigm var chapter logistic regression thus see need evaluate expectation sigmoid respect gaussian approximated exploiting fact sigmoid function similar probit function given cdf standard normal figure plots sigmoid probit functions rescaled axes sigm slope origin advantage using probit one convolve gaussian analytically plug approximation sigm sides equation get sigm sigm applying logistic regression model get following expression ﬁrst suggested spiegelhalter lauritzen sigm figure indicates gives similar results monte carlo approximation using equation sometimes called moderated output since less extreme plug estimate see note hence sigm sigm inequality strict moderated prediction always closer less conﬁdent however decision boundary occurs whenever sigm implies hence decision boundary moderated approximation plug approximation number misclassiﬁcations two methods log likelihood note multiclass case taking account posterior covariance gives different answers plug approach see exercise rasmussen williams 
[logistic, regression, bayesian, logistic, regression, residual, analysis, outlier, detection] sometimes useful detect data cases outliers called residual analysis case analysis regression setting performed computing values follow distribution modelling assumptions correct assessed creating plot plot theoretical quantiles gaussian distribution empirical quantiles points deviate straightline potential outliers online learning stochastic optimization classical methods based residuals work well binary data rely asymptotic normality test statistics however adopting bayesian approach deﬁne outliers points small typically use sigm note estimated data better method exclude estimate predicting deﬁne outliers points low probability cross validated posterior predictive distribution deﬁned efficiently approximated sampling methods gelfand discussion residual analysis logistic regression models see johnson albert sec 
[logistic, regression, online, learning, stochastic, optimization] traditionally machine learning performed offline means batch data optimize equation following form supervised case unsupervised case kind loss function example might use log case trying maximize likelihood alternatively might use prediction function loss function squared error huber loss frequentist decision theory average loss called risk see section overall approach called empirical risk minimization erm see section details however streaming data need perform online learning update estimates new data point arrives rather waiting end may never occur even batch data might want treat like stream large hold main memory discuss learning methods kind scenario simple implementation trick used speed batch learning algorithms applied data sets large hold memory first note naive implementation makes pass data ﬁle beginning end accumulating sufficient statistics gradients goes update performed process repeats unfortunately end pass data beginning ﬁle evicted cache since assuming cannot memory rather going back beginning ﬁle reloading simply work backwards end ﬁle already memory repeat forwards backwards pattern data simple trick known rocking chapter logistic regression 
[logistic, regression, online, learning, stochastic, optimization, online, learning, regret, minimization] suppose step nature presents sample learner must respond parameter estimate theoretical machine learning community objective used online learning regret averaged loss incurred relative best could gotten hindsight using single ﬁxed parameter value regret min example imagine investing stock market let amount invest stock let return stock loss function regret much better worse trading step rather adopting buy hold strategy using oracle choose stocks buy one simple algorithm online learning online gradient descent zinkevich follows step update parameters using proj proj argmin projection vector onto space gradient step size projection step needed parameter must constrained live certain subset see section details see approach regret minimization relates traditional objectives mle variety approaches regret minimization beyond scope book see cesa bianchi lugosi details 
[logistic, regression, online, learning, stochastic, optimization, stochastic, optimization, risk, minimization] suppose instead minimizing regret respect past want minimize expected loss future common frequentist statistical learning theory want minimize expectation taken future data optimizing functions variables objective random called stochastic optimization suppose receive inﬁnite stream samples distribution one way optimize stochastic objectives equation perform update equation step called stochastic gradient descent sgd nemirovski yudin since typically want single parameter estimate use running average note stochastic optimization objective stochastic therefore algorithms however also possible apply stochastic optimization algorithms deterministic objectives examples include simulated annealing section stochastic gradient descent applied empirical risk minimization problem interesting theoretical connections online learning stochastic optimization cesa bianchi lugosi beyond scope book online learning stochastic optimization called polyak ruppert averaging implemented recursively follows see spall kushner yin details setting step size discuss sufficient conditions learning rate guarantee convergence sgd known robbins monro conditions set values time called learning rate schedule various formulas used following bottou bach moulines slows early iterations algorithm controls rate old values forgotten need adjust tuning parameters one main drawback stochastic optimization one simple heuristic bottou follows store initial subset data try range values subset choose one results fastest decrease objective apply rest data note may result convergence algorithm terminated performance improvement hold set plateaus called early stopping per parameter step sizes one drawback sgd uses step size parameters brieﬂy present method known adagrad short adaptive gradient duchi similar spirit diagonal hessian approximation see also schaul similar approach particular parameter time gradient make update follows diagonal step size vector gradient vector squared summed time steps recursively updated follows result per parameter step size adapts curvature loss function method original derived regret minimization case applied generally chapter logistic regression sgd compared batch learning inﬁnite data stream simulate one sampling data points random training set essentially optimizing equation treating expectation respect empirical distribution algorithm stochastic gradient descent initialize repeat randomly permute data proj update converged theory sample replacement although practice usually better randomly permute data sample without replacement repeat single pass entire data set called epoch see algorithm pseudocode offline case often better compute gradient mini batch data cases standard sgd standard steepest descent typically used although simple ﬁrst order method sgd performs surprisingly well problems especially ones large data sets bottou intuitive reason one get fairly good estimate gradient looking examples carefully evaluating precise gradients using large datasets often waste time since algorithm recompute gradient anyway next step often better use computer time noisy estimate move rapidly parameter space extreme example suppose double training set duplicating every example batch methods take twice long online methods unaffected since direction gradient changed doubling size data changes magnitude gradient irrelevant since gradient scaled step size anyway addition enhanced speed sgd often less prone getting stuck shallow local minima adds certain amount noise consequently quite popular machine learning community ﬁtting models non convex objectives neural networks section deep belief networks section 
[logistic, regression, online, learning, stochastic, optimization, lms, algorithm] example sgd let consider compute mle linear regression online fashion derived batch gradient equation online gradient iteration given online learning stochastic optimization black line lms trajectory towards soln red cross rss iteration figure illustration lms algorithm left start slowly converging least squares solution red cross right plot objective function time note decrease monotonically figure generated lmsdemo training example use iteration data set streaming use shall assume notational simplicity equation easy interpret feature vector weighted difference predicted true response hence gradient acts like error signal computing gradient take step along follows need projection step since unconstrained optimization problem algorithm called least mean squares lms algorithm also known delta rule widrow hoff rule figure shows results applying algorithm data shown figure start converge sense drops threshold iterations note lms may require multiple passes data ﬁnd optimum contrast recursive least squares algorithm based kalman ﬁlter uses second order information ﬁnds optimum single pass see section see also exercise 
[logistic, regression, online, learning, stochastic, optimization, perceptron, algorithm] let consider binary logistic regression model online manner batch gradient given equation online case weight update simple form see exactly form lms algorithm indeed property holds generalized linear models section chapter logistic regression consider approximation algorithm speciﬁcally let arg max represent probable class label replace sigm gradient expression thus approximate gradient becomes make algebra simpler assume rather case prediction becomes sign made error guessed right label step update weight vector adding gradient key observation predicted correctly approximate gradient zero change weight vector misclassiﬁed update weights follows negative gradient negative gradient absorb factor learning rate write update case misclassiﬁcation since sign weights matter magnitude set see algorithm pseudocode one show method known perceptron algorithm rosenblatt converge provided data linearly separable exist parameters predicting sign achieves error training set however data linearly separable algorithm converge even converge may take long time much better ways train logistic regression models using proper sgd without gradient approximation irls discussed section however perceptron algorithm historically important one ﬁrst machine learning algorithms ever derived frank rosenblatt even implemented analog hardware addition algorithm used models computing marginals expensive computing map output arg max arises structured output classiﬁcation problems see section details 
[logistic, regression, online, learning, stochastic, optimization, bayesian, view] another approach online learning adopt bayesian view conceptually quite simple apply bayes rule recursively obvious advantage returning posterior instead point estimate also allows online adaptation hyper parameters important since cross validation cannot used online setting finally less obvious advantage generative discriminative classiﬁers algorithm perceptron algorithm input linearly separable data set initialize repeat mod else converged quicker sgd see note modeling posterior variance parameter addition mean effectively associate different learning rate parameter freitas simple way model curvature space variances adapted using usual rules probability theory contrast getting second order optimization methods work online tricky see schraudolph sunehag bordes simple example section show use kalman ﬁlter linear regression model online unlike lms algorithm converges optimal offline answer single pass data extension learn robust non linear regression model online fashion described ting glm case use assumed density ﬁlter section approximate posterior gaussian diagonal covariance variance terms serve per parameter step size see section details another approach use particle ﬁltering section used andrieu sequentially learning kernelized linear logistic regression model 
[logistic, regression, generative, discriminative, classiﬁers] section showed posterior class labels induced gaussian discrim inant analysis gda exactly form logistic regression namely sigm decision boundary therefore linear function cases note however many generative models give rise logistic regression posterior class conditional density poisson poi assumptions made gda much stronger assumptions made logistic regression difference models way trained ﬁtting discrim inative model usually maximize conditional log likelihood log whereas ﬁtting generative model usually maximize joint log likelihood log clear general give different results see exercise gaussian assumptions made gda correct model need less training data logistic regression achieve certain level performance gaussian chapter logistic regression assumptions incorrect logistic regression better jordan discriminative models need model distribution features illustrated figure see class conditional densities rather complex particular multimodal distribution might hard estimate however class posterior simple sigmoidal function centered threshold value suggests general discriminative methods accurate since job sense easier however accuracy important factor choosing method discuss advantages disadvantages approach 
[logistic, regression, generative, discriminative, classiﬁers, pros, cons, approach] easy seen usually easy generative classiﬁers example sections show naive bayes model lda model simple counting averaging contrast logistic regression requires solving convex optimization problem see section details much slower fit classes separately generative classiﬁer estimate parameters class conditional density independently retrain model add classes contrast discriminative models parameters interact whole model must retrained add new class also case train generative model maximize discriminative objective salojarvi handle missing features easily sometimes inputs components observed generative classiﬁer simple method dealing discuss section however discriminative classiﬁer principled solution problem since model assumes always available conditioned although see marlin heuristic approaches handle unlabeled training data much interest semi supervised learning uses unlabeled data help solve supervised task fairly easy using generative models see lasserre liang much harder discriminative models symmetric inputs outputs run generative model backwards infer probable inputs given output computing possible discriminative model reason generative model deﬁnes joint distribution hence treats inputs outputs symmetrically handle feature preprocessing big advantage discriminative methods allow preprocess input arbitrary ways replace could basis function expansion illustrated figure often hard deﬁne generative model pre processed data since new features correlated complex ways well calibrated probabilities generative models naive bayes make strong independence assumptions often valid result extreme poste rior class probabilities near discriminative models logistic regression usually better calibrated terms probability estimates see arguments kinds models therefore useful kinds toolbox see table summary classiﬁcation generative discriminative classiﬁers linear multinomial logistic regression kernel rbf multinomial logistic regression figure multinomial logistic regression classes original feature space basis function expansion using rbf kernels bandwidth using data points centers figure generated logregmultinomkerneldemo ditio nal siti figure class conditional densities left may complex class posteriors right based figure bishop figure generated generativevsdiscrim regression techniques cover book 
[logistic, regression, generative, discriminative, classiﬁers, dealing, missing, data] sometimes inputs components observed could due sensor failure failure complete entry survey etc called missing data problem little rubin ability handle missing data principled way one biggest advantages generative models formalize assumptions associate binary response variable speciﬁes whether value observed joint model form parameters controlling whether item chapter logistic regression model classif regr gen discr param non section discriminant analysis classif gen param sec naive bayes classiﬁer classif gen param sec tree augmented naive bayes classiﬁer classif gen param sec linear regression regr discrim param sec logistic regression classif discrim param sec sparse linear logistic regression discrim param mixture experts discrim param sec multilayer perceptron mlp neural network discrim param conditional random ﬁeld crf classif discrim param sec nearest neighbor classiﬁer classif gen non sec inﬁnite mixture discriminant analysis classif gen non sec classiﬁcation regression trees cart discrim non sec boosted model discrim non sec sparse kernelized lin logreg sklr discrim non sec relevance vector machine rvm discrim non sec support vector machine svm discrim non sec gaussian processes discrim non smoothing splines regr discrim non section table list various models classiﬁcation regression discuss book columns follows model name model suitable classiﬁcation regression model generative discriminative model parametric non parametric list sections book discuss model see also http pmtk googlecode com svn trunk docs tutorial html tsupervised html pmtk equivalents models generative probabilistic model hmms boltzmann machines bayesian networks etc turned classiﬁer using class conditional density observed assume say data missing completely random mcar assume observed part say data missing random mar neither assumptions hold say data missing random nmar case model missing data mechanism since pattern missingness informative values missing data corresponding parameters case collaborative ﬁltering problems example see marlin discussion henceforth assume data mar dealing missing data helpful distinguish cases missing ness test time training data complete data harder case missingness also training time discuss two cases note class label always missing test time deﬁnition class label also sometimes missing training time problem called semi supervised learning generative discriminative classiﬁers missing data test time generative classiﬁer handle features mar marginalizing example missing value compute make naive bayes assumption marginalization performed follows exploited fact hence naive bayes classiﬁer simply ignore missing features test time similarly discriminant analysis matter regularization method used estimate parameters always analytically marginalize missing variables see section missing data training time missing data training time harder deal particular computing mle map estimate longer simple optimization problem reasons discussed section however soon study variety sophisticated algorithms algo rithm section ﬁnding approximate map estimates cases 
[logistic, regression, generative, discriminative, classiﬁers, fisher’s, linear, discriminant, analysis, flda] discriminant analysis generative approach classiﬁcation requires ﬁtting mvn features discussed problematic high dimensions alternative approach reduce dimensionality features mvn resulting low dimensional features simplest approach use linear projection matrix matrix one approach ﬁnding would use pca section result would similar rda section since svd pca essentially equivalent however pca unsupervised technique take class labels account thus resulting low dimensional features necessarily optimal classiﬁcation illustrated figure alternative approach ﬁnd matrix low dimensional data classiﬁed well possible using gaussian class conditional density model assumption gaussianity reasonable since computing linear combinations potentially non gaussian features approach called fisher linear discriminant analysis flda flda interesting hybrid discriminative generative techniques drawback technique restricted using dimensions regardless reasons explain two class case means seeking single vector onto project data derive optimal two class case chapter logistic regression means fisher pca fisher pca figure example fisher linear discriminant two class data dashed green line ﬁrst principal basis vector dotted red line fisher linear discriminant vector solid black line joins class conditional means projection points onto fisher vector shows good class separation projection points onto pca vector shows poor class separation figure generated fisherldademo generative discriminative classiﬁers generalize multi class case ﬁnally give probabilistic interpretation technique derivation optimal projection derive optimal direction two class case following presentation bishop sec deﬁne class conditional means let projection mean onto line also let projection data onto line variance projected points proportional goal ﬁnd maximize distance means also ensuring projected clusters tight rewrite right hand side terms follows class scatter matrix given within class scatter matrix given see note equation ratio two scalars take derivative respect equate zero one show exercise maximized chapter logistic regression figure pca projection vowel data flda projection vowel data see better class separation flda case based figure hastie figure generated fisherdiscrimvoweldemo hannes bretschneider equation called generalized eigenvalue problem invertible convert regular eigenvalue problem however two class case simpler solution particular since equation since care directionality scale factor set optimal solution two class case meaning pooled covariance matrix isotropic proportional vector joins class means intuitively reasonable direction project onto shown figure extension higher dimensions multiple classes extend idea multiple classes higher dimensional subspaces ﬁnding projection matrix maps maximize generative discriminative classiﬁers solution shown leading eigenvectors assuming non singular singular ﬁrst perform pca data figure gives example method applied dimensional speech data representing different vowel sounds see flda gives better class separation pca note flda restricted ﬁnding dimensional linear subspace matter large rank class covariance matrix term arises term linear function rather severe restriction limits usefulness flda probabilistic interpretation flda ﬁnd valid probabilistic interpretation flda follow approach kumar andreo zhou proposed model known heteroscedastic lda hlda works follows let invertible matrix let transformed version data full covariance gaussians transformed data one per class constraint ﬁrst components class speciﬁc remaining components shared across classes thus discriminative use shared dimensional mean shared covariace pdf original untransformed data given ﬁxed easy derive mle one optimize using gradient methods chapter logistic regression special case diagonal closed form solution gales special case equal recover classical lda zhou view result clear hlda outperform lda class covariances equal within discriminative subspace assumption independent poor assumption easy demonstrate synthetic data also case challenging tasks speech recognition kumar andreo furthermore extend model allowing class use projection matrix known multiple lda gales 
[logistic, regression, exercises] exercise spam classiﬁcation using logistic regression consider email spam data set discussed hastie consists email messages features extracted follows features giving percentage words given message match given word list list contains words business free george etc data collected george forman name occurs quite lot features giving percentage characters email match given character list characters feature average length uninterrupted sequence capital letters max mean feature length longest uninterrupted sequence capital letters max mean feature sum lengts uninterrupted sequence capital letters max mean load data spamdata mat contains training set size test set size one imagine performing several kinds preprocessing data try following separately standardize columns mean unit variance transform features using log binarize features using version data logistic regression model use cross validation choose strength regularizer report mean error rate training test sets get numbers similar method train test stnd log binary precise values depend regularization value choose turn code numerical results see also exercise exercise spam classiﬁcation using naive bayes examine dataset exercise generative discriminative classiﬁers use naivebayesfit naivebayespredict binarized spam data training test error try different settings pseudocount like corresponds beta prior although default probably ﬁne turn error rates modify code handle real valued features use gaussian density feature maximum likelihood training test error rates standardized data log transformed data turn error rates code exercise gradient hessian log likelihood logistic regression let sigmoid function show using previous result chain rule calculus derive expression gradient log likelihood equation hessian written diag show positive deﬁnite may assume elements strictly positive full rank exercise gradient hessian log likelihood multinomial logistic regression let prove jacobian softmax hence show hint use chain rule fact show block submatrix hessian classes given exercise symmetric version regularized multinomial logistic regression source hastie multiclass logistic regression form exp exp weight matrix arbitrarily deﬁne one classes say since case model form exp exp chapter logistic regression clamp one vectors constant value parameters unidentiﬁable however suppose clamp using equation add regularization optimizing log show optimum unregularized terms still need enforce ensure identiﬁability offset exercise elementary properties regularized logistic regression source jaaakkola consider minimizing train log average log likelihood data set answer following true false questions multiple locally optimal solutions let arg min global optimum sparse many zero entries training data linearly separable weights might become inﬁnite train always increases increase test always increases increase exercise regularizing separate terms logistic regression source jaaakkola consider data figure model suppose model maximum likelihood minimize train train log likelihood training set sketch possible decision boundary corresponding copy ﬁgure ﬁrst rough sketch enough superimpose answer copy since need multiple versions ﬁgure answer decision boundary unique many classiﬁcation errors method make training set suppose regularize parameter minimize train suppose large number regularize way parameters unregularized sketch possible decision boundary many classiﬁcation errors method make training set hint consider behavior simple linear regression suppose heavily regularize parameter minimize train sketch possible decision boundary many classiﬁcation errors method make training set generative discriminative classiﬁers figure data logistic regression question suppose heavily regularize parameter sketch possible decision boundary many classiﬁcation errors method make training set 
[generalized, linear, models, exponential, family, introduction] encountered wide variety probability distributions gaussian bernoulli student uniform gamma etc turns members broader class distributions known exponential family chapter discuss various properties family allows derive theorems algorithms broad applicability see easily use member exponential family class conditional density order make generative classiﬁer addition discuss build discriminative models response variable exponential family distribution whose mean linear function inputs known generalized linear model generalizes idea logistic regression kinds response variables 
[generalized, linear, models, exponential, family, exponential, family] deﬁning exponential family mention several reasons important shown certain regularity conditions exponential family family distributions ﬁnite sized sufficient statistics meaning compress data ﬁxed sized summary without loss information particularly useful online learning see later exponential family family distributions conjugate priors exist simpliﬁes computation posterior see section exponential family shown family distributions makes least set assumptions subject user chosen constraints see section exponential family core generalized linear models discussed section exponential family core variational inference discussed section exceptions student right form uniform distribution ﬁxed support independent parameter values chapter generalized linear models exponential family 
[generalized, linear, models, exponential, family, exponential, family, deﬁnition] pdf pmf said exponential family form exp exp exp log called natural parameters canonical parameters called vector sufficient statistics called partition function called log partition function cumulant function scaling constant often say natural exponential family equation generalized writing exp function maps parameters canonical parameters dim dim called curved exponential family means sufficient statistics parameters model said canonical form assume models canonical form unless state otherwise 
[generalized, linear, models, exponential, family, exponential, family, examples] let consider examples make things clearer bernoulli bernoulli written exponential family form follows ber exp log log exp log log however representation complete since linear dependendence features consequently uniquely identiﬁable common require representation minimal means unique associated distribution case deﬁne ber exp log exponential family log log odds ratio recover mean parameter canonical parameter using sigm multinoulli represent multinoulli minimal exponential family follows cat exp log exp log log exp log log exp log log write exponential family form follows cat exp log log recover mean parameters canonical parameters using ﬁnd hence log deﬁne write log softmax function equation chapter generalized linear models exponential family univariate gaussian univariate gaussian written exponential family form follows exp exp exp exp log log non examples distributions interest belong exponential family example uniform distribution unif since support distribution depends parameters also student distribution section belong since required form 
[generalized, linear, models, exponential, family, exponential, family, log, partition, function] important property exponential family derivatives log partition function used generate cumulants sufficient statistics reason sometimes called cumulant function prove parameter distribution generalized parameter distribution straightforward way ﬁrst ﬁrst second cumulants distribution mean variance var whereas ﬁrst second moments mean exponential family derivative log exp exp exp exp exp exp second derivative exp var used fact multivariate case hence cov since covariance positive deﬁnite see convex function see section example bernoulli distribution example consider bernoulli distribution log mean given sigm variance given chapter generalized linear models exponential family 
[generalized, linear, models, exponential, family, exponential, family, mle, exponential, family] likelihood exponential family model form exp see sufficient statistics example bernoulli model univariate gaussian also need know sample size pitman koopman darmois theorem states certain regularity conditions exponential family family distributions ﬁnite sufficient statistics ﬁnite means size independent size data set one conditions required theorem support distribution dependent parameter simple example distribution consider uniform distribution likelihood given max sufficient statistics max ﬁnite size uni form distribution exponential family support set depends parameters descibe compute mle canonical exponential family model given iid data points log likelihood log since concave linear see log likelihood concave hence unique global maximum derive maximum use fact derivative log partition function yields expected value sufficient statistic vector section log setting gradient zero see mle empirical average sufficient statistics must equal model theoretical expected sufficient statistics must satisfy exponential family called moment matching example bernoulli distribution mle satisﬁes 
[generalized, linear, models, exponential, family, exponential, family, bayes, exponential, family] seen exact bayesian analysis considerably simpliﬁed prior conjugate likelihood informally means prior form likelihood make sense require likelihood ﬁnite sufficient statistics write suggests family distributions conjugate priors exist exponential family derive form prior posterior likelihood likelihood exponential family given exp terms canonical parameters becomes exp prior natural conjugate prior form exp let write separate size prior pseudo data mean sufficient statistics pseudo data canonical form prior becomes exp posterior posterior given see update hyper parameters adding canonical form becomes exp see posterior hyper parameters convex combination prior mean hyper parameters average sufficient statistics chapter generalized linear models exponential family posterior predictive density let derive generic expression predictive density future observables given past data follows notational brevity combine sufficient statistics size data follows prior becomes exp likelihood posterior similar form hence exp becomes marginal likelihood reduces familiar form normalizer posterior divided normalizer prior multiplied constant example bernoulli distribution simple example let revisit beta bernoulli model new notation likelihood given exp log hence conjugate prior given exp log deﬁne see beta distribution derive posterior follows sufficient statistic derive posterior predictive distribution follows assume beta let number heads past data predict probability exponential family given sequence future heads sufficient statistic follows beta 
[generalized, linear, models, exponential, family, exponential, family, maximum, entropy, derivation, exponential, family] although exponential family convenient deeper justiﬁcation use turns distribution makes least number assumptions data subject speciﬁc set user speciﬁed constraints explain particular suppose know expected values certain features functions known constants arbitrary function principle maximum entropy maxent says pick distribution maximum entropy closest uniform subject constraints moments distribution match empirical moments speciﬁed functions maximize entropy subject constraints equation constraints need use lagrange multipliers lagrangian given log use calculus variations take derivatives wrt function adopt simpler approach treat ﬁxed length vector since assuming discrete log setting yields exp chapter generalized linear models exponential family figure visualization various features glm based figure jordan using sum one constraint exp hence normalization constant given exp thus maxent distribution form exponential family section also known gibbs distribution 
[generalized, linear, models, exponential, family, generalized, linear, models, glms] linear logistic regression examples generalized linear models glms mccullagh nelder models output density exponential family section mean parameters linear combination inputs passed possibly nonlinear function logistic function describe glms detail focus scalar outputs notational simplicity excludes multinomial logistic regression simplify presentation 
[generalized, linear, models, exponential, family, generalized, linear, models, glms, basics] understand glms let ﬁrst consider case unconditional dstribution scalar response variable exp dispersion parameter often set natural parameter partition function normalization constant example case logistic regression log odds ratio log mean parameter see section convert mean parameter natural parameter generalized linear models glms distrib link identity bin logit log sigm poi log log table canonical link functions inverses common glms use function function uniquely determined form exponential family distribution fact invertible mapping furthermore know section mean given derivative partition function let add inputs covariates ﬁrst deﬁne linear function inputs make mean distribution invertible monotonic function linear combination convention function known mean function denoted see figure summary basic model inverse mean function namely called link function free choose almost function like long invertible long appropriate range example logistic regression set sigm one particularly simple form link function use called canonical link function case model becomes exp table list distributions canonical link functions see bernoulli binomial distribution canonical link logit function log whose inverse logistic function sigm based results section show mean variance response variable follows var make notation clearer let consider simple examples linear regression log log var chapter generalized linear models exponential family binomial regression log log log log sigm log log var poisson regression log log log exp log var poisson regression widely used bio statistical applications might represent number diseases given person place number reads genomic location high throughput sequencing context see kuan 
[generalized, linear, models, exponential, family, generalized, linear, models, glms, map, estimation] one appealing properties glms using exactly methods used logistic regression particular log likelihood following form log compute gradient vector using chain rule follows use canonical link simpliﬁes sum input vectors weighted errors used inside stochastic gradient descent procedure discussed section however improved efficiency use second order method use canonical link hessian given probit regression name formula logistic sigm probit log log exp exp complementary log log exp exp table summary possible mean functions binary regression diag diagonal weighting matrix used inside irls algorithm section speciﬁcally following newton update extend derivation handle non canonical links ﬁnd hessian another term however turns expected hessian equation using expected hessian known fisher information matrix instead actual hessian known fisher scoring method straightforward modify procedure perform map estimation gaus sian prior modify objective gradient hessian added regularization logistic regression section 
[generalized, linear, models, exponential, family, generalized, linear, models, glms, bayesian, inference] bayesian inference glms usually conducted using mcmc chapter possible methods include metropolis hastings irls based proposal gamerman gibbs sampling using adaptive rejection sampling ars full conditional dellaportas smith etc see dey futher information also possible use gaussian approximation section variational inference section 
[generalized, linear, models, exponential, family, probit, regression] binary logistic regression use model form sigm general write function maps several possible mean functions listed table section focus case cdf standard normal known probit regression probit function similar logistic function shown figure however model advantages logistic regression see chapter generalized linear models exponential family 
[generalized, linear, models, exponential, family, probit, regression, ml/map, estimation, using, gradient-based, optimization] ﬁnd mle probit regression using standard gradient methods let let gradient log likelihod speciﬁc case given log log standard normal pdf cdf similarly hessian single case given log modify expressions compute map estimate straightforward manner particular use prior gradient hessian penalized log likelihood form expressions passed gradient based optimizer see probitregdemo demo 
[generalized, linear, models, exponential, family, probit, regression, latent, variable, interpretation] interpret probit logistic model follows first let associate item two latent utilities corresponding possible choices assume observed choice whichever action larger utility precisely model follows error terms representing factors might relevant decision making chosen unable model called random utility model rum mcfadden train since difference utilities matters let deﬁne gaussian distribution thus write following fruhwirth schnatter fruhwirth call difference rum drum model marginalize recover probit model probit regression used symmetry gaussian latent variable interpretation provides alternative way model discussed section interestingly use gumbel distribution induce logistic distibution model reduces logistic regression see section details 
[generalized, linear, models, exponential, family, probit, regression, ordinal, probit, regression] one advantage latent variable interpretation probit regression easy extend case response variable ordinal take discrete values ordered way low medium high called ordinal regression basic idea follows introduce thresholds set identiﬁability reasons set example reduces standard binary probit model whereby produces produces partition real line intervals vary parameter ensure right relative amount probability mass falls interval match empirical frequencies class label finding mles model bit trickier binary probit regression since need optimize latter must obey ordering constraint see kawakatsu largey approach based also possible derive simple gibbs sampling algorithm model see hoff 
[generalized, linear, models, exponential, family, probit, regression, multinomial, probit, models] consider case response variable take unordered categorical values multinomial probit model deﬁned follows arg max see dow endersby scott fruhwirth schnatter fruhwirth details model connection multinomial logistic regression deﬁning recover familiar formulation since relative utilities matter constrain correlation matrix instead setting argmax use get model known multivariate probit one way model correlated binary outcomes see talhouk note assumption gaussian noise term zero mean unit variance made without loss generality see suppose used mean variance could easily rescale add offset term without changing likelihood since chapter generalized linear models exponential family 
[generalized, linear, models, exponential, family, multi-task, learning] sometimes want many related classiﬁcation regression models often reasonable assume input output mapping similar across different models get better performance ﬁtting parameters time machine learning setup often called multi task learning caruana transfer learning raina learning learn thrun pratt statistics usually tackled using hierarchical bayesian models bakker heskes discuss although possible methods see chai 
[generalized, linear, models, exponential, family, multi-task, learning, hierarchical, bayes, multi-task, learning] let response item group example might index schools might index students within school might test score section might index people might index purchaes might identity item purchased known discrete choice modeling train let feature vector associated goal models although groups may lots data often long tail majority groups little data thus reliably model separately want use model groups compromise separate model group encourage model parameters similar across groups precisely suppose link function glm furthermore suppose model groups small sample size borrow statistical strength groups larger sample size correlated via latent common parents see section discussion point term controls much group depends common parents term controls strength overall prior suppose simplicity known could set cross validation overall log probability form log log log perform map estimation using standard gradient methods alter natively perform iterative optimization scheme alternating optimizing since likelihood prior convex guaranteed converge global optimum note models trained discard use model separately 
[generalized, linear, models, exponential, family, multi-task, learning, application, personalized, email, spam, ﬁltering] interesting application multi task learning personalized spam ﬁltering suppose want one classiﬁer per user since users label email spam hard estimate models independently let common prior representing parameters generic user multi task learning case emulate behavior model simple trick daume attenberg weinberger make two copies feature one concatenated user one effect learn predictor form user words thus estimated everyone email whereas estimated user email see correspondence hierarchical bayesian model deﬁne log probability original model rewritten log assume effect using augmented feature trick regularizer strength however one typically gets better performance requiring equal finkel manning 
[generalized, linear, models, exponential, family, multi-task, learning, application, domain, adaptation] domain adaptation problem training set classiﬁers data drawn different distributions email newswire text problem obviously special case multi task learning tasks finkel manning used hierarchical bayesian model perform domain adaptation two nlp tasks namely named entity recognition parsing report reason ably large improvements ﬁtting separate models dataset small improvements approach pooling data ﬁtting single model 
[generalized, linear, models, exponential, family, multi-task, learning, kinds, prior] multi task learning common assume prior gaussian however sometimes priors suitable example consider task conjoint analysis requires ﬁguring features product customers like best modelled using hierarchical bayesian setup use sparsity promoting prior rather gaussian prior called multi task feature selection see lenk argyriou possible approaches always reasonable assume tasks equally similar pool parameters across tasks qualitatively different performance worse using pooling inductive bias prior wrong indeed found experimentally sometimes multi task learning worse solving task separately called negative transfer chapter generalized linear models exponential family one way around problem use ﬂexible prior mixture gaussians ﬂexible priors provide robustness prior mis speciﬁcation see xue jacob details one course combine mixtures sparsity promoting priors many variants possible 
[generalized, linear, models, exponential, family, generalized, linear, mixed, models] suppose generalize multi task learning scenario allow response include infor mation group level well item level similarly allow parameters vary across groups tied across groups gives rise following model basis functions model represented pictorially shown figure ﬁgures explained chapter note number parameters grows number groups whereas size ﬁxed frequentists call terms random effects since vary randomly across groups call ﬁxed effect since viewed ﬁxed unknown constant model ﬁxed random effects called mixed model glm overall model called generalized linear mixed effects model glmm models widely used statistics 
[generalized, linear, models, exponential, family, generalized, linear, mixed, models, example, semi-parametric, glmms, medical, data] consider following example wand suppose amount spinal bone mineral density sbmd person measurement let age person let ethnicity one white asian black hispanic primary goal determine signiﬁcant differences mean sbmd among four ethnic groups accounting age data shown light gray lines figure see nonlinear effect sbmd age use semi parametric model combines linear regression non parametric regression ruppert also see variation across individuals within group use mixed effects model speciﬁcally use account random effect person since coefficients person speciﬁc spline basis functions see section account nonlinear effect age account effect different ethnicities furthermore use linear link function overall model therefore contains non parametric part model related age contains parametric part model related ethnicity random offset person endow regression coefficients separate gaussian priors perform posterior inference compute see section generalized linear mixed models age years spinal bone miner density asian black hispanic white figure directed graphical model generalized linear mixed effects model groups spinal bone mineral density age four different ethnic groups raw data shown light gray lines fitted model shown black solid posterior predicted mean dotted posterior predictive variance figure wand used kind permission matt wand chapter generalized linear models exponential family computational details ﬁtting model compute prediction group see figure results also perform signiﬁcance testing computing ethnic group relative baseline say white section 
[generalized, linear, models, exponential, family, generalized, linear, mixed, models, computational, issues] principle problem glmms difficult two reasons first may conjugate prior second two levels unknowns model namely regression coefficients means variances priors one approach adopt fully bayesian inference methods variational bayes hall mcmc gelman hill discuss section mcmc section alternative approach use empirical bayes discuss general terms section context glmm use algorithm section step compute step optimize linear regression setting step performed exactly general need use approximations traditional methods use numerical quadrature monte carlo see breslow clayton faster approach use variational see braun mcauliffe application variational multi level discrete choice modeling problem frequentist statistics popular method ﬁtting glmms called generalized estimating equations gee hardin hilbe however recommend approach since statistically efficient likelihood based methods see section addition provide estimates population parameters random effects sometimes interest 
[generalized, linear, models, exponential, family, learning, rank] section discuss learning rank letor problem want learn function rank order set items precise common application information retrieval speciﬁcally suppose query set documents might relevant documents contain string would like sort documents decreasing order relevance show top user similar problems arise areas collaborative ﬁltering ranking players game tournament setting slightly different kind problem see section summarize methods solving problem following presentation liu material based glms include chapter anyway lack better place standard way measure relevance document query use probabilistic language model based bag words model deﬁne sim word term multinoulli distribution estimated document practice need smooth estimated distribution example using dirichlet prior representing overall frequency word learning rank estimated documents system precisely use len background frequency term document len number words smoothing parameter see zhai lafferty details however might many signals use measure relevance example pagerank web document measure authoritativeness derived web link structure see section details also compute often query occurs document discuss learn combine signals 
[generalized, linear, models, exponential, family, learning, rank, pointwise, approach] suppose collect training data representing relevance set documents query speciﬁcally query suppose retrieve possibly relevant documents query document pair deﬁne feature vector example might contain query document similarity score page rank score document furthermore suppose set labels representing degree relevance document query labels might binary relevant irrelevant may represent degree relevance relevant somewhat relevant irrelevant labels obtained query logs thresholding number times document clicked given query binary relevance labels solve problem using standard binary clas siﬁcation scheme estimate ordered relevancy labels use ordinal regression predict rating either case sort documents scoring metric called pointwise approach letor widely used simplicity however method take account location document list thus penalizes errors end list much errors beginning often desired behavior addition decision relevance made myopically 
[generalized, linear, models, exponential, family, learning, rank, pairwise, approach] evidence carterette people better judging relative relevance two items rather absolute relevance consequently data might tell relevant given query vice versa model kind data using binary classiﬁer form set rel rel otherwise one way model function follows sigm rather surprisingly google least using learning methods search engine source peter norvig quoted http anand typepad com datawocky human experts less rone catastrophic errors machine learned models html chapter generalized linear models exponential family scoring function often taken linear special kind neural network known ranknet burges see section general discussion neural networks ﬁnd mle maximizing log likelihood equivalently minimizing cross entropy loss given ijk ijk ijk log ijk ijk log ijk optimized using gradient descent variant ranknet used microsoft bing search engine 
[generalized, linear, models, exponential, family, learning, rank, listwise, approach] pairwise approach suffers problem decisions relevance made based pair items documents rather considering full context consider methods look entire list items time deﬁne total order list specifying permutation indices model uncertainty use plackett luce distribution derives name independent work plackett luce following form score document ranked position understand equation let consider simple example suppose probability ranked ﬁrst times probability ranked second given ranked ﬁrst times probabilty ranked third given ranked ﬁrst second words incorporate features deﬁne often take linear function known listnet model cao train model let relevance scores documents query minimize cross entropy term log course stated intractable since term needs sum permutations make tractable consider permutations top positions source http www bing com community site_blogs search archive user needs eatures science behind bing aspx learning rank permutations set evaluate cross entropy term derivative time special case one document presented list deemed relevant say instead use multinomial logistic regression exp exp often performs least well ranking methods least context collaborative ﬁltering yang 
[generalized, linear, models, exponential, family, learning, rank, loss, functions, ranking] variety ways measure performance ranking system summa rize mean reciprocal rank mrr query let rank position ﬁrst relevant document denoted deﬁne mean reciprocal rank simple performance measure mean average precision map case binary relevance labels deﬁne precision ordering follows num relevant documents top positions deﬁne average precision follows num relevant documents iff document relevant example relevancy labels finally deﬁne mean average precision averaged queries normalized discounted cumulative gain ndcg suppose relevance labels multi ple levels deﬁne discounted cumulative gain ﬁrst items ordering follows dcg log relevance item log term used discount items later list table gives simple numerical example alternative deﬁnition places stronger emphasis retrieving relevant documents uses dcg log trouble dcg varies magnitude length returned list may vary therefore common normalize measure ideal dcg chapter generalized linear models exponential family log log table illustration compute ndcg http wikipedia org wiki discounted _cumulative_gain value relevance score item position see dcg maximum dcg obtained using ordering scores hence ideal dcg normalized dcg dcg obtained using optimal ordering idcg argmax dcg easily computed sorting computing dcg finally deﬁne normalized discounted cumulative gain ndcg dcg idcg table gives simple numerical example ndcg averaged queries give measure performance rank correlation measure correlation ranked list relevance judegment using variety methods one approach known weighted kendall statistics deﬁned terms weighted pairwise inconsistency two lists ult sgn sgn ult variety measures commonly used loss functions used different ways bayesian approach ﬁrst model using posterior inference depends likelihood prior loss choose actions test time minimize expected future loss one way sample parameters posterior evaluate say precision different thresholds averaging see zhang example approach frequentist approach try minimize empirical loss training set problem loss functions differentiable functions model parameters either use gradient free optimization methods minimize surrogate loss function instead cross entropy loss negative log likelihood example widely used surrogate loss function another loss known weighted approximate rank pairwise warp loss proposed usunier extended weston provides better approximation precision loss warp deﬁned follows warp rank rank learning rank vector scores possible output label terms possible document corresponding input query expression rank measures rank true label assigned scoring function finally transforms integer rank real valued penalty using jgt would optimize proportion top ranked correct labels setting non zero values would optimize top ranked list induce good performance measured map precision stands warp loss still hard optimize approximated monte carlo sampling optimized gradient descent described weston 
[generalized, linear, models, exponential, family, exercises] exercise conjugate prior univariate gaussian exponential family form derive conjugate prior univariate gaussian using exponential family analogy section suitable reparameterization show prior form thus free parameters exercise mvn exponential family show write mvn exponential family form hint use information form deﬁned section 
[directed, graphical, models, bayes, nets, introduction] basically know two principles treating complicated systems simple ways ﬁrst principle modularity second principle abstraction apologist computational probability machine learning believe probability theory implements two principles deep intriguing ways namely factorization averaging exploiting two mechanisms fully possible seems way forward machine learning michael jordan quoted frey suppose observe multiple correlated variables words document pixels image genes microarray compactly represent joint distribution use distribution infer one set variables given another reasonable amount computation time learn parameters distribution reasonable amount data questions core probabilistic modeling inference learning form topic chapter 
[directed, graphical, models, bayes, nets, introduction, chain, rule] chain rule probability always represent joint distribution follows using ordering variables number variables matlab like notation denotes set dropped conditioning ﬁxed parameters brevity problem expression becomes complicated represent conditional distributions gets large example suppose variables states represent table numbers representing discrete distribution actually free parameters due sum one constraint write simplicity similarly represent table numbers writing say stochastic matrix since satisﬁes constraint rows entries similarly represent table chapter directed graphical models bayes nets numbers called conditional probability tables cpts see parameters model would need awful lot data learn many parameters one solution replace cpt parsimonius conditional probability distri bution cpd multinomial logistic regression total number parameters making compact density model neal frey adequate want evaluate probability fully observed vector example use model deﬁne class conditional density thus making generative classiﬁer bengio bengio however model useful kinds prediction tasks since variable depends previous variables need another approach 
[directed, graphical, models, bayes, nets, introduction, conditional, independence] key efficiently representing large joint distributions make assumptions conditional independence recall section conditionally inde pendent given denoted iff conditional joint written product conditional marginals let see might help suppose assume words future independent past given present called ﬁrst order markov assumption using assumption plus chain rule write joint distribution follows called ﬁrst order markov chain characterized initial distribution states plus state transition matrix see section information 
[directed, graphical, models, bayes, nets, introduction, graphical, models] although ﬁrst order markov assumption useful deﬁning distributions sequences deﬁne distributions images videos general arbitrary collections variables genes belonging biological pathway graphical models come graphical model way represent joint distribution making assumptions particular nodes graph represent random variables lack edges represent assumptions better name models would fact independence diagrams term graphical models entrenched several kinds graphical model depending whether graph directed undirected combination directed undirected chapter study directed graphs consider undirected graphs chapter introduction 
[graph, terminology] continue must deﬁne basic terms intuitive graph consists set nodes vertices set edges represent graph adjacency matrix write denote edge graph iff say graph undirected otherwise directed usually assume means self loops terms commonly use parent directed graph parents node set nodes feed child directed graph children node set nodes feed family directed graph family node node parents fam root directed graph root node parents leaf directed graph leaf node children ancestors directed graph ancestors parents grand parents etc node ancestors set nodes connect via trail anc descendants directed graph descendants children grand children etc node descendants set nodes reached via trails desc neighbors graph deﬁne neighbors node set immediately connected nodes nbr undirected graph chapter directed graphical models bayes nets write indicate neighbors edge graph degree degree node number neighbors directed graphs speak degree degree count number parents children cycle loop graph deﬁne cycle loop series nodes get back started following edges graph directed may speak directed cycle example figure directed cycles undirected cycle dag directed acyclic graph dag directed graph directed cycles see figure example topological ordering dag topological ordering total ordering numbering nodes parents lower numbers children example figure use etc path trail path trail series directed edges leading tree undirected tree undirectecd graph cycles directed tree dag directed cycles allow node multiple parents call polytree otherwise call moral directed tree forest forest set trees subgraph node induced subgraph graph created using nodes corresponding edges clique undirected graph clique set nodes neighbors maximal clique clique cannot made larger without losing clique property example figure clique maximal since add still maintain clique property fact maximal cliques follows 
[directed, graphical, models] directed graphical model dgm whose graph dag commonly known bayesian networks however nothing inherently bayesian bayesian networks way deﬁning probability distributions models also called belief networks term belief refers subjective probability nothing inherently subjective kinds probability distributions represented dgms finally models sometimes called causal networks directed arrows sometimes interpreted representing causal relations however nothing inherently causal dgms see section discussion causal dgms reasons use neutral less glamorous term dgm key property dags nodes ordered parents come children called topological ordering constructed dag given order deﬁne ordered markov property assumption node depends immediate parents predecessors ordering pred parents node pred predecessors node ordering natural generalization ﬁrst order markov property chains general dags examples figure naive bayes classiﬁer represented dgm assume features simplicity shaded nodes observed unshaded nodes hidden tree augmented naive bayes classiﬁer features general tree topology change depending value example dag figure encodes following joint distribution general term cpd written distribution emphasize equation holds assumptions encoded dag correct however usual drop explicit conditioning brevity node parents states number parameters model much less needed model makes assumptions 
[examples] section show wide variety commonly used probabilistic models conve niently represented dgms 
[examples, naive, bayes, classiﬁers] section introduced naive bayes classiﬁer assumes features condi tionally independent given class label assumption illustrated figure allows write joint distirbution follows naive bayes assumption rather naive since assumes features conditionally independent one way capture correlation features use graphical model particular model tree method known tree augmented naive bayes chapter directed graphical models bayes nets figure ﬁrst second order markov chain figure ﬁrst order hmm classiﬁer tan model friedman illustrated figure reason use tree opposed generic graph two fold first easy ﬁnd optimal tree structure using chow liu algorithm explained section second easy handle missing features tree structured model explain section 
[examples, markov, hidden, markov, models] figure illustrates ﬁrst order markov chain dag course assumption immediate past captures everything need know entire history bit strong relax little adding dependence well called second order markov chain illustrated figure corresponding joint following form create higher order markov models similar way see section detailed discussion markov models unfortunately even second order markov assumption may inadequate long range correlations amongst observations keep building ever higher order models since number parameters blow alternative approach assume underlying hidden process modeled ﬁrst order markov chain data noisy observation process result known hidden markov model hmm illustrated figure known hidden variable time observed variable put time quotation marks since models applied kind sequence data genomics language represents location rather time cpd transition model cpd observation model examples table noisy cpd parents augmented leak node omitted subscript brevity hidden variables often represent quantities interest identity word someone currently speaking observed variables measure acoustic waveform would like estimate hidden state given data compute called state estimation another form probabilistic inference see chapter details hmms 
[examples, medical, diagnosis] consider modeling relationship various variables measured intensive care unit icu breathing rate patient blood pressure etc alarm network figure one way represent dependencies beinlich model variables parameters since model created hand process called knowledge engineering known probabilistic expert system section discuss learn parameters dgms data assuming graph structure known chapter discuss learn graph structure different kind medical diagnosis network known quick medical reference qmr network shwe shown figure designed model infectious diseases qmr model bipartite graph structure diseases causes top symptoms ﬁndings bottom nodes binary write distribution follows represent hidden nodes diseases represent visible nodes symptoms cpd root nodes bernoulli distributions representing prior probability disease representing cpds leaves symptoms using cpts would require many parameters fan number parents many leaf nodes high natural alternative use logistic regression model cpd sigm dgm cpds logistic regression distributions known sigmoid belief net neal however since parameters model created hand alternative cpd known noisy model used noisy model assumes parent child usually also since gate occasionally links parents child may fail independently random case even parent child may model precisely let probability link fails chapter directed graphical models bayes nets hrbp errcauter hrsat tpr minvol pvsat pap pulm embolus shunt intubation press disconnect ventmach venttube ventlung ventalv artco anaphy laxis hypo volemia pcwp lvfailure lved volume stroke volume history cvp errlow output hrekg insuff anesth catechol sao expco minvolset kinked tube fio iox khduw glvhdvh erwxolvp degrphq sdlq xglvhdvhv pswrpv amp frxqw figure alarm network figure generated visualizealarmnetwork qmr network examples table cpt encodes mapping genotype phenotype bloodtype determin istic many one mapping probability activate causal power way child links parents fail independently random thus obviously observe parents contradicts model data case would get probability zero model problematic possible someone exhibits symptom speciﬁed diseases handle add dummy leak node always represents causes parameter represents probability background leak cause effect modiﬁed cpd becomes see table numerical example deﬁne log rewrite cpd exp see similar logistic regression model bipartite models noisy cpds called bno models relatively easy set parameters hand based domain expertise however also possible learn data see neal meek heckerman noisy cpds also proved useful modeling human causal learning griffiths tenenbaum well general binary classiﬁcation settings yuille zheng 
[examples, genetic, linkage, analysis] another important historically early application dgms problem genetic linkage analysis start pedigree graph dag representing relationship parents children shown figure convert dgm explain finally perform probabilistic inference resulting model chapter directed graphical models bayes nets rfxv rfxv figure left family tree circles females squares males individuals disease interest highlighted right dgm two loci blue nodes observed phenotype individual locus nodes hidden orange nodes paternal maternal allele small red nodes ijl paternal maternal selection switching variables linked across loci founder root nodes parents hence need switching variables based figure friedman examples detail person animal location locus along genome create three nodes observed marker property blood type fragment dna measured two hidden alleles one inherited mother maternal allele father paternal allele together ordered pair constitutes hidden genotype locus obviously must add arcs representing fact genotypes cause phenotypes observed manifestations genotypes cpd called penetrance model simple example suppose represents person observed bloodtype genotype repre sent penetrance model using deterministic cpd shown table example dominates person genotype phenotype addition add arcs mother father reﬂecting mendelian inheritance genetic material one parents precisely let mother could either equal maternal allele copy one mother two alleles let hidden variable speciﬁes choice model using following cpd known inheritance model deﬁne similarly father values said specify phase genotype values constitute haplotype person locus next need specify prior root nodes called founder model represents overall prevalence difference kinds alleles population usually assume independence loci founder alleles finally need specify priors switch variables control inheritance process variables spatially correlated since adjacent sites genome typically inherited together recombination events rare model imposing two state markov chain probability switching state locus given distance loci called recombination model resulting dgm shown figure series replicated pedigree dags augmented switching variables linked using markov chains related model known phylogenetic hmm siepel haussler used model evolution amongst phylogenies simpliﬁed example model used suppose one locus corresponding blood type brevity drop index suppose observe possible genotypes ambiguity genotype phenotype mapping many one want reverse mapping known inverse problem fortunately use blood types relatives help disambiguate evidence information ﬂow across via pedigree dag thus combine local evidence sometimes observed marker equal unphased genotype unordered set however phased hidden genotype directly measurable chapter directed graphical models bayes nets informative prior conditioned data get less entropic local posterior practice model used try determine along genome given disease causing gene assumed lie genetic linkage analysis task method works follows first suppose parameters model including distance marker loci known unknown location disease causing gene marker loci construct models model postulate disease gene comes marker estimate markov switching parameter hence distance disease gene nearest known locus measure quality model using likelihood pick model highest likelihood equivalent map model uniform prior note however computing likelihood requires marginalizing hidden variables see fishelson geiger references therein exact methods task based variable elimination algorithm discuss section unfortunately reasons explain section exact methods computationally intractable number individuals loci large see albers approximate method computing likelihood based form variational inference discuss section 
[examples, directed, gaussian, graphical, models] consider dgm variables real valued cpds following form called linear gaussian cpd show multiplying cpds together results large joint gaussian distribution form called directed ggm gaussian bayes net explain derive cpd parameters following shachter kenley app convenience rewrite cpds following form conditional standard deviation given parents strength edge local mean easy see global mean concatenation local means derive global covariance let diag diagonal matrix containing standard deviations rewrite equation matrix vector form follows subtract parent mean use derivation much messier seen looking bishop inference let vector noise terms rearrange get since lower triangular topological ordering lower triangular diagonal hence since always invertible write usz deﬁned thus regression weights correspond cholesky decomposition show cov cov cov usz cov 
[inference] seen graphical models provide compact way deﬁne joint probability distribu tions given joint distribution main use joint distribution perform probabilistic inference refers task estimating unknown quantities known quantities example section introduced hmms said one goals estimate hidden states words observations speech signal section discussed genetic linkage analysis said one goals estimate likelihood data various dags corresponding different hypotheses location disease causing gene general pose inference problem follows suppose set correlated random variables joint distribution section assuming parameters model known discuss learn parameters section let partition vector visible variables observed hidden variables unobserved inference refers computing posterior distribution unknowns given knowns essentially conditioning data clamping visible variables observed values normalizing normalization constant likelihood data also called probability evidence chapter directed graphical models bayes nets sometimes hidden variables interest let partition hidden variables query variables whose value wish know remaining nuisance variables interested compute interested marginalizing nuisance variables section saw perform operations multivariate gaussian time number variables discrete random variables say states joint distribution represented multi dimensional table always perform operations exactly take time chapter explain exploit factorization encoded perform operations time quantity known treewidth graph measures tree like graph graph tree chain models inference takes time linear number nodes unfortunately general graphs exact inference take time exponential number nodes explain section therefore examine various approximate inference schemes later book 
[learning] figure left data points conditionally independent given right plate notation represents model one left except repeated nodes inside box known plate number lower right hand corner speciﬁes number repetitions node assumption data case generated independently distribution notice data cases independent conditional parameters marginally data cases dependent nevertheless see example order data cases arrive makes difference beliefs since orderings sufficient statistics hence say data exchangeable avoid visual clutter common use form syntactic sugar called plates simply draw little box around repeated variables convention nodes within box get repeated model unrolled often write number copies repetitions bottom right corner box see figure simple example corresponding joint distribution form dgm represents assumptions behind models considered chapter slightly complex example shown figure left show naive bayes classiﬁer unrolled features uses plate represent repetition cases version right shows model using nested plate notation variable inside two plates two sub indices example write represent parameter feature class conditional density note plates nested crossing notational devices modeling complex parameter tying patterns devised heckerman widely used clear ﬁgure used generate iff otherwise ignored example context speciﬁc independence since relationship holds chapter directed graphical models bayes nets figure naive bayes classiﬁer dgm single plates nested plates 
[learning, plate, notation] inferring parameters data often assume data iid represent assumption explicitly using graphical model shown figure illustrates learning 
[learning, learning, complete, data] variables fully observed case missing data hidden variables say data complete dgm complete data likelihood given data associated node parents family product terms one per cpd say likelihood decomposes according graph structure suppose prior factorizes well clearly posterior also factorizes means compute posterior cpd independently words factored prior plus factored likelihood implies factored posterior let consider example cpds tabular thus extending earlier results secion discussed bayesian naive bayes separate row separate multinoulli distribution conditioning case combination parent values table formally write cpt cat tck number learning states node number parent combinations number nodes obviously tck row cpt let put separate dirichlet prior row cpt dir compute posterior simply adding pseudo counts empirical counts get dir tck number times node state parents state tck equation mean distribution given following tck tck tck tck tck example consider dgm figure suppose training data consists following cases list sufficient statistics tck posterior mean parameters ick dirichlet prior ick corresponding add one smoothing node tck tck tck tck easy show mle form equation except without tck terms tck tck tck course mle suffers zero count problem discussed section important use prior regularize estimation problem 
[learning, learning, missing, and/or, latent, variables] missing data hidden variables likelihood longer factorizes indeed longer convex explain detail section means usually compute locally optimal map estimate bayesian inference parameters even harder discuss suitable approximate inference techniques later chapters chapter directed graphical models bayes nets 
[conditional, independence, properties, dgms] figure dgm example figure see since path blocked observed path blocked hidden path blocked hidden however also see since path longer blocked observed exercise gives practice determining relationships dgms 
[conditional, independence, properties, dgms, d-separation, bayes, ball, algorithm, global, markov, properties] first introduce deﬁnitions say undirected path separated set nodes containing evidence iff least one following conditions hold contains chain contains tent fork contains collider structure descendant next say set nodes separated different set nodes given third observed set iff undirected path every node every node separated finally deﬁne properties dag follows separated given bayes ball algorithm shachter simple way see separated given based deﬁnition idea shade nodes indicating observed place balls node let bounce around according rules ask balls reach nodes three main rules shown figure notice balls travel opposite edge directions see ball pass chain shaded middle similarly ball pass fork shaded middle however ball cannot pass structure unless shaded middle justify rules bayes ball follows first consider chain structure encodes conditional independence properties dgms figure bayes ball rules shaded node one condition arrow hitting bar means ball cannot pass otherwise ball pass based jordan condition independent therefore observing middle node chain breaks two markov chain consider tent structure joint chapter directed graphical models bayes nets 
[conditional, independence, properties, dgms, markov, properties, dgms] separation criterion one conclude non descendants node nodes except descendants desc equation called directed local markov property example figure special case property look predecessors node according topological ordering pred follows since pred called ordered markov property justiﬁes equation example figure use ordering ﬁnd pred described three markov properties dags directed global markov property equation ordered markov property equation directed local markov property equation obvious less obvious nevertheless true see koller friedman proof hence properties equivalent furthermore distribution markov wrt factorized equation called factorization property obvious one show converse also holds see koller friedman proof 
[conditional, independence, properties, dgms, markov, blanket, full, conditionals] set nodes renders node conditionally independent nodes graph called markov blanket denote one show markov blanket node dgm equal parents children parents chapter directed graphical models bayes nets nodes also parents children copa example figure parent share common child namely see parents markov blanket note derive terms involve cancel numerator denominator left product cpds contain scope hence example figure resulting expression called full conditional prove important study gibbs sampling section 
[inﬂuence, decision, diagrams] represent multi stage bayesian decision problems using graphical notation known decision diagram inﬂuence diagram howard matheson kjaerulff madsen extends directed graphical models adding decision nodes also called tion nodes represented rectangles utility nodes also called value nodes represented diamonds original random variables called chance nodes represented ovals usual figure gives simple example illustrating famous oil wild catter problem problem decide whether drill oil well two possible actions means drill means drill assume states nature means well dry means wet oil means soaking lot oil suppose prior beliefs finally must specify utility function since states actions discrete represent table analogous cpt dgm suppose use following numbers dollars see drill incur costs also make money drill dry well lose drill wet well gain drill soaking well gain prior expected utility drill given example originally raiffa presentation based notes daphne koller inﬂuence decision diagrams oil utility drill sound test cost figure inﬂuence diagram basic oil wild catter problem extension information arc sound chance node drill decision node extension get decide whether perform test expected utility drill maximum expected utility max max therefore optimal action drill arg max let consider slight extension model suppose perform sounding estimate state well sounding observation one states diffuse reﬂection pattern suggesting oil open reﬂection pattern suggesting oil closed reﬂection pattern indicating lots oil since caused add arc model addition assume outcome sounding test available decide whether drill hence add information arc illustrated figure let model reliability sensor using following conditional distribution chapter directed graphical models bayes nets suppose sounding test observe posterior oil state posterior expected utility performing action gives however since drilling incurs cost observe better drilling makes sense suppose sounding test observe similar reasoning one show higher similarly observe much higher hence optimal policy follows choose get choose get choose get compute expected proﬁt maximum expected utility follows expected utility given possible outcomes sounding test assuming act optimally given outcome prior marginal outcome test hence maximum expected utility suppose choose whether test modelled shown figure add new test node test enter states determined exactly test enters special unknown state also cost associated performing test worth test depends much meu changes know outcome test namely state test equation test equation improvement utility test act optimally outcome inﬂuence decision diagrams figure pomdp shown inﬂuence diagram hidden world states implicitly make forgetting assumption effectively means arrows coming previous observations mdp shown inﬂuence diagram called value perfect information vpi test long costs less terms graphical models vpi variable determined computing meu base inﬂuence diagram computing meu inﬂuence diagram add information arcs action nodes computing difference words vpi meu meu decision node variable measuring possible modify variable elimination algorithm section computes optimal policy given inﬂuence diagram methods essentially work backwards ﬁnal time step computing optimal decision step assuming following actions chosen optimally see lauritzen nilsson kjaerulff madsen details could continue extend model various ways example could imagine dynamical system test observe outcomes perform actions move next oil well continue drilling polluting way fact many problems robotics business medicine public policy etc usefully formulated inﬂuence diagrams unrolled time raiffa lauritzen nilsson kjaerulff madsen generic model form shown figure known partially observed markov decision process pomdp pronounced pom basically hidden markov model section augmented action reward nodes used model perception action cycle intelligent agents use see kaelbling details special case pomdp states fully observed called markov decision process mdp shown figure much easier solve since compute mapping observed states actions solved using dynamic programming see sutton barto details pomdp case information arc sufficient uniquely determine chapter directed graphical models bayes nets figure dgms best action since state fully observed instead need choose actions based belief state since belief updating process deterministic see section compute belief state mdp details compute policies models see kaelbling spaan vlassis 
[exercises] exercise marginalizing node dgm source koller consider dag figure assume minimal map consider marginalizing construct new dag minimal map specify justify extra edges need added exercise bayes ball source jordan compute global independence statements directed graphical models use bayes ball algorithm separation criterion method converting undirected graph give results consider dag figure list variables independent given evidence consider dag figure list variables independent given evidence exercise markov blanket dgm prove full conditional node dgm given children parents exercise hidden variables dgms consider dgms figure deﬁne number empty nodes left right top bottom graph left deﬁnes joint inﬂuence decision diagrams figure weather fishing marginalized hidden variable graph right deﬁnes joint points assuming nodes including binary cpds tabular prove model left free parameters points assuming nodes binary cpds tabular prove model right free parameters points suppose data set observe want estimate parameters cpds using maximum likelihood model easier explain answer exercise bayes nets rainy day source nando freitas question must model problem binary variables gray vancouver rain sad consider directed graphical model describing relation ship variables shown figure write expression terms write expression different explain find maximum likelihood estimates using following data set row training case may state answers without proof exercise fishing nets source duda consider bayes net shown figure nodes represent following variables winter spring summer autumn salmon sea bass light medium dark wide thin chapter directed graphical models bayes nets figure qmr style network hidden leaves removing barren nodes corresponding conditional probability tables note rows represent columns row sums one represents child cpd thus thin sea bass thin salmon etc answer following queries may use matlab hand either case show work suppose ﬁsh caught december end autumn beginning winter thus let instead prior called soft evidence since know exact value distribution suppose lightness measured known ﬁsh thin classify ﬁsh salmon sea bass suppose know ﬁsh thin medium lightness season likely use exercise removing leaves networks consider qmr network symtpoms observed example fig ure hidden show safely remove hidden leaf nodes without affecting posterior disease nodes prove compute using network figure called barren node removal applied dgm suppose partition leaves three groups unknown clearly remove unknown leaves since hidden affect parents show analytically remove leaves state absorbing effect prior parents trick works noisy cpds exercise handling negative ﬁndings qmr network consider qmr network let hidden diseases negative ﬁndings leaf nodes positive ﬁndings leaf nodes compute posterior two steps ﬁrst absorb negative ﬁndings absorb positive ﬁndings show ﬁrst step done time number dieases number negative ﬁndings simplicity ignore leak nodes intuitively reason correlation induced amongst parents ﬁnding since explaining away inﬂuence decision diagrams exercise moralization introduce new independence statements recall process moralizing dag means connecting together unmarried parents share common child dropping arrows let moralization dag show set conditional independence statements implied model 
[mixture, models, algorithm, latent, variable, models] chapter showed graphical models used deﬁne high dimensional joint probability distributions basic idea model dependence two variables adding edge graph technically graph represents conditional independence get point alternative approach assume observed variables correlated arise hidden common cause model hidden variables also known latent variable models lvms see chapter models harder models latent variables however signiﬁcant advantages two main reasons first lvms often fewer parameters models directly represent correlation visible space illustrated figure nodes including binary cpds tabular model left free parameters whereas model right free parameters second hidden variables lvm serve bottleneck computes compressed representation data forms basis unsupervised learning see figure illustrates generic lvm structures used purpose general latent variables visible variables usually many latent factors contributing observation many many mapping single latent variable case usually discrete one many mapping also many one mapping representing different competing factors causes observed variable models form basis probabilistic matrix factorization discussed section finally one one mapping represented allowing vector valued representation subsume others depending form likelihood prior generate variety different models summarized table 
[mixture, models, algorithm, mixture, models] simplest form lvm representing discrete latent state use discrete prior cat likelihood use chapter mixture models algorithm xsdudphwhuv xsdudphwhuv figure dgm without hidden variables leaves represent medical symptoms roots represent primary causes smoking diet exercise hidden variable represent mediating factors heart disease might directly visible figure latent variable model represented dgm many many one many many one one one base distribution observations type overall model known mixture model since mixing together base distributions follows convex combination since taking weighted sum mixing weights satisfy give examples mixture models name section mvn discrete mixture gaussians prod discrete discrete mixture multinomials prod gaussian prod gaussian factor analysis probabilistic pca prod gaussian prod laplace probabilistic ica sparse coding prod discrete prod gaussian multinomial pca prod discrete dirichlet latent dirichlet allocation prod noisy prod bernoulli qmr prod bernoulli prod bernoulli sigmoid belief net table summary popular directed latent variable models prod means product prod discrete likelihood means factored distribution form cat prod gaussian means factored distribution form pca stands principal components analysis ica stands indepedendent components analysis figure mixture gaussians show contours constant probability component mixture surface plot overall density based figure bishop figure generated mixgaussplotdemo 
[mixture, models, algorithm, mixture, models, mixtures, gaussians] widely used mixture model mixture gaussians mog also called gaussian mixture model gmm model base distribution mixture multivariate gaussian mean covariance matrix thus model form figure shows mixture gaussians mixture component represented different set eliptical contours given sufficiently large number mixture components gmm used approximate density deﬁned chapter mixture models algorithm 
[mixture, models, algorithm, mixture, models, mixture, multinoullis] use mixture models deﬁne density models many kinds data example suppose data consist dimensional bit vectors case appropriate class conditional density product bernoullis ber probability bit turns cluster latent variables meaning might simply introduce latent variables order make model powerful example one show exercise mean covariance mixture distribution given cov diag although component distributions factorized joint distribution thus mixture distribution capture correlations variables unlike single product bernoullis model 
[mixture, models, algorithm, mixture, models, using, mixture, models, clustering] two main applications mixture models ﬁrst use black box density model useful variety tasks data compression outlier detection creating generative classiﬁers model class conditional density mixture distribution see section second common application mixture models use clustering discuss topic detail chapter basic idea simple ﬁrst mixture model compute represents posterior probability point belongs cluster known responsibility cluster point computed using bayes rule follows procedure called soft clustering identical computations performed using generative classiﬁer difference two models arises training time mixture case never observe whereas generative classiﬁer observe plays role represent amount uncertainty cluster assignment using max assuming small may reasonable compute hard clustering using map estimate given arg max arg max log log mixture models time genes yeast microarray data means centroids figure yeast gene expression data plotted time series visualizing cluster centers produced means figure generated kmeansyeastdemo figure mixture bernoullis binarized mnist digit data show mle corresponding cluster means numbers top image represent mixing weights labels used training model figure generated mixbermnistem hard clustering using gmm illustrated figure cluster data rep resenting height weight people colors represent hard assignments note identity labels colors used immaterial free rename clusters without affecting partitioning data called label switching another example shown figure data vectors represent expression levels different genes different time points clustered using gmm see several kinds genes whose expression level goes monotonically time response given stimulus whose expression level goes monotonically complex response patterns clustered series groups see section details choose example represent cluster prototype centroid shown figure example clustering binary data consider binarized version mnist handwrit ten digit dataset see figure ignore class labels mixture chapter mixture models algorithm bernoullis using visualize resulting centroids shown figure see method correctly discovered digit classes overall results great created multiple clusters digits clusters others several possible reasons errors model simple capture relevant visual characteristics digit example pixel treated independently notion shape stroke although think clusters digits actually exhibit fair degree visual variety example two ways writing without cross bar figure illustrates range writing styles thus need clusters adequately model data however set large nothing model algorithm preventing extra clusters used create multiple versions digit indeed happens use model selection prevent many clusters chosen looks visually appealing makes good density estimator may quite different likelihood function convex may stuck local optimum explain section example typical mixture modeling goes show one must cautious trying interpret clusters discovered method adding little bit supervision using informative priors help lot 
[mixture, models, algorithm, mixture, models, mixtures, experts] section described use mixture models context generative classiﬁers also use create discriminative models classiﬁcation regression example consider data figure seems like good model would three different linear regression functions applying different part input space model allowing mixing weights mixture densities input dependent cat see figure dgm model called mixture experts moe jordan jacobs idea submodel considered expert certain region input space function called gating function decides expert use depending input values example figure shows three experts carved input space figure shows predictions expert individually case experts linear regression models figure shows overall prediction model obtained using discuss model section mixture models expert predictions fixed mixing weights gating functions fixed mixing weights predicted mean var fixed mixing weights figure data three separate regression lines gating functions three different experts conditionally weighted average three expert predictions figure generated mixexpdemo 
[mixture, models, algorithm] figure mixture experts hierarchical mixture experts chapter mixture models algorithm forwards problem expert predictions prediction mean mode figure data simple forwards model data inverse model mixture linear regressions training points color coded responsibilities predictive mean red cross mode black square based figures bishop figure generated mixexpdemoonetomany clear plug model expert example use neural networks chapter represent gating functions experts result known mixture density network models slower train ﬂexible mixtures experts see bishop details also possible make expert mixture experts gives rise model known hierarchical mixture experts see figure dgm section details application inverse problems mixtures experts useful solving inverse problems problems invert many one mapping typical example robotics location end effector hand uniquely determined joint angles motors however given location many settings joints produce thus inverse mapping unique another example kinematic tracking people video mapping image appearance pose unique due self occlusion etc parameter estimation mixture models figure lvm represented dgm left model unrolled examples right model using plate notation simpler example illustration purposes shown figure see deﬁnes function since every value along horizontal axis unique response sometimes called forwards model consider problem computing corresponding inverse model shown figure obtained simply interchanging axes see values along horizontal axis multiple possible outputs inverse uniquely deﬁned example could consequently predictive distribution multimodal mixture linear experts data figure shows prediction expert figure shows plugin approximation posterior predictive mode mean note posterior mean yield good predictions fact model trained minimize mean squared error even model ﬂexible nonlinear model neural network work poorly inverse problems however posterior mode mode input dependent provides reasonable approximation 
[mixture, models, algorithm, parameter, estimation, mixture, models] seen compute posterior hidden variables given observed variables assuming parameters known section discuss learn parameters section showed complete data factored prior posterior parameters also factorizes making computation simple unfortunately longer true hidden variables missing data reason apparent looking figure observed separation see hence posterior factorize since lvm hidden parameters longer independent posterior factorize making much harder chapter mixture models algorithm figure left data points sampled mixture gaussians right likelihood surface parameters set true values see two symmetric modes reﬂecting unidentiﬁability parameters figure generated mixgaussliksurfacedemo compute also complicates computation map estimates discus 
[mixture, models, algorithm, parameter, estimation, mixture, models, unidentiﬁability] main problem computing lvm posterior may multiple modes see consider gmm observed would unimodal posterior parameters dir niw consequently easily ﬁnd globally optimal map estimate hence globally optimal mle suppose hidden case possible ways ﬁlling get different unimodal likelihood thus marginalize get multi modal posterior modes correspond different labelings clusters illustrated figure plot likelihood function gmm data shown figure see two peaks one corresponding case case say parameters identiﬁable since unique mle therefore cannot unique map estimate assuming prior rule certain labelings hence posterior must multimodal question many modes confuse multimodality parameter posterior multimodality deﬁned model latter case clusters would expect get peaks although theoretically possible get least carreira perpinan williams parameter estimation mixture models parameter posterior hard answer possible labelings peaks might get merged nevertheless exponential number since ﬁnding optimal mle gmm hard aloise drineas unidentiﬁability cause problem bayesian inference example suppose draw samples posterior average try approximate posterior mean kind monte carlo approach explained detail chapter samples come different modes average meaningless note however reasonable average posterior predictive distributions since likelihood function invariant mode parameters came variety solutions proposed unidentiﬁability problem solutions depend details model inference algorithm used example see stephens approach handling unidentiﬁability mixture models using mcmc approach adopt chapter much simpler compute single local mode perform approximate map estimation say approximate since ﬁnding globally optimal mle hence map estimate hard least mixture models aloise far common approach simplicity also reasonable approximation least sample size sufficiently large see consider figure see latent variables gets see one data point however two latent parameters gets see data points posterior uncertainty parameters typically much less posterior uncertainty latent variables justiﬁes common strategy computing bothering compute section study hierarchical bayesian models essentially put structure top parameters models important model parameters send information used point estimate would possible 
[mixture, models, algorithm, parameter, estimation, mixture, models, computing, map, estimate, non-convex] previous sections argued rather heuristically likelihood function multiple modes hence ﬁnding map estimate hard section show result algebraic means sheds additional insight problem presentation based part rennie consider log likelihood lvm log log unfortunately objective hard maximize since cannot push log inside sum precludes certain algebraic simplications prove problem hard suppose joint probability distribution exponential family means written follows exp chapter mixture models algorithm sufficient statistics normalization constant see sec tion details shown exercise mvn exponential family nearly distributions encountered far including dirichlet multinomial gamma wishart etc student distribution notable exception mixtures exponential families also exponential family providing mixing indicator variables observed exercise assumption complete data log likelihood written follows log ﬁrst term clearly linear one show convex function boyd vandenberghe overall objective concave due minus sign hence unique maximum consider happens missing data observed data log likelihood given log log log one show log sum exp function convex boyd vandenberghe know convex however difference two convex functions general convex objective neither convex concave local optima disadvantage non convex functions usually hard ﬁnd global timum optimization algorithms ﬁnd local optimum one ﬁnd depends start algorithms simulated annealing sec tion genetic algorithms claim always ﬁnd global optimum unrealistic assumptions allowed cooled inﬁnitely slowly lowed run inﬁnitely long practice run local optimizer perhaps using multiple random restarts increase chance ﬁnding good local optimum course careful initialization help lot give examples case case basis note convex method ﬁtting mixtures gaussians proposed idea assign one cluster per data point select amongst using convex type penalty rather trying optimize locations cluster centers see lashkari golland details essentially unsupervised version approach used sparse kernel logistic regression discuss section note however penalty although convex necessarily good way promote sparsity discussed chapter fact see chapter best sparsity promoting methods use non convex penalties use optimie moral story afraid non convexity 
[mixture, models, algorithm, algorithm] many models machine learning statistics computing map parameter estimate easy provided observe values relevant random variables algorithm model section mix gaussians mix experts factor analysis student probit regression dgm hidden variables mvn missing data hmms shrinkage estimates gaussian means exercise table models discussed book easily applied ﬁnd map parameter estimate complete data however missing data latent variables computing map estimate becomes hard one approach use generic gradient based optimizer ﬁnd local minimum negative log likelihood nll given nll log however often enforce constraints fact covariance matrices must positive deﬁnite mixing weights must sum one etc tricky see exercise cases often much simpler always faster use algorithm called expectation maximization short dempster meng van dyk mclachlan krishnan simple iterative algorithm often closed form updates step furthermore algorithm automatically enforce required constraints exploits fact data fully observed map estimate would easy compute particular iterative algorithm alternates inferring missing values given parameters step optimizing parameters given ﬁlled data step give details followed several examples end theoretical discussion put algorithm larger context see table summary applications book 
[mixture, models, algorithm, algorithm, basic, idea] let visible observed variables case let hidden missing variables goal maximize log likelihood observed data log log unfortunately hard optimize since log cannot pushed inside sum chapter mixture models algorithm gets around problem follows deﬁne complete data log likelihood log cannot computed since unknown let deﬁne expected complete data log likelihood follows current iteration number called auxiliary function expectation taken wrt old parameters observed data goal step compute rather terms inside mle depends known expected sufficient statistics ess step optimize function wrt arg max perform map estimation modify step follows argmax log step remains unchanged section show algorithm monotonically increases log likelihood observed data plus log prior map estimation stays objective ever goes must bug math code surprisingly useful debugging tool explain perform steps several simple models make things clearer 
[mixture, models, algorithm, algorithm, gmms] section discuss mixture gaussians using fitting kinds mixture models requires straightforward modiﬁcation see exercise assume number mixture components known see section discussion point algorithm auxiliary function expected complete data log likelihood given log log log log log log responsibility cluster takes data point computed step described step step following simple form mixture model step step optimize wrt obviously weighted number points assigned cluster derive step terms look parts depend see result log log weighted version standard problem computing mles mvn see section one show exercise new parameter estimates given chapter mixture models algorithm equations make intuitive sense mean cluster weighted average points assigned cluster covariance proportional weighted empirical scatter matrix computing new estimates set next step example example algorithm action shown figure start color code points blue points come cluster red points cluster precisely set color color blue red ambiguous points appear purple iterations algorithm converged good clustering data standardized removing mean dividing standard deviation processing often helps convergence means algorithm popular variant algorithm gmms known means algorithm discuss consider gmm make following assumptions ﬁxed ﬁxed cluster centers estimated consider following delta function approximation posterior computed step argmax sometimes called hard since making hard assignment points clusters since assumed equal spherical covariance matrix cluster probable cluster computed ﬁnding nearest prototype arg min hence step must ﬁnd euclidean distance data points cluster centers takes time however sped using various techniques applying triangle inequality avoid redundant computations elkan given hard cluster assignments step updates cluster center computing mean points assigned see algorithm pseudo code algorithm figure illustration gmm applied old faithful data initial random values parameters posterior responsibility point computed ﬁrst step degree redness indicates degree point belongs red cluster similarly blue purple points roughly uniform posterior clusters show updated parameters ﬁrst step iterations iterations iterations based bishop figure figure generated mixgaussdemofaithful chapter mixture models algorithm algorithm means algorithm initialize repeat assign data point closest cluster center arg min update cluster center computing mean points assigned converged figure image compressed using vector quantization codebook size figure generated vqdemo vector quantization since means proper algorithm maximizing likelihood instead interpreted greedy algorithm approximately minimizing loss function related data compression explain suppose want perform lossy compression real valued vectors simple approach use vector quantization basic idea replace real valued vector discrete symbol index codebook prototypes data vector encoded using index similar prototype similarity measured terms euclidean distance encode arg min deﬁne cost function measures quality codebook computing reconstruction error distortion induces decode encode decode means algorithm thought simple iterative scheme minimizing objective course achieve zero distortion assign one prototype every data vector takes space number real valued data vectors algorithm length number bits needed represent real valued scalar quantization accuracy however many data sets see similar vectors repeatedly rather storing many times store create pointers hence reduce space requirement log kdc log term arises data vectors needs specify codewords using pointers kdc term arises store codebook entry dimensional vector typically ﬁrst term dominates second approximate rate encoding scheme number bits needed per object log typically much less one application image compression consider pixel image figure gray scale use one byte represent pixel gray scale intensity need bits represent image compressed image need log bits factor compression factor compression negligible perceptual loss see figure greater compression could achieved modelled spatial correlation pixels encoded blocks used jpeg residual errors differences model predictions would smaller would take fewer bits encode initialization avoiding local minima means need initialized common pick data points random make initial cluster centers pick centers sequentially try cover data pick initial point uniformly random subsequent point picked remaining points probability proportional squared distance points closest cluster center known farthest point clustering gonzales means arthur vassilvitskii bahmani surprisingly simple trick shown guarantee distortion never log worse optimal arthur vassilvitskii heuristic commonly used speech recognition community incrementally grow gmms initially give cluster score based mixture weight round training consider splitting cluster highest score two new centroids random perturbations original centroid new scores half old scores new cluster small score narrow variance removed continue way desired number clusters reached see figueiredo jain similar incremental approach map estimation usual mle may overﬁt overﬁtting problem particularly severe case gmms understand problem suppose simplicity possible get inﬁnite likelihood assigning one centers say single data point say since term makes following contribution likelihood chapter mixture models algorithm dimensionality fraction times gmm fails mle map figure illustration singularities arise likelihood function gmms based bishop figure figure generated mixgausssingularity illustration beneﬁt map estimation estimation ﬁtting gaussian mixture model plot fraction times random trials method encounters numerical problems dimensionality problem samples solid red upper curve mle dotted black lower curve map figure generated mixgaussmlvsmap hence drive term inﬁnity letting shown figure call collapsing variance problem easy solution perform map estimation new auxiliary function expected complete data log likelihood plus log prior old log log log log note step remains unchanged step needs modiﬁed explain prior mixture weights natural use dirichlet prior dir since conjugate categorical distribution map estimate given use uniform prior reduces equation prior parameters class conditional densities depends form class conditional densities discuss case gmms leave map estimation mixtures bernoullis exercise simplicity let consider conjugate prior form niw algorithm section map estimate given illustrate beneﬁts using map estimation instead estimation context gmms apply synthetic data dimensions using either map estimation count trial failure numerical issues involving singular matrices dimensionality conduct random trials results illustrated figure using see soon becomes even moderately large estimation crashes burns whereas map estimation never encounters numerical problems using map estimation need specify hyper parameters mention simple heuristics setting fraley raftery set unregularized since numerical problems arise case map estimates simplify quite scary looking discuss set one possibility use diag pooled variance dimension reason term resulting volume ellipsoid given diag parameter controls strongly believe prior weakest prior use still proper set common choice 
[mixture, models, algorithm, algorithm, mixture, experts] mixture experts model using straightforward manner expected complete data log likelihood given old log old old old step standard mixture model except replace computing chapter mixture models algorithm step need maximize old wrt regression parameters model objective form old recognize weighted least squares problem makes intuitive sense small data point downweighted estimating model parameters section immediately write mle diag mle variance given replace estimate unconditional mixing weights estimate gating parameters objective form log recognize equivalent log likelihood multinomial logistic regression equation except replace hard encoding soft encoding thus estimate ﬁtting logistic regression model soft target labels 
[mixture, models, algorithm, algorithm, dgms, hidden, variables] generalize ideas behind mixtures experts compute mle map estimate arbitrary dgm could use gradient based methods binder much simpler use lauritzen step estimate hidden variables step compute mle using ﬁlled values give details simplicity presentation assume cpds tabular based section let write cpt follows tck log likelihood complete data given log tck log tck tck empirical counts hence expected complete data log likelihood form log tck log tck algorithm tck visible variables case quantity known family marginal computed using inference algorithm tjk expected sufficient statistics constitute output step given ess step simple form tck tck tjk proved adding lagrange multipliers enforce constraint tjk expected complete data log likelihood optimizing parameter vector separately modify perform map estimation dirichlet prior simply adding pseudo counts expected counts 
[mixture, models, algorithm, algorithm, student, distribution] one problem gaussian distribution sensitive outliers since log probability decays quadratically distance center robust alternative student distribution discussed section unlike case gaussian closed form formula mle student even missing data must resort iterative optimization methods easiest one use since automatically enforces constraints positive symmetric positive deﬁnite addition resulting algorithm turns simple intuitive form see ﬁrst blush might apparent used since missing data key idea introduce artiﬁcial hidden auxiliary variable order simplify algorithm particular exploit fact student distribution written gaussian scale mixture see exercise proof case thought inﬁnite mixture gaussians one slightly different covariance matrix treating missing data write complete data log likelihood log log log log log log log log chapter mixture models algorithm deﬁned mahalanobis distance partition two terms one involving involving dropping irrelevant constants log log log log known let ﬁrst derive algorithm assumed known simplicity case ignore term need ﬁgure compute wrt old parameters section hence step iteration step obtained maximizing yield results quite intuitive quantity precision measurement small corresponding data point weighted estimating mean covariance student achieves robustness outliers unknown compute mle degrees freedom ﬁrst need compute expectation involves log one show log log algorithm errors using gauss red error bankrupt solvent errors using student red error bankrupt solvent figure mixture modeling bankruptcy data set left gaussian class conditional densities right student class conditional densities points belong class shown triangles points belong class shown circles estimated labels based posterior probability belonging mixture component computed incorrect point colored red otherwise colored blue training data black figure generated mixstudentbankruptcydemo log digamma function hence equation log log log substituting equation log log gradient expression equal log unique solution interval found using constrained optimizer performing gradient based optimization step rather closed form update example known generalized algorithm one show still converge local optimum even perform partial improvement parameters step mixtures student distributions easy extend methods mixture student distributions see exercise details let consider small example data set regarding bankrupty patterns certain companies ﬁrst feature speciﬁes ratio chapter mixture models algorithm retained earnings total assets second feature speciﬁes ratio earnings interests taxes ebit total assets two models data ignoring class labels mixture gaussians mixture students use ﬁtted model classify data compute probable cluster membership treat compare true labels compute error rate permute latent labels consider cluster represent class vice versa recompute error rate points misclassiﬁed shown red result shown figure see student model made errors gaussian model made class conditional densities contain extreme values causing gaussian poor choice 
[mixture, models, algorithm, algorithm, probit, regression] section described latent variable interpretation probit regression recall form latent show model using although possible probit regression models using gradient based methods shown section based approach advantage generalized many kinds models see later complete data log likelihood following form assuming prior log log log log const posterior step truncated gaussian equation see depends linearly need compute exercise asks show posterior mean given step estimate using ridge regression output trying predict speciﬁcally algorithm simple much slower direct gradient methods illustrated figure posterior entropy step quite high since observe positive negative given information likelihood magnitude using stronger regularizer help speed convergence constrains range plausible values addition one use various speedup tricks data augmentation van dyk meng discuss algorithm iter penalized nll probit regression regularizer minfunc figure fitting probit regression model using quasi newton method figure generated probitregdemo 
[mixture, models, algorithm, algorithm, theoretical, basis] section show monotonically increases observed data log likelihood reaches local maximum saddle point although points usually unstable derivation also serve basis various generalizations discuss later expected complete data log likelihood lower bound consider arbitrary distribution hidden variables observed data log likelihood written follows log log log concave function jensen inequality equation following lower bound log let denote lower bound follows log entropy argument holds positive distribution one choose intuitively pick yields tightest lower bound lower bound sum chapter mixture models algorithm terms following form log log log log log term independent maximize lower bound setting course unknown instead use estimate parameters iteration output step plugging lower bound get log recognize ﬁrst term expected complete data log likelihood second term constant wrt step becomes arg max arg max log usual comes punchline since used divergence becomes zero log hence log see lower bound tight step since lower bound touches function maximizing lower bound also push function step guaranteed modify parameters increase likelihood observed data unless already local maximum process sketched figure dashed red curve original function observed data log likelihood solid blue curve lower bound evaluated touches objective function set maximum lower bound blue curve new bound point dotted green curve maximum new bound becomes etc compare newton method figure repeatedly ﬁts optimizes quadratic approximation monotonically increases observed data log likelihood prove monotonically increases observed data log likelihood reaches local optimum algorithm figure illustration bound optimization algorithm based figure bishop figure generated emloglikelihoodmax ﬁrst inequality follows since lower bound second inequality follows since deﬁnition max ﬁnal equality follows equation consequence result observe monotonic increase observed data log likelihood must error math code performing map estimation must add log prior term objective surprisingly powerful debugging tool 
[mixture, models, algorithm, algorithm, online] dealing large streaming datasets important able learn online discussed section two main approaches online literature ﬁrst approach known incremental neal hinton optimizes lower bound one time however requires storing expected sufficient statistics data case second approach known stepwise sato ishii cappe mouline cappe based stochastic approximation theory requires constant memory use explain approaches detail following presentation liang klein liang klein batch review explaining online review batch abstract setting let vector sufficient statistics single data case example mixture multinoullis would count vector number cluster used plus matrix number times hidden state observed letter let expected sufficient statistics case sum ess given derive map estimate parameters step denote operation example case mixtures multinoullis need normalize row notation belt pseudo code batch shown algorithm chapter mixture models algorithm algorithm batch algorithm initialize repeat new example new new new converged incremental incremental neal hinton keep track well come data case swap old replace new new shown code algorithm note exploit sparsity new speedup computation since components wil changed algorithm incremental algorithm initialize repeat example random order new new new converged viewed maximizing lower bound optimizing etc method guaranteed monotonically converge local maximum lower bound log likelihood stepwise stepwise whenever compute new move towards shown algorithm iteration stepsize value must satisfy robbins monro conditions equation example liang klein liang klein use get somewhat better behavior using minibatch size update possible optimize maximize training set likelihood detail written update exploit sparsity storing jlt instead using sparse update jlt affect results since scaling counts global constant effect algorithm figure illustration deterministic annealing based http wikipedia org wiki grad uated_optimization trying different values parallel initial trial period signiﬁcantly speed algorithm algorithm stepwise algorithm initialize repeat example random order converged liang klein liang klein compare batch incremental stepwise four different unsupervised language modeling tasks found stepwise using faster incremental much faster batch terms accuracy stepwise usually good sometimes even better batch incremental often worse either methods 
[mixture, models, algorithm, algorithm, variants] one widely used algorithms statistics machine learning surpris ingly many variations proposed brieﬂy mention use later chapters see mclachlan krishnan information annealed general converge local maximum increase chance ﬁnding global maximum use variety methods one approach use method known deterministic annealing rose basic idea smooth posterior landscape raising temperature gradually cooling slowly tracking global maximum see figure sketch stochastic version chapter mixture models algorithm true log likelihood lower bound training time true log likelihood lower bound training time figure illustration possible behaviors variational lower bound increases iteration likelihood lower bound increases likelihood decreases case algorithm closing gap approximate true posterior regularizing effect based figure saul figure generated varembound algorithm described section annealed version described ueda nakano variational section showed optimal thing step make exact posterior latent variables case lower bound log likelihood tight step push log likelihood however sometimes computationally intractable perform exact inference step may able perform approximate inference ensure step performing inference based lower bound likelihood step seen monotonically increasing lower bound see figure called variational neal hinton see chapter variational inference methods used step monte carlo another approach handling intractable step use monte carlo approximation expected sufficient statistics draw samples posterior compute sufficient statistics completed vector average results called monte carlo mcem wei tanner draw single sample called stochastic celeux diebolt one way draw samples use mcmc see chapter however wait mcmc converge inside step method becomes slow alternative use stochastic approximation perform brief sampling step followed partial parameter update called stochastic approximation delyon tends work better mcem another alternative apply mcmc infer parameters well latent variables fully bayesian approach thus eliminating distinction steps see chapter details generalized sometimes perform step exactly cannot perform step exactly however still monotonically increase log likelihood performing partial step merely increase expected complete data log likelihood rather maximizing example might follow gradient steps called algorithm iterations loglik iterations loglik figure illustration adaptive relaxed applied mixture gaussians dimensions show algorithm applied two different datasets randomly sampled mixture gaussians plot convergence different update rates using gives results regular actual running time printed legend figure generated mixgaussoverrelaxedemdemo generalized gem algorithm unfortunate term since many ways generalize ecm algorithm ecm algorithm stands expectation conditional maximization refers optimizing parameters step sequentially turn dependent ecme algorithm stands ecm either liu rubin variant ecm maximize expected complete data log likelihood function usual observed data log likelihood one conditional maximization steps latter much faster since ignores results step directly optimizes objective interest standard example ﬁtting student distribution ﬁxed update usual update replace standard update form arg max arg max log see mclachlan krishnan information relaxed vanilla quite slow especially lots missing data adaptive overrelaxed algorithm salakhutdinov roweis performs update form step size parameter usual update computed step obviously reduces standard using larger values result faster convergence see figure illustration unfortunately using large value cause algorithm fail converge finally note fact special case larger class algorithms known bound optimization algorithms stands minorize maximize see hunter lange discussion chapter mixture models algorithm 
[mixture, models, algorithm, model, selection, latent, variable, models] using lvms must specify number latent variables controls model complexity particuarl case mixture models must specify number clusters choosing parameters example model selection discuss approaches 
[mixture, models, algorithm, model, selection, latent, variable, models, model, selection, probabilistic, models] optimal bayesian approach discussed section pick model largest marginal likelihood argmax two problems first evaluating marginal likelihood lvms quite difficult practice simple approximations bic used see fraley raftery alternatively use cross validated likelihood performance measure although slow since requires ﬁtting model times number folds second issue need search potentially large number models usual approach perform exhaustive search candidate values however sometimes set model maximal size rely power bayesian occam razor kill unwanted components example shown section discuss variational bayes alternative approach perform stochastic sampling space models traditional approaches green lunn based reversible jump mcmc use birth moves propose new centers death moves kill old centers however slow difficult implement simpler approach use dirichlet process mixture model using gibbs sampling still allows unbounded number mixture components see section details perhaps surprisingly sampling based methods faster simple approach evaluating quality separately reason ﬁtting model often slow contrast sampling methods often quickly determine certain value poor thus need waste time part posterior 
[mixture, models, algorithm, model, selection, latent, variable, models, model, selection, non-probabilistic, methods] using probabilistic model example choose means algorithm since correspond probability model likelihood none methods described used obvious proxy likelihood reconstruction error deﬁne squared recon struction error data set using model complexity follows case means reconstruction given argmin explained section figure plots reconstruction error test set means notice error decreases increasing model complexity reason behavior follows model selection latent variable models mse test means nll test set gmm figure test set performance data generated mixture gaussians data shown figure mse test set means negative log likelihood test set gmm figure generated kmeansmodelseld xtrain mse mse mse mse mse mse nll nll nll nll nll nll figure synthetic data generated mixture gaussians histogram training data test data looks essentially centroids estimated means gmm density model estimated values figure generated kmeansmodelseld chapter mixture models algorithm add centroids means tile space densely shown figure hence given test vector likely ﬁnd close prototype accurately represent increases thus decreasing reconstruction error however use probabilistic model gmm plot negative log likelihood get usual shaped curve test set shown figure supervised learning always use cross validation select non probabilistic models different complexity case unsupervised learning although novel observation mentioned passing hastie one standard references ﬁeld perhaps widely appreciated fact one compelling arguments favor probabilistic models given cross validation work supposing one unwilling use probabilistic models bizarre reason one choose common approach plot reconstruction error training set versus try identify knee kink curve idea true number clusters rate decrease error function high since splitting apart things grouped together however splitting apart natural clusters reduce error much kink ﬁnding process automated use gap statistic tibshirani nevertheless identifying kinks hard shown figure since loss function usually drops gradually different approach kink ﬁnding described section 
[mixture, models, algorithm, fitting, models, missing, data] suppose want joint density model maximum likelihood holes data matrix due missing data usually represented nans formally let component data case observed let otherwise let visible data missing hidden data goal compute argmax missing random assumption see section vector created row columns indexed set hence log likelihood form log log fitting models missing data vector hidden variables case assumed discrete notational simplicity substituting get log log unfortunately objective hard maximize since cannot push log inside sum however use algorithm compute local optimum give example 
[mixture, models, algorithm, fitting, models, missing, data, mle, mvn, missing, data] suppose want mvn maximum likelihood missing data use ﬁnd local maximum objective explain getting started get algorithm started compute mle based rows data trix fully observed rows use hoc imputation procedures compute initial mle step compute expected complete data log likelihood iteration follows log log log log log drop conditioning expectation brevity see need compute expected sufficient statistics compute quantities use results section speciﬁcally consider case components observed components unobserved chapter mixture models algorithm hence expected sufficient statistics assumed without loss generality unobserved variables come observed variables node ordering compute use result cov hence step solving show step equivalent plugging ess usual mle equations get thus see equivalent simply replacing variables expectations applying standard mle formula would ignore posterior variance would result incorrect estimate instead must compute expectation sufficient statistics plug usual equation mle easily modify algorithm perform map estimation plugging ess equation map estimate implementation see gaussmissingfitem example example procedure action let reconsider imputation problem section dimensional data cases missing data let parameters using call resulting parameters use model predictions computing figure indicates results obtained using learned parameters almost good true parameters surprisingly performance improves data fraction missing data reduced extension gmm case straightforward mixture gaussians presence partially observed data vectors leave details exercise 
[mixture, models, algorithm, exercises] exercise student inﬁnite mixture gaussians derive equation simplicity assume one dimensional distribution fitting models missing data truth imputed truth imputed truth imputed imputation true params truth imputed truth imputed truth imputed truth imputed imputation truth imputed figure illustration data imputation scatter plot true values imputed values ing true parameters using parameters estimated figure generated gaussimputationdemo exercise mixtures gaussians show step estimation mixture gaussians given exercise mixtures bernoullis show step estimation mixture bernoullis given show step map estimation mixture bernoullis prior given exercise mixture student distributions derive algorithm estimation mixture multivariate student distributions exercise gradient descent ﬁtting gmm consider gaussian mixture model deﬁne log likelihood log chapter mixture models algorithm figure mixture gaussians two discrete latent indicators speciﬁes mean use speciﬁes variance use deﬁne posterior responsibility cluster datapoint follows show gradient log likelihood wrt derive gradient log likelihood wrt ignore constraints one way handle constraint reparameterize using softmax function unconstrained parameters show may constant factor missing expression hint use chain rule fact follows exercise derive gradient log likelihood wrt ignore constraints one way handle constraint symmetric positive deﬁnite matrix reparame terize using cholesky decomposition upper triangular otherwise unconstrained matrix derive gradient log likelihood wrt exercise ﬁnite scale mixture gaussians source jaakkola consider graphical model figure deﬁnes following fitting models missing data parameters equivalent mixture weights think mixture non gaussian components component distribution scale mixture combining gaussians different variances scales derive generalized algorithm model recall generalized partial update step rather ﬁnding exact maximum derive expression responsibilities needed step write full expression expected complete log likelihood new old old log new solving step would require jointly optimize means variances turn simpler ﬁrst solve given ﬁxed subsequently solve given new values brevity ﬁrst part derive expression maximizing given ﬁxed solve new exercise manual calculation step gmm source freitas question consider clustering data mixture gaussians using algorithm given data points suppose output step following matrix entry probability obervation belonging cluster responsibility cluster data point compute step may state equations maximum likelihood estimates quantities know without proof apply equations data set may leave answer fractional form show work write likelihood function trying optimize performing step mixing weights new values performing step means new values exercise moments mixture gaussians consider mixture gaussians show chapter mixture models algorithm figure data points circles represent initial guesses show cov hint use fact cov exercise means clustering hand source jaakkola figure show data points lie integer grid note axis compressed distances measured using actual grid coordinates suppose apply means algorithm data using centers initialized two circled data points draw ﬁnal clusters obtained means converges show approximate location new centers group together points assigned center hint think shortest euclidean distance exercise deriving means cost function show hint note since amp exercise visible mixtures gaussians exponential family show joint distribution gmm represented exponential family form fitting models missing data inverse temperature survival time regression censored data red censored green predicted ols figure example censored linear regression black circles observed training points red crosses observed censored training points green stars predicted values censored training points also show lines least squares ignoring censoring based figure tanner figure generated linregcensoredschmeehahndemo written hannes bretschneider exercise robust linear regression student likelihood consider model form derive algorithm compute mle may assume ﬁxed simplicity hint see section exercise estimation gaussian shrinkage model extend results section case equal known hint treat hidden variables integrate step maximize step exercise censored linear regression censored regression refers case one knows outcome least certain value precise value unknown arises many different settings example suppose one trying learn model predict long program take run different settings parameters one may abort certain runs seem taking long resulting run times said right censored runs know censoring time min true running time observed running time also deﬁne left censored interval censored models derive algorithm ﬁtting linear regression model right censored data hint use results exercise see figure example based data schmee hahn notice line tilted upwards since model takes account fact truncated values actually higher observed values closely related model econometrics called tobit model max get observe positive outcomes example represents desired investment actual investment probit regression section another example chapter mixture models algorithm exercise posterior mean variance truncated gaussian let sometimes probit regression censored regression observe observe fact threshold namely observe event see exercise details censored regression section probit regression show deﬁned pdf standard gaussian cdf hint event interest hint shown hence 
[latent, linear, models, factor, analysis] one problem mixture models use single latent variable generate observations particular observation come one prototypes one think mixture model using hidden binary variables representing one hot encoding cluster identity variables mutually exclusive model still limited representational power alternative use vector real valued latent variables simplest prior use gaussian consider choices later observations also continuous may use gaussian likelihood linear regression assume mean linear function hidden inputs thus yielding matrix known factor loading matrix covariance matrix take diagonal since whole point model force explain correlation rather baking observation covariance overall model called factor analysis special case called probabilistic principal components analysis ppca reason name become apparent later generative process diagonal illustrated figure take isotropic gaussian spray slide along line deﬁned induces ellongated hence correlated gaussian 
[latent, linear, models, factor, analysis, low, rank, parameterization, mvn] thought way specifying joint density model using small number parameters see note equation induced marginal distribution gaussian chapter latent linear models x_z_ figure illustration ppca generative process latent dimension generating observed dimensions based figure bishop see set without loss generality since always absorb similarly set without loss generality always emulate correlated prior using deﬁning new weight matrix ﬁnd cov thus see approximates covariance matrix visible vector using low rank decomposition cov uses parameters allows ﬂexible compromise full covariance gaussian parameters diagonal covariance parameters note restrict diagonal could trivially set full covariance matrix could set case latent factors would required 
[latent, linear, models, factor, analysis, inference, latent, factors] although thought way deﬁne density often used hope latent factors reveal something interesting data need compute posterior latent factors use bayes rule gaussians give note model actually independent denote computing matrix takes time computing takes time sometimes called latent scores latent factors factor analysis retail dealer engine cylinders horsepower citympg highwaympg weight wheelbase length width component component rotation none porsche honda insight gmc yukon slt mercedes benz kia sorento mercedes benz saturn ion nissan pathfinder armada figure projection cars data based factor analysis blue text names cars corresponding certain chosen points figure generated fabiplotdemo let give simple example based shalizi consider dataset variables cases describing various aspects cars engine size number cylinders miles per gallon mpg price etc ﬁrst dimensional model plot scores points visualize data shown figure get better understanding meaning latent factors project unit vectors corresponding feature dimensions etc low dimensional space shown blue lines figure known biplot see horizontal axis represents price corresponding features labeled dealer retail expensive cars right vertical axis represents fuel efficiency measured terms mpg versus size heavy vehicles less efficient higher whereas light vehicles efficient lower verify interpretation clicking points ﬁnding closest exemplars training set printing names figure however general interpreting latent variable models fraught difficulties discuss section 
[latent, linear, models, factor, analysis, unidentiﬁability] like mixture models also unidentiﬁable see suppose arbitrary orthogonal rotation matrix satisfying let deﬁne likelihood chapter latent linear models function modiﬁed matrix unmodiﬁed matrix since cov wrr geometrically multiplying orthogonal matrix like rotating generating since drawn isotropic gaussian makes difference likelihood consequently cannot unique identify therefore cannot uniquely identify latent factors either ensure unique solution need remove degrees freedom since number orthonormal matrices size total model free parameters excluding mean ﬁrst term arises obviously require less equal number parameters unconstrained symmetric covariance matrix gives upper bound follows max example implies usually never choose upper bound since would result overﬁtting see discussion section choose unfortunately even set max still cannot uniquely identify parameters since rotational ambiguity still exists non identiﬁability affect predictive per formance model however affect loading matrix hence interpretation latent factors since factor analysis often used uncover structure data problem needs addressed commonly used solutions forcing orthonormal perhaps cleanest solution identiﬁability problem force orthonormal order columns decreasing variance corresponding latent factors approach adopted pca discuss section result necessarily interpretable least unique forcing lower triangular one way achieve identiﬁability popular bayesian community lopes west ensure ﬁrst visible feature generated ﬁrst latent factor second visible feature generated ﬁrst two latent factors example correspond factor loading matrix given also require total number parameters constrained matrix equal number uniquely identiﬁable parameters disadvantage method ﬁrst visible variables see note free parameters ﬁrst column since column vector must normalized unit length free parameters second column must orthogonal ﬁrst factor analysis figure mixture factor analysers dgm known founder variables affect interpretation latent factors must chosen carefully sparsity promoting priors weights instead pre specifying entries zero encourage entries zero using regularization zou ard bishop archambeau bach spike slab priors rattray called sparse factor analysis necessarily ensure unique map estimate encourage interpretable solutions see section choosing informative rotation matrix variety heuristic methods try ﬁnd rotation matrices used modify hence latent factors try increase interpretability typically encouraging approximately sparse one popular method known varimax kaiser use non gaussian priors latent factors section dicuss placing non gaussian distribution enable sometimes uniquely identify well latent factors technique known ica 
[latent, linear, models, factor, analysis, mixtures, factor, analysers] model assumes data lives low dimensional linear manifold reality data better modeled form low dimensional curved manifold approximate curved manifold piecewise linear manifold suggests following model let linear subspace dimensionality represented suppose latent indicator specifying subspace use generate data sample gaussian prior pass matrix add noise precisely model follows cat chapter latent linear models figure mixture ppcas dataset figure generated mixppcademonetlab called mixture factor analysers mfa hinton assumptions represented figure another way think model low rank version mixture gaussians particular model needs kld parameters instead parameters needed mixture full covariance gaussians reduce overﬁtting fact mfa good generic density model high dimensional real valued data 
[latent, linear, models, factor, analysis, factor, analysis, models] using results chapter straightforward derive algorithm model little work mixture fas state results without proof derivation found ghahramani hinton however deriving equations useful exercise want become proﬁcient math obtain results single factor analyser set equations section see simpliﬁcation equations arises ﬁtting ppca model results turn particularly simple elegant intepretation step compute posterior responsibility cluster data point using conditional posterior given step easiest estimate time deﬁning principal components analysis pca also deﬁne step follows diag note updates vanilla much faster version algorithm based ecm described zhao 
[latent, linear, models, factor, analysis, fitting, models, missing, data] many applications collaborative ﬁltering missing data one virtue approach ﬁtting ppca model easy extend case however overﬁtting problem lot missing data consequently important perform map estimation use bayesian inference see ilin raiko details 
[latent, linear, models, principal, components, analysis, pca] consider model constrain orthonormal shown tipping bishop model reduces classical non probabilistic principal components analysis pca also known karhunen loeve transform version known probabilistic pca ppca tipping bishop sensible pca roweis equivalent result derived independently different perspective moghaddam pentland make sense result ﬁrst learn classical pca connect pca svd ﬁnally return discuss ppca 
[latent, linear, models, principal, components, analysis, pca, classical, pca, statement, theorem] synthesis view classical pca summarized forllowing theorem theorem suppose want ﬁnd orthogonal set linear basis vectors corresponding scores minimize average reconstruction error chapter latent linear models figure illustration pca ppca circles original data points crosses reconstructions red star data mean pca points orthogonally projected onto line figure generated pcademod ppca projection longer orthogonal reconstructions shrunk towards data mean red star based figure nabney figure generated ppcademod subject constraint orthonormal equivalently write objective follows matrix rows frobenius norm matrix deﬁned optimal solution obtained setting contains eigenvectors largest eigenvalues empirical covariance matrix assume zero mean notational simplicity furthermore optimal low dimensional encoding data given orthogonal projection data onto column space spanned eigenvectors example shown figure diagonal line vector called ﬁrst principal component principal direction data points orthogonally projected onto line get best dimensional approximation data discuss figure later general hard visualize higher dimensional data data happens set images easy figure shows ﬁrst three principal vectors reshaped images well reconstruction speciﬁc image using varying number basis vectors discuss choose section show principal directions ones along data shows maximal variance means pca misled directions variance high merely measurement scale figure shows example vertical axis weight uses large range horizontal axis height resulting line looks somewhat unnatural therefore standard practice standardize data ﬁrst principal components analysis pca mean principal basis principal basis principal basis reconstructed bases reconstructed bases reconstructed bases reconstructed bases figure mean ﬁrst three basis vectors eigendigits based images digit mnist dataset reconstruction image based basis vectors figure generated pcaimagedemo height weight height weight figure effect standardization pca applied height weight dataset left pca raw data right pca standardized data figure generated pcademoheightweight equivalently work correlation matrices instead covariance matrices beneﬁts apparent figure 
[latent, linear, models, principal, components, analysis, pca, proof] proof use denote principal direction denote high dimensional observation denote low dimensional representation denote component low dimensional vectors let start estimating best solution corresponding projected points ﬁnd remaining bases etc later reconstruction error chapter latent linear models given since orthonormality assumption taking derivatives wrt equating zero gives optimal reconstruction weights obtained orthogonally projecting data onto ﬁrst principal direction see figure plugging back gives const variance projected coordinates given var since data centered see minimizing reconstruction error equivalent maximizing variance projected data arg min arg max var often said pca ﬁnds directions maximal variance called analysis view pca variance projected data written empirical covariance matrix correlation matrix data standardized principal components analysis pca trivially maximize variance projection hence minimize recon struction error letting impose constraint instead maximize lagrange multiplier taking derivatives equating zero hence direction maximizes variance eigenvector covariance matrix left multiplying using ﬁnd variance projected data since want maximize variance pick eigenvector corresponds largest eigenvalue let ﬁnd another direction minimize reconstruction error subject error optimizing wrt gives solution exercise asks show yields words second principal encoding gotten projecting onto second principal direction substituting yields const dropping constant term adding constraints yields exercise asks show solution given eigenvector second largest eigenvalue proof continues way formally one use induction chapter latent linear models 
[latent, linear, models, principal, components, analysis, pca, singular, value, decomposition, svd] deﬁned solution pca terms eigenvectors covariance matrix however another way obtain solution based singular value decomposition svd basically generalizes notion eigenvectors square matrices kind matrix particular real matrix decomposed follows matrix whose columns orthornormal matrix whose rows columns orthonormal matrix containing min singular values main diagonal ﬁlling rest matrix columns left singular vectors columns right singular vectors see figure example since singular values assuming last columns irrelevant since multiplied economy sized svd thin svd avoids computing unnecessary elements let denote decomposition uˆs figure computing economy sized svd takes min time golub van loan connection eigenvectors singular vectors following arbitrary real matrix usv usv vdv diagonal matrix containing squares singular values hence eigenvectors equal right singular vectors eigenvalues equal squared singular values similarly usv eigenvectors equal left singular vectors also eigenvalues equal squared singular values summarize follows evec evec eval eval principal components analysis pca figure svd decomposition non square matrices usv shaded parts diagonal terms zero shaded entries computed economy sized version since needed truncated svd approximation rank since eigenvectors unaffected linear scaling matrix see right singular vectors equal eigenvectors empirical covariance furthermore eigenvalues scaled version squared singular values means perform pca using lines code see pcapmtk however connection pca svd goes deeper equation represent rank matrix follows singular values die quickly figure produce rank approximation matrix follows called truncated svd see figure total number parameters needed represent matrix using rank approximation chapter latent linear models rank rank rank rank figure low rank approximations image top left original image size rank subsequent images ranks figure generated svdimagedemo log original randomized figure first log singular values clown image solid red line data matrix obtained randomly shuffling pixels dotted green line figure generated svdimagedemo principal components analysis pca example consider pixel image figure top left numbers see rank approximation numbers good approximation one show error approximation given furthermore one show svd offers best rank approximation matrix best sense minimizing frobenius norm let connect back pca let usv truncated svd know usv furthermore optimal reconstruction given ﬁnd usv precisely truncated svd approximation another illustration fact pca best low rank approximation data 
[latent, linear, models, principal, components, analysis, pca, probabilistic, pca] ready revisit ppca one show following remarkable result theorem tipping bishop consider factor analysis model orthogonal observed data log likelihood given log assuming centered data notational simplicity maxima log likelihood given arbitrary orthogonal matrix matrix whose columns ﬁrst eigenvectors corresponding diagonal matrix eigenvalues without loss generality set furthermore mle noise variance given average variance associated discarded dimensions thus classical pca easy see posterior latent factors given chapter latent linear models confuse hence ﬁnd thus posterior mean obtained orthogonal projection data onto column space classical pca note however posterior mean orthogonal projection since shrunk somewhat towards prior mean illustrated figure sounds like undesirable property means reconstructions closer overall data mean 
[latent, linear, models, principal, components, analysis, pca, algorithm, pca] although usual way pca model uses eigenvector methods svd also use turn advantages discuss pca relies probabilistic formulation pca however algorithm continues work zero noise limit shown roweis let matrix storing posterior means low dimensional representations along columns similarly let store original data along columns equation constitutes step notice orthogonal projection data equation step given exploited fact cov worth comparing expression mle multi output linear regression equation form thus see step like linear regression replace observed inputs expected values latent variables summary entire algorithm step step tipping bishop showed stable ﬁxed point algorithm globally optimal solution algorithm converges solution spans linear subspace deﬁned ﬁrst eigenvectors however want orthogonal contain eigenvectors descending order eigenvalue orthogonalize resulting matrix done quite cheaply alternatively modify give principal basis directly ahn algorithm simple physical analogy case roweis consider points attached springs rigid rod whose orientation deﬁned vector let location spring attaches rod step hold rod ﬁxed let attachment points slide around minimize spring energy proportional sum squared residuals step hold attachment principal components analysis pca step step step step figure illustration pca green stars original data points black circles reconstructions weight vector represented blue line start random initial guess step represented orthogonal projections update rod step keeping projections onto rod black circles ﬁxed another step black circles slide along rod rod stays ﬁxed another step based figure bishop figure generated pcaemstepbystep points ﬁxed let rod rotate minimize spring energy see figure illustration apart pleasing intuitive interpretation pca following advantages eigenvector methods faster particular assuming dominant cost pro jection operation step overall time number chapter latent linear models figure illustration estimating effective dimensionalities mixture factor analysers using vbem blank columns forced via ard mechanism data generated clusters intrinsic dimensionalities method successfully estimated source figure beal used kind permission matt beal iterations roweis showed experimentally number iterations usually small mean regardless results depends ratio eigenval ues empirical covariance matrix much faster min time required straightforward eigenvector methods although sophisticated eigenvec tor methods lanczos algorithm running times comparable implemented online fashion update estimate data streams handle missing data simple way see section extended handle mixtures ppca models modiﬁed variational variational bayes complex models 
[latent, linear, models, choosing, number, latent, dimensions] section discussed choose number components mixture model section discuss choose number latent dimensions pca model 
[latent, linear, models, choosing, number, latent, dimensions, model, selection, fa/ppca] use probabilistic model principle compute argmax however two problems first evaluating marginal likelihood lvms quite difficult practice simple approximations bic variational lower bounds see section used see also minka alternatively use cross validated likelihood performance measure although slow since requires ﬁtting model times number folds second issue need search potentially large number models usual approach perform exhaustive search candidate values however sometimes set model maximal size use technique called automatic relevancy determination section combined automatically prune irrelevant weights choosing number latent dimensions number points per cluster intrinsic dimensionalities figure show estimated number clusters estimated dimensionalities function sample size vbem algorithm found two different solutions note clusters larger effective dimensionalities discovered sample sizes increases source table beal used kind permission matt beal technique described supervised context chapter adapted context shown bishop ghahramani beal figure illustrates approach applied mixture fas small synthetic dataset ﬁgures visualize weight matrices cluster using hinton diagrams size square proportional value entry matrix see many sparse figure shows degree sparsity depends amount training data accord bayesian occam razor particular sample size small method automatically prefers simpler models sample size gets sufficiently large method converges correct solution one subspaces dimensionality although ard method elegant still needs perform search done using birth death moves ghahramani beal alternative approach perform stochastic sampling space models traditional approaches lopes west based reversible jump mcmc also use birth death moves however slow difficult implement recent approaches use non parametric priors combined gibbs sampling see paisley carin 
[latent, linear, models, choosing, number, latent, dimensions, model, selection, pca] since pca probabilistic model cannot use methods described obvious proxy likelihood reconstruction error case pca reconstruction given estimated train geoff hinton english professor computer science university toronto chapter latent linear models rmse num pcs train set reconstruction error rmse num pcs test set reconstruction error figure reconstruction error mnist number latent dimensions used pca training set test set figure generated pcaoverfitdemo figure plots train mnist training data figure see drops quite quickly indicating capture empirical correlation pixels small number factors illustrated qualitatively figure exercise asks prove residual error using terms given sum discarded eigenvalues train therefore alternative plotting error plot retained eigenvalues decreasing order called scree plot plot looks like side mountain scree refers debris fallen mountain lying base shape residual error plot related quantity fraction variance explained deﬁned train max captures information scree plot course use rank get zero reconstruction error training set avoid overﬁtting natural plot reconstruction error test set shown figure see error continues even model becomes complex thus get usual shaped curve typically expect see going problem pca proper generative model data merely compression technique give latent dimensions able approximate test data accurately contrast probabilistic model enjoys bayesian occam razor effect section gets punished wastes probability mass parts space little data illustrated figure plots quotation http janda org workshop factoranalysis spssrun spss htm choosing number latent dimensions negloglik num pcs train set negative loglik negloglik num pcs test set negative loglik figure negative log likelihood mnist number latent dimensions used ppca training set test set figure generated pcaoverfitdemo negative log likelihood computed using ppca test set see usual shaped curve results analogous section discussed issue choosing means algorithm using gmm proﬁle likelihood although shape sometimes regime change plots relatively large errors relatively small one way automate detection described zhu ghodsi idea let measure error incurred model size max pca eigenvalues method also applied means consider partitioning values two groups depending whether threshold determine measure quality use simple change point model important models prevent overﬁtting case one regime less data within two regimes assume iid obviously incorrect adequate present purposes model max partitioning data computing mles using pooled estimate variance kgt kgt evaluate proﬁle log likelihood log log finally choose arg max illustrated figure left plot scree plot shape figure right plot proﬁle chapter latent linear models num pcs eigenvalue scree plot num pcs profile log likelihood figure scree plot training set corresponding figure proﬁle likelihood figure generated pcaoverfitdemo likelihood rather miraculously see fairly well determined peak 
[latent, linear, models, pca, categorical, data] section consider extending factor analysis model case observed data categorical rather real valued data form number observed response variables assume generated latent variable gaussian prior passed softmax function follows cat factor loading matrix response offset term response need explicit offset term since clamping one element cause problems computing posterior covariance factor analysis deﬁned prior mean prior covariance since capture non zero mean changing non identity covariance changing call categorical pca see chapter discussion related models interesting study kinds distributions induce observed variables varying parameters simplicity assume single ternary response variable lives probability simplex figure shows happens vary parameters prior equivalent varying parameters likelihood see deﬁne fairly complex distributions simplex induced distribution known logistic normal distribution aitchison model data using modiﬁed version basic idea infer gaussian approximation posterior step maximize step details multiclass case found khan see pca categorical data figure examples logistic normal distribution deﬁned simplex diagonal covariance non zero mean negative correlation states positive correlation states source figure blei lafferty used kind permission david blei figure left synthetic dimensional bit vectors right embedding learned binary pca using variational color coded points identity true prototype generated figure generated binaryfademotipping also section details binary case sigmoid link found exercise probit link exercise one application model visualize high dimensional categorical data fig ure shows simple example dimensional bit vectors clear sample noisy copy one three binary prototypes catfa model yielding approximate mles figure plot see three distinct clusters expected khan show model outperforms ﬁnite mixture models task imputing missing entries design matrices consisting real categorical data useful analysing social science survey data often missing data variables mixed type chapter latent linear models figure gaussian latent factor models paired data supervised pca partial least squares canonical correlation analysis 
[latent, linear, models, pca, paired, multi-view, data] common pair related datasets gene expression gene copy number movie ratings users movie reviews natural want combine together low dimensional embedding example data fusion cases might want predict one element pair say one via low dimensional bottleneck discuss various latent gaussian models tasks following presentation virtanen models easily generalize pairs sets data focus case case joint distribution multivariate gaussian easily models using gibbs sampling generalize models handle discrete count data using exponential family response distribution instead gaussian explain section however require use approximate inference step analogous modiﬁcation mcmc pca paired multi view data 
[latent, linear, models, pca, paired, multi-view, data, supervised, pca, latent, factor, regression] consider following model illustrated figure called supervised pca west called bayesian factor regression model like pca except target variable taken account learning low dimensional embedding since model jointly gaussian although joint density model infer implied conditional distribution show interesting connection zellner prior suppose let svd diag contains squared singular values one show west dependence prior arises fact derived indirectly joint model discussion focussed regression guo generalizes cca exponential family appropriate discrete although longer compute conditional closed form model similar interpretation regression case namely predicting response via latent bottleneck basic idea compressing predict formulated using information theory particular might want ﬁnd encoding distribution minimize parameter controlling tradeoff compression predictive accuracy known information bottleneck tishby often taken discrete clustering however gaussian case closely related cca chechik easily generalize cca case vector responses predicted multi label classiﬁcation williamson ghahramani used model perform collaborative ﬁltering goal predict rating person gives movie side information takes form list friends intuition behind approach knowledge friends well ratings users help predict movies like general setting tasks correlated could beneﬁt cca adopt probabilistic view various extensions straightforward example easily generalize semi supervised case observe chapter latent linear models discriminative supervised pca one problem model puts much weight predicting inputs outputs partially alleviated using weighted objective following form rish control relative importance data sources gaussian data see controls noise variance exp exp interpretation holds generally exponential family note however hard estimate parameters changing changes normalization constant likelihood give alternative approach weighting heavily 
[latent, linear, models, pca, paired, multi-view, data, partial, least, squares] technique partial least squares pls gustafsson sun asym metric discriminative form supervised pca key idea allow variance input features explained subspace let rest subspace shared input output model form see figure corresponding induced distribution visible variables form choose large enough shared subspace capture covariate speciﬁc variation model easily generalized discrete data using exponential family virtanen independent component analysis ica 
[latent, linear, models, pca, paired, multi-view, data, canonical, correlation, analysis] canonical correlation analysis cca like symmetric unsupervised version pls allows view private subspace also shared subspace two observed variables three latent variables shared private write model follows bach jordan see figure corresponding observed joint distribution form one compute mle model using bach jordan show resulting mle equivalent rotation scaling classical non probabilistic view however advantages probabilistic view many trivially generalize observed variables create mixtures cca viinikanoja create sparse versions cca using ard archambeau bach generalize exponential family klami perform bayesian inference parameters wang klami kaski handle non parametric sparsity promoting priors rai daume 
[latent, linear, models, independent, component, analysis, ica] consider following situation crowded room many people speaking ears essentially act two microphones listening linear combination different speech signals room goal deconvolve mixed signals constituent parts known cocktail party problem example blind signal separation bss blind source separation blind means know nothing source signals besides obvious applications acoustic signal processing problem also arises analysing eeg meg signals ﬁnancial data dataset necessarily temporal latent sources factors get mixed together linear way formalize problem follows let observed signal sensors time vector source signals assume chapter latent linear models truth observed signals pca estimate ica estimate figure illustration ica applied iid samples source signal latent signals observations pca estimate ica estimate figure generated icademo written aapo hyvarinen matrix section treat time point independent observation model temporal correlation could replace index stick consistent much ica literature goal infer source signals illustrated figure context called mixing matrix number sources number sensors square matrix often assume noise level zero simplicity far model identical factor analysis pca noise except general require orthogonality however use different prior pca assume source independent gaussian distribution relax gaussian assumption let source distributions non gaussian independent component analysis ica uniform data uniform data linear mixing pca applied mixed data uniform source ica applied mixed data uniform source figure illustration ica pca applied iid samples source signal uniform distribution latent signals observations pca estimate ica estimate figure generated icademouniform written aapo hyvarinen distribution without loss generality constrain variance source distributions variance modelled scaling rows appropriately resulting model known independent component analysis ica reason gaussian distribution disallowed source prior ica permit unique recovery sources illustrated figure pca likelihood invariant orthogonal transformation sources mixing matrix pca recover best linear subspace signals lie cannot uniquely recover signals chapter latent linear models illustrate suppose two independent sources uniform distributions shown figure suppose following mixing matrix observe data shown figure assuming noise apply pca followed scaling get result figure corresponds whitening data uniquely recover sources need perform additional rotation trouble information symmetric gaussian posterior tell angle rotate sense pca solves half problem since identiﬁes linear subspace ica identify appropriate rotation hence see ica different methods varimax seek good rotations latent factors enhance interpretability figure shows ica recover source permutation indices possible sign change ica requires square hence invertible non square case sources sensors cannot uniquely recover true signal compute posterior represents beliefs source cases need estimate well source distributions discuss 
[latent, linear, models, independent, component, analysis, ica, maximum, likelihood, estimation] section discuss ways estimate square mixing matrices noise free ica model usual assume observations centered hence also assume zero mean addition assume observations whitened done pca data centered whitened noise free case also cov hence see must orthogonal reduces number parameters estimate also simplify math algorithms let often called recognition weights opposed generative weights since equation det det hence write log likelihood assuming iid samples follows log log det log literature common denote generative weights recognition weights trying consistent notation used earlier chapter independent component analysis ica row since constraining orthogonal ﬁrst term constant drop also replace average data expectation operator get following objective nll log want minimize subject constraint rows orthogonal also want unit norm since ensures variance factors unity since whitened data necessary scale weights otherwords orthonormal matrix straightforward derive gradient descent algorithm model however rather slow one also derive faster algorithm follows natural gradient see mackay details popular alternative use approximate newton method discuss section another approach use discuss section 
[latent, linear, models, independent, component, analysis, ica, fastica, algorithm] describe fast ica algorithm based hyvarinen oja show approximate newton method ﬁtting ica models simplicity presentation initially assume one latent factor addition initially assume source distributions known write log let constrained objective gradient hessian given lagrange multiplier let make approximation makes hessian easy invert giving rise following newton update one rewrite following way practice expectations replaced monte carlo estimates training set gives efficient online learning algorithm performing update one project back onto constraint surface using new chapter latent linear models gaussian laplace uniform figure illustration gaussian sub gaussian uniform super gaussian laplace distributions figure generated subsupergaussplot written kevin swersky one iterates algorithm convergence due sign ambiguity values may converge direction deﬁned vector converge one assess convergence monitoring new approach since objective convex multiple local optima use fact learn multiple different weight vectors features either learn features sequentially project part lies subspace deﬁned earlier features learn parallel orthogonalize parallel latter approach usually preferred since unlike pca features ordered way ﬁrst feature important second hence better treat symmetrically independent component analysis ica modeling source densities far assumed log known kinds models might reasonable signal priors know using gaussians correspond quadratic functions work want kind non gaussian distribution general several kinds non gaussian distributions following super gaussian distributions distributions big spike mean hence order ensure unit variance heavy tails laplace distribution classic example see figure formally say distribution super gaussian leptokurtic lepto coming greek thin kurt kurt kurtosis distribution deﬁned kurt standard deviation central moment moment mean mean variance conventional subtract deﬁnition kurtosis make kurtosis gaussian variable equal zero sub gaussian distributions sub gaussian platykurtic platy coming greek broad distribution negative kurtosis distributions much ﬂatter gaussian uniform distribution classic example see figure skewed distributions another way non gaussian asymmetric one measure skewness deﬁned skew example right skewed distribution gamma distribution see figure one looks empirical distribution many natural signals images speech passed certain linear ﬁlters tend super gaussian result holds kind linear ﬁlters found certain parts brain simple cells found primary visual cortex well kinds linear ﬁlters used signal processing wavelet transforms one obvious choice modeling natural signals ica therefore laplace distribution mean zero variance log pdf given log log since laplace prior differentiable origin common use smoother super gaussian distributions one example logistic distribution corre sponding log pdf case mean zero variance given following log log cosh log chapter latent linear models figure modeling source distributions using mixture univariate gaussians independent factor analysis model moulines attias various ways estimating log discussed seminal paper pham garrat however ﬁtting ica maximum likelihood critical exact shape source distribution known although important know whether sub super gaussian consequently common use log cosh instead complex expressions 
[latent, linear, models, independent, component, analysis, ica, using] alternative assuming particular form equivalently use ﬂexible non parametric density estimator mixture uni variate gaussians approach proposed moulines attias corresponding graph ical model shown figure possible derive exact algorithm model key observation possible compute exactly summing combinations variables number mixture components per source expensive one use variational mean ﬁeld approximation attias estimate source distributions parallel ﬁtting standard gmm source gmms independent component analysis ica known compute marginals easily using given use ica algorithm estimate course steps interleaved details found attias 
[latent, linear, models, independent, component, analysis, ica, estimation, principles] quite common estimate parameters ica models using methods seem different maximum likelihood review methods give additional insight ica however also see methods fact equivalent maximum likelihood presentation based hyvarinen oja maximizing non gaussianity early approach ica ﬁnd matrix distribution far gaussian possible related approach statistics called projection pursuit one measure non gaussianity kurtosis sensitive outliers another measure negentropy deﬁned negentropy var since gaussian maximum entropy distribution measure always non negative becomes large distributions highly non gaussian deﬁne objective maximizing negentropy orthogonal whiten data covariance independently ﬁrst term constant hence const log const see equal sign change irrelevant constants log likelihood equation minimizing mutual information one measure dependence set random variables multi information chapter latent linear models would like minimize since trying ﬁnd independent components put another way want best possible factored approximation joint distribution since constrain orthogonal drop last term since since multiplying change shape distribution constant solely determined empirical distribution hence minimizing equivalent maximizing negentropy equivalent maximum likelihood maximizing mutual information infomax instead trying minimize mutual information components let imagine neural network input noisy output nonlinear scalar function seems reasonable try maximize information ﬂow system principle known infomax bell sejnowski want maximize mutual information internal neural representation observed input signal latter term constant assume noise constant variance one show approximate former term follows log log det usual drop last term orthogonal deﬁne cdf pdf expression equivalent log likelihood particular use logistic nonlinearity sigm corresponding pdf logistic distribution log log cosh ignoring irrelevant constants thus see infomax equivalent maximum likelihood 
[latent, linear, models, exercises] exercise step model show mle step given equation exercise map estimation model derive step model using conjugate priors parameters exercise heuristic assessing applicability pca source press let empirical covariance matrix eigenvalues explain variance evalues good measure whether pca would useful analysing data higher value useful pca independent component analysis ica exercise deriving second principal component let show yields show value minimizes given eigenvector second largest eigenvalue hint recall exercise deriving residual error pca prove hint ﬁrst consider case use fact also recall show amp hint recall truncation use show error using terms given hint partition sum exercise derivation fisher linear discriminant show maximum given hint recall derivative ratio two scalars given also recall exercise pca via successive deﬂation let ﬁrst eigenvectors largest eigenvalues principal basis vectors satisfy construct method ﬁnding sequentially chapter latent linear models showed class ﬁrst principal eigenvector satisﬁes deﬁne orthogonal projection onto space orthogonal deﬁne deﬂated matrix rank obtained removing dimensional data component lies direction ﬁrst principal direction using facts hence show covariance deﬂated matrix given let principal eigenvector explain may assume unit norm suppose simple method ﬁnding leading eigenvector eigenvalue matrix denoted write pseudo code ﬁnding ﬁrst principal basis vectors uses special function simple vector arithmetic code use svd eig function hint simple iterative routine takes lines write input function output worry syntactically correct exercise latent semantic indexing source freitas exercise study technique called latent semantic indexing applies svd document term matrix create low dimensional embedding data designed capture semantic similarity words ﬁle lsidocuments pdf contains documents various topics list unique words terms occur documents lsiwords txt document term matrix lsimatrix txt let transpose lsimatrix column represents document compute svd make approximation using ﬁrst singular values vectors plot low dimensional representation documents get something like figure consider ﬁnding documents alien abductions look lsiwords txt versions word term abducted term abduction term abductions suppose want ﬁnd documents containing word abducted documents contain document however document clearly related topic thus lsi also ﬁnd document create test document containing one word abducted project subspace make compute cosine similarity low dimensional representation documents top closest matches exercise imputation model derive expression model exercise efficiently evaluating ppca density derive expression ppca model based plugging mles using matrix inversion lemma independent component analysis ica figure projection documents dimensions figure generated lsicode exercise ppca source exercise hastie due hinton generate observations following model fit pca model latent factor hence show corresponding weight vector aligns maximal variance direction dimension pca case maximal correlation direction dimensions case 
[sparse, linear, models, introduction] introduced topic feature selection section discussed methods ﬁnding input variables high mutual information output trouble approach based myopic strategy looks one variable time fail interaction effects example xor neither predict response together perfectly predict response real world example consider genetic association studies sometimes two genes may harmless present together cause recessive disease balding chapter focus selecting sets variables time using model based approach model generalized linear model form link function perform feature selection encouraging weight vector sparse lots zeros approach turns offer signiﬁcant computational advantages see applications feature selection sparsity useful many problems many dimensions training cases cor responding design matrix short fat rather tall skinny called small large problem becoming increasingly prevalent develop high throughput measurement devices example gene microarrays common measure expression levels genes get examples perhaps sign times even data seems getting fatter may want ﬁnd smallest set features accurately predict response growth rate cell order prevent overﬁtting reduce cost building diagnostic device help scientiﬁc insight problem chapter use basis functions centered training examples kernel function resulting design matrix size feature selection context equivalent selecting subset training examples help reduce overﬁtting computational cost known sparse kernel machine signal processing common represent signals images speech etc terms wavelet basis functions save time space useful ﬁnd sparse representation chapter sparse linear models signals terms small number basis functions allows estimate signals small number measurements well compress signal see section information note topic feature selection sparsity currently one active areas machine learning statistics chapter space give overview main results 
[sparse, linear, models, bayesian, variable, selection, spike, slab, model] posterior given ﬁrst consider prior likelihood common use following prior bit vector ber probability feature relevant pseudo norm number non zero elements vector comparison later models useful write log prior follows log log log log log const const log controls sparsity model write likelihood follows notational simplicity assumed response centered ignore offset term discuss prior feature irrelevant expect expect non zero standardize inputs reasonable prior controls big expect coefficients associated relevant variables scaled overall noise level summarize prior follows ﬁrst term spike origin distribution approaches uniform distribution thought slab constant height hence called spike slab model mitchell beauchamp drop coefficients model since clamped zero prior hence equation becomes following assuming gaussian likelihood bayesian variable selection number non zero elements follows generalize slightly deﬁning prior form positive deﬁnite matrix given priors compute marginal likelihood noise variance known write marginal likelihood using equation follows noise unknown put prior integrate common use guidelines setting found kohn use recover jeffrey prior integrate noise get following complicated expression marginal likelihood brown rss see also exercise marginal likelihood cannot computed closed form using logistic regression nonlinear model approximate using bic form log log log map estimate based degrees freedom model zou adding log prior overall objective becomes log log log const see two complexity penalties one arising bic approximation marginal likelihood arising prior obviously combined one overall complexity parameter denote 
[sparse, linear, models, bayesian, variable, selection, regularization] another model sometimes used kuo mallick zhou soussen following ber common use prior form reasons explained section see also exercise various approaches proposed setting including cross validation empirical bayes minka george foster hierarchical bayes liang etc chapter sparse linear models signal processing literature soussen called bernoulli gaussian model although could also call binary mask model since think variables masking weights unlike spike slab model integrate irrelevant coefficients always exist addition binary mask model form whereas spike slab model form binary mask model product identiﬁed likelihood one interesting aspect model used derive objective function widely used non bayesian subset selection literature first note joint prior form hence scaled unnormalized negative log posterior form log const log let split two subvectors indexed zero non zero entries respectively since set consider case regularize non zero weights complexity penalty coming marginal likelihood bic approximation case objective becomes similar bic objective instead keeping track bit vector deﬁne set relevant variables support set non zero entries rewrite equation follows called regularization converted discrete optimization problem continuous one however pseudo norm makes objective non smooth still hard optimize discuss different solutions rest chapter 
[sparse, linear, models, bayesian, variable, selection, algorithms] since models cannot explore full posterior ﬁnd globally optimal model instead resort heuristics one form another methods discuss involve searching space models evaluating cost bayesian variable selection subset size training set error subsets prostate cancer figure lattice subsets residual sum squares versus subset size prostate cancer data set lower envelope best rss achievable set given size based figure hastie figure generated prostatesubsets point requires ﬁtting model computing argmax evaluating marginal likelihood computing step sometimes called wrapper method since wrap search best model set good models around generic model ﬁtting procedure order make wrapper methods efficient important quickly evaluate score function new model given score previous model done provided efficiently update sufficient statistics needed compute possible provided differs one bit corresponding adding removing single variable provided depends data via case use rank one matrix updates downdates efficiently compute updates usually applied decomposition see miller schniter details greedy search suppose want ﬁnd map model use regularized objective equation exploit properties least squares derive various efficient greedy forwards search methods summarize details see miller soussen single best replacement simplest method use greedy hill climbing step deﬁne neighborhood current model models reached ﬂipping single bit variable currently model consider adding currently model consider removing soussen call single best replacement sbr since expecting sparse solution start empty set essentially moving lattice subsets shown figure continue adding removing improvement possible orthogonal least squares set equation complexity penalty reason perform deletion steps case sbr algorithm equivalent orthogonal least squares chen wigger turn equivalent chapter sparse linear models greedy forwards selection algorithm start empty set add best feature step error monotonically shown figure pick next best feature add current set solving arg min min update active set setting choose next feature add step need solve least squares problems step cardinality current active set chosen best feature add need solve additional least squares problem compute orthogonal matching pursuits orthogonal least squares somewhat expensive simpli ﬁcation freeze current weights current value pick next feature add solving arg min min inner optimization easy solve simply set current residual vector columns unit norm arg max looking column correlated current residual update active set compute new least squares estimate using method called orthogonal matching pursuits omp mallat requires one least squares calculation per iteration faster orthogonal least squares quite accurate blumensath davies matching pursuits even aggressive approximation greedily add feature correlated current residual called matching pursuits mallat zhang also equivalent method known least squares boosting section backwards selection backwards selection starts variables model called saturated model deletes worst one step equivalent performing greedy search top lattice downwards give better results bottom search since decision whether keep variable made context variables might depende however method typically infeasible large problems since saturated model expensive foba forwards backwards algorithm zhang similar single best replacement algorithm presented except uses omp like approximation choosing next move make similar dual pass algorithm described moghad dam bayesian matching pursuit algorithm schniter similiar omp except uses bayesian marginal likelihood scoring criterion spike slab model instead least squares objective addition uses form beam search explore multiple paths lattice regularization basics stochastic search want approximate posterior rather computing mode want compute marginal inclusion probabilities one option use mcmc standard approach use metropolis hastings proposal distribution ﬂips single bits enables efficiently compute given probability state bit conﬁguration estimated counting many times random walk visits state see hara sillanpaa review methods bottolo richardson recent method based evolutionary mcmc however discrete state space mcmc needlessly inefficient since compute unnormalized probability state directly using exp thus need ever revisit state much efficient alternative use kind stochastic search algorithm generate set high scoring models make following approximation see heaton scott review recent methods kind variational inference tempting apply spike slab model form compute step optimize step however work compute comparing delta function gaussian pdf replace delta function narrow gaussian step amounts classifying two possible gaussian models however likely suffer severe local minima alternative apply bernoulli gaussian model form case posterior intractable compute bits become correlated due explaining away however possible derive mean ﬁeld approximation form huang rattray 
[sparse, linear, models, regularization, basics, regularization, yield, sparse, solutions] explain regularization results sparse solutions whereas regularization focus case linear regression although similar arguments hold logistic regression glms regularization basics objective following non smooth objective function min rss rewrite constrained smooth objective quadratic function linear constraints min rss upper bound norm weights small tight bound corresponds large penalty vice versa equation known lasso stands least absolute shrinkage selection operator tibshirani see name later similarly write ridge regression min rss bound constrained form min rss figure plot contours rss objective function well contours constraint surfaces theory constrained optimization know optimal solution occurs point lowest level set objective function intersects constraint surface assuming constraint active geometrically clear relax constraint grow ball meets objective corners ball likely intersect ellipse one sides especially high dimensions corners stick corners correspond sparse solutions lie coordinate axes contrast grow ball intersect objective point corners preference sparsity see another away notice ridge regression prior cost sparse solution cost dense solution long norm however lasso setting cheaper setting since rigorous way see regularization results sparse solutions examine conditions hold optimum section 
[sparse, linear, models, regularization, basics, optimality, conditions, lasso] lasso objective form rss equation example quadratic program since quadratic objective subject linear inequality constraints lagrangian given equation chapter sparse linear models figure illustration sub derivatives function point based ﬁgure http wikipedia org wiki subderivative figure generated subgradientplot unfortunately term differentiable whenever example non smooth optimization problem handle non smooth functions need extend notion derivative deﬁne subderivative subgradient convex function point scalar interval containing see figure illustration deﬁne set subderivatives interval one sided limits lim lim set subderivatives called subdifferential function denoted example case absolute value function subderivative given function everywhere differentiable analogy standard calculus result one show point local minimum iff general vector valued function say subgradient vectors linear lower bound function regularization basics figure left soft thresholding ﬂat region interval right hard thresholding let apply concepts lasso problem let initially ignore non smooth penalty term one show exercise rss without component similarly see proportional correlation feature residual due features hence magnitude indication relevant feature predicting relative features current parameters adding penalty term ﬁnd subderivative given write compact fashion follows depending value solution occur different values follows chapter sparse linear models feature strongly negatively correlated residual subgradient zero feature weakly correlated residual subgradient zero feature strongly positively correlated residual subgra dient zero summary write follows soft soft sign max positive part called soft thresholding illustrated figure plot dotted line line corresponding least squares solid line represents regularized estimate shifts dotted line except case sets contrast figure illustrate hard thresholding sets values shrink values outside interval slope soft thresholding line coincide diagonal means even large coefficients shrunk towards zero consequently lasso biased estimator undesirable since likelihood indicates via coefficient large want shrink discuss issue detail section ﬁnally understand tibshirani invented term lasso tibshirani stands least absolute selection shrinkage operator since selects subset variables shrinks coefficients penalizing absolute values get ols solution minimal norm max get max max value computed using fact optimal general maximum penalty regularized objective max max regularization basics 
[sparse, linear, models, regularization, basics, comparison, least, squares, lasso, ridge, subset, selection] gain insight regularization comparing least squares regularized least squares simplicity assume features orthonormal case rss given rss const see factorizes sum terms one per dimension hence write map estimates analytically follows mle ols solution given ols column follows trivially equation see ols orthogonal projection feature onto response vector see section ridge one show ridge estimate given ridge ols lasso equation using fact ols lasso sign ols ols corresponds soft thresholding shown figure subset selection pick best features using subset selection parameter estimate follows ols rank ols otherwise rank refers location sorted list weight magnitudes corresponds hard thresholding shown figure figure plots mse lasso degree polynomial figure plots mse polynomial order see lasso gives similar results subset selection method another example consider data set concerning prostate cancer features training cases goal predict log prostate speciﬁc antigen levels see hastie biological details table shows lasso gives better prediction accuracy least particular data set least squares ridge best subset regression case strength regularizer chosen cross validation lasso also gives rise sparse solution course problems ridge may give better predictive accuracy practice combination lasso ridge known elastic net often performs best since provides good combination sparsity regularization see section chapter sparse linear models lambda mse train test degree mse performance mle train test figure mse lasso degree polynomial note decreases move right figure generated linregpolylassodemo mse versus polynomial degree note model order increases move right see figure plot polynomial regression models figure generated linregpolyvsdegree term best subset ridge lasso intercept lcavol lweight age lbph svi lcp gleason pgg test error table results different methods prostate cancer data features training cases methods least squares subset best subset regression ridge lasso rows represent coefficients see subset regression lasso give sparse solutions bottom row mean squared error test set cases based table hastie figure generated prostatecomparison 
[sparse, linear, models, regularization, basics, regularization, path] increase solution vector tend get sparser although necessarily monotonically plot values feature known regularization path illustrated ridge regression figure plot regularizer decreases see coefficients zero ﬁnite value coefficients non zero furthermore increase magnitude decreased figure plot analogous result lasso move right upper bound penalty increases coefficients zero increase regularization basics lcavol lweight age lbph svi lcp gleason pgg lcavol lweight age lbph svi lcp gleason pgg figure proﬁles ridge coefficients prostate cancer example bound norm small large left vertical line value chosen fold using rule based figure hastie figure generated ridgepathprostate proﬁles lasso coefficients prostate cancer example bound norm small large left based figure hastie figure generated lassopathprostate lcavol lweight age lbph svi lcp gleason pgg lars step lcavol lweight age lbph svi lcp gleason pgg figure illustration piecewise linearity regularization path lasso prostate cancer example plot critical values plot steps lars algorithm figure generated lassopathprostate coefficients gradually turn value max ols solution sparse remarkably shown solution path piecewise linear function efron set critical values active set non zero coefficients changes values critical values non zero coefficient increases decreases linear fashion illustrated figure furthermore one solve critical values analytically basis lars algorithm efron stands least angle regression shrinkage see section details remarkably lars compute entire regularization path roughly common plot solution versus shrinkage factor deﬁned max rather merely affects scale horizontal axis shape curves chapter sparse linear models original number nonzeros reconstruction lambda mse debiased mse minimum norm solution mse figure example recovering sparse signal using lasso see text details based figure figueiredo figure generated sparsesensingdemo written mario figueiredo computational cost single least squares namely min figure plot coefficients computed critical value piecewise linearity evident display actual coefficient values step along regularization path last line least squares solution listing output lassopathprostate changing max solution weights zero solution weights non zero unfortunately subset sizes achievable using lasso one show optimal solution variables reaching complete set corresponding ols solution minimal norm section see using regularizer well regularizer method known elastic net achieve sparse solutions contain variables training cases lets explore model sizes regularization basics 
[sparse, linear, models, regularization, basics, model, selection] tempting use regularization estimate set relevant variables cases recover true sparsity pattern parameter vector generated data method recover true model limit called model selection consistent details methods enjoy property beyond scope book see buhlmann van geer details instead going theoretical discussion show small example ﬁrst generate sparse signal size consisting randomly placed spikes next generate random design matrix size finally generate noisy observation estimate original shown ﬁrst row figure second row estimate using max see spikes right places small third row least squares estimate coefficients estimated non zero based supp called debiasing necessary lasso shrinks relevant coefficients well irrelevant ones last row least squares estimate coefficients jointly ignoring sparsity see debiased sparse estimate excellent estimate original signal contrast least squares without sparsity assumption performs poorly course perform model selection pick common use cross validation however important note cross validation picking value results good predictive accuracy usually value one likely recover true model see recall regularization performs selection shrinkage chosen coefficients brought closer order prevent relevant coefficients shrunk way cross validation tend pick value large course result less sparse model contains irrelevant variables false positives indeed proved meinshausen buhlmann prediction optimal value result model selection consistency section discuss adaptive mechanisms automatically tuning per dimension basis result model selection consistency downside using regularization select variables give quite different results data perturbed slightly bayesian approach estimates posterior marginal inclusion probabilities much robust frequentist solution use bootstrap resampling see section rerun estimator different versions data computing often variable selected across different trials approximate posterior inclusion probabilities method known stability selection meinshausen bãijhlmann threshold stability selection bootstrap inclusion probabilities level say thus derive sparse estimator known bootstrap lasso bolasso bach include variable occurs least sets returned lasso ﬁxed process intersecting sets way eliminating false positives vanilla lasso produces theoretical results bach prove bolasso model selection consistent wider range conditions vanilla lasso illustration reproduced experiments bach particular created chapter sparse linear models log variable index log variable index log correct support lasso bolasso figure probability selection variable white large probabilities black small proba bilities regularization parameter lasso move left right decrease amount regularization therefore select variables bolasso probability correct sign estimation regularization parameter bolasso red dashed lasso black plain number bootstrap replications based figures bach figure generated bolassodemo datasets size variables relevant see bach detail experimental setup dataset variable sparsity level deﬁne deﬁne average datasets figure plot log lasso bolasso see bolasso large range true variables selected case lasso emphasized figure plot empirical probability correct set variables recovered lasso bolasso increasing number bootstrap samples course using samples takes longer practice bootstraps seems good compromise speed accuracy bolasso usual issue picking obviously could use cross validation plots figure suggest another heuristic shuffle rows create large black block pick middle region course operationalizing intuition may tricky require various hoc thresholds reminiscent ﬁnd knee curve heuristic discussed section discussing pick mixture models bayesian approach provides principled method selecting 
[sparse, linear, models, regularization, basics, bayesian, inference, linear, models, laplace, priors] focusing map estimation sparse linear models also possible perform bayesian inference see park casella seeger however posterior mean median well samples posterior sparse mode sparse another example phenomenon discussed section said map estimate often untypical bulk posterior another argument favor using posterior mean comes equation showed plugging posterior mean rather posterior mode optimal thing want minimize squared prediction error schniter shows experimentally elad yavnch shows theoretically using posterior mean spike slab prior results better prediction accuracy using posterior mode laplace prior albeit slightly higher computational cost regularization algorithms 
[sparse, linear, models, regularization, algorithms] section give brief review algorithms used solve regularized estimation problems focus lasso case quadratic loss however algorithms extended general settings logistic regression see yaun comprehensive review regularized logistic regression note area machine learning advancing rapidly methods may state art time read chapter see schmidt yaun yang recent surveys 
[sparse, linear, models, regularization, algorithms, coordinate, descent] sometimes hard optimize variables simultaneously easy optimize one one particular solve coefficient others held ﬁxed argmin unit vector either cycle coordinates deterministic fashion sample random choose update coordinate gradient steepest coordinate descent method particularly appealing one dimensional optimization problem solved analytically example shooting algorithm lange lasso uses equation compute optimal value given coefficients see algorithm pseudo code lassoshooting matlab code see yaun extensions method logistic regression case resulting algorithm fastest method experimental comparison concerned document classiﬁcation large sparse feature vectors representing bags words types data dense features regression problems might call different algorithms algorithm coordinate descent lasso aka shooting algorithm initialize repeat soft converged 
[sparse, linear, models, regularization, algorithms, lars, homotopy, methods] problem coordinate descent updates one variable time slow converge active set methods update many variables time unfortunately chapter sparse linear models complicated need identify variables constrained zero free updated active set methods typically add remove variables time take long started far solution ideally suited generating set solutions different values starting empty set generating regularization path algorithms exploit fact one quickly compute known warm starting fact even want solution single value call sometimes computationally efficient compute set solutions max using warm starting called continuation method homotopy method often much faster directly cold starting particularly true small perhaps well known example homotopy method machine learning lars algorithm stands least angle regression shrinkage efron similar algorithm independently invented osborne compute possible values efficient manner lars works follows starts large value variable correlated response vector chosen decreased second variable found correlation terms magnitude current residual ﬁrst variable residual step deﬁned current active set equation remarkably one solve new value analytically using geometric argument hence term least angle allows algorithm quickly jump next point regularization path active set changes repeats variables added necessary allow variables removed active set want sequence solutions correspond regularization path lasso disallow variable removal get slightly different algorithm called lar tends faster particular lar costs single ordinary least squares namely min lar similar greedy forward selection method known least squares boosting see section many attempts extend lars algorithm compute full regulariza tion path regularized glms logistic regression general one cannot analytically solve critical values instead standard approach start max slowly decrease tracking solution called continuation method homotopy method methods exploit fact quickly compute known warm starting even want full path method often much faster directly cold starting desired value particularly true small method described friedman combines coordinate descent warm starting strategy computes full regularization path regularized glm implemented glmnet package bundled pmtk 
[sparse, linear, models, regularization, algorithms, proximal, gradient, projection, methods] section consider methods suitable large scale problems homotopy methods made slow methods also easy extend kinds regularization algorithms regularizers beyond see later presentation section based vandenberghe yang consider convex objective form representing loss convex differentiable representing regularizer convex necessarily differentiable example rss corresponds bpdn problem another example lasso problem formulated follows rss indicator function convex set deﬁned otherwise cases easy optimize functions form equation example suppose rss design matrix simply obective becomes minimizer given prox proximal operator convex function deﬁned prox argmin intuitively returning point minimizes also close proximal general use operator inside iterative optimizer case want stay close previous iterate case use prox argmin key issues efficiently compute proximal operator different regu larizers extend technique general loss functions discuss issues proximal operators proximal operator given componentwise soft thresholding prox soft showed section proximal operator given componen twise hard thresholding prox hard hard proximal operator given projection onto set prox argmin proj chapter sparse linear models figure illustration projected gradient descent step along negative gradient takes outside feasible set project point onto closest point set get proj derive implicit update direction using used kind permission mark schmidt convex sets easy compute projection operator example project onto rectangular set deﬁned box constraints use proj project onto euclidean ball use proj project onto norm ball use proj soft otherwise solution equation max implement whole procedure time explained duchi see application different projection methods section proximal gradient method discuss use proximal operator inside gradient descent routine basic idea minimize simple quadratic approximation loss function centered regularization algorithms argmin gradient loss constant discussed last term arises simple approximation hessian loss form dropping terms independent multiplying rewrite expression terms proximal operator follows argmin prox equivalent gradient descent method equivalent projected gradient descent sketched figure method known iterative soft thresholding several ways pick equivalently given approxi mation hessian require least squares sense hence argmin known barzilai borwein spectral stepsize barzilai borwein fletcher raydan stepsize used gradient method whether proximal lead monotonic decrease objective much faster standard line search techniques ensure convergence require objective decrease average average computed sliding window size combine stepsize iterative soft thresholding technique plus continuation method gradually reduces get fast method bpdn problem known sparsa algorithm stands sparse reconstruction separable approximation wright however call iterative shrinkage thresholding algorithm see algorithm pseudocode sparsa matlab code see also exercise related approach based projected gradient descent nesterov method faster version proximal gradient descent obtained epxanding quadratic approximation around point recent parameter value particular consider performing updates form prox chapter sparse linear models algorithm iterative shrinkage thresholding algorithm ista input parameters initialize repeat max adapt regularizer repeat soft update using stepsize equation increased much within past steps update residual figure representing lasso using gaussian scale mixture prior known nesterov method nesterov tseng variety ways setting typically one uses line search method combined iterative soft thresholding technique plus continuation method gradually reduces get fast method bpdn problem known fast iterative shrinkage thesholding algorithm fista beck teboulle regularization algorithms 
[sparse, linear, models, regularization, algorithms, lasso] section show solve lasso problem using lasso ﬁrst sight might seem odd since hidden variables key insight represent laplace distribution gaussian scale mixture gsm andrews mallows west follows lap thus laplace gsm mixing distibution variances exponential distribution expon using decomposition represent lasso model shown figure corresponding joint distribution form diag assumed notational simplicity stan dardized centered ignore offset term expanding get exp exp exp exp describe apply algorithm model figure brief step infer step estimate resulting estimate lasso estimator approach ﬁrst proposed figueiredo see also griffin brown caron doucet ding harrison extensions going details worthwhile asking presenting approach given variety often much faster algorithms directly solve map estimation problem see linregfitltest empirical comparison reason latent variable perspective brings several advantages following provides easy way derive algorithm ﬁnd regularized parameter estimates variety models robust linear regression exercise probit regression exercise ensure posterior unimodal one follow park casella slightly modify model making prior variance weights depend observation noise algorithm easy modify chapter sparse linear models suggests trying priors variances besides consider various extensions makes clear compute full posterior rather map estimate technique known bayesian lasso park casella hans objective function equation complete data penalized log likelihood follows dropping terms depend const diag precision matrix step key compute derive directly see exercise alternatively derive full posterior given following park casella inversegaussian note inverse gaussian distribution also known wald distribution hence let diag denote result step also need infer easy show posterior hence step step consists computing argmax map estimation gaussian prior regularization extensions however since expect many many making inverting numerically unstable fortunately use svd given udv follows diag diag caveat since lasso objective convex method always ﬁnd global optimum unfor tunately sometimes happen numerical reasons particular suppose true solution suppose set step following step infer set thus never undo mistake fortunately practice situation seems rare see hunter discussion 
[sparse, linear, models, regularization, extensions] section discuss various extensions vanilla regularization 
[sparse, linear, models, regularization, extensions, group, lasso] standard regularization assume correspondence parameters variables interpret mean variable excluded complex models may many parameters associated given variable particular may vector weights input examples multinomial logistic regression feature associated different weights one per class linear regression categorical inputs scalar input one hot encoded vector length multi task learning multi task learning multiple related prediction problems example might separate regression binary classiﬁcation problems thus feature associated different weights may want use feature tasks none tasks thus select weights group level obozinski use regularizer form may end elements zero prevent kind situation partition parameter vector groups minimize following objective nll chapter sparse linear models norm group weight vector nll least squares method called group lasso yuan lin often use larger penalty larger groups setting number elements group example groups objective becomes nll note used square norms model would become equivalent ridge regression since using square root penalizing radius ball containing group weight vector way radius small elements small thus square root results group sparsity variant technique replaces norm inﬁnity norm turlach zhao max clear also result group sparsity illustration difference shown figures cases true signal size divided groups size randomly choose groups assign non zero values ﬁrst example values drawn second example values set pick random design matrix size finally generate given data estimate support using group estimate non zero values using least squares see group lasso much better job vanilla lasso since respects known group structure also see norm tendency make elements within block similar magnitude appropriate second example ﬁrst value examples chosen hand gsm interpretation group lasso group lasso equivalent map estimation using following prior exp slight non zero noise group lasso results presumably due numerical errors regularization extensions original number groups active groups standard debiased tau mse block debiased tau mse block linf debiased tau mse figure illustration group lasso original signal piecewise gaussian top left original signal bottom left vanilla lasso estimate top right group lasso estimate using norm blocks bottom right group lasso estimate using norm blocks based figures wright figure generated grouplassodemo based code mario figueiredo one show exercise prior written gsm follows size group see one variance term per group comes gamma prior whose shape parameter depends group size whose rate parameter controlled figure gives example groups one size one size picture also makes clearer grouping effect suppose small estimated small force small converseley suppose large estimated large allow become large well chapter sparse linear models original number groups active groups standard debiased tau mse block debiased tau mse block linf debiased tau mse figure figure except original signal piecewise constant figure graphical model group lasso groups ﬁrst size second size regularization extensions algorithms group lasso variety algorithms group lasso brieﬂy mention two ﬁrst approach based proximal gradient descent discussed section since regularizer separable proximal operator decomposes separate operators form prox argmin one show combettes wajs implemented follows prox proj ball using equation prox otherwise prox combine vectorial soft threshold function follows wright prox max max use ball project onto time using algorithm described duchi another approach modify algorithm method almost vanilla lasso deﬁne group dimension belongs use full conditionals changes follows must modify full conditional weight precisions estimated based shared set weights inversegaussian step use must modify full conditional tuning parameter estimated based values chapter sparse linear models index cgh figure example fused lasso vertical axis represents array cgh chromosomal genome hybridization intensity horizontal axis represents location along genome source figure hoeﬂing noisy image fused lasso estimate using lattice prior source figure hoeﬂing used kind permission holger hoeﬂing 
[sparse, linear, models, regularization, extensions, fused, lasso] problem settings functional data analysis want neighboring coefficients similar addition sparse example given figure want signal mostly addition property neighboring locations typically similar value model using prior form exp known fused lasso penalty context functional data analysis often use one coefficient location signal see section case overall objective form sparse version equation possible generalize idea beyond chains consider graph structures using penalty form called graph guided fused lasso see chen graph might come prior knowledge database known biological pathways another example shown figure graph structure lattice regularization extensions gsm interpretation fused lasso one show kyung fused lasso model equivalent following hierarchical model expon expon tridiagonal precision matrix main diagonal diagonal deﬁned similar model section used chain structured gaussian markov random ﬁeld prior ﬁxed vari ance let variance random case graph guided lasso structure graph reﬂected zero pattern gaussian precision matrix see section algorithms fused lasso possible generalize algorithm fused lasso model exploiting markov structure gaussian prior efficiency direct solvers use latent variable trick also derived see hoeﬂing however model undeniably expensive variants considered 
[sparse, linear, models, regularization, extensions, elastic, net, ridge, lasso, combined] although lasso proved effective variable selection technique several problems zou hastie following group variables highly correlated genes pathway lasso tends select one chosen rather arbitrarily evident lars algorithm one member group chosen remaining members group correlated new residual hence chosen usually better select relevant variables group know grouping structure use group lasso often know grouping structure case lasso select variables saturates variables correlated empirically observed prediction performance ridge better lasso chapter sparse linear models zou hastie zou hastie proposed approach called elastic net hybrid lasso ridge regression solves problems apparently called elastic net like stretchable ﬁshing net retains big ﬁsh zou hastie vanilla version vanilla version model deﬁnes following objective function notice penalty function strictly convex assuming unique global minimum even full rank shown zou hastie strictly convex penalty exhibit grouping effect means regression coefficients highly correlated variables tend equal change sign negatively correlated example two features equal one show estimates also equal contrast lasso may vice versa algorithms vanilla elastic net simple show exercise elastic net problem reduced lasso problem modiﬁed data particular deﬁne solve arg min set use lars solve subproblem known lars algorithm stop algorithm variables included cost note use wish since rank contrast lasso cannot select variables jumping ols solution using lars solvers one typically uses cross validation select improved version unfortunately turns vanilla elastic net produce functions predict accurately unless close either pure ridge pure lasso intuitively reason performs shrinkage twice due penalty due penalty solution simple undo shrinkage scaling estimates vanilla version words solution equation better estimate non convex regularizers call corrected estimate one show corrected estimates given arg min elastic net like lasso use version shrunk towards see section discussion regularized estimates covariance matrices gsm interpretation elastic net implicit prior used elastic net obviously form exp product gaussian laplace distributions written hierarchical prior follows kyung chen expon clearly reduces regular lasso possible perform map estimation model using bayesian inference using mcmc kyung variational bayes chen 
[sparse, linear, models, non-convex, regularizers] although laplace prior results convex optimization problem statistical point view prior ideal two main problems first put enough probability mass near sufficiently suppress noise second put enough probability mass large values causes shrinkage relevant coefficients corresponding signal seen figure see estimates large coefficients signiﬁcantly smaller estimates phenomenon known bias problems solved going ﬂexible kinds priors larger spike heavier tails even though cannot ﬁnd global optimum anymore non convex methods often outperform regularization terms predictive accuracy detecting relevant variables fan schniter give examples chapter sparse linear models 
[sparse, linear, models, non-convex, regularizers, bridge, regression] natural generalization regularization known bridge regression frank friedman form nll corresponds map estimation using exponential power distribution given exppower exp get gaussian distribution corresonding ridge regression set get laplace distribution corresponding lasso set get regression equivalent best subset selection unfortunately objective convex sparsity promoting norm tightest convex approximation norm effect changing illustrated figure plot prior assume also plot posterior seeing single observation imposes single linear constraint form certain tolerance controlled observation noise compare figure see see mode laplace vertical axis corresponding contrast two modes using corresponding two different sparse solutions using gaussian map estimate sparse mode lie either coordinate axes 
[sparse, linear, models, non-convex, regularizers, hierarchical, adaptive, lasso] recall one principal problems lasso results biased estimates needs use large value squash irrelevant parameters penalizes relevant parameters would better could associate different penalty parameter parameter course completely infeasible tune parameters cross validation poses problem bayesian simply make private tuning parameter treated random variables coming conjugate prior full model follows see figure called hierarchical adaptive lasso hal lee see also lee cevher armagan integrate induces lap distribution result scaled mixture laplacians turns model compute local posterior mode using explain resulting estimate hal often works non convex regularizers figure top plot log prior three different distributions unit variance gaussian laplace exponential power bottom plot log posterior observing single observation corresponding single linear constraint precision observation shown diagonal lines top ﬁgure case gaussian prior posterior unimodal symmetric case laplace prior posterior unimodal asymmetric skewed case exponential prior posterior bimodal based figure seeger figure generated sparsepostplot written florian steinke much better estimate returned lasso sense likely contain zeros right places model selection consistency likely result good predictions prediction consistency lee give explanation behavior section hal since inverse gamma conjugate laplace ﬁnd step given step vanilla lasso prior following form exp hence step must optimize argmax log chapter sparse linear models hal figure dgm hierarchical adaptive lasso contours hierarchical adpative laplace based figure lee figure generated normalgammapenaltyplotdemo expectation given thus step becomes weighted lasso problem argmin easily solved using standard methods lars note coefficient esti mated large previous iteration large scaling factor small large coefficients penalized heavily conversely small coefficients get penalized heavily way algorithm adapts penalization strength coefficient result estimate often much sparser returned lasso also less biased note set perform iteration get method closely related adaptive lasso zou zou algorithm also closely related iteratively reweighted methods proposed signal processing community chartrand yin candes understanding behavior hal get better understanding hal integrating get following marginal distribution non convex regularizers lasso mle map mle map hal figure thresholding behavior two penalty functions negative log priors laplace hierarchical adaptive laplace based figure lee figure generated normalgammathresholdplotdemo instance generalized distribution mcdonald newey cevher armagan called double pareto distribution deﬁned scale parameter controls degree sparsity related degrees freedom recover standard distribution recover exponential power distribution get laplace distribution context current model see resulting penalty term form log log const tuning parameters plot penalty plot figure various values compared diamond shaped laplace penalty shown figure see hal penalty looks like star ﬁsh puts much density along spines thus enforcing sparsity aggressively note penalty clearly convex gain understanding behavior penalty function considering applying problem linear regression orthogonal design matrix case chapter sparse linear models ref fixed lap andrews mallows west lee cevher armagan neg griffin brown chen fixed griffin brown figueiredo fixed andrews mallows west horseshoe carvahlo table scale mixtures gaussians abbreviations half rectiﬁed cauchy gamma shape rate parameterization generalized inverse gamma neg normal exponential gamma normal gamma normal jeffreys horseshoe distribution name give distribution induced prior described carvahlo simple analytic form deﬁnitions neg densities bit complicated found references distributions deﬁned text one show objective becomes mle mle mle mle thus compute map estimate one dimension time solving following optimization problem argmin mle figure plot lasso estimate estimate mle see estimator usual soft thresholding behavior seen earlier figure however behavior undesirable since large magnitude coefficients also shrunk towards whereas would like equal unshrunken estimates figure plot hal estimate hal estimate mle see approximates desirable hard thresholding behavior seen earlier figure much closely 
[sparse, linear, models, non-convex, regularizers, hierarchical, priors] many hierarchical sparsity promoting priors proposed see table brief summary cases analytically derive form marginal prior generally speaking prior concave particularly interesting prior improper normal jeffreys prior used figueiredo puts non informative jeffreys prior variance automatic relevance determination ard sparse bayesian learning sbl resulting marginal form gives rise thresholding rule looks similar hal figure turn similar hard thresholding however prior free parameters good thing nothing tune bad thing ability adapt level sparsity 
[sparse, linear, models, automatic, relevance, determination, ard/sparse, bayesian, learning, sbl] figure illustration ard results sparsity vector inputs point towards vector outputs feature removed ﬁnite probability density spread directions away probability density maximized based figure tipping 
[sparse, linear, models, automatic, relevance, determination, ard/sparse, bayesian, learning, sbl, ard, linear, regression] explain procedure context linear regression ard glms requires use laplace approximation case conventional discussing ard sbl denote weight precisions measurement precision confuse use statistics represent regression coefficients particular assume following model chapter sparse linear models diag marginal likelihood computed analytically follows exp compare marginal likelihood equation spike slab model modulo factor missing second term equations except replaced binary continuous log form objective becomes log log regularize problem may put conjugate prior precision modiﬁed objective becomes log log log log log log useful performing bayesian inference bishop tipping however performing type point estimation use improper prior results maximal sparsity describe optimize wrt precision terms proxy ﬁnding probable model setting spike slab model turn closely related regularization particular shown wipf objective equation many fewer local optima objective hence much easier optimize estimated compute posterior parameters using fact compute posterior simultaneously encouraging sparsity method called sparse bayesian learning nevertheless since many ways sparse bayesian use ard term instead even linear model context addition sbl bayesian values coefficients rather reﬂecting uncertainty set relevant variables typically interest alternative approach optimizing put gamma prior integrate get student posterior buntine weigend however turns results less accurate estimate mackay addition working gaussians easier working student distribution gaussian case generalizes easily cases logistic regression automatic relevance determination ard sparse bayesian learning sbl 
[sparse, linear, models, automatic, relevance, determination, ard/sparse, bayesian, learning, sbl, whence, sparsity] ﬁnd mle since gaussian prior shrinking towards zero precision however ﬁnd prior conﬁdent hence feature irrelevant hence posterior mean thus irrelevant features automatically weights turned pruned give intuitive argument based tipping encour age irrelevant features consider linear regression training examples plot vectors plane shown figure suppose feature irrelevant predicting response points nearly orthogonal direction let see happens marginal likelihood change marginal likelihood given ﬁnite posterior elongated along direction figure however ﬁnd spherical figure held constant latter assigns higher probability density observed response vector preferred solution words marginal likelihood punishes solutions small irrelevant since waste probability mass parsimonious point view bayesian occam razor eliminate redundant dimensions 
[sparse, linear, models, automatic, relevance, determination, ard/sparse, bayesian, learning, sbl, connection, map, estimation] ard seems quite different map estimation methods considering earlier chapter particular ard integrating optimizing vice chapter sparse linear models versa parameters become correlated posterior due explaining away estimate borrowing information features feature consequently effective prior non factorial furthermore depends data however wipf nagarajan shown ard viewed following map estimation problem ard arg min ard ard min log proof based convex analysis little complicated hence omitted furthermore wipf nagarajan wipf prove map estimation non factorial priors strictly better map estimation possible factorial prior following sense non factorial objective always fewer local minima factorial objectives still satisfying property global optimum non factorial objec tive corresponds global optimum objective property regularization local minima enjoy 
[sparse, linear, models, automatic, relevance, determination, ard/sparse, bayesian, learning, sbl, algorithms, ard] section review several different algorithms implementing ard algorithm easiest way implement sbl ard use expected complete data log likelihood given log log log log aww const log log const computed step using equation suppose put prior prior penalized objective becomes log log setting get following step automatic relevance determination ard sparse bayesian learning sbl update becomes update given new deriving exercise fixed point algorithm faster direct approach directly optimize objective equation one show exercise equations lead following ﬁxed point updates quantity measure well determined data mackay hence effective degrees freedom model see section discussion since depend computed using equation laplace approximation need estimate equations convergence convergence properties algorithm studied wipf nagarajan convergence results formally identical obtained since objective non convex results depend initial values iteratively reweighted algorithm another approach solving ard problem based view map estimation problem although log prior rather complex form shown non decreasing concave function means solved iteratively reweighted problem form arg min nll wipf nagarajan following procedure setting penalty terms suggested based convex bound penalty function initialize chapter sparse linear models iteration compute iterating following equation times xdiag diag see new penalty depends old weights quite different adaptive lasso method section understand difference consider noiseless case assume case solutions perfectly reconstruct data sparsity called basic feasible solutions bfs want solutions satsify much sparser suppose method found bfs want increase penalty weight small adaptive lasso since reinforce current local optimum instead want increase penalty weight small covariance term xdiag diag effect bfs matrix full rank penalty increase much sparser matrix full rank penalties associated zero valued coefficients increase thus reinforcing solution wipf nagarajan 
[sparse, linear, models, automatic, relevance, determination, ard/sparse, bayesian, learning, sbl, ard, logistic, regression] consider binary logistic regression ber sigm using gaussian prior longer use estimate since gaussian prior conjugate logistic likelihood step cannot done exactly one approach use variational approximation step discussed section simpler approach use laplace approximation see section step use approximation inside procedure except longer need update note however guaranteed converge alternative use techniques section case use exact methods compute inner weighted regularized logistic regression problem approximations required 
[sparse, linear, models, sparse, coding] far concentrating sparse priors supervised learning section discuss use unsupervised learning section discussed ica like pca except uses non gaussian prior latent factors make non gaussian prior sparsity promoting laplace distribution approximating observed vector sparse combination basis vectors columns note sparsity pattern controlled changes data case data case relax constraint orthogonal get method called algorithm wipf nagarajan equivalent single iteration equation however since equation cheap compute time worth iterating times solving expensive problem sparse coding method orthogonal pca gauss yes gauss ica non gauss yes sparse coding laplace sparse pca gauss laplace maybe sparse laplace laplace table summary various latent factor models dash column means performing parameter estimation rather map parameter estimation summary abbreviations pca principal components analysis factor analysis ica independent components analysis matrix factorization sparse coding context call factor loading matrix dictionary column referred atom view sparse representation common case call representation overcomplete sparse coding dictionary ﬁxed learned ﬁxed common use wavelet dct basis since many natural signals well approximated small number basis functions however also possible learn dictionary maximizing likelihood log log discuss ways optimize present several interesting applications confuse sparse coding sparse pca see witten journee puts sparsity promoting prior regression weights whereas sparse coding put sparsity promoting prior latent factors course two techniques combined call result sparse matrix factorization although term non standard see table summary terminology 
[sparse, linear, models, sparse, coding, learning, sparse, coding, dictionary] since equation hard objective maximize common make following approximation log max log log laplace rewrite nll nll common denote dictionary denote latent factors however stick notation chapter sparse linear models prevent becoming arbitrarily large common constrain norm columns less equal let denote constraint set want solve min nll ﬁxed optimization simple least squares problem ﬁxed dictionary optimization problem identical lasso problem many fast algorithms exist suggests obvious iterative optimization scheme alternate optimizing mumford called kind approach analysis synthesis loop estimating basis analysis phase estimating coefficients synthesis phase cases slow sophisticated algorithms used see mairal variety models result optimization problem similar equation example non negative matrix factorization nmf paatero tapper lee seung requires solving objective form min note hyper parameters tune intuition behind constraint learned dictionary may interpretable positive sum positive parts rather sparse sum atoms may positive negative course combine nmf sparsity promoting prior latent factors called non negative sparse coding hoyer alternatively drop positivity constraint impose sparsity constraint factors dictionary call sparse matrix factorization ensure strict convexity use elastic net type penalty weights mairal resulting min several related objectives one write example replace lasso nll group lasso fused lasso witten also use sparsity promoting priors besides laplace example zhou propose model latent factors made sparse using binary mask model section bit mask generated bernoulli distribution parameter drawn beta distribution alternatively use non parametric prior beta process allows model use dictionaries unbounded size rather specify advance one perform bayesian inference model using gibbs sampling variational bayes one ﬁnds effective size dictionary goes noise level goes due bayesian occam razor prevent overﬁtting see zhou details 
[sparse, linear, models, sparse, coding, results, dictionary, learning, image, patches] one reason sparse coding generated much interest recently explains interesting phenomenon neuroscience particular dictionary learned applying sparse coding figure illustration ﬁlters learned various methods applied natural image patches patch ﬁrst centered normalized unit norm ica figure generated icabasisdemo kindly provided aapo hyvarinen sparse coding pca non negative matrix factorization sparse pca low sparsity weight matrix sparse pca high sparsity weight matrix figure generated sparsedictdemo written julien mairal chapter sparse linear models sparse coding patches natural images consists basis vectors look like ﬁlters found simple cells primary visual cortex mammalian brain olshausen field particular ﬁlters look like bar edge detectors shown figure example parameter chosen number active basis functions non zero components interestingly using ica gives visually similar results shown figure contrast applying pca data results sinusoidal gratings shown figure look like cortical cell response patterns therefore conjectured parts cortex may performing sparse coding sensory input resulting latent representation processed higher levels brain figure shows result using nmf figure show results sparse pca increase sparsity basis vectors 
[sparse, linear, models, sparse, coding, compressed, sensing] although interesting look dictionaries learned sparse coding necessarily useful however practical applications sparse coding discuss imagine instead observing data observe low dimensional projection matrix noise term usually gaussian assume known sensing matrix corresponding different linear projections example consider mri scanner beam direction corresponds vector encoded row figure illustrates modeling assumptions goal infer hope recover measure answer use bayesian inference appropriate prior exploits fact natural signals expressed weighted combination small number suitably chosen basis functions assume sparse prior suitable dictionary called compressed sensing compressive sensing candes baruniak candes wakin bruckstein work important represent signal right basis otherwise sparse traditional applications dictionary ﬁxed standard form wavelets however one get much better performance learning domain speciﬁc dictionary using sparse coding zhou sensing matrix often chosen random matrix reasons explained candes wakin however one get better performance adapting projection matrix dictionary seeger nickish chang 
[sparse, linear, models, sparse, coding, image, inpainting, denoising] suppose image corrupted way text scratches sparsely superimposed top figure might want estimate underlying reason pca discovers sinusoidal grating patterns trying model covariance data case image patches translation invariant means cov function image intensity location one show hyvarinen eigenvectors matrix kind always sinusoids different phases pca discovers fourier basis sparse coding figure schematic dgm compressed sensing observe low dimensional measurement generated passing measurement matrix possibly subject observation noise variance assume sparse decomposition terms dictionary latent variables parameter controlls sparsity level figure example image inpainting using sparse coding left original image right recon struction source figure mairal used kind permission julien mairal clean image called image inpainting one use similar techniques image denoising model special kind compressed sensing problem basic idea follows partition image overlapping patches concatenate form deﬁne row selects patch deﬁne visible uncorrupted components hidden components perform image inpainting compute model parameters specify dictionary sparsity level either learn dictionary offline database images learn dictionary image based non corrupted patches figure shows technique action dictionary size atoms learned undamaged color patches mega pixel image alternative approach use graphical model ﬁelds experts model chapter sparse linear models black directly encodes correlations neighboring image patches rather using latent variable model unfortunately models tend computationally expensive 
[sparse, linear, models, exercises] exercise partial derivative rss deﬁne rss show rss without component without component residual due using features except feature hint partition weights involving involving show rss hence sequentially add features optimal weight feature computed computing orthogonally projecting onto current residual exercise derivation step linear regression derive equations hint following identity useful exercise derivation ﬁxed point updates linear regression derive equations hint easiest way derive result rewrite log equation exactly equivalent since case gaussian prior likelihood posterior also gaussian laplace approximation exact case get log log log log log rest straightforward algebra sparse coding exercise marginal likelihood linear regression suppose use prior form show equation simpliﬁes exercise reducing elastic net lasso deﬁne show arg min arg min hence one solve elastic net problem using lasso solver modiﬁed data exercise shrinkage linear regression source jaakkola consider performing linear regression orthonormal design matrix column feature estimate parameter separately figure plots correlation feature response different esimation methods ordinary least squares ols ridge regression parameter lasso parameter unfortunately forgot label plots method solid dotted dashed line correspond hint see section value value exercise prior bernoulli rate parameter spike slab model consider model section suppose put prior sparsity rates beta derive expression integrating discuss advantages disadvantages approach compared assuming ﬁxed chapter sparse linear models figure plot amount correlation three different estimators exercise deriving step gsm prior show log intn hint exp exp hint log exercise sparse probit regression laplace prior derive algorithm ﬁtting binary probit classiﬁer section using laplace prior weights get stuck see figueiredo ding harrison exercise gsm representation group lasso consider prior ignoring grouping issue marginal distribution induced weights gamma mixing distribution called normal gamma distribution sparse coding given modiﬁed bessel function second kind besselk function matlab suppose following prior variances corresponding marginal group weights form suppose conveniently exp show resulting map estimate equivalent group lasso exercise projected gradient descent regularized least squares consider bpdn problem argmin rss using split variable trick introducted section deﬁning rewrite quadratic program simple bound constraint sketch use projected gradient descent solve problem get stuck consult figueiredo exercise subderivative hinge loss function let hinge loss function max exercise lower bounds convex functions let convex function explain ﬁnd global affine lower bound arbitrary point dom 
[kernels, introduction] far book assuming object wish classify cluster process anyway represented ﬁxed size feature vector typically form however certain kinds objects clear best represent ﬁxed sized feature vectors example represent text document protein sequence variable length molecular structure complex geometry evolutionary tree variable size shape one approach problems deﬁne generative model data use inferred latent representation parameters model features plug features standard methods example chapter discuss deep learning essentially unsupervised way learn good feature representations another approach assume way measuring similarity objects require preprocessing feature vector format example comparing strings compute edit distance let measure similarity objects abstract space call kernel function note word kernel several meanings discuss different interpretation section chapter discuss several kinds kernel functions describe algorithms written purely terms kernel function computations methods used access choose look inside objects processing 
[kernels, kernel, functions] deﬁne kernel function real valued function two arguments typically function symmetric non negative interpreted measure similarity required give several examples chapter kernels 
[kernels, kernel, functions, rbf, kernels] squared exponential kernel kernel gaussian kernel deﬁned exp diagonal written exp interpret deﬁning characteristic length scale dimension corresponding dimension ignored hence known ard kernel spherical get isotropic kernel exp known bandwidth equation example radial basis function rbf kernel since function 
[kernels, kernel, functions, kernels, comparing, documents] performing document classiﬁcation retrieval useful way comparing two documents use bag words representation number times words occurs document use cosine similarity deﬁned quantity measures cosine angle interpreted vectors since count vector hence non negative cosine similarity means vectors orthogonal therefore words common unfortunately simple method work well two main reasons first word common deemed similar even though popular words occur many documents therefore discriminative known stop words second discriminative word occurs many times document similarity artiﬁcially boosted even though word usage tends bursty meaning word used document likely used see section fortunately signiﬁcantly improve performance using simple preprocessing idea replace word count vector new feature vector called idf representa tion stands term frequency inverse document frequency deﬁne follows first term frequency deﬁned log transform count log reduces impact words occur many times within one document second inverse document frequency deﬁned idf log kernel functions total number documents denominator counts many documents contain term finally deﬁne idf idf several ways deﬁne idf terms see manning details use inside cosine similarity measure new kernel form idf gives good results information retrieval manning probabilistic interpretation idf kernel given elkan 
[kernels, kernel, functions, mercer, positive, deﬁnite, kernels] methods study require kernel function satisfy requirement gram matrix deﬁned positive deﬁnite set inputs call kernel mercer kernel positive deﬁnite kernel shown schoelkopf smola gaussian kernel mercer kernel cosine similarity kernel sahami heilman importance mercer kernels following result known mercer theorem gram matrix positive deﬁnite compute eigenvector decomposition follows diagonal matrix eigenvalues consider element let deﬁne write thus see entries kernel matrix computed performing inner product feature vectors implicitly deﬁned eigenvectors general kernel mercer exists function mapping depends eigen functions potentially inﬁnite dimensional space example consider non stationary polynomial kernel one show corresponding feature vector contain terms degree example chapter kernels written using kernel equivalent working dimensional feature space case gaussian kernel feature map lives inﬁnite dimensional space case clearly infeasible explicitly represent feature vectors example kernel mercer kernel called sigmoid kernel deﬁned tanh note uses tanh function even though called sigmoid kernel kernel inspired multi layer perceptron see section real reason use true neural net kernel positive deﬁnite see section general establishing kernel mercer kernel difficult requires techniques functional analysis however one show possible build new mercer kernels simpler ones using set standard rules example mercer see schoelkopf smola details 
[kernels, kernel, functions, linear, kernels] deriving feature vector implied kernel general quite difficult possible kernel mercer however deriving kernel feature vector easy use get linear kernel deﬁned useful original data already high dimensional original features individually informative bag words representation vocabulary size large expression level many genes case decision boundary likely representable linear combination original features necessary work feature space course high dimensional problems linearly separable example images high dimensional individual pixels informative image classiﬁcation typically requires non linear kernels see section 
[kernels, kernel, functions, matern, kernels] matern kernel commonly used gaussian process regression see section following form kernel functions modiﬁed bessel function approaches kernel kernel simpliﬁes exp use kernel deﬁne gaussian process see chapter get ornstein uhlenbeck process describes velocity particle undergoing brownian motion corresponding function continuous differentiable hence jagged 
[kernels, kernel, functions, string, kernels] real power kernels arises inputs structured objects example describe one way comparing two variable length strings using string kernel follow presentation rasmussen williams hastie consider two strings lengths deﬁned alphabet example consider two amino acid sequences deﬁned letter alphabet let following sequence length iptsalvketlallsthrtllianetlripvpvhknhqlcteeifqgigtlesqtvqggtv erlfknlslikkyidgqkkkcgeerrrvnqfldy lqe flgvmntewi let following sequence length phrrdlcsrsiwlarkirsdltaltesyvkhqglwselteaer lqe nlqayrtfhvlla rlledqqvhftptegdfhqaihtlllqvaafayqieelmilleykiprneadgmlfekk lwglkv lqe lsqwtvrsihdlrfisshqtgip strings substring lqe common deﬁne similarity two strings number substrings common formally generally let say substring write usv possibly empty strings let denote number times substring appears string deﬁne kernel two strings set strings length alphabet known kleene star operator mercer kernel computed time certain settings weights using suffix trees leslie vishwanathan smola shawe taylor cristianini various cases interest set get bag characters kernel deﬁnes number times character occurs require bordered white space get bag words kernel counts many times possible word occurs note sparse vector since words chapter kernels optimal partial matching matching figure illustration pyramid match kernel computed two images used kind permission kristen grauman present consider strings ﬁxed length get spectrum kernel used classify proteins scop superfamilies leslie example lqe lqe two strings various extensions possible example allow character mismatches leslie generalize string kernels compare trees described collins duffy useful classifying ranking parse trees evolutionary trees etc 
[kernels, kernel, functions, pyramid, match, kernels] computer vision common create bag words representation image computing feature vector often using sift lowe variety points image commonly chosen interest point detector feature vectors chosen places vector quantized create bag discrete symbols one way compare two variable sized bags kind use pyramid match kernel grauman darrell basic idea illustrated figure feature set mapped multi resolution histogram compared using weighted histogram intersection turns provides good approximation similarity measure one would obtain performing optimal bipartite match ﬁnest spatial resolution summing pairwise similarities matched points however histogram method faster robust missing unequal numbers points mercer kernel kernel functions 
[kernels, kernel, functions, kernels, derived, probabilistic, generative, models] suppose probabilistic generative model feature vectors several ways use model deﬁne kernel functions thereby make model suitable discriminative tasks sketch two approaches probability product kernels one approach deﬁne kernel follows often approximated parameter estimate computed using single data vector called probability product kernel jebara although seems strange model single data point important bear mind ﬁtted model used see similar two objects particular model model thinks likely means similar example suppose ﬁxed use ﬁnd jebara exp constant factor rbf kernel turns one compute equation variety generative models including ones latent variables hmms provides one way deﬁne kernels variable length sequences furthermore technique works even sequences real valued vectors unlike string kernel section see jebara details fisher kernels efficient way use generative models deﬁne kernels use fisher kernel jaakkola haussler deﬁned follows gradient log likelihood score vector evaluated mle log fisher information matrix essentially hessian log note function data similarity computed context data well also note one model intuition behind fisher kernel following let direction parameter space would like parameters move maximize chapter kernels poly rbf prototypes figure xor truth table fitting linear logistic regression classiﬁer using degree polynomial expansion model using rbf kernel centroids speciﬁed black crosses figure generated logregxordemo likelihood call directional gradient say two vectors similar directional gradients similar wrt geometry encoded curvature likelihood function see section interestingly shown saunders string kernel section equivalent fisher kernel derived order markov chain see section also shown elkan kernel deﬁned inner product idf vectors section approximately equal fisher kernel certain generative model text based compound dirichlet multinomial model section 
[kernels, using, kernels, inside, glms] section discuss one simple way use kernels classiﬁcation regression see approaches later 
[kernels, using, kernels, inside, glms, kernel, machines] deﬁne kernel machine glm input feature vector form set centroids rbf kernel called rbf network discuss ways choose parameters call equation kernelised feature vector note approach kernel need mercer kernel use kernelized feature vector logistic regression deﬁning ber provides simple way deﬁne non linear decision boundary example consider data coming exclusive xor function binary valued function two binary inputs truth table shown figure figure show data labeled xor function jittered points make picture clearer see cannot separate data even using degree polynomial jittering common visualization trick statistics wherein points plot display would otherwise land top dispersed uniform additive noise using kernels inside glms figure rbf basis left column ﬁtted function middle column basis functions evaluated grid right column design matrix top bottom show different bandwidths figure generated linregrbfdemo however using rbf kernel prototypes easily solves problem shown figure also use kernelized feature vector inside linear regression model deﬁning example figure shows data set uniformly spaced rbf prototypes bandwidth ranging small large small values lead wiggly functions since predicted function value non zero points close one prototypes bandwidth large design matrix reduces constant matrix since point equally close every prototype hence corresponding function straight line 
[kernels, using, kernels, inside, glms, lvms, rvms, sparse, vector, machines] main issue kernel machines choose centroids input low dimensional euclidean space uniformly tile space occupied data prototypes figure however approach breaks higher numbers dimensions curse dimensionality try perform numerical optimization parameters see haykin use mcmc inference see andrieu kohn resulting objective function posterior highly multimodal furthermore techniques hard extend structured input spaces kernels useful another approach ﬁnd clusters data assign one prototype per cluster chapter kernels center many clustering algorithms need similarity metric input however regions space high density necessarily ones prototypes useful representing output clustering unsupervised task may yield representation useful prediction furthermore need pick number clusters simpler approach make example prototype get see many parameters data points however use sparsity promoting priors discussed chapter efficiently select subset training exemplars call sparse vector machine natural choice use regularization krishnapuram note multi class case necessary use group lasso since exemplar associated weights one per class call lvm stands regularized vector machine analogy deﬁne use regularizer lvm regularized vector machine course sparse get even greater sparsity using ard sbl resulting method called rele vance vector machine rvm tipping one model using generic ard sbl algorithms although practice common method greedy algorithm tipping faul algorithm implemented mike tipping code bundled pmtk another popular approach creating sparse kernel machine use support vector machine svm discussed detail section rather using sparsity promoting prior essentially modiﬁes likelihood term rather unnatural bayesian point view nevertheless effect similar see figure compare lvm lvm rvm svm using rbf kernel binary classiﬁcation problem simplicity chosen hand lvm lvm rvms parameters estimated using empirical bayes svm use pick since svm performance sensitive parameter see section see methods give similar performance however rvm sparsest hence fastest test time lvm svm rvm also fastest train since svm slow despite fact rvm code matlab svm code result fairly typical figure compare lvm lvm rvm svm using rbf kernel regression problem see predictions quite similar rvm sparsest lvm svm illustrated figure 
[kernels, kernel, trick] rather deﬁning feature vector terms kernels instead work original feature vectors modify algorithm replaces inner products form call kernel function called kernel trick turns many algorithms kernelized way give examples note require kernel mercer kernel trick work kernel trick logregl nerr logregl nerr rvm nerr svm nerr figure example non linear binary classiﬁcation using rbf kernel bandwidth lvm lvm rvm svm chosen cross validation black circles denote support vectors figure generated kernelbinaryclassifdemo 
[kernels, kernel, trick, kernelized, nearest, neighbor, classiﬁcation] recall classiﬁer section need compute euclidean distance test vector training points ﬁnd closest one look label kernelized observing allows apply nearest neighbor classiﬁer structured data objects 
[kernels, kernel, trick, kernelized, k-medoids, clustering] means clustering section uses euclidean distance measure dissimilarity always appropriate structured objects describe develop kernelized chapter kernels linregl linregl rvm svm figure example kernel based regression noisy sinc function using rbf kernel bandwidth lvm lvm rvm svm regression chosen cross validation default svmlight red circles denote retained training exemplars figure generated kernelregrdemo version algorithm ﬁrst step replace means algorithm medoids algorothm similar means instead representing cluster centroid mean data vectors assigned cluster make centroid one data vectors thus always deal integer indexes rather data objects assign objects closest centroids update centroids look object belongs cluster measure sum distances others cluster pick one smallest sum argmin kernel trick weights linregl weights linregl weights rvm weights svm figure coefficient vectors length models figure figure generated kernelregrdemo takes work per cluster whereas means takes update cluster pseudo code given algorithm method modiﬁed derive classiﬁer computing nearest medoid class known nearest medoid classiﬁcation hastie algorithm kernelized using equation replace distance computation chapter kernels algorithm medoids algorithm initialize random subset size repeat argmin argmin converged 
[kernels, kernel, trick, kernelized, ridge, regression] applying kernel trick distance based methods straightforward obvious apply parametric models ridge regression however done explain serve good warm studying svms primal problem let feature vector corresponding design matrix want minimize optimal solution given dual problem equation yet form inner products however using matrix inversion lemma equation rewrite ridge estimate follows takes time compute advantageous large see partially kernelize replacing gram matrix leading term let deﬁne following dual variables rewrite primal variables follows tells solution vector linear sum training vectors plug test time compute predictive mean get kernel trick eigenvalue eigenvalue eigenvalue eigenvalue eigenvalue eigenvalue eigenvalue eigenvalue figure visualization ﬁrst kernel principal component basis functions derived data use rbf kernel figure generated kpcascholkopf written bernhard scholkopf succesfully kernelized ridge regression changing primal dual variables technique applied many linear models logistic regression computational cost cost computing dual variables whereas cost computing primal variables hence kernel method useful high dimensional settings even use linear kernel svd trick equation however prediction using dual variables takes time prediction using primal variables takes time speedup prediction making sparse discuss section 
[kernels, kernel, trick, kernel, pca] section saw could compute low dimensional linear embedding data using pca required ﬁnding eigenvectors sample covariance matrix chapter kernels however also compute pca ﬁnding eigenvectors inner product matrix show allow produce nonlinear embedding using kernel trick method known kernel pca schoelkopf first let orthogonal matrix containing eigenvectors corresponding eigenvalues deﬁnition pre multiplying gives see eigenvectors hence eigen values given however eigenvectors normalized since normalized eigenvectors given pca useful trick regular pca since size whereas size also allow use kernel trick show let gram matrix recall mercer theorem use kernel implies underlying feature space implicitly replacing let corresponding notional design matrix corresponding notional covariance matrix feature space eigenvectors given kpca contain eigenvectors eigenvalues course actually compute kpca since potentially inﬁnite dimensional however compute projection test vector onto feature space follows kpca φuλ one ﬁnal detail worry far assumed projected data zero mean case general cannot simply subtract mean feature space however trick use deﬁne centered feature vector gram matrix centered feature vectors given expressed matrix notation follows hkh centering matrix convert algebra pseudocode shown algorithm whereas linear pca limited using components kpca use components since rank potentially inﬁnite dimensionality embedded feature vectors figure gives example method applied dimensional data using rbf kernel project points unit grid onto ﬁrst kernel trick algorithm kernel pca input size size num latent dimensions oko eig pca kpca figure visualization data pca projection kernel pca projection figure generated kpcademo based code van der maaten components visualize corresponding surfaces using contour plot see ﬁrst two component separate three clusters following components split clusters although features learned kpca useful classiﬁcation schoelkopf necessarily useful data visualization example figure shows projection data figure onto ﬁrst principal bases computed using pca kpca obviously pca perfectly represents data kpca represents cluster different line course need project data back let consider different data set use dimensional data set representing three known phases ﬂow oil pipeline data widely used compare data visualization methods synthetic comes bishop james project using pca kpca rbf kernel results shown figure perform nearest neighbor classiﬁcation low dimensional space kpca makes errors pca makes lawrence chapter kernels figure representation dimensional oil ﬂow data different colors symbols represent phases oil ﬂow pca kernel pca gaussian kernel compare figure figure lawrence used kind permission neil lawrence nevertheless kpca projection rather unnatural section discuss make kernelized versions probabilistic pca note close connection kernel pca technique known mul tidimensional scaling mds methods ﬁnds low dimensional embedding euclidean distance embedding space approximates original dissimilarity matrix see williams details 
[kernels, support, vector, machines, svms] svms classiﬁcation regression require specify kernel function parameter typically chosen cross validation note however interacts quite strongly kernel parameters example suppose using rbf kernel precision corresponding narrow kernels need heavy regularization hence small big larger value used see tightly coupled illustrated figure shows estimate risk function authors libsvm recommend hsu using grid values addition important standardize data ﬁrst spherical gaussian kernel make sense choose efficiently one develop path following algorithm spirit lars section basic idea start large margin wide hence points inside slowly decreasing small set points move inside margin outside values change cease support vectors maximal function completely smoothed support vectors remain see hastie details 
[kernels, support, vector, machines, svms, svms, regression] problem kernelized ridge regression solution vector depends training inputs seek method produce sparse estimate vapnik vapnik proposed variant huber loss function section called epsilon insensitive loss function deﬁned otherwise means point lying inside tube around prediction penalized figure corresponding objective function usually written following form chapter kernels regularization constant objective convex unconstrained differentiable absolute value function loss term section discussed lasso problem several possible algorithms could use one popular approach formulate problem constrained optimization problem particular introduce slack variables represent degree point lies outside tube given rewrite objective follows quadratic function must minimized subject linear constraints equations well positivity constraints standard quadratic program variables one show see schoelkopf smola optimal solution form furthermore turns vector sparse care errors smaller called support vectors thse points errors lie outside tube model trained make predictions using plugging deﬁnition get finally replace get kernelized solution 
[kernels, support, vector, machines, svms, svms, classiﬁcation] discuss apply svms classiﬁcation ﬁrst focus binary case discuss multi class case section hinge loss section showed negative log likelihood logistic regression model nll log log support vector machines svms convex upper bound risk binary classiﬁer log odds ratio assumed labels rather section replace nll loss hinge loss deﬁned hinge max conﬁdence choosing label however need probabilistic semantics see figure plot see function looks like door hinge hence name overall objective form min non differentiable max term however introducing slack variables one show equivalent solving min quadratic program variables subjet constraints eliminate primal variables solve dual variables correspond lagrange multipliers constraints standard solvers take time however specialized algorithms avoid use generic solvers developed problem sequential minimal optimization smo algorithm platt practice take however even slow large settings common use linear svms take time train joachims bottou one show solution form sparse hinge loss called support vectors points either incorrectly classiﬁed classiﬁed correctly inside margin disuss margins see figure illustration test time prediction done using sgn sgn using equation kernel trick sgn takes time compute number support vectors depends sparsity level hence regularizer chapter kernels figure illustration large margin principle left separating hyper plane large margin right separating hyper plane small margin figure illustration geometry linear decision boundary point classiﬁed belonging decision region otherwise belongs decision region known discriminant function decision boundary set points vector perpendicular decision boundary term controls distance decision boundary origin signed distance orthogonal projection onto decision boundary given based figure bishop illustration soft margin principle points circles around support vectors also indicate value corresponding slack variables based figure bishop support vector machines svms large margin principle section derive equation form completely different perspective recall goal derive discriminant function linear feature space implied choice kernel consider point induced space referring figure see distance decision boundary whose normal vector orthogonal projection onto boundary hence hence would like make distance large possible reasons illustrated figure particular might many lines perfectly separate training data especially work high dimensional feature space intuitively best one pick one maximizes margin perpendicular distance closest point addition want ensure point correct side boundary hence want objective becomes max min note rescaling parameters using change distance point boundary since factor cancels divide therefore let deﬁne scale factor point closest decision boundary therefore want optimize min fact added convenience affect optimal parameters constraint says want points correct side decision boundary margin least reason say svm example large margin classiﬁer data linearly separable even using kernel trick feasible solution therefore introduce slack variables point inside correct margin boundary otherwise point lies inside margin correct side decision boundary point lies wrong side decision boundary see figure replace hard constraints soft margin constraints new objective becomes min chapter kernels correct log odds rvm svm figure log odds different methods based figure tipping used kind permission mike tipping equation since means point misclassiﬁed interpret upper bound number misclassiﬁed points parameter regularization parameter controls number errors willing tolerate training set common deﬁne using controls fraction misclassiﬁed points allow training phase called svm classiﬁer usually set using cross validation see section probabilistic output svm classiﬁer produces hard labeling sign however often want measure conﬁdence prediction one heuristic approach interpret log odds ratio log convert output svm probability using estimated maximum likelihood separate validation set using training set estimate leads severe overﬁtting technique ﬁrst proposed platt however resulting probabilities particularly well calibrated since nothing svm training procedure justiﬁes interpreting log odds ratio illustrate consider example tipping suppose data unif unif since class conditional distributions overlap middle log odds class class zero inﬁnite outside region sampled points model rvm svm gaussian kenel width models perfectly capture decision boundary achieve generalizaton error bayes optimal problem probabilistic output rvm good approximation true log odds case svm shown figure support vector machines svms figure one versus rest approach green region predicted class class one versus one approach label green region ambiguous based figure bishop svms multi class classiﬁcation section saw could upgrade binary logistic regression model multi class case replacing sigmoid function softmax bernoulli distribution multinomial upgrading svm multi class case easy since outputs calibrated scale hence hard compare obvious approach use one versus rest approach also called one train binary classiﬁers data class treated positive data classes treated negative however result regions input space ambiguously labeled shown figure common alternative pick arg max however technique may work either since guarantee different functions comparable magnitudes addition binary subproblem likely suffer class imbalance problem see suppose equally represented classes training positive examples negative examples hurt performance possible devise ways train classiﬁers simultaneously weston watkins resulting method takes time instead usual time another approach use one versus one ovo approach also called pairs train classiﬁers discriminate pairs classify point class highest number votes however also result ambiguities shown figure also takes time train test data point number support vectors see also allwein approach based error correcting output codes worth remembering difficulties plethora heuristics proposed fundamentally arise svms model uncertainty using probabilities output scores comparable across classes reduce test time structuring classes dag directed acyclic graph performing pairwise comparisons platt however factor training time unavoidable chapter kernels error error figure cross validation estimate error svm classiﬁer rbf kernel different precisions different regularizer applied synthetic data set drawn mixture gaussians slice surface red dotted line bayes optimal error computed using bayes rule applied model used generate data based figure hastie figure generated svmcgammademo 
[kernels, support, vector, machines, svms, summary, key, points] summarizing discussion recognize svm classiﬁers involve three key ingre dients kernel trick sparsity large margin principle kernel trick necessary prevent underﬁtting ensure feature vector sufficiently rich linear classiﬁer separate data recall section mercer kernel viewed implicitly deﬁning potentially high dimensional feature vector original features already high dimensional many gene expression text classiﬁcation problems suf ﬁces use linear kernel equivalent working original features comparison discriminative kernel methods method opt opt kernel sparse prob multiclass non mercer section lvm convex yes yes yes lvm convex yes yes yes yes rvm convex yes yes yes yes svm convex yes indirectly yes yes table comparison various kernel based classiﬁers empirical bayes cross validation see text details sparsity large margin principles necessary prevent overﬁtting ensure use basis functions two ideas closely related arise case use hinge loss function however methods achieving sparsity also methods maximizing margin boosting deeper discussion point takes outside scope book see hastie information 
[kernels, support, vector, machines, svms, probabilistic, interpretation, svms] section saw use kernels inside glms derive probabilistic classiﬁers lvm rvm section discuss gaussian process classiﬁers also use kernels however approaches use logistic probit likelihood opposed hinge loss used svms natural wonder one interpret svm directly probabilistic model must interpret negative log likelihood margin hence exp exp summing values require exp exp constant independent turns possible sollich however willing relax sum one condition work pseudo likelihood derive probabilistic interpretation hinge loss polson scott particular one show exp exp thus exponential negative hinge loss represented gaussian scale mixture allows one svm using gibbs sampling latent variables turn opens door bayesian methods setting hyper parameters prior see polson scott details see also franc different probabilistic interpretation svms 
[kernels, comparison, discriminative, kernel, methods] mentioned several different methods classiﬁcation regression based kernels summarize table stands gaussian process discuss chapter columns following meaning chapter kernels optimize key question whether objective log log convex lvm lvm svms convex objectives rvms gps bayesian methods perform parameter estimation optimize kernel methods require one tune kernel parameters bandwidth rbf kernel well level regularization methods based gaussians including lvm rvms gps use efficient gradient based optimizers maximize marginal likelihood svms lvm must use cross validation slower see section sparse lvm rvms svms sparse kernel methods use subset training examples gps lvm sparse use training examples principle advantage sparsity prediction test time usually faster addition one sometimes get improved accuracy probabilistic methods except svms produce probabilistic output form svms produce conﬁdence value converted probability probabilities usually poorly calibrated see section multiclass methods except svms naturally work multiclass setting using multinoulli output instead bernoulli svm made multiclass classiﬁer various difficulties approach discussed section mercer kernel svms gps require kernel positive deﬁnite techniques apart differences natural question method works best small experiment found methods similar accuracy averaged range problems provided kernel provided regularization constants chosen appropriately given statistical performance roughly computational performance gps lvm generally slowest taking time since exploit sparsity although various speedups possible see section svms also take time train unless use linear kernel case need time joachims however need use cross validation make svms slower rvms lvm faster rvm since rvm requires multiple rounds minimization see section however practice common use greedy method train rvms faster minimization reﬂected empirical results conclusion follows speed matters use rvm well calibrated probabilistic output matters active learning control problems use circumstances using svm seems sensible structured output case likelihood based methods slow attribute enormous popularity svms superiority ignorance alternatives also lack high quality software implementing alternatives section gives extensive experimental comparison supervised learning methods including svms various non kernel methods see http pmtk googlecode com svn trunk docs tutorial html tutkernelclassif html kernels building generative models boxcar epanechnikov tricube gaussian figure comparison popular smoothing kernels boxcar kernel compact support smooth epanechnikov kernel compact support differentiable boundary tri cube compact support two continuous derivatives boundary support gaussian differentiable compact support based figure hastie figure generated smoothingkernelplot 
[kernels, kernels, building, generative, models] different kind kernel known smoothing kernel used create non parametric density estimates used unsupervised density estimation well creating generative models classiﬁcation regression making models form 
[kernels, kernels, building, generative, models, smoothing, kernels] smoothing kernel function one argument satisﬁes following properties simple example gaussian kernel control width kernel introducing bandwidth parameter generalize vector valued inputs deﬁning rbf kernel case gaussian kernel becomes exp chapter kernels although gaussian kernels popular unbounded support alternative kernel compact support epanechnikov kernel deﬁned plotted figure compact support useful efficiency reasons since one use fast nearest neighbor methods evaluate density unfortunately epanechnikov kernel differentiable boundary support alterative tri cube kernel deﬁned follows compact support two continuous derivatives boundary support see figure boxcar kernel simply uniform distribution use kernel 
[kernels, kernels, building, generative, models, kernel, density, estimation, kde] recall gaussian mixture model section parametric density estimator data however requires specifying number locations clusters alternative estimating allocate one cluster center per data point case model becomes generalize approach writing called parzen window density estimator kernel density estimator kde simple non parametric density model advantage parametric model model ﬁtting required except tuning bandwidth usually done cross validation need pick disadvantage model takes lot memory store lot time evaluate also use clustering tasks figure illustrates kde two kinds kernel top use boxcar kernel result equivalent histogram estimate density since count many data points land within interval size around bottom use gaussian kernel results smoother usual way pick minimize estimate cross validation frequentist risk see bowman azzalini section discuss bayesian approach non parametric density estimation based dirichlet process mixture models allows kernels building generative models unif unif gauss gauss figure nonparametric parzen density estimator estimated data points denoted top row uniform kernel bottom row gaussian kernel rows represent increasingly large band width parameters based http wikipedia org wiki kernel_density_estimation figure generated parzenwindowdemo infer mixtures also efficient kde since need store data see also section discuss empirical bayes approach estimating kernel parameters gaussian process model classiﬁcation regression 
[kernels, kernels, building, generative, models, kde, knn] use kde deﬁne class conditional densities generative classiﬁer turns provide alternative derivation nearest neighbors classiﬁer introduced section show follow presentation bishop kde boxcar kernel ﬁxed bandwidth count many data points fall within hyper cube centered datapoint suppose instead ﬁxing bandwidth instead chapter kernels gaussian kernel regression true data estimate figure example kernel regression using gaussian kernel figure generated kernelregressiondemo based code cao allow bandwidth volume different data point speciﬁcally grow volume around encounter data points regardless class label let resulting volume size previously let examples class volume estimate class conditional density follows total number examples class whole data set class prior estimated hence class posterior given used fact since choose total points regardless class around every point equivalent equation since 
[kernels, kernels, building, generative, models, kernel, regression] section discussed use kernel density estimation kde unsupervised learning also use kde regression goal compute conditional expectation kernels building generative models use kde approximate joint density follows hence derive result used two properties smoothing kernels first integrate one second fact follows deﬁning using zero mean property smoothing kernels rewrite result follows see prediction weighted sum outputs training points weights depend similar stored training points method called kernel regression kernel smoothing nadaraya watson model see figure example use gaussian kernel note method one free parameter namely one show bowman azzalini data true density gaussian using gaussian kernels optimal bandwidth given compute robust approximation standard deviation ﬁrst computing mean absolute deviation mad median median using mad mad code used produce figure estimated separately set chapter kernels although heuristics seem work well derivation rests rather dubious assumptions gaussianity true density furthermore heuristics limited tuning single parameter section discuss empirical bayes approach estimating multiple kernel parameters gaussian process model classiﬁcation regression handle many tuning parameters based much transparent principles maximizing marginal likelihood 
[kernels, kernels, building, generative, models, locally, weighted, regression] deﬁne rewrite prediction made kernel regression follows note need smoothing kernel longer need normalization term write model essentially ﬁtting constant function locally improve ﬁtting linear regression model point solving min called locally weighted regression example method loess aka lowess stands locally weighted scatterplot smoothing cleveland devlin see also edakunni bayesian version model compute paramters test case solving following weighted least squares problem design matrix diag corresponding prediction form term combines local smoothing kernel effect linear regression called equivalent kernel see also section 
[kernels, exercises] exercise fitting svm classiﬁer hand source jaakkola consider dataset points consider mapping point using feature vector equivalent kernels building generative models using second order polynomial kernel max margin classiﬁer form min write vector parallel optimal vector hint recall figure apr version perpendicular decision boundary two points feature space value margin achieved hint recall margin distance support vector decision boundary hint think geometry points space line separating one solve using fact margin equal solve using value equations hint points decision boundary inequalities tight write form discriminant function explicit function exercise linear separability source koller consider ﬁtting svm dataset linearly separable resulting decision boundary guaranteed separate classes 
[gaussian, processes, introduction] supervised learning observe inputs outputs assume unknown function possibly corrupted noise optimal approach infer distribution functions given data use make predictions given new inputs compute focussed parametric representations function instead inferring infer chapter discuss way perform bayesian inference functions approach based gaussian processes gps deﬁnes prior functions converted posterior functions seen data although might seem difficult represent distribution function turns need able deﬁne distribution function values ﬁnite arbitrary set points say assumes jointly gaussian mean covariance given positive deﬁnite kernel function see section information kernels key idea deemed kernel similar expect output function points similar see figure illustration turns regression setting computations done closed form time discuss faster approximations section classiﬁcation setting must use approximations gaussian approximation since posterior longer exactly gaussian gps thought bayesian alternative kernel methods discussed chap ter including lvm rvm svm although methods sparser therefore faster give well calibrated probabilistic outputs see section discussion properly tuned probabilistic output important certain applications online tracking vision robotics fox reinforcement learning optimal control engel deisenroth global optimization non convex functions mockus lizotte brochu experiment design santner etc chapter gaussian processes figure gaussian process training points testing point represented mixed directed undirected graphical model representing hidden nodes represent value function data points hidden nodes fully interconnected undirected edges forming gaussian graphical model edge strengths represent covariance terms test point similar training points predicted output similar presentation closely based rasmussen williams con sulted futher details see also diggle ribeiro discusses related approach known kriging widely used spatial statistics literature 
[gaussian, processes, gps, regression] section discuss gps regression let prior regression function denoted mean function kernel covariance function obviously require positive deﬁnite kernel ﬁnite set points process deﬁnes joint gaussian note common use mean function since ﬂexible enough model mean arbitrarily well see however section consider parametric models mean function model residual errors semi parametric approach combines interpretability parametric models accuracy non parametric models gps regression figure left functions sampled prior kernel right samples posterior conditioning noise free observations shaded area represents std based figure rasmussen williams figure generated gprdemonoisefree 
[gaussian, processes, gps, regression, predictions, using, noise-free, observations] suppose observe training set noise free observation function evaluated given test set size want predict function outputs ask predict value already seen want return answer uncertainty words act interpolator training data happen assume observations noiseless consider case noisy observations return prediction problem deﬁnition joint distribution following form standard rules conditioning gaussians section posterior following form process illustrated figure left show sample samples prior use squared exponential kernel aka gaussian kernel rbf kernel given exp controls horizontal length scale function varies controls vertical variation discuss estimate kernel parameters right chapter gaussian processes show samples posterior see model perfectly interpolates training data predictive uncertainty increases move away observed data one application noise free regression computationally cheap proxy behavior complex simulator weather forecasting program simulator stochastic deﬁne mean output note still observation noise one estimate effect changing simulator parameters examining effect predictions rather run simulator many times may prohibitively slow strategy known dace stands design analysis computer experiments santner 
[gaussian, processes, gps, regression, predictions, using, noisy, observations] let consider case observe noisy version underlying function case model required interpolate data must come close observed data covariance observed noisy responses cov words cov second matrix diagonal assumed noise terms independently added observation joint density observed data latent noise free function test points given assuming mean zero notational simplicity hence posterior predictive density case single test input simpliﬁes follows another way write posterior mean follows revisit expression later gps regression figure gps kernels different hyper parameters noisy observations kernel form equation hyper parameters follows based figure rasmussen williams figure generated gprdemochangehparams written carl rasmussen 
[gaussian, processes, gps, regression, effect, kernel, parameters] predictive performance gps depends exclusively suitability chosen kernel suppose choose following squared exponential kernel noisy observations exp horizontal scale function changes controls vertical scale function noise variance figure illustrates effects changing parameters sampled noisy data points kernel using made predictions various parameters conditional data figure use result good figure reduce length scale parameters optimized maximum marginal likelihood technique discuss function looks wiggly also uncertainty goes faster since effective distance training points increases rapidly figure increase length scale function looks smoother chapter gaussian processes input input output input input output input input output figure functions sampled kernel different hyper parameters kernel form equation diag diag based figure rasmussen williams figure generated gprdemoard written carl rasmussen extend kernel multiple dimensions follows exp deﬁne matrix several ways simplest use isotropic matrix see figure example also endow dimension characteristic length scale diag length scales become large corresponding feature dimension deemed irrelevant ard section figure use function changes faster along direction direction also create matrix form diag matrix rasmussen williams calls factor analysis distance function analogy fact factor analysis section approximates covariance matrix low rank matrix plus diagonal matrix columns correspond relevant directions input space figure use function changes mostly rapidly direction perpendicular gps regression 
[gaussian, processes, gps, regression, estimating, kernel, parameters] estimate kernel parameters could use exhaustive search discrete grid values validation loss objective quite slow approach used tune kernels used svms consider empirical bayes approach allow use continuous optimization methods much faster particular maximize marginal likelihood since marginal likelihood given log log log log ﬁrst term data term second term model complexity term third term constant understand tradeoff ﬁrst two terms consider kernel vary length scale hold ﬁxed let log short length scales good small however model complexity high almost diagonal figure top right since points considered near others log large long length scales poor model complexity low almost figure bottom right log small discuss maximize marginal likelhiood let kernel parameters also called hyper parameters denoted one show log takes time compute time per hyper parameter compute gradient form depends form kernel parameter taking derivatives respect often constraints hyper parameters case deﬁne log use chain rule given expression log marginal likelihood derivative estimate kernel parameters using standard gradient based optimizer however since objective convex local minima problem illustrate example consider figure use kernel equation plot log data points shown panels vary two reason called marginal likelihood rather likelihood marginalized latent gaussian vector moves one level bayesian hierarchy reduces chances overﬁtting number kernel parameters usually fairly small compared standard parametric model chapter gaussian processes characteristic lengthscale noise standard deviation input output input output figure illustration local minima marginal likelihood surface plot log marginal likelihood ﬁxed using data points shown panels function corresponding lower left local minimum quite wiggly low noise function corresponding top right local minimum quite smooth high noise data generated using source figure rasmussen williams figure generated gprdemomarglik written carl rasmussen local optima indicated bottom left optimum corresponds low noise short length scale solution shown panel top right optimum corresponds high noise long length scale solution shown panel data points enough evidence conﬁdently decide reasonable although complex model panel marginal likelihood higher simpler model panel data map estimate come dominate figure illustrates interesting typical features region top panel corresponds case noise high regime marginal likelihood insensitive length scale indicated horizontal contours since data explained noise region left hand side panel corresponds case length scale short regime marginal likelihood insensitive noise level since data perfectly interpolated neither regions would chosen good optimizer gps regression log length scale log magn log length scale log magn log length scale log magn figure three different approximations posterior hyper parameters grid based monte carlo central composite design source figure vanhatalo used kind permission jarno vanhatalo bayesian inference hyper parameters alternative computing point estimate hyper parameters compute poste rior let represent kernel parameters well dimensionality small compute discrete grid possible values centered map estimate computed approximate posterior latent variables using denotes weight grid point higher dimensions regular grid suffers curse dimensionality obvious alternative monte carlo slow another approach use form quasi monte carlo whereby place grid points mode distance mode along dimension total points called central composite design rue also used unscented kalman ﬁlter see section make gaussian like approximation reasonable often log transform hyper parameters see figure illustration chapter gaussian processes multiple kernel learning quite different approach optimizing kernel parameters known multiple kernel learning idea deﬁne kernel weighted sum base kernels optimize weights instead kernel parameters particularly useful different kinds data wish fuse together see rakotomamonjy approach based risk minimization convex optimization girolami rogers approach based variational bayes 
[gaussian, processes, gps, regression, computational, numerical, issues] predictive mean given reasons numerical stability unwise directly invert robust alternative compute cholesky decomposition compute predictive mean variance log marginal likelihood shown pseudo code algorithm based rasmussen williams takes time compute cholesky decomposition time solve compute mean using time variance using time test case alternative cholesky decomposition solve linear system using conjugate gradients terminate algorithm iterations takes time run gives exact solution time another approach approximate matrix vector multiplies needed using fast gauss transform yang however scale high dimensional inputs see also section discussion speedup techniques algorithm regression cholesky var log log log 
[gaussian, processes, gps, regression, semi-parametric, gps] sometimes useful use linear model mean process follows models residuals combines parametric non parametric model known semi parametric model assume integrate parameters get new hagan gps meet glms log log log log sigm log table likelihood gradient hessian binary logistic probit regression assume deﬁne sigm logistic regression probit regression also pdf cdf rasmussen williams integrating corresponding predictive distribution test inputs following form rasmussen williams cov cov predictive mean output linear model plus correction term due predictive covariance usual covariance plus extra term due uncertainty 
[gaussian, processes, gps, meet, glms] section extend gps glm setting focussing classiﬁcation case bayesian logistic regression main difficulty gaussian prior conjugate bernoulli multinoulli likelihood several approximations one adopt gaussian approximation section expectation propagation kuss rasmussen nickisch rasmussen variational girolami rogers opper archambeau mcmc neal christensen etc focus gaussian approximation since simplest fastest 
[gaussian, processes, gps, meet, glms, binary, classiﬁcation] binary case deﬁne model following rasmussen williams assume let sigm logistic regression probit regression regression assume computing posterior deﬁne log unnormalized posterior follows log log log log log chapter gaussian processes let function want minimize gradient hessian given log log note log diagonal matrix data iid conditional expressions gradient hessian log likelihood logit probit case given sections summarized table use irls ﬁnd map estimate update form new log log convergence gaussian approximation posterior takes following form computing posterior predictive compute posterior predictive first predict latent function test case mean used equation get mean given noise free compute predictive variance use rule iterated variance var var var probabilities conditioned equation var equation var var cov combining get var cov equation cov using matrix inversion lemma get var gps meet glms summary var convert predictive distribution binary responses use approximated using methods discussed section discussed bayesian logistic regression example using probit approximation sec tion sigm var computing marginal likelihood need marginal likelihood order optimize kernel parameters using laplace approximation equation log log const hence log log log log computing derivatives log complex regression case since well depend details found rasmussen williams numerically stable computation implement equations numerically stable way best avoid inverting rasmussen williams suggest deﬁning eigenvalues bounded max hence safely inverted one use matrix inversion lemma show hence irls update becomes new log chapter gaussian processes cholesky decomposition ﬁtting algorithm takes time space number newton iterations convergence evaluate log marginal likelihood equa tion using log log log exploited fact compute predictive distribution rather using exploit fact mode log hence rewrite predictive mean follows log compute predictive variance exploit fact get var compute whole algorithm summarized algorithm based rasmussen williams fitting takes time prediction takes time number test cases example figure show synthetic binary classiﬁcation problem use kernel left show predictions using hyper parameters set hand use short length scale hence sharp turns decision boundary right show predictions using learned hyper parameters model favors parsimonious explanation data 
[gaussian, processes, gps, meet, glms, multi-class, classiﬁcation] section consider model form cat assume thus one latent function per class priori independent may use different kernels use gaussian approximation posterior similar model using multinomial probit function instead multinomial logit described girolami rogers see training points well predicted model log contribute strongly prediction test points similar behavior support vectors svm see section gps meet glms algorithm binary classiﬁcation using gaussian approximation first compute map estimate using irls repeat log cholesky log converged log log log perform prediction log var sigm var kernel kernel figure contours posterior predictive probability red circle class generated kernel thick black line decision boundary threshold probability manual parameters short length scale learned parameters long length scale figure generated gpcdemod based code carl rasmussen chapter gaussian processes computing posterior unnormalized log posterior given log exp log log dummy encoding layout also block diagonal matrix containing models correlation latent function gradient hessian given diag matrix obtained stacking diag vertically compare expressions standard logistic regression section use irls compute mode newton step form new naively implementing would take time however reduce shown rasmussen williams computing posterior predictive compute posterior predictive manner analogous section mean latent response put vector form writing using similar argument equation show covariance latent response given cov diag gps meet glms diagonal matrix compute posterior predictive visible response need use cat cov use deterministic approximations softmax function discussed sec tion compute alternatively use monte carlo computing marginal likelihood using arguments similar binary case show log log exp log optimized numerically usual way numerical computational issues one implement model ﬁtting time space number newton iterations using techniques described rasmussen williams prediction takes time number test cases 
[gaussian, processes, gps, meet, glms, gps, poisson, regression] section illustrate gps poisson regression interesting application spatial disease mapping example vanhatalo discuss problem modeling relative risk heart attack different regions finland data consists heart attacks finland aggregated lattice cells model following form poi known expected number deaths related population cell overall death rate relative risk cell want infer since data counts small regularize problem sharing information spatial neighbors hence assume log use matern kernel length scale magnitude estimated data figure gives example kind output one obtain method based data locations left plot posterior mean relative risk right posterior variance see higher eastern finland consistent studies also see variance north higher since fewer people living chapter gaussian processes posterior mean relative risk fic posterior variance relative risk fic figure show relative risk heart disease finland using poisson left posterior mean right posterior variance figure generated gpspatialdemolaplace written jarno vanhatalo 
[gaussian, processes, connection, methods] variety methods statistics machine learning closely related regression classiﬁcation give brief review 
[gaussian, processes, connection, methods, linear, models, compared, gps] consider bayesian linear regression dimensional features prior weights posterior predictive distribution given following one show rewrite distribution follows xσx deﬁned xσx size since features ever appear form xσx kernelize expression deﬁning thus see bayesian linear regression equivalent covariance function note however degenerate covariance function since non zero eigenvalues intuitively reﬂects fact model represent limited number functions result underﬁtting since model ﬂexible enough capture data perhaps worse result overconﬁdence since connection methods model prior impoverished posterior become concentrated model wrong think right 
[gaussian, processes, connection, methods, linear, smoothers, compared, gps] linear smoother regression function linear function training outputs called weight function silverman confuse linear model output linear function input vector variety linear smoothers kernel regression section locally weighted regression section smoothing splines section regression see regession linear smoother note mean posterior predictive distribution given kernel regression derive weight function smoothing kernel rather mercer kernel clear weight function local support case things clear since weight function depends inverse certain kernel functions analytically derive form known equivalent kernel silverman one show although may computing linear combination convex combination interestingly local function even original kernel used local futhermore effective bandwidth equivalent kernel automatically decreases sample size increases whereas kernel smoothing bandwidth needs set hand adapt see rasmussen williams sec sec details degrees freedom linear smoothers clear method called linear called smoother best explained terms gps consider prediction training set let eigendecomposition since real symmetric positive deﬁnite eigenvalues real non negative eigenvectors orthonormal let rewrite equation follows chapter gaussian processes equation except working eigenvectors gram matrix instead data matrix case interpretation similar corresponding basis function much inﬂuence consequently high frequency components smoothed effective degrees freedom linear smoother deﬁned dof speciﬁes wiggly curve 
[gaussian, processes, connection, methods, svms, compared, gps] saw section svm objective binary classiﬁcation given equation also know equation optimal solution form kernelizing get equation absorbing term one kernels hence svm objective rewritten compare map estimation classiﬁer log tempting think convert svm ﬁguring likelihood would equivalent hinge loss however turns likelihood sollich although pseudo likelihood matches svm see section figure saw hinge loss logistic loss well probit loss quite similar main difference hinge loss strictly errors larger gives rise sparse solution section discussed ways derive sparse kernel machines discuss connection methods gps 
[gaussian, processes, connection, methods, lvm, rvms, compared, gps] sparse kernel machines linear models basis function expansion form section know equivalent following kernel connection methods diag kernel function two interesting properties first degenerate meaning non zero eigenvalues joint distribution highly constrained second kernel depends training data cause model overconﬁdent extrapolating beyond training data see consider point far outside convex hull data basis functions values close prediction back mean worryingly variance back noise variance contrast using non degenerate kernel function predictive variance increases move away training data desired see rasmussen quiñonero candela discussion 
[gaussian, processes, connection, methods, neural, networks, compared, gps] section discuss neural networks nonlinear generalization glms binary classiﬁcation case neural network deﬁned logistic regression model applied logistic regression model ber sigm sigm turns interesting connection neural networks gaussian processes ﬁrst pointed neal explain connection follow presentation rasmussen williams consider neural network regression one hidden layer form offset bias term output weight hidden unit response inputs weights unit input hidden unit activation function typically sigmoid tanh function smooth function let use following priors weights unspeciﬁed denoting weights last equality follows since hidden units iid let scale since hidden units increase input ﬁnal node scale magnitude weights last term becomes sum iid random variables assuming bounded apply central limit theorem result get gaussian process chapter gaussian processes input input input output figure covariance function samples kernel using various values figure generated gpnndemo written chris williams use activation transfer function erf erf choose williams showed covariance kernel form sin true neural network kernel unlike sigmoid kernel tanh positive deﬁnite figure illustrates kernel diag figure shows functions sampled corresponding equivalent functions superpositions erf random increases variance increases function varies quickly unlike rbf kernel functions sampled kernel tend away data rather tend remain value edge data suppose use rbf network equivalent hidden unit activation function form exp one show coresponding kernel equivalent rbf kernel 
[gaussian, processes, connection, methods, smoothing, splines, compared, gps] smoothing splines widely used non parametric method smoothly interpolating data green silverman special case gps see usually used input dimensional univariate splines basic idea function minimizing discrepancy data plus smoothing term penalizes functions wiggly penalize derivative connection methods function objective becomes one show green silverman solution piecewise polynomial polynomials order interior bins denoted order two outermost intervals example get natural cubic spline series truncated cubic polynomials whose left hand sides located training points fact model linear edges prevents extrapolating wildly beyond range data drop requirement get unrestricted spline clearly model using ridge regression columns however also derive time method green silverman sec regression splines general place polynomials ﬁxed set locations known knots denoted result called regression spline parametric model uses basis function expansion following form drop interior exterior distinction simplicity choosing number locations knots like choosing number values support vectors section impose regularizer regression coefficients method known penalized splines see section practical example penalized splines connection gps one show rasmussen williams cubic spline map estimate following function chapter gaussian processes penalize zero ﬁrst derivatives note kernel equation rather unnatural indeed posterior samples resulting rather unsmooth however posterior mode mean smooth shows regularizers always make good priors input thin plate splines one generalize cubic splines input deﬁning regularizer following form one show solution form log known thin plate spline equivalent map estimation whose kernel deﬁned williams fitzgibbon higher dimensional inputs hard analytically solve form optimal solution using higher order inputs however parametric regression spline setting forego regularizer freedom deﬁning basis functions one way handle multiple inputs use tensor product basis deﬁned cross product basis functions example input deﬁne clear high dimensional data cannot allow higher order interactions many parameters one approach problem use search procedure look useful interaction terms known mars stands multivariate adaptive regression splines see section details 
[gaussian, processes, connection, methods, rkhs, methods, compared, gps] generalize idea penalizing derivatives functions used smoothing splines functions general notion smoothness recall section connection methods mercer theorem says positive deﬁnite kernel function represented terms eigenfunctions form orthormal basis function space deﬁne inner product two functions space follows xef exercise show deﬁnition implies xeκ called reproducing property space functions called reproducing kernel hilbert space rkhs consider optimization problem form norm function xef intuition functions complex wrt kernel large norms need many eigenfunctions represent want pick simple function provides good data one show see schoelkopf smola solution must form known representer theorem holds convex loss functions besides squared error solve substituting using reproducing property get chapter gaussian processes minimizing wrt ﬁnd hence identical equation posterior mean predictive distribution indeed since mean mode gaussian see linear regresson rkhs regularizer equivalent map estimation analogous statement holds logistic regression case also uses convex likelihood loss function 
[gaussian, processes, latent, variable, model] section discussed kernel pca applies kernel trick regular pca section discuss different way combine kernels probabilistic pca resulting method known lvm stands gaussian process latent variable model lawrence explain method start ppca recall section ppca model follows model maximum likelihood integrating maximizing objective given exp showed theorem mle computed terms eigenvectors consider dual problem whereby maximize integrate use prior form corresponding likelihood becomes exp based discussion connection eigenvalues section come surprise also solve dual problem using eigenvalue methods see lawrence details use linear kernel recover pca also use general kernel gram matrix mle longer available latent variable model figure representation dimensional oil ﬂow data different colors symbols represent phases oil ﬂow kernel pca gaussian kernel lvm gaussian kernel shading represents precision posterior lighter pixels higher precision figure lawrence used kind permission neil lawrence via eigenvalue methods instead must use gradient based optimization objective given log gradient given form course depend kernel used example linear kernel pass gradient standard optimizer conjugate gradient descent let compare lvm kernel pca kpca learn kernelized mapping observed space latent space whereas lvm learn kernelized mapping latent space observed space figure illustrates results applying kpca lvm visualize dimensional oil ﬂow data shown figure see embedding produced lvm far better perform nearest neighbor classiﬁcation latent space lvm makes errors kernel pca kernel separately optimized hyper parameters makes errors regular pca makes errors lvm inherits usual advantages probabilistic generative models ability handle missing data data different types ability use gradient based methods instead grid search tune kernel parameters ability handle prior information chapter gaussian processes etc discussion probabilistic methods spectral dimensionality reduction see lawrence 
[gaussian, processes, approximation, methods, large, datasets] principal drawback gps take time use need invert compute cholesky decomposition kernel matrix variety approximation methods devised take time user speciﬁable parameter details see quinonero candela 
[gaussian, processes, exercises] exercise reproducing property prove equation 
[adaptive, basis, function, models, introduction] chapters discussed kernel methods provide powerful way create non linear models regression classiﬁcation prediction takes form deﬁne either training data subset models form essen tially perform form template matching whereby compare input stored prototypes although work well relies good kernel function measure similarity data vectors often coming good kernel function quite difficult example deﬁne similarity two images pixel wise comparison intensities gaussian kernel corresponds work well although possible indeed common hand engineer kernels speciﬁc tasks see pyramid match kernel section would interesting could learn kernel section discussed way learn parameters kernel function maxi mizing marginal likelihood example use ard kernel exp estimate thus perform form nonlinear feature selection however methods computationally expensive another approach known multiple kernel learning see rakotomamonjy uses convex combination base kernels estimates mixing weights relies good base kernels also computationally expensive alternative approach dispense kernels altogether try learn useful features directly input data create call adaptive basis function model abm model form chapter adaptive basis function models basis function learned data framework covers models discuss chapter typically basis functions parametric write parameters basis function use denote entire parameter set resulting model linear parameters anymore able compute locally optimal mle map estimate nevertheless models often signiﬁcantly outperform linear models see 
[adaptive, basis, function, models, classiﬁcation, regression, trees, cart] classiﬁcation regression trees cart models also called decision trees confused decision trees used decision theory deﬁned recursively partitioning input space deﬁning local model resulting region input space represented tree one leaf per region explain 
[adaptive, basis, function, models, classiﬁcation, regression, trees, cart, basics] explain cart approach consider tree figure ﬁrst node asks less threshold yes ask less threshold yes bottom left quadrant space ask less result axis parallel splits partition space regions shown figure associate mean response regions resulting piecewise constant surface shown figure write model following form region mean response region encodes choice variable split threshold value path root leaf makes clear cart model adaptive basis function model basis functions deﬁne regions weights specify response value region discuss ﬁnd basis functions generalize classiﬁcation setting storing distribution class labels leaf instead mean response illustrated figure model used classify data figure example ﬁrst check color object blue follow left branch end leaf labeled means positive examples negative examples match criterion hence predict blue red check shape ellipse end leaf labeled predict red ellipse predict colour check size less predict otherwise probabilities empirical fraction positive examples satisfy conjunction feature values deﬁnes path root leaf classiﬁcation regression trees cart figure simple regression tree two inputs based figure hastie figure generated regtreesurfacedemo shape color size figure simple decision tree data figure leaf labeled means positive examples match path negative examples tree leaves pure meaning examples one class exception leaf representing red ellipses label distribution could distinguish positive negative red ellipses adding test based size however always desirable construct trees perfectly model training data due overﬁtting 
[adaptive, basis, function, models, classiﬁcation, regression, trees, cart, growing, tree] finding optimal partitioning data complete hyaﬁl rivest common use greedy procedure shown algorithm compute locally optimal mle method used cart breiman quinlan quinlan three popular implementations method see dtfit simple matlab implementation split function chooses best feature best value feature follows arg min min cost cost chapter adaptive basis function models algorithm recursive procedure grow classiﬁcation regression tree function ﬁttree node depth node prediction mean class label distribution split worthsplitting depth cost return node else node test anonymous function node left ﬁttree node depth node right ﬁttree node depth return node cost function given dataset deﬁned notational simplicity assumed inputs real valued ordinal makes sense compare feature numeric value set possible thresholds feature obtained sorting unique values example feature values set case categorical inputs common approach consider splits form possible class label although could allow multi way splits resulting non binary trees would result data fragmentation meaning little data might fall subtree resulting overﬁtting function checks node worth splitting use several stopping heuristics following reduction cost small typically deﬁne gain using feature normalized measure reduction cost cost cost cost tree exceeded maximum desired depth distribution response either sufficiently homogeneous labels distribution pure number examples either small remains specify cost measure used evaluate quality proposed split depends whether goal regression classiﬁcation discuss cases regression cost regression setting deﬁne cost follows cost classiﬁcation regression trees cart mean response variable speciﬁed set data alternatively linear regression model leaf using inputs features chosen path root measure residual error classiﬁcation cost classiﬁcation setting several ways measure quality split first multinoulli model data leaf satisfying test estimating class conditional probabilities follows data leaf given several common error measures evaluating proposed partition misclassiﬁcation rate deﬁne probable class label argmax corresponding error rate entropy deviance log note minimizing entropy equivalent maximizing information gain quinlan test class label deﬁned infogain log log since mle distribution categorical use tests form taking expectations values gives mutual information infogain infogain chapter adaptive basis function models error rate gini entropy figure node impurity measures binary classiﬁcation horizontal axis corresponds probability class entropy measure rescaled pass based figure hastie figure generated ginidemo gini index expected error rate see note probability random entry leaf belongs class probability would misclassiﬁed two class case misclassiﬁcation rate max entropy gini index plotted figure see cross entropy gini measures similar sensitive changes class probability misclassiﬁcation rate example consider two class problem cases class suppose one split created nodes created nodes splits produce misclassiﬁcation rate however latter seems preferable since one nodes pure contains one class cross entropy gini measures favor latter choice example example consider two four features class iris dataset shown fig ure resulting tree shown figure decision boundaries shown figure see tree quite complex resulting decision boundaries figure show estimate error much higher training set error indicating overﬁtting discuss perform tree pruning stage simplify tree classiﬁcation regression trees cart sepal length sepal width setosa versicolor virginica unpruned decision tree versicolor setosa virginica figure iris data show ﬁrst two features sepal length sepal width ignore petal length petal width decision boundaries induced decision tree figure versicolor setosa setosa virginica versicolor versicolor versicolor versicolor versicolor virginica virginica virginica versicolor versicolor virginica virginica versicolor versicolor virginica number terminal nodes cost misclassification error cross validation training set min std err best choice figure unpruned decision tree iris data plot misclassiﬁcation error rate depth tree figure generated dtreedemoiris 
[adaptive, basis, function, models, classiﬁcation, regression, trees, cart, pruning, tree] prevent overﬁtting stop growing tree decrease error sufficient justify extra complexity adding extra subtree however tends myopic example xor data figure would might never make splits since feature little predictive power standard approach therefore grow full tree perform pruning done using scheme prunes branches giving least increase error see breiman details determine far prune back evaluate cross validated error subtree pick tree whose error within standard error minimum illustrated figure point minimum error corresponds simple tree figure chapter adaptive basis function models versicolor setosa virginica versicolor setosa pruned decision tree versicolor setosa virginica figure pruned decision tree iris data figure generated dtreedemoiris 
[adaptive, basis, function, models, classiﬁcation, regression, trees, cart, pros, cons, trees] cart models popular several reasons easy interpret easily handle mixed discrete continuous inputs insensitive monotone transformations inputs split points based ranking data points perform automatic variable selection relatively robust outliers scale well large data sets modiﬁed handle missing inputs however cart models also disadvantages primary one predict accurately compared kinds model part due greedy nature tree construction algorithm related problem trees unstable small changes input data large effects structure tree due hierarchical nature tree growing process causing errors top affect rest tree frequentist terminology say trees high variance estimators discuss solution 
[adaptive, basis, function, models, classiﬁcation, regression, trees, cart, random, forests] one way reduce variance estimate average together many estimates example train different trees different subsets data chosen randomly postprocess tree derive series logical rules quinlan standard heuristic handling missing inputs decision trees look series backup variables induce similar partition chosen variable given split used case chosen variable unobserved test time called surrogate splits method ﬁnds highly correlated features thought learning local joint model input advantage generative model modeling entire joint distribution inputs disadvantage entirely hoc simpler approach applicable categorical variables code missing new value treat data fully observed classiﬁcation regression trees cart replacement compute ensemble tree technique called bagging breiman stands bootstrap aggregating unfortunately simply running learning algorithm different subsets data result highly correlated predictors limits amount variance reduction possible technique known random forests breiman tries decorrelate base learners learning trees based randomly chosen subset input variables well randomly chosen subset data cases models often good predictive accuracy caruana niculescu mizil widely used many applications body pose recognition using microsoft popular kinect sensor shotton bagging frequentist concept also possible adopt bayesian approach learning trees particular chipman denison perform approximate inference space trees structure parameters using mcmc reduces variance predictions also perform bayesian inference space ensembles trees tends work much better known bayesian adaptive regression trees bart chipman note cost sampling based bayesian methods comparable sampling based random forest method approaches farily slow train produce high quality classiﬁers unfortunately methods use multiple trees whether derived bayesian frequen tist standpoint lose nice interpretability properties fortunately various post processing measures applied discussed section 
[adaptive, basis, function, models, classiﬁcation, regression, trees, cart, cart, compared, hierarchical, mixture, experts] interesting alternative decision tree known hierarchical mixture experts figure gives illustration two levels experts thought probabilistic decision tree depth since recursively partition space apply different expert partition hastie hastie write hme approach promising competitor cart trees advantages include following model partition input space using set nested linear decision boundaries contrast standard decision trees constrained use axis parallel splits model makes predictions averaging experts contrast standard decision tree predictions made based model corresponding leaf since leaves often contain training examples result overﬁtting fitting hme involves solving smooth continuous optimization problem usually using likely less prone local optima standard greedy discrete optimization methods used decision trees similar reasons computationally easier bayesian parameters hme see peng bishop chapter adaptive basis function models svensén structure parameters decision tree see 
[adaptive, basis, function, models, generalized, additive, models] simple way create nonlinear model multiple inputs use generalized additive model hastie tibshirani model form modeled scatterplot smoother mapped using link function glm hence term generalized additive model use regression splines ﬁxed basis function expansion approach written whole model written however common use smoothing splines section case objective regression setting becomes strength regularizer 
[adaptive, basis, function, models, generalized, additive, models, backﬁtting] discuss model using mle constant uniquely identiﬁable since always add subtract constants functions convention assume case mle rest model center responses subtracting iteratively update turn using target vector residuals obtained omitting term smoother ensure output zero mean using called backﬁtting algorithm hastie tibshirani full column rank objective convex since smoothing spline linear operator shown section procedure guaranteed converge global optimum glm case need modify method somewhat basic idea replace weighted least squares step irls see section weighted backﬁtting algorithm logistic regression case response weight associated sigm generalized additive models 
[adaptive, basis, function, models, generalized, additive, models, computational, efficiency] call smoother takes time total cost number iterations high dimensional inputs ﬁtting gam expensive one approach combine sparsity penalty see spam sparse additive model approach ravikumar alternatively use greedy approach boosting see section 
[adaptive, basis, function, models, generalized, additive, models, multivariate, adaptive, regression, splines, mars] extend gams allowing interaction effects general create anova decomposition jkl course cannot allow many higher order interactions many parameters common use greedy search decide variables add multivariate adaptive regression splines mars algorithm one example hastie sec ﬁts models form equation uses tensor product basis regression splines represent multidimensional regression functions example input might use create function start set candidate basis functions form linear splines knots observed values variable consider splines sloping directions called reﬂecting pair see figure let represent current set basis functions initialize using consider creating new basis function pair multplying one reﬂecting pairs example might initially get obtained multiplying reﬂecting pair involving knot pair added see figure next step might create model obtained multiplying new reﬂecting pair new function shown figure chapter adaptive basis function models figure linear spline function knot solid blue dotted red mars model given equation simple mars model given equation figure generated marsdemo proceed way model becomes large may impose upper bound order interactions prune backwards step eliminating basis function causes smallest increase residual error error stops improving whole procedure closely related cart see suppose replace piecewise linear basis functions step functions multiplying pair reﬂected step functions equivalent splitting node suppose impose constraint variable involved multiplication candidate term variable gets replaced interaction original variable longer available ensures variable split thus guaranteeing resulting model represented tree case mars growing strategy cart growing strategy 
[adaptive, basis, function, models, boosting] boosting schapire freund greedy algorithm ﬁtting adaptive basis function models form equation generated algorithm called weak learner base learner algorithm works applying weak learner sequentially weighted versions data weight given examples misclassiﬁed earlier rounds weak learner classiﬁcation regression algorithm common use cart model late leo breiman called boosting weak learner shallow decision tree best shelf classiﬁer world hastie supported extensive empirical comparison different classiﬁers caruana niculescu mizil showed boosted decision trees best terms misclassiﬁcation error terms producing well calibrated probabilities judged roc curves second best method random forests invented breiman see section contrast single decision trees performed poorly boosting originally derived computational learning theory literature schapire freund schapire focus binary classiﬁcation papers proved one could boost performance training set weak learner arbitrarily boosting train test figure performance adaboost using decision stump weak learner data figure training solid blue test dotted red error number iterations figure generated boostingdemo written richard stapenhurst high provided weak learner could always perform slightly better chance example figure plot training test error boosted decision stumps dataset shown figure see training set error rapidly goes near zero surprising test set error continues decline even training set error reached zero although test set error eventually thus boosting resistant overﬁtting boosted decision stumps form basis successful face detector viola jones used generate results figure used many digital cameras view stunning empirical success statisticians started become interested method breiman breiman showed boosting interpreted form gradient descent function space view extended friedman showed boosting could extended handle variety loss functions including regression robust regression poisson regression etc section shall present statistical inter pretation boosting drawing reviews buhlmann hothorn hastie consulted details 
[adaptive, basis, function, models, boosting, forward, stagewise, additive, modeling] goal boosting solve following optimization problem min loss function assumed abm model equation common choices loss function listed table use squared error loss optimal estimate given argmin chapter adaptive basis function models name loss derivative algorithm squared error lboosting absolute error sgn median gradient boosting exponential loss exp exp log adaboost logloss log log logitboost table commonly used loss functions gradients population minimizers algorithms minimize loss binary classiﬁcation problems assume sigm regression problems assume adapted hastie buhlmann hothorn loss logloss exp figure illustration various loss functions binary classiﬁcation horizontal axis margin vertical axis loss log loss uses log base figure generated hingelossplot showed section course cannot computed practice since requires knowing true conditional distribution hence sometimes called population minimizer expectation interpreted frequentist sense see boosting try approximate conditional expectation binary classiﬁcation obvious loss loss differentiable instead common use logloss convex upper bound loss showed section case one show optimal estimate given log one generalize framework multiclass case discuss alternative convex upper bound exponential loss deﬁned exp see figure plot computational advantages logloss discussed turns optimal estimate loss also boosting log see set derivative expected loss zero cases see boosting try approximate half log odds ratio since ﬁnding optimal hard shall tackle sequentially initialise deﬁning arg min example use squared error set use log loss exponential loss set log could also use powerful model baseline glm iteration compute argmin set key point back adjust earlier parameters method called forward stagewise additive modeling continue ﬁxed number iterations fact main tuning parameter method often pick monitoring performance separate validation set stopping performance starts decrease called early stopping alternatively use model selection criteria aic bic see buhlmann hothorn details practice better test set performance obtained performing partial updates form step size parameter practice common use small value called shrinkage discuss solve suproblem equation depend form loss function however independent form weak learner 
[adaptive, basis, function, models, boosting, lboosting] suppose used squared error loss step loss form chapter adaptive basis function models figure example adaboost using decision stump weak learner degree blackness represents conﬁdence red class degree whiteness represents conﬁdence blue class size datapoints represents weight decision boundary yellow round rounds rounds figure generated boostingdemo written richard stapenhurst current residual set without loss generality hence ﬁnd new basis function using weak learner predict called lboosting least squares boosting buhlmann section see method suitable choice weak learner made give results lars used perform variable selection see section 
[adaptive, basis, function, models, boosting, adaboost] consider binary classiﬁcation problem exponential loss step minimize exp exp exp weight applied datacase rewrite objective follows consequently optimal function add argmin found applying weak learner weighted version dataset weights subsituting solving ﬁnd log err err boosting err overall update becomes weights next iteration become exploited fact otherwise since cancel normalization step drop result algorithm shown algorithm known adaboost example algorithm action using decision stumps weak learner given figure see many iterations carve complex decision boundary rather surprising adaboost slow overﬁt apparent figure see section discussion point algorithm adaboost binary classiﬁcation exponential loss fit classiﬁer training set using weights compute err compute log err err set exp return sgn 
[adaptive, basis, function, models, boosting, logitboost] trouble exponential loss puts lot weight misclassiﬁed examples apparent exponential blowup left hand side figure makes method sensitive outliers mislabeled examples addition logarithm pmf binary variables consequently cannot recover probability estimates friedman called discrete adaboost since assumes base classiﬁer returns binary class label returns probability instead modiﬁed algorithm known real adaboost used see friedman details chapter adaptive basis function models natural alternative use logloss instead punishes mistakes linearly clear figure furthermore means able extract probabilities ﬁnal learned function using goal minimze expected log loss given log exp performing newton upate objective similar irls one derive algorithm shown algorithm known logitboost friedman generalized multi class setting explained friedman algorithm logitboost binary classiﬁcation log loss compute working response compute weights argmin update compute exp return sgn 
[adaptive, basis, function, models, boosting, boosting, functional, gradient, descent] rather deriving new versions boosting every different loss function possible derive generic version known gradient boosting friedman mason explain imagine minimizing argmin parameters solve stagewise using gradient descent step let gradient evaluated gradients common loss functions given table make update boosting step length chosen argmin called functional gradient descent current form much use since optimizes ﬁxed set points learn function generalize however modify algorithm ﬁtting weak learner approximate negative gradient signal use update argmin overall algorithm summarized algorithm omitted line search step strictly necessary argued buhlmann hothorn algorithm gradient boosting initialize argmin compute gradient residual using use weak learner compute minimizes update return apply algorithm using squared loss recover lboosting apply algorithm log loss get algorithm known binomialboost buhlmann hothorn advantage logitboost need able weighted ﬁtting applies black box regression model gradient vector also relatively easy extend multi class case see hastie also apply algorithm loss functions huber loss section robust outliers squared error loss 
[adaptive, basis, function, models, boosting, sparse, boosting] suppose use weak learner following algorithm search possible variables pick one best predicts residual vector argmin chapter adaptive basis function models method known sparse boosting buhlmann identical matching pursuit algorithm discussed section clear result sparse estimate least small see let rewrite update follows non zero entry occurs location known forward stagewise linear regression hastie becomes equivalent lar algorithm discussed section increasing number steps boosting analogous decreasing regularization penalty modify boosting allow variable deletion steps zhao make equivalent lars algorithm computes full regularization path lasso problem algorithm used sparse logistic regression simply modifying residual appropriate negative gradient consider weak learner similar except uses smoothing spline instead linear regression mapping residual result sparse generalized additive model see section obviously extended pick pairs variables time resulting method often works much better mars buhlmann 
[adaptive, basis, function, models, boosting, multivariate, adaptive, regression, trees, mart] quite common use cart models weak learners usually advisable use shallow tree variance low even though bias high since shallow tree likely far truth compensated subsequent rounds boosting height tree additional tuning parameter addition number rounds boosting shrinkage factor suppose restrict trees leaves get stump split single variable allow two variable interactions etc general recommended hastie caruana niculescu mizil use combine gradient boosting algorithm shallow regression trees get model known mart stands multivariate adaptive regression trees actually includes slight reﬁnement basic gradient boosting algorithm ﬁtting regression tree residual negative gradient estimate parameters leaves tree minimize loss argmin region leaf tree corresponding parameter mean response regression problems probable class label classiﬁcation problems 
[adaptive, basis, function, models, boosting, boosting, work, well] seen boosting works well especially classiﬁers two main reasons first seen form regularization known help feedforward neural networks multilayer perceptrons prevent overﬁtting eliminating irrelevant features see imagine pre computing possible weak learners deﬁning feature vector form could use regularization select subset alternatively use boosting step weak learner creates new possible combine boosting regularization get algorithm known adaboost duchi singer essentially method greedily adds best features weak learners using boosting prunes irrelevant ones using regularization another explanation concept margin introduced sec tion schapire ratsch proved adaboost maximizes margin training data rosset generalized loss functions log loss 
[adaptive, basis, function, models, boosting, bayesian, view] far presentation boosting frequentist since focussed greedily minimizing loss functions likelihood interpretation algorithm given neal mackay meek idea consider mixture experts model form expert like weak learner usually experts using imagine sequential scheme whereby update parameters one expert time step posterior responsibilities reﬂect well existing experts explain given data point poor data points inﬂuence next expert ﬁtted view naturally suggest way use boosting like algorithm unsupervised learning simply sequentially mixture models instead mixtures experts notice rather broken mle procedure since never goes back update parameters old expert similarly boosting ever wants change weight assigned weak learner way add weak learner new weight result unnecessarily large models contrast bart model chipman uses bayesian version backﬁtting small sum weak learners typically trees 
[adaptive, basis, function, models, feedforward, neural, networks, multilayer, perceptrons] feedforward neural network aka multi layer perceptron mlp series logistic regression models stacked top ﬁnal layer either another logistic regression linear regression model depending whether solving classiﬁcation regression problem example two layers solving regression problem model form non linear activation transfer function commonly logistic function called hidden layer deterministic function input chapter adaptive basis function models figure neural network one hidden layer number hidden units weight matrix inputs hidden nodes weight vector hidden nodes output important non linear otherwise whole model collapses large linear regression model form one show mlp universal approximator meaning model suitably smooth function given enough hidden units desired level accuracy hornik handle binary classiﬁcation pass output sigmoid glm ber sigm easily extend mlp predict multiple outputs example regression case see figure illustration add mutual inhibition arcs output units ensuring one turns enforce sum one constraint used multi class classiﬁcation resulting model form cat 
[adaptive, basis, function, models, feedforward, neural, networks, multilayer, perceptrons, convolutional, neural, networks] purpose hidden units learn non linear combinations original inputs called feature extraction feature construction hidden features passed input ﬁnal glm approach particularly useful problems original input features individually informative example pixel image informative combination pixels tells objects present conversely task document classiﬁcation using bag words representation feature word count informative extracting higher order features less important suprisingly much work neural networks motivated visual pattern feedforward neural networks multilayer perceptrons figure convolutional neural network simard source http www codep roject com library neuralnetrecognition aspx used kind permission mike neill recognition lecun although also applied types data including text collobert weston form mlp particularly well suited signals like speech text signals like images convolutional neural network mlp hidden units local receptive ﬁelds primary visual cortex weights tied shared across image order reduce number parameters intuitively effect spatial parameter tying useful features discovered portion image used everywhere else without independently learned resulting network exhibits translation invariance meaning classify patterns matter occur inside input image figure gives example convolutional network designed simard colleagues simard layers layers adjustable parameters designed classify gray scale images handwritten digits mnist dataset see section layer feature maps size hidden node one feature maps computed convolving image weight matrix sometimes called kernel adding bias passing result form nonlinearity therefore neurons layer weights bias share parameters would weights ﬁrst layer layer feature maps obtained convolving feature map layer weight matrix adding adding bias passing nonlinearity therefore neurons layer adjustable weights one kernel pair feature chapter adaptive basis function models maps layers connections layer fully connected layer neurons weights finally layer also fully connected neurons weights adding numbers total neurons adjustable weights connections model usually trained using stochastic gradient descent see section details single pass data set called epoch mike neill experiments found single epoch took minutes recall training examples mnist since took epochs error rate converge total training time hours using technique obtained misclassiﬁcation rate test cases reduce error rate standard trick expand training set including distorted versions original data encourage network invariant small changes affect identity digit created applying random ﬂow ﬁeld shift pixels around see figure examples use online training stochastic gradient descent create distortions rather store using technique mike neill obtained misclassiﬁcation rate test cases close current state art yann cun colleagues lecun obtained similar performance using slightly complicated architecture shown figure model known lenet historically came model figure two main differences first lenet subsampling layer convolutional layer either averages computes max small window previous layer order reduce size obtain small amount shift invariance convolution sub sampling combination inspired hubel wiesel model simple complex cells visual cortex hubel wiesel continues popular neurally inspired models visual object recognition riesenhuber poggio similar idea ﬁrst appeared fukushima neocognitron fukushima though globally supervised training algorithm available time second difference lenet simard architecture ﬁnal layer actually rbf network rather standard sigmoidal softmax layer model gets test error rate trained distortions trained distortions figure shows errors made system genuinely ambiguous several errors person would never make web based demo lenet found http yann lecun com exdb lenet index html course classifying isolated digits limited applicability real world people usually write strings digits letters requires segmentation classiﬁcation cun colleagues devised way combine convolutional neural networks model similar conditional random ﬁeld described section solve problem system eventually deployed postal service see lecun detailed account system remains one best performing systems task implementation details mike used code variety speedup tricks using standard era hardware intel pentium hyperthreaded processor running ghz see http www codeproject com library neuralnetrecognition aspx details list various methods along misclassiﬁcation rates mnist test set available http yann lecun com exdb mnist error rates within statistically signiﬁcantly different feedforward neural networks multilayer perceptrons figure several synthetic warpings handwritten digit based figure bishop figure generated elasticdistortionsdemo written kevin swersky input convolutions subsampling convolutions feature maps subsampling maps maps layer maps layer full connection full connection gaussian connections output figure lenet convolutional neural net classifying handwritten digits source figure lecun used kind permission yann lecun chapter adaptive basis function models figure errors made lenet test cases mnist image label form correct label estimated label source figure lecun used kind permission yann lecun compare figure shows results deep generative model 
[adaptive, basis, function, models, feedforward, neural, networks, multilayer, perceptrons, kinds, neural, networks] network topologies possible besides ones discussed example skip arcs directly input output skipping hidden layer sparse connections layers etc however mlp always requires weights form directed acyclic graph allow feedback connections model known recurrent neural network deﬁnes nonlinear dynamical system simple probabilistic interpretation rnn models currently best approach language modeling performing word prediction natural language tomas signiﬁcantly outperforming standard gram based methods discussed section allow symmetric connections hidden units model known hop ﬁeld network associative memory probabilistic counterpart known boltzmann machine see section used unsupervised learning 
[adaptive, basis, function, models, feedforward, neural, networks, multilayer, perceptrons, brief, history, ﬁeld] neural networks subject great interest many decades due desire understand brain build learning machines possible review entire history instead give edited highlights ﬁeld generally viewed starting mcculloch pitts mccullich pitts devised simple mathematical model neuron approximated feedforward neural networks multilayer perceptrons output weighted sum inputs passed threshold function threshold similar sigmoidal activation function frank rosenblatt invented perceptron learning algorithm way estimate parameters mcculloch pitts neuron see section details similar model called adaline adaptive linear element invented widrow hoff minsky papert minsky papert published famous book called percep trons showed linear models hidden layers limited power since cannot classify data linearly separable considerably reduced interest ﬁeld rumelhart hinton williams rumelhart discovered backpropa gation algorithm see section allows one models hidden layers backpropagation algorithm originally discovered bryson independently werbos however rumelhart brought algorithm people attention spawned decade intense interest models sejnowski rosenberg sejnowski rosenberg created famous nettalk system learned mapping english words phonetic symbols could fed speech synthesizer audio demo system learns time found http www cnl salk edu parallelnetspronounce nettalk systems starts babbling gradually learns pronounce english words nettalk learned distributed representation via hidden layer various sounds success spawned big debate psychology connectionism based neural networks computationalism based syntactic rules debate lives extent machine learning community still arguments whether learning best performed using low level neural like representations using structured models yann cun others lecun created famous lenet system described section support vector machine see section invented boser svms provide similar prediction accuracy neural networks considerably easier train since use convex objective function spawned decade interest kernel methods general note however svms use adaptive basis functions require fair amount human expertise design right kernel function geoff hinton invented contrastive divergence training procedure hinton provided way ﬁrst time learn deep networks training one layer time unsupervised fashion see section details turn spawned renewed interest neural networks last years see chapter 
[adaptive, basis, function, models, feedforward, neural, networks, multilayer, perceptrons, backpropagation, algorithm] unlike glm nll mlp non convex function parameters nevertheless ﬁnd locally optimal map estimate using standard gradient based optimization methods since mlps lots parameters often trained large data sets became part folklore get published top machine learning conference known nips stands neural information processing systems important ensure paper contain word neural network chapter adaptive basis function models tanh sigmoid figure two possible activation functions tanh maps preferred nonlin earity hidden nodes sigm maps preferred nonlinearity binary nodes output layer figure generated tanhplot consequently common use ﬁrst order online methods stochastic gradient descent section whereas glms usually irls second order offline method discuss compute gradient vector nll applying chain rule calculus resulting algorithm known backpropagation reasons become apparent notational simplicity shall assume model one hidden layer helpful distinguish pre post synaptic values neuron apply nonlinearity let input pre synaptic hidden layer post synaptic hidden layer transfer function typically use sigm may also use tanh see figure comparison input sigm tanh vector assume applied component wise convert hidden layer output layer follows let pre synaptic output layer post synaptic output layer another nonlinearity corresponding canonical link glm reserve notation without hat output corresponding training case regression model use binary classifcation use sigm sigm multi class classiﬁcation use write overall model follows parameters model ﬁrst second layer weight matrices offset bias terms accomodated clamping element regression setting easily estimate variance output noise using empirical variance residual errors training complete one value output node performing multi target regression usually assume feedforward neural networks multilayer perceptrons regression case outputs nll given squared error classiﬁcation case classes nll given cross entropy log task compute derive separately overall gradient obtained summing although often use mini batch see section let start considering output layer weights since assuming canonical link function output glm equation tells error signal overall gradient pre synaptic input output layer namely times error signal namely input layer weights exploited fact remains compute ﬁrst level error signal tanh units tanh tanh sech sigmoid units hence chapter adaptive basis function models thus layer errors computed passing layer errors back matrix hence term backpropagation key property compute gradients locally node needs know immediate neighbors supposed make algorithm neurally plausible although interpretation somewhat controversial putting together compute gradients follows ﬁrst perform forwards pass compute compute error output layer pass backwards using equation compute error hidden layer compute overall gradient follows 
[adaptive, basis, function, models, feedforward, neural, networks, multilayer, perceptrons, identiﬁability] easy see parameters neural network identiﬁable example change sign weights going one hidden units long change sign weights going effects cancel since tanh odd function tanh tanh sign ﬂip symmetries leading equivalent settings parameters similarly change identity hidden units without affecting likelihood permutations total number equivalent parameter settings likelihood therefore addition may local minima due non convexity nll serious problem although enough data local optima often quite shallow simple stochastic optimization methods avoid addition common perform multiple restarts pick best solution average resulting predictions make sense average parameters since identiﬁable 
[adaptive, basis, function, models, feedforward, neural, networks, multilayer, perceptrons, regularization] usual mle overﬁt especially number nodes large simple way prevent called early stopping means stopping training procedure error validation set ﬁrst starts increase method works usually initialize small random weights model initially simple since tanh sigm functions nearly linear near origin training progresses weights become larger model becomes nonlinear eventually overﬁt another way prevent overﬁtting keeping approaches used elsewhere book impose prior parameters use map estimation standard use prior equivalent regularization precision strength prior neural networks literature called weight decay since encourages small weights hence simpler models penalized nll objective becomes log feedforward neural networks multilayer perceptrons note penalize bias terms gradient modiﬁed objective becomes section regularization sufficiently strong matter many hidden units apart wasted computation hence advisable set large afford say choose appropriate regularizer set parameter cross validation empirical bayes see section ridge regression good practice standardize inputs zero mean unit variance spherical gaussian prior makes sense consistent gaussian priors one show mackay using regularization parameter ﬁrst second layer weights results lack certain desirable invariance property particular suppose linearly scale shift inputs outputs neural network regression model would like model learn predict function suitably scaling internal weights bias terms however amount scaling needed ﬁrst second layer weights compensate change inputs outputs therefore need use different regularization strength ﬁrst second layer fortunately easy use following prior bias terms get feeling effect hyper parameters sample mlp parameters prior plot resulting random functions figure shows examples decreasing allows ﬁrst layer weights get bigger making sigmoid like shape functions steeper decreasing allows ﬁrst layer biases get bigger allows center sigmoid shift left right decreasing allows second layer weights get bigger making functions wiggly greater sensitivity change input hence larger dynamic range decreasing allows second layer biases get bigger allowing mean level function move chapter see easier way deﬁne priors functions weight pruning since many weights neural network often helpful encourage sparsity various hoc methods names optimal brain damage devised see bishop details since regularizing output bias terms helpful case regression normalize target responses training set zero mean consistent fact prior output bias zero mean chapter adaptive basis function models figure effects changing hyper parameters mlp default parameter values decreasing factor decreasing factor decreasing factor decreasing factor figure generated mlppriorsdemo feedforward neural networks multilayer perceptrons neural network data deep neural net figure deep sparse neural network connections pruned using regularization level nodes numbered clamped outgoing weights correspond offset bias terms predictions made model training set figure generated sparsennetdemo written mark schmidt however also use principled sparsity promoting techniques discussed chapter one approach use regularizer see figure example another approach use ard discussed detail section soft weight sharing another way regularize parameters encourage similar weights share statistical strength know parameters group together learn using mixture model model mixture diagonal gaussians parameters assigned cluster share mean variance thus similar values assuming variance cluster low called soft weight sharing nowlan hinton practice technique widely used see bishop want know details semi supervised embedding interesting way regularize deep feedforward neural networks encourage hidden layers assign similar objects similar representations useful often easy obtain side information consisting sets pairs similar dissimilar objects example video classiﬁcation task neighboring frames deemed similar frames distant time deemed dis similar mobahi note done without collecting labels let examples similar otherwise let embedding item hidden layer neural network deﬁne loss function depends embedding two objects chapter adaptive basis function models observed similarity measure example might want force similar objects similar embeddings force embeddings dissimilar objects minimal distance apart max minimal margin deﬁne augmented loss function training neural network nll labeled training set unlabeled training set tradeoff parameter called semi supervised embedding weston objective easily optimized stochastic gradient descent itera tion pick random labeled training example take gradient step optimize nll pick random pair similar unlabeled examples sometimes generated rather stored advance make gradient step optimize finally pick random unlabeled example high probability dissimilar make gradient step optimize note technique effective leverage massive amounts data related approach collobert weston trained neural network distinguish valid english sentences invalid ones done taking million words english wikipedia wikipedia org creating windows length containing neighboring words constitutes positive examples create negative examples middle word window replaced random english word likely invalid sentence either grammatically semantically high probability neural network trained course week latent representation used input supervised semantic role labeling task little labeled training data available see also ando zhang related work 
[adaptive, basis, function, models, feedforward, neural, networks, multilayer, perceptrons, bayesian, inference] although map estimation succesful way reduce overﬁtting still good reasons want adopt fully bayesian approach ﬁtting neural networks integrating parameters instead optimizing much stronger form regu larization map estimation use bayesian model selection determine things like hyper parameter settings number hidden units likely much faster cross validation especially many hyper parameters ard modelling uncertainty parameters induce uncertainty predictive distribu tions important certain problems active learning risk averse decision making feedforward neural networks multilayer perceptrons use online inference methods extended kalman ﬁlter online learning haykin one adopt variety approximate bayesian inference techniques context section discuss laplace approximation ﬁrst suggested mackay one also use hybrid monte carlo neal variational bayes hinton camp barber bishop parameter posterior regression start considering regression following presentation bishop sec summarizes work mackay use prior form represents weights combined denote precision noise posterior approximated follows exp data error prior error overall error negative log prior plus log likelihood let make second order taylor series approximation around minimum map estimate hessian hessian data error computed exactly time number parameters using variant backpropagation see bishop sec details alternatively use quasi newton method ﬁnd mode use internally computed low rank approximation note diagonal approximations usually inaccurate either case using quadratic approximation posterior becomes gaussian chapter adaptive basis function models parameter posterior classiﬁcation classiﬁcation case regression case except cross entropy error form predictive posterior regression posterior predictive density given analytically tractable nonlinearity let therefore construct ﬁrst order taylor series approximation around mode linear gaussian model gaussian prior weights equation predictive variance depends input follows error bars larger regions input space little training data see figure example predictive posterior classiﬁcation section discuss approximate case binary classiﬁcation situation similar case logistic regression discussed section except addition posterior predictive mean non linear function speciﬁcally sigm pre synaptic output ﬁnal layer let make linear approximation found modiﬁed version backpropagation clearly feedforward neural networks multilayer perceptrons input target data function network error bars data function prediction samples figure posterior predictive density mlp hidden nodes trained data points dashed green line true function result using laplace approximation performing empirical bayes optimize hyperparameters solid red line posterior mean prediction dotted blue lines standard deviation mean figure generated mlpregevidencedemo result using hybrid monte carlo using trained hyperparameters solid red line posterior mean prediction dotted blue lines samples posterior predictive figure generated mlpreghmcdemo written ian nabney hence posterior predictive output sigm sigm deﬁned equation repeat convenience course simpler potentially accurate alternative draw samples gaussian posterior approximate posterior predictive using monte carlo either case effect taking uncertainty parameters account sec tion moderate conﬁdence output decision boundary unaf fected however ard neural networks made laplace approximation posterior optimize marginal likelihood wrt hyper parameters using ﬁxed point equations section typically use one hyper parameter weight vector leaving node achieve effect similar group lasso section prior form ﬁnd input feature irrelevant weight vector pruned similarly ﬁnd hidden feature irrelevant known automatic chapter adaptive basis function models relevancy determination ard discussed detail section applying neural networks gives efficient means variable selection non linear models software package netlab contains simple example ard applied neural network called demard demo creates data according nonlinear regression function sin noisy copy see irrelevant predicting target however correlated relevant using ard ﬁnal hyper parameters follows clearly indicates feature irrelevant feature weakly relevant feature relevant 
[adaptive, basis, function, models, ensemble, learning] ensemble learning refers learning weighted combination base models form tunable parameters ensemble learning sometimes called committee method since base model gets weighted vote clearly ensemble learning closely related learning adaptive basis function models fact one argue neural net ensemble method represents hidden unit output layer weights also think boosting kind ensemble learning weights base models determined sequentially describe forms ensemble learning 
[adaptive, basis, function, models, ensemble, learning, stacking] obvious way estimate weights equation use argmin however result overﬁtting large complex model simple solution use cross validation particular use loocv estimate argmin predictor obtained training data excluding known stacking stands stacked generalization wolpert technique robust case true model model class standard bma clarke approach used netﬂix team known ensemble tied submission winning team bellkor pragmatic chaos terms accuracy sill stacking also used problems image segmentation labeling ensemble learning class table part bit error correcting output code class problem row deﬁnes two class problem based table hastie 
[adaptive, basis, function, models, ensemble, learning, error-correcting, output, codes] interesting form ensemble learning known error correcting output codes ecoc dietterich bakiri used context multi class classiﬁcation idea trying decode symbol namely class label possible states could use bit vector length encode class label train separate binary classiﬁers predict bit however using bits designing codewords maximal hamming distance get method resistant individual bit ﬂipping errors misclassiﬁcation example table use bits encode class problem minimum hamming distance pair rows decoding rule min bit codeword class james hastie showed random code worked well optimal code methods work averaging results multiple classiﬁers thereby reducing variance 
[adaptive, basis, function, models, ensemble, learning, ensemble, learning, equivalent, bayes, model, averaging] section discussed bayesian model selection alternative picking best model using make predictions make weighted average predictions made model compute called bayes model averaging bma sometimes give better performance using single model hoeting course averaging models typically computationally infeasible analytical integration obviously possible discrete space although one sometimes use dynamic programming perform computation exactly meila jaakkola simple approximation sample models posterior even simpler approximation one widely used practice use map model important note bma equivalent ensemble learning minka latter technique corresponds enlarging model space deﬁning single new model chapter adaptive basis function models model bst bag svm ann knn bst stmp logreg table fraction time method achieved speciﬁed rank sorting mean performance across datasets metrics based table caruana niculescu mizil used kind permission alexandru niculescu mizil convex combination base models follows principle perform bayesian inference compute make pre dictions using however much common use point estimation methods saw 
[adaptive, basis, function, models, experimental, comparison] described many different methods classiﬁcation regression one use depends inductive bias think appropriate domain usually hard assess common try several different methods see perform empirically summarize two comparisons carefully conducted although data sets used relatively small see website mlcomp org distributed way perform large scale comparisons kind course must always remember free lunch theorem section tells universally best learning method 
[adaptive, basis, function, models, experimental, comparison, low-dimensional, features] rich caruana alex niculescu mizil caruana niculescu mizil conducted extensive experimental comparison different binary classiﬁcation methods different data sets data sets training cases test sets containing examples average number features ranged much lower dimensional nips feature selection challenge fold cross validation used assess average test error separate internal method may need use model selection experimental comparison methods compared follows listed roughly decreasing order performance assessed table bst boosted decision trees random forest bag bagged decision trees svm support vector machine ann artiﬁcial neural network knn nearest neighbors bst stmp boosted stumps decision tree logreg logistic regression naive bayes used different performance measures divided three groups thresh old metrics require point estimate output include accuracy score sec tion etc ordering ranking metrics measure well positive cases ordered negative cases include area roc curve section average precision precision recall break even point finally probability metrics included cross entropy log loss squared error methods svms produce calibrated probabilities post processed using platt logistic regression trick section using isotonic regression performance measures standardized scale could compared obviously results vary dataset metric therefore averaging performance necessarily give reliable conclusions however one perform bootstrap analysis shows robust conclusions changes results shown table see time boosted decision trees best method followed random forests bagged decision trees svms neural networks however following methods relatively poorly knn stumps single decision trees logistic regression naive bayes results generally consistent conventional wisdom practioners ﬁeld course conclusions may change features high dimensional lots irrelevant features section lots noise etc 
[adaptive, basis, function, models, experimental, comparison, high-dimensional, features] nips conference ran competition goal solve binary classiﬁcation problems large numbers mostly irrelevant features given small training sets called feature selection challenge performance measured terms predictive accuracy terms ability select features ﬁve datasets used summarized table term probe refers artiﬁcal variables added problem make harder predictive power correlated original features results competition discussed guyon overall winner approach based bayesian neural networks neal zhang follow study chapter adaptive basis function models dataset domain type probes train val test aracene mass spectrometry dense dexter text classiﬁcation sparse dorothea drug discovery sparse gisette digit recognition dense madelon artiﬁcial dense table summary data used nips feature selection challenge dorothea datasets features binary others features real valued screened features ard method avg rank avg time avg rank avg time hmc mlp boosted mlp bagged mlp boosted trees random forests table performance different methods nips feature selection challenge hmc stands hybrid monte carlo see section report average rank lower better across datasets also report average training time minutes standard error brackets mcmc bagged mlps use two hidden layers units boosted mlps use one hidden layer hidden units boosted trees used depths shrinkage tree trained data chosen random step called stochastic gradient boosting table hastie johnson bayesian neural nets mlps hidden layers compared several methods based bagging boosting note methods quite similar case prediction form bayesian mlp mcmc hybrid monte carlo set set draw posterior bagging set estimated ﬁtting model bootstrap sample data boosting set estimated sequentially improve computational statistical performance feature selection performed two methods considered simple uni variate screening using tests method based mlp ard results follow study shown table see bayesian mlps winner second place either random forests boosted mlps depending preprocessing however clear statistically signiﬁcant differences since test sets relatively small terms training time see mcmc much slower methods would interesting see well deterministic bayesian inference laplace approximation would perform obviously much faster question much would one lose interpreting black box models par tial dependence par tial dependence figure partial dependence plots predictors friedman synthetic dimensional gression problem source figure chipman used kind permission hugh chipman statistical performance 
[adaptive, basis, function, models, interpreting, black-box, models] linear models popular part easy interpet however often poor predictors makes poor proxy nature mechanism thus conclusions importance particular variables based models good predictive accuracy breiman interestingly many standard statistical tests goodness test predictive accuracy model chapter studied black box models good predictive accuracy unfortunately hard interpret directly fortunately various heuristics use probe models order assess input variables important simple example consider following non linear function ﬁrst proposed friedman illustrate power mars sin see output complex function inputs augmenting vector additional irrelevant random variables drawn uniform create challenging feature selection problem experiments add extra dummy variables chapter adaptive basis function models usage figure average usage variable bart model data ﬁrst features relevant different coloured lines correspond different numbers trees ensemble source figure chipman used kind permission hugh chipman one useful way measure effect set variables output compute partial dependence plot friedman plot deﬁned response predictors averaged figure shows example use sets corresponding single variable data generated equation irrelevant variables added bart model section computed partial dependence plots see predicted response invariant indicating variables marginally irrelevant response roughly linear roughly quadratic error bars obtained computing empirical quantiles based posterior samples alternatively use bootstrap another useful summary computes relative importance predictor variables thought nonlinear even model free way performing variable selection although technique restricted ensembles trees basic idea originally proposed breiman count often variable used node trees particular let proportion splitting rules use tree sample posterior trees easily compute posterior alternatively use bootstrap figure gives example using bart see ﬁve relevant variables chosen much ﬁve irrelevant variables increase number trees variables likely chosen reducing sensitivity method small method farily diagnostic interpreting black box models 
[adaptive, basis, function, models, exercises] exercise nonlinear regression inverse dynamics question model predict torques robot needs apply order make arm reach desired point space data collected sarcos robot arm degrees freedom input vector encodes desired position velocity accelaration joints output vector encodes torques applied joints reach point mapping highly nonlinear training points test testing points simplicity following standard practice focus predicting scalar output namely torque ﬁrst joint download data http www gaussianprocess org gpml standardize inputs zero mean unit variance training set center outputs zero mean training set apply corresponding transformations test data describe various models transformed data make predictions compute standardized mean squared error test set follows smse test test train ntrain variance output computed training set ﬁrst method try standard linear regression turn numbers code according rasmussen williams able achieve smse using method try running means clustering using cross validation pick rbf network data using estimated means use estimate rbf bandwidth smse get turn numbers code according rasmussen williams gaussian process regression get smse goal get close try ﬁtting feedforward neural network use pick number hidden units strength regularizer smse get turn numbers code 
[markov, hidden, markov, models, introduction] chapter discuss probabilistic models sequences observations arbitrary length models applications computational biology natural language processing time series forecasting etc focus case observations occur discrete time steps although time may also refer locations within sequence 
[markov, hidden, markov, models, markov, models] recall section basic idea behind markov chain assume captures relevant information predicting future assume sufficient statistic assume discrete time steps write joint distribution follows called markov chain markov model assume transition function independent time chain called homogeneous stationary time invariant example parameter tying since parameter shared multiple variables assumption allows model arbitrary number variables using ﬁxed number parameters models called stochastic processes assume observed variables discrete called discrete state ﬁnite state markov chain make assumption throughout rest section 
[markov, hidden, markov, models, markov, models, transition, matrix] discrete conditional distribution written matrix known transition matrix probability going state state row matrix sums one called stochastic matrix chapter markov hidden markov models figure state transition diagrams simple markov chains left state chain right state left right chain stationary ﬁnite state markov chain equivalent stochastic automaton common visualize automata drawing directed graph nodes represent states arrows represent legal transitions non zero elements known state transition diagram weights associated arcs probabilities example following state chain illustrated figure left following state chain illustrated figure right called left right transition matrix com monly used speech recognition section element transition matrix speciﬁes probability getting one step step transition matrix deﬁned probability getting exactly steps obviously chapman kolmogorov equations state words probability getting steps probability getting steps steps summed write matrix multiplication hence thus simulate multiple steps markov chain powering transition matrix markov models says cards legendary reconnaissance rollie democracies unsustainable could strike redlining visits profit booking wait madison square garden county courthouse done three already way teacher table example output gram word model trained using backoff smoothing broadcast news corpus ﬁrst words speciﬁed hand model generates word results fed back model source http www fit vutbr imikolov rnnlm gen gra txt 
[markov, hidden, markov, models, markov, models, application, language, modeling] one important application markov models make statistical language models probability distributions sequences words deﬁne state space words english language marginal probabilities called unigram statistics use ﬁrst order markov model called bigram model use second order markov model called trigram model general called gram models example figure shows gram grams counts letters represents space estimated darwin origin species language models used several things following sentence completion language model predict next word given previous words sentence used reduce amount typing required particularly important disabled users see david mackay dasher system uses mobile devices data compression density model used deﬁne encoding scheme assigning short codewords probable strings accurate predictive model fewer number bits requires store data text classiﬁcation density model used class conditional density hence turned generative classiﬁer note using gram class conditional density unigram statistics would equivalent naive bayes classiﬁer see section automatic essay writing one sample generate artiﬁcial text one way assessing quality model table give example text generated gram model trained corpus million words tomas describes much better language model based recurrent neural networks generates much semantically plausible text http www inference phy cam dasher chapter markov hidden markov models unigrams bigrams figure unigram bigram counts darwin origin species picture right hinton diagram joint distribution size white squares proportional value entry corresponding vector matrix based mackay figure generated ngramplot mle markov language models discuss simple way estimate transition matrix training data proba bility particular sequence length given hence log likelihood set sequences sequence length given log log log log deﬁne following counts markov models hence write mle normalized counts results extended straightforward way higher order markov models however problem zero counts becomes acute whenever number states order chain large gram models parameters words vocabulary gram model billion free parameters corresponding possible word pairs unlikely see training data however want predict particular word string totally impossible happen seen training text would severe form overﬁtting simple solution use add one smoothing simply add one empirical counts normalizing bayesian justiﬁcation given section however add one smoothing assumes grams equally likely realistic sophisticated bayesian approach discussed section alternative using smart priors gather lots lots data example google gram models based one trillion words extracted web data uncompressed publically available example data set grams shown serve incoming serve incubator serve independent serve index serve indication serve indicator serve indicators serve indispensable serve indispensible serve individual although approach based brute force ignorance successful rather unsatisfying since clear humans learn see tenenbaum reﬁned bayesian approach needs much less data described section empirical bayes version deleted interpolation common heuristic used sparse data problem called deleted interpolation chen goodman deﬁnes transition matrix convex combination bigram famous example improbable syntactically valid english word string due noam chomsky colourless green ideas sleep furiously would want model predict string impossible even ungrammatical constructs allowed model certain probability since people frequently violate grammatical rules especially spoken language see http googleresearch blogspot com gram belong html tails chapter markov hidden markov models frequencies unigram frequencies term usually set cross validation also closely related technique called backoff smoothing idea small back reliable estimate namely show deleted interpolation heuristic approximation predic tions made simple hierarchical bayesian model presentation follows mckay peto first let use independent dirichlet prior row transition matrix dir dir dir row transition matrix prior mean satisfying prior strength use prior row see figure posterior given dir vector records number times transitioned state states equation posterior predictive density similar equation identical main difference bayesian model uses context dependent weight combine empirical frequency rather ﬁxed weight like adaptive deleted interpolation furthermore rather backing empirical marginal frequencies back model parameter remaining question values use let use empirical bayes since assume row transition matrix priori independent given marginal likelihood markov model found applying equation row counts leaving state generalized beta function using methods discussed minka however also use following approximation mckay peto says prior probability word given number different contexts occurs rather number times occurs justify reasonableness result mackay peto mckay peto give following example markov models figure markov chain put different dirichlet prior every row transition matrix hyperparameters dirichlet shared imagine see language see see frequently occuring couplet see see second word couplet see follows first word high probability see marginal statistics see going become hugely dominated see words see equal frequency see use standard smoothing formula equation novel see novel novel context word seen would turn since marginal frequencies see times however seems unreasonable appears many contexts novel high see follows see novel low use bayesian formula equation get effect free since back large small see equation unfortunately although elegant bayesian model beat state art lan guage model known interpolated kneser ney kneser ney chen goodman however teh shown one build non parametric bayesian model outperforms interpolated kneser ney using variable length contexts wood method extended create sequence memoizer currently best performing language model handling vocabulary words smoothing methods handle case counts small even zero none deal case test set may contain completely novel word particular assume words vocabulary state space ﬁxed known typically set unique words training data dictionary interestingly non parametric methods based posterior inference using mcmc section particle ﬁltering section rather optimization methods despite quite efficient chapter markov hidden markov models 
[markov, hidden, markov, models] figure markov chains state aperiodic chain reducible state chain even non zero none models predict novel word outside set hence assign zero probability test sentence unfamiliar word unfamiliar words bound occur set words open class example set proper nouns names people places unbounded standard heuristic solve problem replace novel words special symbol unk stands unknown certain amount probability mass held aside event principled solution would use dirichlet process generate countably inﬁnite state space amount data increases see section novel words accepted genuine words system predictive power since misspelling considered new word novel word seen frequently enough warrant added vocabulary see friedman singer griffiths tenenbaum details 
[markov, hidden, markov, models, stationary, distribution, markov, chain] focussing markov models way deﬁning joint probability distributions sequences however also interpret stochastic dynamical systems hop one state another time step case often interested long term distribution states known stationary distribution chain section discuss relevant theory later consider two important applications google pagerank algorithm ranking web pages section mcmc algorithm generating samples hard normalize probability distributions chapter stationary distribution let one step transition matrix let probability state time conventional context assume row vector initial distribution states time matrix notation markov models imagine iterating equations ever reach stage say reached stationary distribution also called invariant distribution equilibrium distribution enter stationary distribution never leave example consider chain figure ﬁnd stationary distribution write general words probability state times net ﬂow state must equal probability state times net ﬂow state called global balance equations solve equations subject constraint computing stationary distribution ﬁnd stationary distribution solve eigenvector equation set eigenvector eigenvalue sure eigenvector exists since row stochastic matrix also recall eigenvalues course since eigenvectors unique constants proportionality must normalize end ensure sums one note however eigenvectors guaranteed real valued matrix positive hence due sum one constraint general approach handle chains transition probabilities figure follows resnick constraints constraint since unknowns overconstrained let replace column last get new matrix call next deﬁne last position corresponds column solve example state chain solve linear system chapter markov hidden markov models chain figure ﬁnd easily verify correct since see mcstatdist matlab code unfortunately chains stationary distribution explain stationary distribution exist consider state chain figure start state stay forever since absorbing state thus one possible stationary distribution however start oscillate two states ever another possible stationary distribution start state could end either stationary distributions see example necessary condition unique stationary distribution state transition diagram singly connected component get state state chains called irreducible consider state chain figure irreducible provided suppose clear symmetry chain spend time state thus suppose case chain oscillate two states long term distribution states depends start start state every odd time step state start state every odd time step state example motivates following deﬁnition let say chain limiting distribution lim exists independent holds long run distribution states independent starting state let characterize limiting distribution exists deﬁne period state gcd gcd stands greatest common divisor largest integer divides members set example figure gcd gcd say state aperiodic sufficient condition ensure state self loop necessary condition say chain aperiodic states aperiodic one show following important result theorem every irreducible singly connected aperiodic ﬁnite state markov chain limiting distribution equal unique stationary distribution special case result says every regular ﬁnite state chain unique stationary distribution regular chain one whose transition matrix satisﬁes integer possible get state state steps consequently steps chain could state matter started one show sufficient conditions ensure regularity chain irreducible singly connected every state self transition handle case markov chains whose state space ﬁnite countable set integers uncountable set reals need generalize earlier markov models deﬁnitions since details rather technical brieﬂy state main results without proof see grimmett stirzaker details stationary distribution exist require irreducibility singly connected aperiod icity also require state recurrent chain states recurrent called recurrent chain recurrent means return state probability simple example non recurrent state transient state consider figure states transient one immediately leaves either spins around state forever oscillates states forever way return state clear ﬁnite state irreducible chain recurrent since always get back started consider example inﬁnite state space suppose perform random walk integers let probability moving right probability moving left suppose start shoot guaranteed return similarly shoot cases chain recurrent even though irreducible intuitively obvious require states recurrent stationary distribution exist however sufficient see consider random walk integers suppose case return origin inﬁnite number times chain recurrent however takes inﬁnitely long prohibits stationary distribution intuitive reason distribution keeps spreading larger larger set integers never converges stationary distribution formally deﬁne state non null recurrent expected time return state ﬁnite chain states non null called non null chain brevity say state ergodic aperiodic recurrent non null say chain ergodic states ergodic state main theorem theorem every irreducible singly connected ergodic markov chain limiting distri bution equal unique stationary distribution generalizes theorem since irreducible ﬁnite state chains states recurrent non null detailed balance establishing ergodicity difficult give alternative condition easier verify say markov chain time reversible exists distribution called detailed balance equations says ﬂow must equal ﬂow weighted appropriate source probabilities following important result theorem markov chain transition matrix regular satisﬁes detailed balance wrt distribution stationary distribution chain chapter markov hidden markov models figure small world wide web figure generated pagerankdemo written tim davis proof see note hence note condition sufficient necessary see figure example chain stationary distribution satisfy detailed balance section discuss markov chain monte carlo mcmc methods take input desired distribution construct transition matrix general transition kernel satisﬁes detailed balance wrt thus sampling states chain eventually enter stationary distribution visit states probabilities given 
[markov, hidden, markov, models, application, google’s, pagerank, algorithm, web, page, ranking] results section form theoretical underpinnings google pagerank algorithm used information retrieval world wide web sketch basic idea see byran leise detailed explanation treat web giant directed graph nodes represent web pages documents edges represent hyper links perform process called web crawling start designated root nodes home open directory project follows links storing pages encounter run time next words web page entered data structure called inverted index word store list documents word occurs practice store list hash codes representing urls test time user enters google said indexed trillion unique urls assume urls per page average means billion unique web pages estimates billion unique web pages source thenextweb com shareables infographic big internet markov models query look documents containing word intersect lists since queries deﬁned conjunction search terms get reﬁned search storing location word document test words document occur order query let give example http wikipedia org wiki inverted_index documents banana create following inverted index pair represents document word location banana what example see word occurs location counting document location document suppose search ignore word order retrieve following documents require word order matches document would returned generally allow order matches give bonus points documents whose word order matches query word order features words occur title document return matching documents decreasing order score relevance called document ranking far described standard process information retrieval link structure web provides additional source information basic idea web pages authoritative others ranked higher assuming match query web page authority linked many pages protect effect called link farms dummy pages link given site boost apparent relevance weight incoming link source authority thus get following recursive deﬁnition authoritativeness page also called pagerank probability following link recognize equation stationary distribution markov chain simplest setting deﬁne uniform distribution states connected however ensure distribution unique need make chain regular chain done allowing state jump state including small probability effectively makes transition matrix aperiodic fully connected although adjacency matrix web highly sparse discuss efficient methods computing leading eigenvector giant matrix ﬁrst let give example pagerank algorithm consider small web figure chapter markov hidden markov models figure web graph sites rooted www harvard edu corresponding page rank vector figure generated pagerankdemopmtk based code cleve moler moler ﬁnd stationary distribution random surfer visit site time see node higher pagerank nodes even though number links linked inﬂuential nodehelps increase pagerank score linked less inﬂuential node slightly larger example figure shows web graph derived root harvard edu figure shows corresponding pagerank vector efficiently computing pagerank vector let iff link imagine performing random walk graph every time step probability follow one outlinks uniformly random probability jump random node chosen uniformly random outlinks jump random page random jumps including self transitions ensure chain irreducible singly connected regular hence solve unique stationary distribution using eigenvector methods deﬁnes following transition matrix number nodes probability jumping one page another without following link represents degree page stochastic matrix columns sum one note earlier notation represent transition matrix compactly follows deﬁne diagonal matrix entries hidden markov models deﬁne vector components rewrite equation follows matrix sparse rank one modiﬁcation sparse matrix elements equal small constant obviously need stored explicitly goal solve one efficient method ﬁnd leading eigenvector large matrix known power method simply consists repeated matrix vector multiplication followed normalization gdv possible implement power method without using matrix multiplications simply sampling transition matrix counting often visit state essentially monte carlo approximation sum implied applying data figure yields stationary distribution figure took iterations converge starting uniform distribution see also function pagerankdemo tim davis animation algorithm action applied small web example handle changing web structure run algorithm every day every week starting old distribution langville meyer details perform monte carlo power method parallel distributed computing environment see rajaraman ullman web spam pagerank foolproof example consider strategy adopted penney depart ment store usa christmas season planted many links home page irrelevant web pages thus increasing ranking google search engine segal even though source pages low pagerank many effect added businesses call search engine optimization google calls web spam google notiﬁed scam new york times manually downweighted penney since behavior violates google code conduct result penney dropped rank rank essentially making disappear view automatically detecting scams relies various techniques beyond scope chapter 
[markov, hidden, markov, models, hidden, markov, models] mentioned section hidden markov model hmm consists discrete time discrete state markov chain hidden states plus observation model chapter markov hidden markov models figure data sampled state hmm state emits gaussian hidden state sequence based figure bishop figure generated hmmlillypaddemo corresponding joint distribution form observations hmm discrete continuous discrete common observation model observation matrix observations continuous common observation model conditional gaussian figure shows example states emits different gaussian resulting model similar gaussian mixture model except cluster membership markovian dynamics indeed hmms sometimes called markov switching models fruhwirth schnatter see tend get multiple observations location sudden jump new cluster 
[markov, hidden, markov, models, hidden, markov, models, applications, hmms] hmms used black box density models sequences advantage markov models represent long range dependencies observations mediated via latent variables particular note assume markov property holds observations black box models useful time series prediction fraser also used deﬁne class conditional densities inside generative classiﬁer however common imbue hidden states desired meaning try estimate hidden states observations compute hidden markov models bat rat cat gnat goat figure dna sequences state transition diagram proﬁle hmm source figure durbin used kind permission richard durbin online scenario offline scenario see section discussion differences two approaches give examples applications use hmms way automatic speech recognition represents features extracted speech signal represents word spoken transition model represents language model observation model represents acoustic model see jelinek jurafsky martin details activity recognition represents features extracted video frame class activity person engaged running walking sitting etc see szeliski details part speech tagging represents word represents part speech noun verb adjective etc see section information pos tagging chapter markov hidden markov models related tasks gene ﬁnding represents dna nucleotides represents whether inside gene coding region see schweikerta details protein sequence alignment represents amino acid represents whether matches latent consensus sequence location model called proﬁle hmm illustrated figure hmm states called match insert delete match state equal value consensus insert state generated uniform distribution unrelated consensus sequence delete state way generate noisy copies consensus sequence different lengths figure consensus agc see various versions path state transition diagram shown figure speciﬁes align sequence consensus gnat probable path means delete parts consensus sequence insert match ﬁnal estimate model parameters counting number transitions number emissions kind state shown figure see section information training hmm durbin details proﬁle hmms note tasks conditional random ﬁelds essentially discrimi native versions hmms may suitable see chapter details 
[markov, hidden, markov, models, inference, hmms] discuss infer hidden state sequence hmm assuming parameters known exactly algorithms apply chain structured graphical models chain crfs see section chapter generalize methods arbitrary graphs section show use output inference context parameter estimation 
[markov, hidden, markov, models, inference, hmms, types, inference, problems, temporal, models] several different kinds inferential tasks hmm ssm general illustrate differences consider example called occasionally dishonest casino durbin model represents dice face shows represents identity dice used time casino uses fair dice occasionally switches loaded dice short period observation distribution uniform multinoulli symbols observation distribution skewed towards face see figure sample model may observe data following listing example output casinodemo die rolls refers observed symbol die refers hidden state loaded fair thus see model generates sequence symbols statistics inference hmms figure hmm occasionally dishonest casino blue arrows visualize state transition diagram based durbin roll number loaded filtered roll number loaded smoothed roll number map state fair loaded viterbi figure inference dishonest casino vertical gray bars denote samples generated using loaded die filtered estimate probability using loaded dice smoothed estimates map trajectory figure generated casinodemo distribution changes abruptly every typical application see rolls want infer dice used different kinds inference summarize filtering means compute belief state online recursively data streams called ﬁltering reduces noise simply estimating hidden state using current estimate see perform ﬁltering simply applying bayes rule sequential fashion see figure example smoothing means compute offline given evidence see figure example conditioning past future data uncertainty signiﬁcantly reduced understand intuitively consider detective trying ﬁgure com mitted crime moves crime scene uncertainty high ﬁnds key clue aha moment uncertainty reduced previously confusing observations hindsight easy explain chapter markov hidden markov models 
[markov, hidden, markov, models, riiolqh] main kinds inference state space models shaded region interval data arrow represents time step want perform inference current time sequence length lag prediction horizon see text details fixed lag smoothing interesting compromise online offline estimation involves computing called lag gives better performance ﬁltering incurs slight delay changing size lag one trade accuracy delay prediction instead predicting past given future ﬁxed lag smoothing might want predict future given past compute called prediction horizon example suppose straightforward perform computation power transition matrix apply current belief state quantity prediction future hidden states converted prediction future observations using posterior predictive density used time series forecasting see fraser details see figure sketch relationship ﬁltering smoothing prediction map estimation means computing arg max prob able state sequence context hmms known viterbi decoding see inference hmms section figure illustrates difference ﬁltering smoothing map decoding occasionally dishonest casino hmm see smoothed offline estimate indeed smoother ﬁltered online estimate threshold estimates compare true sequence ﬁnd ﬁltered method makes errors smoothed method makes map path makes errors surprising smoothing makes fewer errors viterbi since optimal way min imize bit error rate threshold posterior marginals see section nevertheless applications may prefer viterbi decoding discuss section posterior samples one plausible interpretation data useful sample posterior sample paths contain much information sequence marginals computed smoothing probability evidence compute probability evidence summing hidden paths used classify sequences hmm used class conditional density model based clustering anomaly detection etc 
[markov, hidden, markov, models, riiolqh, forwards, algorithm] describe recursively compute ﬁltered marginals hmm algorithm two steps first comes prediction step compute one step ahead predictive density acts new prior time next comes update step absorb observed data time using bayes rule normalization constant given process known predict update cycle distribution called ﬁltered belief state time vector numbers often denoted matrix vector notation write update following simple form local evidence time transition matrix hadamard product representing elementwise vector multiplication see algorithm pseudo code hmmfilter matlab code chapter markov hidden markov models addition computing hidden states use algorithm compute log probability evidence log log log need work log domain avoid numerical underﬂow algorithm forwards algorithm input transition matrices local evidence vectors initial state distribution normalize normalize return log log subroutine normalize 
[markov, hidden, markov, models, riiolqh, forwards-backwards, algorithm] section explained compute ﬁltered marginals using online inference discuss compute smoothed marginals using offline inference basic idea key decomposition relies fact break chain two parts past future conditioning let ﬁltered belief state also deﬁne conditional likelihood future evidence given hidden state time note probability distribution states since need satisfy finally deﬁne desired smoothed posterior marginal equation inference hmms already described recursively compute left right fashion section describe recursively compute right left fashion already computed compute follows write resulting equation matrix vector form base case probability non event computed forwards backwards messages combine compute overall algorithm known forwards backwards algorithm pseudo code similar forwards case see hmmfwdback implementation think algorithm passing messages left right right left combining node generalize intuition section discuss belief propagation two slice smoothed marginals estimate parameters transition matrix using see section need compute expected number transitions state state term called smoothed two slice marginal computed follows chapter markov hidden markov models matrix vector form another interpretation equations see section time space complexity clear straightforward implementation takes time since must perform matrix multiplication step applications speech recognition large term becomes prohibitive fortunately transition matrix sparse reduce substantially example left right transition matrix algorithm takes time cases exploit special properties state space even transition matrix sparse particular suppose states represent discretization underlying continuous state space transition matrix form exp continuous vector represented state one implement forwards backwards algorithm log time useful models large state spaces see section details cases bottleneck memory time expected sufficient statistics needed takes constant space independent however compute need working space since must store backwards pass possible devise simple divide conquer algorithm reduces space complexity log cost increasing running time log see binder zweig padmanabhan details 
[markov, hidden, markov, models, riiolqh, viterbi, algorithm] viterbi algorithm viterbi used compute probable sequence states chain structured graphical model compute arg max equivalent computing shortest path trellis diagram figure nodes possible states time step node edge weights log probabilities weight path given log log log log map mpe discussing algorithm works let make one important remark jointly probable sequence states necessarily sequence marginally probable states former given equation viterbi computes whereas latter given maximizer posterior marginals mpm arg max arg max inference hmms figure trellis states time markov chain based rabiner simple example difference consider chain two time steps deﬁning following joint joint map estimate whereas sequence marginal mpms advantage joint map estimate always globally consistent example suppose performing speech recognition someones says recognize speech could mis heard wreck nice beach locally may appear beach probable interpretation particular window sound add requirement data explained single linguistically plausible path interpretation becomes less likely hand mpm estimates robust marroquin see note viterbi estimate max variables arg max max whereas use forwards backwards sum variables makes mpm equation robust since estimate node averaging neighbors rather conditioning speciﬁc value neighbors general may want mix max sum example consider joint distribution observe chapter markov hidden markov models details algorithm tempting think implement viterbi replacing sum operator forwards backwards max operator former called sum product latter max product algorithm unique mode running max product computing using equation give result using equation weiss freeman general lead incorrect results multiple equally probably joint assignments reasons node breaks ties independently hence may manner inconsistent neighbors viterbi algorithm therefore quite simple replacing sum max particular forwards pass use max product backwards pass uses traceback procedure recover probable path trellis states essentially picks probable state previous nodes condition event therefore break ties consistently detail deﬁne max probability ending state time given take probable path key insight probable path state time must consist probable path state time followed transition hence max also keep track likely previous state possible state end argmax tells likely previous state probable path initialize setting terminate computing probable ﬁnal state arg max compute probable sequence states using traceback usual worry numerical underﬂow free normalize terms step affect maximum however unlike forwards backwards case want query let remaining nuisance variables deﬁne map estimate arg max max sum contrast deﬁne mpe probable explanation arg max max terminology due pearl although widely used outside bayes net literatire obviously map mpe however summing nuisance variables give different results maxing summing nuisance variables sensible computationally harder need combine max sum operations lerner parr inference hmms amp amp amp amp amp amp amp figure illustration viterbi decoding simple hmm speech recognition state hmm single phone visualizing state transition diagram assume observations vector quantized possible symbols state different distribution symbols based figure russell norvig illustration viterbi algorithm applied model data sequence columns represent time rows represent states arrow state state annotated two numbers ﬁrst probability transition second probability generating observation state bold lines circles represent probable sequence states based figure russell norvig also easily work log domain key difference log max max log whereas log log hence use log max log max log log log case gaussian observation models result signiﬁcant constant factor speedup since computing log much faster computing high dimensional gaussian one reason viterbi algorithm widely used step section training large speech recognition systems based hmms example figure gives worked example viterbi algorithm based russell suppose observe discrete sequence observations representing codebook entries vector quantized version speech signal model starts state probability generating states next self transition probability transition proabability end probability generating end chapter markov hidden markov models probability generating hence thus state probable see second column figure time step see two paths bold arrow indicates latter probable hence one remember algorithm continues way reached end sequence one reached end follow black arrows back recover map path time space complexity time complexity viterbi clearly general space complexity forwards backwards transition matrix form exp continuous vector represented state implement viterbi time instead log needed forwards backwards see section details best list viterbi algorithm returns one probable paths extended return top paths schwarz chow nilsson goldberger called best list use discriminative method rerank paths based global features derived fully observed state sequence well visible features technique widely used speech recognition example consider sentence recognize speech possible probable interpretation system acoustic signal wreck nice speech maybe wreck nice beach maybe correct interpretation much lower list however using ranking system may able improve score correct interpretation based global context one problem best list often top paths similar rather representing qualitatively different interpretations data instead might want generate diverse set paths accurately represent posterior uncertainty one way sample paths posterior discuss ways generate diverse map estimates see yadollahpour kulesza taskar 
[markov, hidden, markov, models, riiolqh, forwards, ﬁltering, backwards, sampling] often useful sample paths posterior follow run forwards backwards compute two slice smoothed posteri ors next compute conditionals normalizing sample initial pair states ﬁnally recursively sample note solution requires forwards backwards pass additional forwards sampling pass alternative forwards pass perform sampling learning hmms backwards pass key insight write joint right left using sample given future sampled states using sampling distribution given base case algorithm forms basis blocked gibbs sampling methods parameter inference see 
[markov, hidden, markov, models, learning, hmms] discuss estimate parameters initial state distribution transition matrix parameters class conditional densities ﬁrst consider case observed training set harder case hidden 
[markov, hidden, markov, models, learning, hmms, training, fully, observed, data] observe hidden state sequences compute mles exactly section use conjugate prior also easily compute posterior details estimate depend form observation model situation identical ﬁtting generative classiﬁer example state multinoulli distribution associated parameters represents observed symbol mle given chapter markov hidden markov models result quite intuitive simply add number times state see symbol divide number times state similarly state gaussian distribution associated sec tion following mles sufficient statistics given analogous results derived kinds distributions one also easily extend results compute map estimates even full posteriors parameters 
[markov, hidden, markov, models, learning, hmms, hmms, the, baum-welch, algorithm] variables observed situation analogous ﬁtting mixture model common approach use algorithm ﬁnd mle map parameters although course one could use gradient based methods see baldi chauvin section derive algorithm applied hmms also known baum welch algorithm baum step straightforward show expected complete data log likelihood given old log log old log expected counts given old old old learning hmms expected sufficient statistics computed running forwards backwards algo rithm sequence particular algorithm computes following smoothed node edge marginals step based section step normalize expected counts result quite intuitive simply add expected number transitions divide expected number times transition anything else multinoulli observation model expected sufficient statistics step form result quite intuitive simply add expected number times state see symbol divide expected number times state gaussian observation model expected sufficient statistics given step becomes regularized way regularize gmms initialization usual must take care ensure initialize parameters carefully minimize chance getting stuck poor local optima several ways chapter markov hidden markov models use fully labeled data initialize parameters initially ignore markov dependencies estimate observation parameters using standard mixture model estimation methods means randomly initialize parameters use multiple restarts pick best solution techniques deterministic annealing ueda nakano rao rose help mitigate effect local minima also means often used initialize gmms common initialize hmms using viterbi training means approximating posterior paths single probable path necessarily good idea since initially parameters often poorly estimated viterbi path fairly arbitrary safer option start training using forwards backwards switch viterbi near convergence 
[markov, hidden, markov, models, learning, hmms, bayesian, methods, “ﬁtting”, hmms] returns map estimate parameters section brieﬂy discuss methods bayesian parameter estimation hmms methods rely material cover later book one approach use variational bayes vbem discuss general terms section details hmm case found mackay beal basic idea step uses forwards backwards roughly speaking plug posterior mean parameters instead map estimates step updates parameters conjugate posteriors instead updating parameters alternative vbem use mcmc particularly appealing algorithm block gibbs sampling discuss general terms section details hmm case found fruhwirth schnatter basic idea sample given data parameters using forwards ﬁltering backwards sampling sample parameters posteriors conditional sampled latent paths simple implement one need take care unidentiﬁability label switching mixture models see section 
[markov, hidden, markov, models, learning, hmms, discriminative, training] sometimes hmms used class conditional density inside generative classiﬁer case computed using forwards algorithm easily maximize joint likelihood using method hmm class conditional density separately however might like ﬁnd parameters maximize conditional likelihood expensive maximizing joint likelihood since denominator couples class conditional hmms together furthermore longer used one must resort generalizations hmms generic gradient based methods nevertheless discriminative training result improved accuracies standard practice speech recognition initially train generative models separately using ﬁne tune discriminatively jelinek 
[markov, hidden, markov, models, learning, hmms, model, selection] hmms two main model selection issues many states topology use state transition diagram discuss issues choosing number hidden states choosing number hidden states hmm analogous problem choosing number mixture components possible solutions use grid search range using objective function cross validated likelihood bic score variational lower bound log marginal likelihood use reversible jump mcmc see fruhwirth schnatter details note slow widely used use variational bayes extinguish unwanted components analogy gmm case discussed section see mackay beal details use inﬁnite hmm based hierarchical dirichlet process see beal teh details structure learning term structure learning context hmms refers learning sparse transition matrix want learn structure state transition diagram structure graphical model ﬁxed large number heuristic methods proposed alternate parameter estimation kind heuristic split merge method see stolcke omohundro alternatively one pose problem map estimation using minimum entropy prior form exp prior prefers states whose outgoing distribution nearly deterministic hence low entropy brand corresponding step cannot solved closed form numerical methods used trouble might prune incoming transitions state creating isolated islands state space inﬁnite hmm presents interesting alternative methods see beal teh details 
[markov, hidden, markov, models, generalizations, hmms] many variants basic hmm model proposed brieﬂy discuss chapter markov hidden markov models figure encoding hidden semi markov model dgm deterministic duration counters 
[markov, hidden, markov, models, generalizations, hmms, variable, duration, semi-markov, hmms] standard hmm probability remain state exactly steps exp log self loop probability called geometric distribution however kind exponentially decaying function sometimes unrealistic allow general durations one use semi markov model called semi markov predict next state sufficient condition past state also need know long state state space observed directly result called hidden semi markov model hsmm variable duration hmm explicit duration hmm hsmms widely used many gene ﬁnding programs since length distribution exons introns geometric see schweikerta chip seq data analysis programs see kuan hsmms useful model waiting time state accurately also model distribution whole batch observations instead assuming observations conditionally iid use likelihood models form generate correlated observations duration state time steps useful modeling data piecewise linear shows local trends ostendorf hsmm augmented hmms one way represent hsmm use graphical model shown figure ﬁgure assumed observations iid within state required mentioned node state duration counter maximum duration state ﬁrst enter state sample duration distribution state thereafer deterministically counts generalizations hmms figure markov chain repeated states self loops resulting distribution sequence lengths various figure generated hmmselfloopdist state allowed change make stochastic transition new state precisely deﬁne cpds follows otherwise otherwise note could represented table non parametric approach kind parametric distribution gamma distribution geometric distribution emulates standard hmm one perform inference model deﬁning mega variable however rather inefficient since deterministic possible marginalize derive special purpose inference procedures see guedon kobayashi details unfortunately methods take time sequence length number states maximum duration state approximations semi markov models efficient less ﬂexible way model non geometric waiting times replace state new states emission probabilities original state example consider model figure obviously smallest sequence generate length path length model probability multiplying number possible paths ﬁnd total probability path length chapter markov hidden markov models need words phones sub phones end end end end end figure example hhmm asr system recognize words top level represents bigram word probabilities middle level represents phonetic spelling word bottom level represents subphones phone traditional represent phone state hmm representing beginning middle end based figure jurafsky martin equivalent negative binomial distribution adjusting self loop probabilities state model wide range waiting times see figure let number expansions state needed approximate forwards backwards model takes time average number predecessor states compared hsmm typical speech recognition applications similar ﬁgures apply problems gene ﬁnding also often uses hsmms since expanded state method much faster hsmm see johnson details 
[markov, hidden, markov, models, generalizations, hmms, hierarchical, hmms] hierarchical hmm hhmm fine extension hmm designed model domains hierarchical structure figure gives example hhmm used automatic speech recognition phone subphone models called different higher level contexts always ﬂatten hhmm regular hmm factored representation often easier interpret allows efficient inference model ﬁtting hhmms used many application domains speech recognition bilmes gene ﬁnding plan recognition bui monitoring transportation patterns liao indoor robot localization theocharous etc hhmms less expressive stochastic context free grammars scfgs since allow hierarchies bounded depth support efficient inference particular inference scfgs using inside outside algorithm jurafsky martin takes whereas inference hhmm takes time murphy paskin represent hhmm directed graphical model shown figure represents state time level state transition level allowed generalizations hmms figure hhmm represented dgm state time level hmm level ﬁnished entered exit state otherwise shaded nodes observed remaining nodes hidden may optionally clamp length observation sequence ensure models ﬁnished end sequence source figure murphy paskin chain level ﬁnished determined node chain ﬁnishes chooses enter end state mechanism ensures higher level chains evolve slowly lower level chains lower levels nested within higher levels variable duration hmm thought special case hhmm top level deterministic counter bottom level regular hmm change states counter timed see murphy paskin details 
[markov, hidden, markov, models, generalizations, hmms, input-output, hmms] straightforward extend hmm handle inputs shown figure deﬁnes conditional density model sequences form input time sometimes called control signal inputs outputs continuous typical parameterization would cat thus transition matrix logistic regression model whose parameters depend previous state observation model gaussian whose parameters depend current chapter markov hidden markov models figure input output hmm first order auto regressive hmm second order buried markov model depending value hidden variables effective graph structure com ponents observed variables non zero elements regression matrix precision matrix change although shown state whole model thought hidden version maximum entropy markov model section conditional inputs parameters one apply standard forwards backwards algorithm estimate hidden states also straightforward derive algorithm estimate parameters see bengio frasconi details 
[markov, hidden, markov, models, generalizations, hmms, auto-regressive, buried, hmms] standard hmm assumes observations conditionally independent given hidden state practice often case however straightforward direct arcs well figure known auto regressive hmm regime switching markov model continuous data observation model becomes linear regression model parameters chosen according current hidden state also consider higher order extensions condition last observations models widely used econometrics hamilton similar models deﬁned discrete observations hmm essentially combines two markov chains one hidden variables capture long range dependencies one observed variables capture short range dependen cies berchtold since nodes observed connections generalizations hmms figure factorial hmm chains coupled hmm chains change computation local evidence inference still performed using stan dard forwards backwards algorithm parameter estimation using also straightforward step unchanged step transition matrix assume scalar observations notational simplicty step involves minimizing log focussing terms see requires solving weighted least squares problems smoothed posterior marginal weighted linear regression problem design matrix toeplitz form subproblem solved efficiently using levinson durbin method durbin koopman buried markov models generalize hmms allowing dependency structure observable nodes change based hidden state figure model called dynamic bayesian multi net since mixture different networks linear gaussian setting change structure arcs using sparse regression matrices change structure connections within components using sparse gaussian graphical models either directed undirected see bilmes details 
[markov, hidden, markov, models, generalizations, hmms, factorial, hmm] hmm represents hidden state using single discrete random variable represent bits information would require states contrast consider distributed representation hidden state represents chapter markov hidden markov models bit hidden state represent bits using binary variables illustrated figure model called factorial hmm ghahramani jordan hope kind model could capture different aspects signal one chain would represent speaking style another words spoken unfortunately conditioned hidden variables correlated due explaining away common observed child make exact state estimation intractable however derive efficient approximate inference algorithms discuss section 
[markov, hidden, markov, models, generalizations, hmms, coupled, hmm, inﬂuence, model] multiple related data streams use coupled hmm brand illustrated figure series hmms state transitions depend states neighboring chains represent joint conditional distribution used various tasks audio visual speech recognition neﬁan modeling freeway traffic ﬂows kwon murphy trouble model requires parameters specify chains states per chain state depends past plus past two neighbors closely related model known inﬂuence model asavathiratham uses fewer parameters models joint conditional distribution use convex combination pairwise transition matrices parameter speciﬁes much inﬂuence chain chain model takes parameters specify furthermore allows chain inﬂuenced chains nearest neighbors hence corresponding graphical model similar figure except node incoming edges previous nodes used various tasks modeling conversational interactions people basu unfortunately inference models takes time since chains become fully correlated even interaction graph sparse various approximate inference methods applied discuss later 
[markov, hidden, markov, models, generalizations, hmms, dynamic, bayesian, networks, dbns] dynamic bayesian network way represent stochastic process using directed graphical model note network dynamic structure parameters ﬁxed acronym dbn stand either dynamic bayesian network deep belief network section depending context geoff hinton invented term deep belief network suggested acronyms dybn deebn avoid ambiguity generalizations hmms sensorvalid fydotdiff fcloseslow xdot xdot inlane inlane leftclr leftclr rightclr rightclr lataction lataction fwdaction fwdaction ydot ydot stopped stopped bxdot engstatus engstatus bclosefast frontbackstatus frontbackstatus bydotdiff fclr bclr xdotsens ydotsens leftclrsens rightclrsens turnsignal fydotdiffsens fclrsens bxdotsens bclrsens bydotdiffsens slice slice evidence figure batnet dbn transient nodes shown second slice minimize clutter dotted lines ignored used kind permission daphne koller rather network representation dynamical system hmm variants seen could considered dbns however prefer reserve term dbn graph structures irregular problem speciﬁc example shown figure dbn designed monitor state simulated autonomous car known bayesian automated taxi batmobile forbes deﬁning dbns straightforward need specify structure ﬁrst time slice structure two time slices form cpds learning also easy main problem exact inference computationally expensive hidden variables become correlated time known entanglement see koller friedman sec details thus sparse graph necessarily result tractable exact inference however later see algorithms exploit graph structure efficient approximate inference 
[markov, hidden, markov, models, exercises] exercise derivation function hmm derive equation exercise two ﬁlter approach smoothing hmms assuming derive recursive algorithm updating hint similar standard forwards algorithm using time reversed transition matrix show compute posterior marginals chapter markov hidden markov models backwards ﬁltered messages forwards ﬁltered messages stationary distribution exercise hmms mixture gaussian observations consider hmm observation model form draw dgm derive step derive step exercise hmms tied mixtures many applications common observations high dimensional vectors speech recognition often vector cepstral coefficients derivatives estimating full covariance matrix values number mixture components per hidden state exercise requires lot data alternative use gaussians rather gaussians let state inﬂuence mixing weights means covariances called semi continuous hmm tied mixture hmm draw corresponding graphical model derive step derive step 
[state, space, models, introduction] state space model ssm like hmm except hidden states continuous model written following generic form hidden state optional input control signal observation transition model observation model system noise time observation noise time assume parameters model known included hidden state discuss one primary goals using ssms recursively estimate belief state note often drop conditioning brevity discuss algorithms later chapter also discuss convert beliefs hidden state predictions future observables computing posterior predictive important special case ssm cpds linear gaussian words assume transition model linear function observation model linear function system noise gaussian observation noise gaussian model called linear gaussian ssm ssm linear dynamical system lds parameters independent time model called stationary chapter state space models observed truth observed filtered observed smoothed figure illustration kalman ﬁltering smoothing observations green cirles generated object moving right true location denoted black squares filtered estimated shown dotted red line red cross posterior mean blue circles conﬁdence ellipses derived posterior covariance clarity plot ellipses every time step using offline kalman smoothing figure generated kalmantrackingdemo ssm important supports exact inference see particular initial belief state gaussian subsequent belief states also gaussian denote notation denotes similarly thus denotes prior seen data brevity denote posterior belief states using compute quantities efficiently using celebrated kalman ﬁlter show section discussing algorithms discuss important applications 
[state, space, models, applications, ssms] local level figure local level model sample output black solid line deterministic system noisy observations red dotted line noisy system deterministic observation blue dot dash line noisy system observations figure generated ssmtimeseriessimple local trend figure local trend sample output color code figure figure generated ssmtimeseriessimple local linear trend many time series exhibit linear trends upwards downwards least locally model letting level change amount step follows see figure write standard form deﬁning constant deﬁning slope line addition unrolling applications ssms seasonal model figure seasonal model sample output period color code figure figure generated ssmtimeseriessimple hence thus generalization classic constant linear trend model example shown black line figure seasonality many time series ﬂuctuate periodically illustrated figure modeled adding latent process consisting series offset terms sum zero average complete cycle steps see figure graphical model case need seasonal vari able sum zero constraint writing standard ssm form left exercise arma models classical approach time series forecasting based arma models arma stands auto regressive moving average refers model form independent gaussian noise terms pure model example model chapter state space models figure model model represented directed graph arma model source figure choi used kind permission myung choi shown figure nodes implicit gaussian cpd ﬁrst order markov chain pure model example model shown figure nodes hidden common causes induces dependencies adjacent time steps models short range correlation get arma model shown figure captures correlation short long time scales turns arma models represented ssms explained aoki harvey west harrison durbin koopman petris prado west however structural approach time series often easier understand arma approach addition allows parameters evolve time makes models adaptive non stationarity 
[state, space, models, applications, ssms, ssms, object, tracking] one earliest applications kalman ﬁltering tracking objects airplanes missiles noisy measurements radar give simpliﬁed example illustrate key ideas consider object moving plane let horizontal vertical locations object corresponding velocity represent state vector follows applications ssms let assume object moving constant velocity perturbed random gaussian noise due wind thus model system dynamics follows system noise sampling period says new location old location plus times old velocity plus random noise also new velocity old velocity plus random noise called random accelerations model since object moves according newton laws subject random changes velocity suppose observe location object velocity let represent observation assume subject gaussian noise model follows measurement noise finally need specify initial prior beliefs state object assume gaussian represent prior ignorance making suitably broad fully speciﬁed model perform sequential bayesian updating compute using algorithm known kalman ﬁlter described section figure gives example object moves right generates observation time step think blips radar screen observe blips ﬁlter noise using kalman ﬁlter every step compute marginalizing dimensions corresponding velocities easy since posterior gaussian best guess location object posterior mean denoted red cross figure uncertainty associated represented ellipse contains probability mass see uncertainty goes time effects initial uncertainty get washed also see estimated trajectory ﬁltered noise obtain much smoother plot figure need use kalman smoother computes depends future well past data discussed section 
[state, space, models, applications, ssms, robotic, slam] consider robot moving around unknown world needs learn map keep track location within map problem known simultaneous localization chapter state space models figure illustration graphical model underlying slam ﬁxed location landmark location robot observation trace robot sees landmarks time step landmark landmark etc based figure koller friedman robot pose figure illustration slam problem robot starts top left moves clockwise circle back started see posterior uncertainty robot location increases decreases returns familar location closing loop performed smoothing new information would propagate backwards time disambiguate entire trajectory show precision matrix representing sparse correlations landmarks landmarks robot position pose sparse precision matrix visualized gaussian graphical model shown source figure koller friedman used kind permission daphne koller applications ssms mapping slam short widely used mobile robotics well applications indoor navigation using cellphones since gps work inside buildings let assume represent map locations ﬁxed set landmarks denote vector simplicity assume uniquely identiﬁable let represent unknown location robot time deﬁne state space assume landmarks static motion model constant system noise measures distance set closest landmarks robot update estimate landmark locations based sees figure shows corresponding graphical model case ﬁrst step sees landmarks landmark landmark etc assume observation model linear gaussian use gaussian motion model use kalman ﬁlter maintain belief state location robot location landmarks smith cheeseman choset nagatani time uncertainty robot location increase due wheel slippage etc robot returns familiar location uncertainty decrease called closing loop illustrated figure see uncertainty ellipses representing cov grow shrink note section assume human joysticking robot environment given input address decision theoretic issue choosing explore since belief state gaussian visualize posterior covariance matrix tually interesting visualize posterior precision matrix since fairly sparse shown figure reason zeros precision matrix correspond absent edges corresponding undirected gaussian graphical model see section initially landmarks uncorrelated assuming diagonal prior ggm disconnected graph diagonal however robot moves induce correlation nearby landmarks intuitively robot estimating position based distance landmarks landmarks locations estimated based robot position become inter dependent seen clearly graphical model figure clear separated path via unknown sequence nodes consequence precision matrix becoming denser exact inference takes time example entanglement problem inference dbns prevents method applied large maps two main solutions problem ﬁrst notice correlation pattern moves along location robot see figure remaining correlations become weaker time consequently dynamically prune weak edges ggm using technique called thin junction tree ﬁlter paskin junction trees explained section second approach notice conditional knowing robot path landmark locations independent forms basis method known fastslam combines kalman ﬁltering particle ﬁltering discussed section thrun provides detailed account slam mobile robotics chapter state space models time online linear regression batch batch figure dynamic generalization linear regression illustration recursive least squares algorithm applied model plot marginal posterior number data points error bars represent var seeing data converge offline least squares solution represented horizontal lines figure generated linregonlinedemokalman 
[state, space, models, applications, ssms, online, parameter, learning, using, recursive, least, squares] perform online bayesian inference parameters various statistical models using ssms section focus linear regression section discuss logistic regression basic idea let hidden state represent regression parameters let time varying observation model represent current data vector detail deﬁne prior want online estimation set let hidden state assume regression parameters change set let parameters change time get called dynamic linear model harvey west harrison petris let non stationary observation model form applying kalman ﬁlter model provides way update posterior beliefs parameters data streams known recursive least squares rls algorithm derive explicit form updates follows section show kalman update posterior mean form applications ssms known kalman gain matrix based equation one show context hence update parameters becomes approximate recover least mean squares lms algorithm discussed section lms need specify adapt update parameter ensure convergence mle furthermore algorithm may take multiple passes data contrast rls algorithm automatically performs step size adaptation converges optimal posterior one pass data see figure example 
[state, space, models, applications, ssms, ssm, time, series, forecasting] ssms well suited time series forecasting explain focus case scalar one dimensional time series simplicity presentation based varian see also aoki harvey west harrison durbin koopman petris prado west good books topic ﬁrst sight might apparent ssms useful since goal forecasting predict future visible variables estimate hidden states system indeed classical methods time series forecasting functions form hidden variables play role see section idea state space approach time series create generative model data terms latent processes capture different aspects signal integrate hidden variables compute posterior predictive visibles since model linear gaussian add processes together explain observed data called structural time series model explain basic building blocks local level model simplest latent process known local level model form hidden state model asserts observed data equal unknown level term plus observation noise variance addition level evolves time subject system noise variance see figure examples chapter state space models 
[state, space, models, inference, lg-ssm] section discuss exact inference ssm models ﬁrst consider online case analogous forwards algorithm hmms consider offline case analogous forwards backwards algorithm hmms 
[state, space, models, inference, lg-ssm, kalman, ﬁltering, algorithm] kalman ﬁlter algorithm exact bayesian ﬁltering linear gaussian state space models represent marginal posterior time since everything gaussian perform prediction update steps closed form explain resulting algorithm gaussian analog hmm ﬁlter section inference ssm prediction step prediction step straightforward derive measurement step measurement step computed using bayes rule follows section show given residual innovation given difference predicted observa tion actual observation kalman gain matrix given cov observation noise term independent noise sources note using matrix inversion lemma kalman gain matrix also written quantities need implement algorithm see kalmanfilter matlab code let try make sense equations particular consider equation mean update says new mean old mean plus chapter state space models correction factor times error signal amount weight placed error signal depends kalman gain matrix ratio covariance prior dynamic model covariance measurement error strong prior noisy sensors small place little weight correction term conversely weak prior high precision sensors large place lot weight correction term marginal likelihood byproduct algorithm also compute log likelihood sequence using log log posterior predictive one step ahead posterior predictive density observations computed follows useful time series forecasting computational issues two dominant costs kalman ﬁlter matrix inversion compute kalman gain matrix takes time matrix matrix multiply compute takes time applications robotic mapping latter cost dominates however cases sometimes use sparse approximations see thrun cases precompute since suprisingly depend actual observations unusual property speciﬁc linear gaussian systems iterative equations updating called ricatti equations time invariant systems converge ﬁxed point steady state solution used instead using time speciﬁc gain matrix practice sophisticated implementations kalman ﬁlter used rea sons numerical stability one approach information ﬁlter recursively updates canonical parameters gaussian instead moment parameters another approach square root ﬁlter works cholesky composition decomposition much numerically stable directly updating details found http www unc edu welch kal man various books simon inference ssm derivation derive kalman ﬁlter equations notational simplicity ignore input terms bayes rule gaussians equation posterior precision given matrix inversion lemma equation rewrite bayes rule gaussians equation posterior mean given massage form stated earlier applying second matrix inversion lemma equation ﬁrst term equation applying matrix inversion lemma equation second term equation putting two together get 
[state, space, models, inference, lg-ssm, kalman, smoothing, algorithm] section described kalman ﬁlter sequentially computes useful online inference problems tracking however offline setting wait data arrived compute conditioning past future data uncertainty signiﬁcantly reduced illustrated figure see posterior covariance ellipsoids smaller smoothed trajectory ﬁltered trajectory ellipsoids larger beginning end trajectory since states near boundary many useful neighbors borrow information chapter state space models explain compute smoothed estimates using algorithm called rts smoother named inventors rauch tung striebel rauch also known kalman smoothing algorithm algorithm analogous forwards backwards algorithm hmms although small differences discuss algorithm kalman ﬁltering regarded message passing graph left right messages reached end graph successfully computed work backwards right left sending information future back past combining two information sources question compute backwards equations ﬁrst give equations derivation backwards kalman gain matrix algorithm initialized kalman ﬁlter note backwards pass need access data need allows throw away potentially high dimensional observation vectors keep ﬁltered belief states usually requires less memory derivation derive kalman smoother following presentation jordan sec key idea leverage markov property says independent future data long known course known distribution condition integrate follows induction assume already computed smoothed distribution question perform integration first compute ﬁltered two slice distribution follows inference ssm use gaussian conditioning compute follows compute smoothed distribution using rules iterated expectation iterated covariance first mean covariance cov cov cov cov cov cov algorithm initialized last step ﬁltering algo rithm comparison forwards backwards algorithm hmms note forwards backwards passes lds always worked normalized distributions either conditioned past data conditioned data furthermore backwards pass depends results forwards pass different usual presentation forwards backwards hmms backwards pass computed independently forwards pass see section turns rewrite kalman smoother modiﬁed form makes similar forwards backwards hmms particular chapter state space models conditional likelihood future data backwards message computed independently forwards message however approach several disadvantages needs access original observation sequence backwards message likelihood posterior need integrate fact may always possible represent gaussian positive deﬁnite covariance problem arise discrete state spaces used hmms exact inference possible makes sense try approximate smoothed distribution rather backwards likelihood term see section yet another variant known two ﬁlter smoothing whereby compute forwards pass usual ﬁltered posterior backwards pass easily combined compute see kitagawa briers details 
[state, space, models, learning, lg-ssm] section brieﬂy discuss estimate parameters ssm control theory community known systems identiﬁcation ljung using ssms time series forecasting also physical state estimation problems observation matrix transition matrix known ﬁxed deﬁnition model cases needs learned noise covariances initial state estimate often less important since get washed away data time steps encouraged setting initial state covariance large representing weak prior although estimate offline using methods described also possible derive recursive procedure exactly compute posterior form normal inverse wishart see west harrison prado west details 
[state, space, models, learning, lg-ssm, identiﬁability, numerical, stability] general setting hidden states pre speciﬁed meaning need learn however case set without loss generality since arbitrary noise covariance modeled appropriately modifying also analogy factor analysis require diagonal without loss generality reduces number free parameters improves numerical stability another constraint useful impose eigenvalues dynamics matrix see important consider case system noise case hidden state time given matrix eigenvectors diag contains eigenvalues large blow magnitude consequently ensure stability useful require eigenvalues less siddiqi course eigenvalues less large state return origin fortunately add noise state become non zero model degenerate approximate online inference non linear non gaussian ssms discuss estimate parameters however simplicity presentation impose constraints mentioned 
[state, space, models, learning, lg-ssm, training, fully, observed, data] observe hidden state sequences model computing mles even full posteriors parameters solving multivariate linear regression problem estimate solving least squares problem similarly estimate system noise covariance residuals predicting estimate observation noise covariance residuals predicting 
[state, space, models, learning, lg-ssm, lg-ssm] observe output sequence compute map estimates parameters using method conceptually quite similar baum welch algorithm hmms section except use kalman smoothing instead forwards backwards step use different calculations step leave details exercise 
[state, space, models, learning, lg-ssm, subspace, methods] always give satisfactory results sensitive initial parameter estimates one way avoid use different approach known subspace method overschee moor katayama understand approach let initially assume observation noise system noise case hence consequently observations must generated dim dimensional linear mani fold subspace identify subspace using pca see references details estimate model fully observed either use estimates right use initialize 
[state, space, models, learning, lg-ssm, bayesian, methods, “ﬁtting”, lg-ssms] various offline bayesian alternatives algorithm including variational bayes beal barber chiappa blocked gibbs sampling carter kohn cappe fruhwirth schnatter bayesian approach also used perform online learning discussed section unfortunately add ssm parameters state space model generally longer linear gaussian consequently must use approximate online inference methods discussed 
[state, space, models, approximate, online, inference, non-linear, non-gaussian, ssms] section discussed perform exact online inference ssms however many models non linear example moving objects move straight lines even assume parameters model unknown add chapter state space models state space model becomes nonlinear furthermore non gaussian noise also common due outliers inferring parameters glms instead linear regression general models need use approximate inference approximate inference algorithms discuss approximate posterior gaus sian general gaussian distribution non linear function two main ways approximate gaussian ﬁrst use ﬁrst order approximation second use exact project onto space gaussians moment matching discuss methods turn see also section discuss particle ﬁltering stochastic algorithm approximate online inference uses non parametric approximation posterior often accurate slower compute 
[state, space, models, approximate, online, inference, non-linear, non-gaussian, ssms, extended, kalman, ﬁlter, ekf] section focus non linear models assume noise gaussian consider models form transition model observation model nonlinear differentiable functions furthermore focus case approximate posterior single gaussian simplest way handle general posteriors multi modal discrete etc use particle ﬁltering discuss section extended kalman ﬁlter ekf applied nonlinear gaussian dynamical systems form basic idea linearize previous state estimate using ﬁrst order taylor series expansion apply standard kalman ﬁlter equations noise variance equations changed additional error due linearization modeled thus approximate stationary non linear dynamical system non stationary linear dynamical system intuition behind approach shown figure shows happens pass gaussian distribution shown bottom right nonlinear function shown top right resulting distribution approximated monte carlo shown shaded gray area top left corner best gaussian approximation computed var monte carlo shown solid black line ekf approximates gaussian follows linearizes function current mode passes gaussian distribution linearized function example result quite good approximation ﬁrst second moments much less cost approximation detail method works follows approximate measurement model using jacobian matrix evaluated prior mode approximate online inference non linear non gaussian ssms gaussian mean ekf gaussian mean ekf function taylor approx mean mean figure nonlinear transformation gaussian random variable prior shown bottom right function shown top right transformed distribution shown top left linear function induces gaussian distribution non linear function induces complex distribution solid line best gaussian approximation dotted line ekf approximation source figure thrun used kind permission sebastian thrun similarly approximate system model using jacobian matrix evaluated prior mode given apply kalman ﬁlter compute posterior follows chapter state space models mean covariance sigma points actual sampling sigma point linearized ekf 
[state, space, models, unscented, kalman, ﬁlter, ukf] approximate online inference non linear non gaussian ssms ukf basically uses unscented transform twice approximate passing system model approximate passing measurement model give details note ukf ekf perform operations per time step size latent state space however ukf accurate least second order whereas ekf ﬁrst order approximation although ekf ukf extended capture higher order terms furthermore unscented transform require analytic evaluation derivatives jacobians called derivative free ﬁlter making simpler implement widely applicable unscented transform explaining ukf ﬁrst explain unscented transform assume consider estimating nonlinear function unscented transform follows first create set sigma points given scaling parameter speciﬁed notation means column matrix sigma points propagated nonlinear function yield mean covariance computed follows weighting terms given see figure illustration general optimal values problem dependent thus case sigma points ukf algorithm ukf algorithm simply two applications unscented tranform one compute compute give details chapter state space models ﬁrst step approximate predictive density passing old belief state system model follows second step approximate likelihood passing prior observation model finally use bayes rule gaussians get posterior 
[state, space, models, assumed, density, ﬁltering, adf] section discuss inference perform exact update step approx imate posterior distribution certain convenient form gaussian precisely let unknowns want infer denoted suppose set tractable distributions gaussians diagonal covariance matrix product discrete distributions suppose approximate prior update new measurement get approximate posterior approximate online inference non linear non gaussian ssms 
[state, space, models, hybrid, discrete/continuous, ssms] many systems contain discrete continuous hidden variables known hybrid systems example discrete variables may indicate whether measurement sensor faulty regime system see examples special case hybrid system combine hmm ssm called switching linear dynamical system slds jump markov linear system jmls switching state space model sssm precisely discrete latent variable continuous latent variable continuous observed response optional continuous observed input control assume continuous variables linear gaussian cpds conditional discrete states see figure dgm representation chapter state space models figure switching linear dynamical system squares represent discrete nodes circles represent continuous nodes illustration number modes belief state grows exponentially time assume two binary states 
[state, space, models, hybrid, discrete/continuous, ssms, inference] unfortunately inference state estimation hybrid models including switching ssm model intractable see suppose binary dynamics depend observation matrix initial belief state mixture gaussians corresponding one step ahead predictive density mixture gaussians obtained passing prior modes possible transition models belief state step also mixture gaussians obtained updating distributions step belief state mixture gaussians see exponential explosion number modes see figure various approximate inference methods proposed model following prune low probability trajectories discrete tree basis multiple hypothesis tracking bar shalom fortmann bar shalom use monte carlo essentially sample discrete trajectories apply analytical ﬁlter continuous variables conditional trajectory see section details use adf approximate exponentially large mixture gaussians smaller mixture gaussians see section details gaussian sum ﬁlter switching ssms gaussian sum ﬁlter sorenson alspach approximates belief state step mixture gaussians implemented running kalman ﬁlters hybrid discrete continuous ssms filter filter filter filter bbn merge merge merge filter filter figure adf switching linear dynamical system gpb method imm method see text details parallel particularly well suited switching ssms describe one version algorithm known second order generalized pseudo bayes ﬁlter gpb bar shalom fortmann assume prior belief state mixture gaussians one per discrete state pass different linear models get tij tij finally value collapse gaussian mixtures single mixture give chapter state space models see figure sketch optimal way approximate mixture gaussians single gaussian given arg min solved moment matching cov graphical model literature called weak marginalization lauritzen since preserves ﬁrst two moments applying equations model follows drop subscript brevity algorithm requires running ﬁlters step cheaper alternative represent belief state single gaussian marginalizing discrete switch step straightforward application adf offline extension method called expectation correction described barber mesot barber another heuristic approach known interactive multiple models imm bar shalom fortmann obtained ﬁrst collapsing prior single gaussian moment matching updating using different kalman ﬁlters one per value see figure sketch 
[state, space, models, hybrid, discrete/continuous, ssms, application, data, association, multi-target, tracking] suppose tracking objects airplanes time observe detection events blips radar screen due occlusion missed detections due clutter false alarms case need ﬁgure correspondence detections objects called problem data association arises many application domains figure gives example tracking objects time step unknown mapping speciﬁes objects caused observations speciﬁes wiring diagram time slice standard way solve problem compute weight measures compatibility object measurement typically based close model thinks called nearest neighbor data association heuristic gives weight matrix make hybrid discrete continuous ssms figure model tracking two objects presence data assocation ambiguity observe detections ﬁrst three time steps square matrix size max adding dummy background objects explain false alarms adding dummy observations explain missed detections compute maximal weight bipartite matching using hungarian algorithm takes time see burkard conditional perform kalman ﬁlter update objects assigned dummy observations perform measurement update extension method handle variable unknown number objects known multi target tracking requires dealing variable sized state space many ways perhaps simplest robust methods based sequential monte carlo ristic mcmc khan 
[state, space, models, hybrid, discrete/continuous, ssms, application, fault, diagnosis] consider model figure represents industrial plant consisting various tanks liquid interconnected pipes example two tanks simplicity want estimate pressure inside tank based noisy measurement ﬂow tank however measurement devices sometimes fail furthermore pipes burst get blocked call resistance failure model widely used benchmark fault diagnosis community mosterman biswas create probabilistic model system shown figure square nodes represent discrete variables measurement failures resistance failures remaining variables continuous variety approximate inference algorithms applied model see koller lerner one approach based rao blackwellized particle ﬁltering explained section chapter state space models figure two tank system goal infer pipes blocked burst sensors broken noisy observations ﬂow tank tank tanks hidden variable representing resistance pipe tank hidden variable representing pressure tank etc source figure koller lerner used kind permission daphne koller dynamic bayes net representation two tank system discrete nodes squares continuous nodes circles abbreviations resistance pressure ﬂow measurement resistance failure measurement failure based figure koller lerner 
[state, space, models, hybrid, discrete/continuous, ssms, application, econometric, forecasting] switching ssm model widely used econometric forecasting called regime switching model example combine two linear trend models see sec tion one reﬂects growing economy one reﬂects shrinking economy see west harrison details 
[state, space, models, exercises] exercise derivation ssm derive steps computing locally optimal mle ssm model hint results ghahramani hinton task derive results exercise seasonal ssm model standard form write seasonal model figure ssm deﬁne matrices 
[undirected, graphical, models, markov, random, ﬁelds, introduction] chapter discussed directed graphical models dgms commonly known bayes nets however domains forced choose direction edges required dgm rather awkward example consider modeling image might suppose intensity values neighboring pixels correlated create dag model lattice topology shown figure known causal mrf markov mesh abend however conditional independence properties rather unnatural particular markov blanket deﬁned section node middle colored nodes rather nearest neighbors one might expect alternative use undirected graphical model ugm also called markov random ﬁeld mrf markov network require specify edge orientations much natural problems image analysis spatial statistics example undirected lattice shown figure markov blanket node nearest neighbors show section roughly speaking main advantages ugms dgms symmetric therefore natural certain domains spatial relational data discrimi nativel ugms aka conditional random ﬁelds crfs deﬁne conditional densities form work better discriminative dgms reasons explain section main disadvantages ugms compared dgms parameters less interpretable less modular reasons explain section parameter estimation com putationally expensive reasons explain section see domke empirical comparison two approaches image processing task 
[undirected, graphical, models, markov, random, ﬁelds, conditional, independence, properties, ugms, key, properties] ugms deﬁne relationships via simple graph separation follows sets nodes say iff separates graph means remove nodes paths connecting node node property holds called global markov property ugms example figure chapter undirected graphical models markov random ﬁelds figure lattice represented dag dotted red node independent nodes black given markov blanket include parents blue children green parents orange model represented ugm red node independent black nodes given neighbors blue nodes 
[undirected, graphical, models, markov, random, ﬁelds, conditional, independence, properties, ugms] figure dgm moralized version represented ugm set nodes renders node conditionally independent nodes graph called markov blanket denote formally markov blanket satisﬁes following property closure node one show ugm node markov blanket set immediate neighbors called undirected local markov property example figure local markov property easily see two nodes conditionally indepen dent given rest direct edge called pairwise markov property symbols written using three markov properties discussed derive following properties amongst others ugm figure pairwise rest local rest conditional independence properties ugms relationship markov properties ugms 
[undirected, graphical, models, markov, random, ﬁelds] figure ancestral graph induced dag figure wrt moralized version global obvious global markov implies local markov implies pairwise markov less obvious nevertheless true assuming positive density pairwise implies global hence markov properties illustrated figure see koller friedman proof importance result usually easier empirically assess pairwise conditional independence pairwise statements used construct graph global statements extracted 
[undirected, graphical, models, markov, random, ﬁelds, undirected, alternative, d-separation] seen determinining relationships ugms much easier dgms worry directionality edges section show determine relationships dgm using ugm tempting simply convert dgm ugm dropping orientation edges clearly incorrect since structure quite different properties corresponding undirected chain latter graph incorrectly states avoid incorrect statements add edges unmarried parents drop arrows edges forming case fully connected undirected graph process called moralization figure gives larger restriction positive densities arises deterministic constraints result independencies present distribution explicitly represented graph see koller friedman examples distributions non graphical properties said unfaithful graph chapter undirected graphical models markov random ﬁelds uredelolvwlf xrghov udsklfdo xrghov luhfwhg qgluhfwhg amp krugdo figure dgms ugms perfectly represent different sets distributions distributions perfectly represented either dgms ugms corresponding graph must chordal example moralization interconnect since common child interconnect since common child unfortunately moralization loses information therefore cannot use moralized ugm determine properties dgm example figure using separation see adding moralization arc would lose fact see figure however notice moralization edge due common child needed observe descendants suggests following approach determining first form ancestral graph dag respect means remove nodes ancestors moralize ancestral graph apply simple graph separation rules ugms example figure show ancestral graph figure using figure show moralized version graph clear correctly conclude 
[undirected, graphical, models, markov, random, ﬁelds, comparing, directed, undirected, graphical, models] model expressive power dgm ugm formalize question recall say map distribution deﬁne perfect map words graph represent properties distribution turns dgms ugms perfect maps different sets distributions see figure sense neither powerful representation language example relationships perfectly modeled dgm ugm consider structure asserts drop arrows get asserts incorrect fact ugm precisely represent two statements encoded structure general properties ugms monotonic following sense dgms properties non monotonic since conditioning parameterization mrfs figure ugm two failed attempts represent dgm source figure koller friedman used kind permission daphne koller extra variables eliminate conditional independencies due explaining away example relationships perfectly modeled ugm dgm consider cycle shown figure one attempt model dgm shown figure correctly asserts however incorrectly asserts figure another incorrect dgm correctly encodes incorrectly encodes fact dgm precisely represent statements encoded ugm distributions perfectly modeled either dgm ugm resulting graphs called decomposable chordal roughly speaking means following collapse together variables maximal clique make mega variables resulting graph tree course graph already tree includes chains special case chordal see section details 
[undirected, graphical, models, markov, random, ﬁelds, parameterization, mrfs] although properties ugm simpler natural dgms representing joint distribution ugm less natural dgm see 
[undirected, graphical, models, markov, random, ﬁelds, parameterization, mrfs, hammersley-clifford, theorem] since topological ordering associated undirected graph use chain rule represent instead associating cpds node associate potential functions factors maximal clique graph denote potential function clique potential function non negative function arguments joint distribution deﬁned proportional product clique potentials rather surprisingly one show positive distribution whose properties represented ugm represented way state result formally chapter undirected graphical models markov random ﬁelds theorem hammersley clifford positive distribution satisﬁes prop erties undirected graph iff represented product factors one per maximal clique set maximal cliques partition function given note partition function ensures overall distribution sums proof never published found koller friedman example consider mrf figure satisﬁes properties graph write follows deep connection ugms statistical physics particular model known gibbs distribution written follows exp energy associated variables clique convert ugm deﬁning exp see high probability states correspond low energy conﬁgurations models form known energy based models commonly used physics biochemistry well branches machine learning lecun note free restrict parameterization edges graph rather maximal cliques called pairwise mrf figure get form widely used due simplicity although general partition function denoted german word zustandssumme means sum states reﬂects fact lot pioneering working statistical physics done germans parameterization mrfs 
[undirected, graphical, models, markov, random, ﬁelds, parameterization, mrfs, representing, potential, functions] variables discrete represent potential energy functions tables non negative numbers cpts however potentials probabilities rather represent relative compatibility different assignments potential see examples general approach deﬁne log potentials linear function parameters log feature vector derived values variables resulting log probability form log also known maximum entropy log linear model example consider pairwise mrf edge associate feature vector length follows weight feature convert potential function follows exp exp see easily represent tabular potentials using log linear form log linear form general see useful suppose interested making probabilistic model english spelling since certain letter combinations occur together quite frequently ing need higher order factors capture suppose limit letter trigrams tabular potential still parameters however triples never occur alternative approach deﬁne indicator functions look certain special triples ing etc deﬁne potential trigram follows exp indexes different features corresponding ing etc corre sponding binary feature function tying parameters across locations deﬁne probability word length using exp raises question feature functions come many applications created hand reﬂect domain knowledge see examples later also possible learn data discuss section chapter undirected graphical models markov random ﬁelds 
[undirected, graphical, models, markov, random, ﬁelds, examples, mrfs] section show several popular probability models conveniently expressed ugms 
[undirected, graphical, models, markov, random, ﬁelds, examples, mrfs, ising, model] ising model example mrf arose statistical physics originally used modeling behavior magnets particular let represent spin atom either spin magnets called ferro magnets neighboring spins tend line direction whereas kinds magnets called anti ferromagnets spins want different neighbors model mrf follows create graph form lattice connect neighboring variables figure deﬁne following pairwise clique potential coupling strength nodes two nodes connected graph set assume weight matrix symmetric often assume edges strength assuming weights positive neighboring spins likely state used model ferromagnets example associative markov network weights sufficiently strong corresponding probability distribution two modes corresponding state state called ground states system weights negative spins want different neighbors used model anti ferromagnet results frustrated system constraints satisﬁed time corresponding probability distribution multiple modes interestingly computing partition function done polynomial time associative markov networks hard general cipra interesting analogy ising models gaussian graphical models first assuming write unnormalized log probability ising model follows log factor arises sum edge twice get low energy hence high probability neighboring states agree sometimes external ﬁeld energy term added spin modelled using local energy term form sometimes called ernst ising german american physicist examples mrfs bias term modiﬁed distribution given log deﬁne rewrite form looks similar gaussian exp one important difference case gaussians normalization constant requires computation matrix determinant computed time whereas case ising model normalization constant requires summing bit vectors equivalent computing matrix permanent hard general jerrum 
[undirected, graphical, models, markov, random, ﬁelds, examples, mrfs, hopﬁeld, networks] hopﬁeld network hopﬁeld fully connected ising model symmetric weight matrix weights plus bias terms learned training data using approximate maximum likelihood described section main application hopﬁeld networks associative memory content dressable memory idea suppose train set fully observed bit vectors corresponding patterns want memorize test time present partial pattern network would like estimate missing variables called pattern com pletion see figure example thought retrieving example memory based piece example hence term associative memory since exact inference intractable model standard use coordinate descent algorithm known iterative conditional modes icm sets node likely lowest energy state given neighbors full conditional shown sigm picking probable state amounts using rule using otherwise much better inference algorithms discussed later book since inference deterministic also possible interpret model recurrent neural network quite different feedforward neural nets studied section univariate conditional density models form used supervised learning see hertz details hopﬁeld networks boltzmann machine generalizes hopﬁeld ising model including hidden nodes makes model representationally powerful inference models often uses gibbs sampling stochastic version icm see section details estimation works much better outer product rule proposed hopﬁeld lowers energy observed patterns also raises energy non observed patterns order make distribution sum one hillar chapter undirected graphical models markov random ﬁelds figure examples associative memory reconstruct images binary images size pixels top training images row partially visible test images row estimate iterations bottom ﬁnal state estimate based figure hertz figure generated hopfielddemo figure visualizing sample state potts model size different association strengths regions labeled according size blue largest red smallest used kind permission erik sudderth see gibbsdemoising matlab code produce similar plot ising model however could equally well apply gibbs hopﬁeld net icm boltzmann machine inference algorithm part model deﬁnition see section details boltzmann machines examples mrfs figure grid structured mrf local evidence nodes 
[undirected, graphical, models, markov, random, ﬁelds, examples, mrfs, potts, model] easy generalize ising model multiple discrete states common use potential function following form called potts model neighboring nodes encouraged label samples model shown figure see large clusters occur many small clusters occur critical value mix small large clusters rapid change behavior vary parameter system called phase transition widely studied physics community analogous phenomenon occurs ising model see mackay details potts model used prior image segmentation since says neighboring pixels likely discrete label hence belong segment combine prior likelihood term follows probability observing pixel given corresponding segment belongs class observation model modeled using gaussian non parametric density note label hidden nodes observed nodes compatible section corresponding graphical model mix undirected directed edges shown figure undirected lattice represents prior addition directed edge corresponding representing local evidence technically speak ing combination undirected directed graph called chain graph however renfrey potts australian mathematician chapter undirected graphical models markov random ﬁelds since nodes observed absorbed model thus leaving behind undirected backbone model analog hmm could called partially observed mrf hmm goal perform posterior inference compute function unfortunately case provably much harder case must resort approximate methods discuss later chapters although potts prior adequate regularizing supervised learning problems sufficiently accurate perform image segmentation unsupervised way since segments produced model accurately represent kinds segments one sees natural images morris unsupervised case one needs use sophisticated priors truncated gaussian process prior sudderth jordan 
[undirected, graphical, models, markov, random, ﬁelds, examples, mrfs, gaussian, mrfs] undirected ggm also called gaussian mrf see rue held pairwise mrf following form exp exp note could easily absorb node potentials edge potentials kept separate clarity joint distribution written follows exp recognize multivariate gaussian written information form pairwise term connecting factorization theorem theorem conclude zero entries called structural zeros since represent absent edges graph thus undirected ggms correspond sparse precision matrices fact exploit section efficiently learn structure graph comparing gaussian dgms ugms section saw directed ggms correspond sparse regression matrices hence sparse cholesky factorizations covariance matrices whereas undirected ggms correspond inﬂuential paper geman geman introduced idea gibbs sampler section proposed using potts model prior image segmentation results paper misleading run gibbs sampler long enough see figure vivid illustration point examples mrfs figure var process represented dynamic chain graph source dahlhaus eichler used kind permission rainer dahlhaus oxford university press sparse precision matrices advantage dag formulation make regression weights hence conditional covariate information pourahmadi without worrying positive deﬁnite constraints disadavantage dag formulation dependence order although certain domains time series already natural ordering variables actually possible combine representations resulting gaussian chain graph example consider discrete time second order markov chain states continuous transition function represented vector valued linear gaussian cpd called vector auto regressive var process order models widely used econometrics time series forecasting time series aspect naturally modeled using dgm however sparse correlation amongst components within time slice naturally modeled using ugm example suppose chapter undirected graphical models markov random ﬁelds figure directed graph equivalent dag nodes latent confounders based figures choi used kind permission myung choi resulting graphical model illustrated figure zeros transition matrices correspond absent directed arcs zeros precision matrix correspond absent undirected arcs nodes sometimes sparse covariance matrix rather sparse precision matrix represented using directed graph edge arrows directions figure nodes connected unconditionally independent example figure see gaussian case means graph representing sparse covariance matrix called covariance graph contrast undirected model would bidirected graph converted dag latent variables bidirected edge replaced hidden variable representing hidden common cause confounder illustrated figure relevant properties determined using separation combine bidirected directed edges get directed mixed graphical model useful representing variety models arma models section structural equation models section etc 
[undirected, graphical, models, markov, random, ﬁelds, examples, mrfs, markov, logic, networks] section saw could unroll markov models hmms arbitrary number time steps order model variable length sequences similarly section saw could expand lattice ugm model images size complex domains variable number objects relationships creating models scenarios often done using ﬁrst order logic see russell norvig example consider sentences smoking causes cancer two people friends one smokes write sentences ﬁrst order examples mrfs friends smokes smokes friends friends friends cancer cancer figure example ground markov logic network represented pairwise mrf people based figure domingos lowd used kind permission pedro domingos logic follows predicates relation course rules always true indeed brittleness main reason logical approaches longer widely used least pure form variety attempts combine ﬁrst order logic probability theory area known statistical relational probabilistic relational modeling kersting one simple approach take logical rules attach weights known certainty factors interpret conditional probability distributions example might say unfortunately rule say predict furthermore combining cpds way guaranteed deﬁne consistent joint distribution resulting graph may dag alternative approach treat rules way deﬁning potential functions unrolled ugm result known markov logic network domingos lowd specify network ﬁrst rewrite rules conjunctive normal form cnf also known clausal form case get ﬁrst clause read either smoke cancer logically equivalent equation note clause unbound variable assumed universally quantiﬁed predicate function one argument known object evaluates true false depending whether property holds object logical relation function two arguments objects evaluates true false depending whether relationship holds set objects chapter undirected graphical models markov random ﬁelds inference ﬁrst order logic semi decidable common use restricted subset common approach used prolog restrict language horn clauses clauses contain one positive literal essentially means model series rules right hand side rules part consequence single term encoded knowledge base set clauses attach weights one weights parameter model deﬁne clique potentials follows exp logical expression evaluates clause applied variables weight attach clause roughly speaking weight clause speciﬁes probability world clause satsiﬁed relative world satisﬁed suppose two objects people world anna bob denote constant symbols make ground network clauses creating binary random variables wiring according clauses result ugm figure binary nodes note encoded fact symmetric relation might different values similarly degenerate nodes since enforce equation add constraints model compiler generates ground network could avoid creating redundant nodes summary think mlns convenient way specifying ugm template get unrolled handle data arbitrary size several ways deﬁne relational probabilistic models see koller friedman kersting details cases uncertainty number existence objects relations called open universe problem section gives concrete example context multi object tracking see russell norvig kersting references therein details 
[undirected, graphical, models, markov, random, ﬁelds, learning] section discuss perform map parameter estimation mrfs see quite computationally expensive reason rare perform bayesian inference parameters mrfs although see 
[undirected, graphical, models, markov, random, ﬁelds, learning, training, maxent, models, using, gradient, methods] consider mrf log linear form exp learning indexes cliques scaled log likelihood given log log since mrfs exponential family know function convex see section unique global maximum ﬁnd using gradient based optimizers particular derivative weights particular clique given log exercise asks show derivative log partition function wrt expectation feature model log hence gradient log likelihood ﬁrst term observed values sometimes called clamped term second term free sometimes called unclamped term contrastive term note computing unclamped term requires inference model must done per gradient step makes ugm training much slower dgm training gradient log likelihood rewritten expected feature vector according empirical distribution minus model expectation feature vector emp optimum gradient zero empirical distribution features match model predictions emp called moment matching observation motivates different optimization algorithm discuss section 
[undirected, graphical, models, markov, random, ﬁelds, learning, training, partially, observed, maxent, models] suppose missing data hidden variables model general represent models follows exp chapter undirected graphical models markov random ﬁelds log likelihood form log log exp unnormalized distribution term partition function whole model except ﬁxed hence gradient expected features clamp average log overall gradient given ﬁrst set expectations computed clamping visible nodes observed values second set computed letting visible nodes free cases marginalize alternative approach use generalized use gradient methods step see koller friedman details 
[undirected, graphical, models, markov, random, ﬁelds, learning, approximate, methods, computing, mles, mrfs] ﬁtting ugm general closed form solution map estimate parameters need use gradient based optimizers gradient requires inference models inference intractable learning also becomes intractable motivated various computationally faster alternatives map estimation list table dicsuss alternatives defer others later sections 
[undirected, graphical, models, markov, random, ﬁelds, learning, pseudo, likelihood] one alternative mle maximize pseudo likelihood besag deﬁned follows emp log log optimize product full conditionals also known composite likeli hood lindsay compare objective maximum likelihood emp log log learning method restriction exact mle section closed form chordal mrf exact section ipf tabular gaussian mrf exact section gradient based optimization low tree width exact section max margin training crfs section pseudo likelihood hidden variables approximate section stochastic exact error section contrastive divergence approximate section minimum probability ﬂow integrate hiddens approximate sohl dickstein table methods used compute approximate map parameter estimates mrfs crfs low tree width means order method efficient graph must tree like see section details figure small lattice representation used pseudo likelihood solid nodes observed neighbors based figure carbonetto case gaussian mrfs equivalent besag true general liang jordan approach illustrated figure grid learn predict node given neighbors objective generally fast compute since full conditional requires summing states single node order compute local normalization constant approach similar ﬁtting full conditional separately method used train dependency networks discussed section except parameters tied adjacent nodes one problem hard apply models hidden variables parise welling another subtle problem node assumes neighbors known values node nbr perfect predictor node learn rely completely node even expense ignoring potentially useful information local evidence however experiments parise welling hoeﬂing tibshirani suggest works well exact fully observed ising models course much faster 
[undirected, graphical, models, markov, random, ﬁelds, learning, stochastic, maximum, likelihood] recall gradient log likelihood fully observed mrf given chapter undirected graphical models markov random ﬁelds gradient partially observed mrf similar cases approximate model expectations using monte carlo sampling combine stochastic gradient descent section takes samples empirical distribution pseudocode resulting method shown algorithm algorithm stochastic maximum likelihood ﬁtting mrf initialize weights randomly epoch minibatch size sample sample training case minibatch decrease step size typically use mcmc generate samples course running mcmc convergence step inner loop would extremely slow fortunately shown younes start mcmc chain previous value take steps otherwords sample initializing mcmc chain run iterations valid since likely close since changed parameters small amount call algorithm stochastic maximum likelihood sml closely related algorithm called persistent contrastive divergence discuss section 
[undirected, graphical, models, markov, random, ﬁelds, learning, feature, induction, maxent, models] mrfs require good set features one unsupervised way learn features known feature induction start base set features continually create new feature combinations old ones greedily adding best ones model approach ﬁrst proposed pietra zhu later extended crf case mccallum illustrate basic idea present example pietra described build unconditional probabilistic models represent english spelling initially model features represents uniform distribution algorithm starts choosing add feature learning checks letter lower case feature added parameters maximum likelihood feature turns means word lowercase letter position times likely word without lowercase letter position samples model generated using annealed gibbs sampling section shown xevo ijjiir gsr msmgh pcp ozivlal hzagh yzop advzmxnv ijv_bolft emx kayerf mlj rawzyb ctdnnnbg wgdw kguv spxcq uzflbbf dxtkkn cxwx jpd ztzh zhpkvnu qee nynrx atzen lrh yrqyka zcngotcnx igcump zjcjs lqpwiqu cefmfhc fdcy tzby yopxmvk govyccm ijyiduwfzo duh ejv pjw second feature added algorithm checks two adjacent characters lower case model form exp continuing way algorithm adds features strings sgt inggt represents end word various regular expressions etc samples model features generated using annealed gibbs sampling shown reaser homes thing reloverated ther conists fores anditing proveral prolling prothere mento yaou chestraing intrally qut best compers cluseliment uster deveral thise offect inatever thifer constranded stater vill thase youse menttering verate approach feature learning thought form graphical model structure learning chapter except ﬁne grained add features useful regardless resulting graph structure however resulting graphs become densely connected makes inference hence parameter estimation intractable 
[undirected, graphical, models, markov, random, ﬁelds, learning, iterative, proportional, ﬁtting, ipf] consider pairwise mrf potentials represented tables one parameter per variable setting represent log linear form using exp similarly thus feature vectors indicator functions thank john lafferty sharing example chapter undirected graphical models markov random ﬁelds equation maximum likelihood empirical expectation features equals model expectation emp emp emp empirical probability emp general graph condition must hold optimum emp special family graphs known decomposable graphs deﬁned section one show however even graph decomposable imagine trying enforce condition suggests iterative coordinate ascent scheme step compute emp multiplication elementwise known iterative proportional ﬁtting ipf fienberg bishop see algorithm pseudocode algorithm iterative proportional fitting algorithm tabular mrfs initialize repeat emp converged example let consider simple example http wikipedia org wiki iterative_propo rtional_fitting two binary variables man left handed otherwise similarly woman left handed otherwise summarize data using following contingency table right handed left handed total male female total learning suppose want disconnected graphical model containing nodes edge want ﬁnd vectors model expected counts empirical counts moment matching ﬁnd row column sums model must exactly match row column sums data one possible solution use show model predictions right handed left handed total male female total easy see matches required constraints see ipfdemox matlab code computes numbers method easily generalized arbitrary graphs speed ipf ipf ﬁxed point algorithm enforcing moment matching constraints guaranteed converge global optimum bishop number iterations depends form model graph decomposable ipf converges single iteration general ipf may require many iterations clear dominant cost ipf computing required marginals model efficient methods junction tree algorithm section used resulting something called efficient ipf jirousek preucil nevertheless coordinate descent slow alternative method update parameters simply following gradient likelihood gradient approach signiﬁcant advantage works models clique potentials may fully parameterized features may consist possible indicators clique instead arbitrary although possible adapt ipf setting general features resulting method known iterative scaling practice gradient method much faster malouf minka generalizations ipf use ipf gaussian graphical models instead working empirical counts work empirical means covariances speed kiiveri also possible create bayesian ipf algorithm sampling posterior model parameters see dobra massam ipf decomposable graphical models special family undirected graphical models known decomposable graphical models formally deﬁned section basic idea contains graphs tree like graphs represented ugms dgms without loss information case decomposable graphical models ipf converges one iteration fact chapter undirected graphical models markov random ﬁelds mle closed form solution lauritzen particular tabular potentials gaussian potentials using conjugate priors also easily compute full posterior model rameters decomposable case dgm case see lauritzen details 
[undirected, graphical, models, markov, random, ﬁelds, conditional, random, ﬁelds, crfs] conditional random ﬁeld crf lafferty sometimes discriminative random ﬁeld kumar hebert version mrf clique potentials conditioned input features crf thought structured output extension logistic regression usually assume log linear representation potentials exp feature vector derived global inputs local set labels give examples make notation clearer advantage crf mrf analogous advantage discriminative classiﬁer generative classiﬁer see section namely need waste resources modeling things always observe instead focus attention modeling care namely distribution labels given data another important advantage crfs make potentials factors model data dependent example image processing applications may turn label smoothing two neighboring nodes observed discontinuity image intensity pixels similarly natural language processing problems make latent labels depend global properties sentence language written hard incorporate global features generative models disadvantage crfs mrfs require labeled training data slower train explain section analogous strengths weaknesses logistic regression naive bayes discussed section 
[undirected, graphical, models, markov, random, ﬁelds, conditional, random, ﬁelds, crfs, chain-structured, crfs, memms, label-bias, problem] widely used kind crf uses chain structured graph model correlation amongst neighboring labels models useful variety sequence labeling tasks see sec tion conditional random ﬁelds crfs figure various models sequential data generative directed hmm discriminative directed memm discriminative undirected crf traditionally hmms discussed detail chapter used tasks joint density models form dropped initial term simplicity see figure observe easy train models using techniques described section hmm requires specifying generative observation model difficult furthemore required local since hard deﬁne generative model whole stream observations obvious way make discriminative version hmm reverse arrows figure deﬁnes directed discriminative model form global features features speciﬁc node partition local global necessary helps comparing hmms called maximum entropy markov model memm mccallum kakade memm simply markov chain state transition probabilities conditioned input features therefore special case input output hmm discussed section seems like natural generalization logistic regression structured output setting suffers subtle problem known rather obscurely label bias problem lafferty problem local features time inﬂuence states prior time follows examining dag shows separated earlier time points structure hidden child thus blocking information ﬂow understand means practice consider part speech pos tagging task suppose see word banks could verb banks boa noun river banks overﬂowing locally pos tag word ambiguous however chapter undirected graphical models markov random ﬁelds figure example handwritten letter recognition word brace look similar disambiguated using context source taskar used kind permission ben taskar suppose later sentence see word ﬁshing gives enough context infer sense banks river banks however memm unlike hmm crf ﬁshing evidence ﬂow backwards able disambiguate banks consider chain structured crf model form graph figure see label bias problem longer exists since block information reaching nodes label bias problem memms occurs directed models locally normalized meaning cpd sums contrast mrfs crfs globally normalized means local factors need sum since partition function sums joint conﬁgurations ensure model deﬁnes valid distribution however solution comes price get valid probability distribution seen whole sentence since normalize conﬁgurations consequently crfs useful dgms whether discriminative generative online real time inference furthermore fact depends nodes hence parameters makes crfs much slower train dgms see section 
[undirected, graphical, models, markov, random, ﬁelds, conditional, random, ﬁelds, crfs, applications, crfs] crfs applied many interesting problems give representative sample applications illustrate several useful modeling tricks also provide motivation inference techniques discuss chapter handwriting recognition natural application crfs classify hand written digit strings illustrated figure key observation locally letter may ambiguous depending known labels one neighbors possible use context reduce error rate note node potential often taken probabilistic discriminative classiﬁer conditional random ﬁelds crfs withdrawal ual airways rose announcing british deal adj prp pos begin noun phrase within noun phrase noun phrase noun adjective adj verb preposition possesive pronoun determiner prp key figure crf joint pos tagging segmentation source figure koller friedman used kind permission daphne koller neural network rvm trained isolated letters edge potentials often taken language bigram model later discuss train potentials jointly noun phrase chunking one common nlp task noun phrase chunking refers task segmenting sentence distinct noun phrases nps simple example technique known shallow parsing detail tag word sentence meaning beginning new meaning inside meaning outside called bio notation example following sentence nps marked brackets british airways rose announcing withdrawl uai deal need symbol distinguish meaning two words within single meaning two separate nps standard approach problem would ﬁrst convert string words string pos tags convert pos tags string bios however pipeline method propagate errors robust approach build joint probabilistic model form pos words one way use crf figure connections adjacent labels encode probability transitioning states enforce constraints fact must preceed features usually hand engineered include things like word begin capital letter word followed full stop word noun etc typically features per node number features minimal impact inference time since features observed need summed small increase cost chapter undirected graphical models markov random ﬁelds mrs green spoke today new york green chairs ﬁnance committee per per oth oth oth loc loc per oth oth oth oth key begin person name within person name begin location name per per loc within location name entitiy loc oth figure skip chain crf named entity recognition source figure koller friedman used kind permission daphne koller evaluating potential functions many features usually negligible one use regularization prune irrelevant features however graph structure dramatic effect inference time model figure tractable since essentially fat chain use forwards backwards algorithm section exact inference pos time pos number pos tags number tags however seemingly similar graph figure explained computationally intractable named entity recognition task related chunking named entity extraction instead segmenting noun phrases segment phrases people locations similar techniques used automatically populate calendar email messages called information extraction simple approach use chain structured crf expand state space bio per per loc loc however sometimes ambiguous whether word person location something else proper nouns particularly difficult deal belong open class unbounded number possible names unlike set nouns verbs large essentially ﬁxed get better performance considering long range correlations words example might add link occurrences word force word tag occurence technique also helpful resolving identity pronouns known skip chain crf see figure illustration see graph structure changes depending input additional advantage crfs generative models unfortunately inference model gener ally expensive simple chain local connections reasons explained section conditional random ﬁelds crfs figure illustration simple parse tree based context free grammar chomsky normal form feature vector counts number times production rule used source figure altun used kind permission yasemin altun natural language parsing generalization chain structured models language use probabilistic grammars particular probabilistic context free grammar pcfg set write production rules form non terminals analogous parts speech terminals words see figure example rule associated probability resulting model deﬁnes probability distribution sequences words compute probability observing particular sequence summing trees generate done time using inside outside algorithm see jurafsky martin manning schuetze details pcfgs generative models possible make discriminative versions encode probability labeled tree given sequence words using crf form exp example might deﬁne count number times production rule used analogous number state transitions chain structured model see taskar details hierarchical classiﬁcation suppose performing multi class classiﬁcation label taxonomy groups classes hierarchy encode position within hierarchy deﬁning binary vector turn bit component children combined input features using tensor product see figure example method widely used text classiﬁcation manually constructed taxnomies open directory project www dmoz org quite common beneﬁt information shared parameters nearby categories enabling generalization across classes chapter undirected graphical models markov random ﬁelds figure illustration simple label taxonomy used compute distributed representation label class ﬁgure denoted denoted source figure altun used kind permission yasemin altun protein side chain prediction interesting analog skip chain model arises problem predicting structure protein side chains residue side chain dihedral angles usually discretized values called rotamers goal predict discrete sequence angles discrete sequence amino acids deﬁne energy function include various pairwise interaction terms nearby residues elements vector energy usually deﬁned weighted sum individual energy terms energy contribution due various electrostatic charges hydrogen bonding potentials etc parameters model see yanover details given model compute probable side chain conﬁguration using argmin general problem hard depending nature graph induced terms due long range connections variables nevertheless special cases efficiently handled using methods discussed section stereo vision low level vision problems problems input image set images output processed version image cases common use lattice structured models models similar figure except features global generated model assume pairwise crf classic low level vision problem dense stereo reconstruction goal estimate depth every pixel given two images taken slightly different angles section based sudderth freeman give sketch simple crf used solve task see sun sophisticated model using standard preprocessing techniques one convert depth estimation conditional random ﬁelds crfs problem estimating disparity pixel location left image corresponding pixel location right image typically assume corresponding pixels similar intensity deﬁne local node potential form exp left image right image equation generalized model intensity small windows around location highly textured regions usually possible ﬁnd corresponding patch using cross correlation regions low texture considerable ambiguity correct value easily add gaussian prior edges mrf encodes assumption neighboring disparities similar follows exp resulting model gaussian crf however using gaussian edge potentials oversmooth estimate since prior fails account occasional large changes disparity occur neighboring pixels different sides occlusion boundary one gets much better results using truncated gaussian potential form exp min encodes expected smoothness encodes maximum penalty imposed disparities signiﬁcantly different called discontinuity preserving potential note penalties convex local evidence potential made robust similar way order handle outliers due specularities occlusions etc figure illustrates difference two forms prior top left image standard middlebury stereo benchmark dataset scharstein szeliski bottom left corresponding true disparity values remaining columns represent estimated disparity inﬁnite number rounds loopy belief propagation see section inﬁnite mean results convergence top row shows results using gaussian edge potential bottom row shows results using truncated potential latter clearly better unfortunately performing inference real valued variables computationally difficult unless model jointly gaussian consequently common discretize variables example figure bottom used states edge potentials still form given equation resulting model called metric crf since potentials form metric inference metric crfs efficient crfs discrete labels natural ordering explain section see section comparison various approximate inference methods applied low level crfs see blake prince details probabilistic models computer vision function said metric satisﬁes following three properties reﬂexivity iff symmetry triangle inequality satisﬁes ﬁrst two properties called semi metric chapter undirected graphical models markov random ﬁelds figure illustration belief propagation stereo depth estimation left column image true disparities remaining columns initial estimate estimate iteration estimate convergence top row gaussian edge potentials bottom row robust edge potentials source figure sudderth freeman used kind permission erik sudderth 
[undirected, graphical, models, markov, random, ﬁelds, conditional, random, ﬁelds, crfs, crf, training] modify gradient based optimization mrfs described section crf case straightforward way particular scaled log likelihood becomes log log gradient becomes log note perform inference every single training case inside gradient step times slower mrf case partition function depends inputs applications crfs applications mrfs size graph structure vary hence need use parameter tying ensure deﬁne distribution arbitrary size pairwise case write model follows exp structural svms node edge parameters summed node edge features sufficient statistics gradient expression easily modiﬁed handle case practice important use prior regularization prevent overﬁtting use gaussian prior new objective becomes log simple modify gradient expression alternatively use regularization example could use edge weights learn sparse graph structure node weights schmidt words objective becomes log unfortunately optimization algorithms complicated use see sec tion although problem still convex handle large datasets use stochastic gradient descent sgd described section possible useful deﬁne crfs hidden variables example allow unknown alignment visible features hidden labels see schnitzspan case objective function longer convex nevertheless ﬁnd locally optimal map parameter estimate using gradient methods 
[undirected, graphical, models, markov, random, ﬁelds, structural, svms] seen training crf requires inference order compute expected sufficient statistics needed evaluate gradient certain models computing joint map estimate states provably simpler computing marginals discuss section section discuss way train structured output classiﬁers leverages existence fast map solvers avoid confusion map estimation parameters often refer map estimation states decoding methods known structural support vector machines ssvms tsochantaridis also similar class methods known max margin markov networks mnets taskar see section discussion differences 
[undirected, graphical, models, markov, random, ﬁelds, structural, svms, ssvms, probabilistic, view] book mostly concentrated ﬁtting models using map parameter estimation minimizing functions form log log chapter undirected graphical models markov random ﬁelds however test time pick label minimize posterior expected loss deﬁned section argmin loss incur estimate truth therefore seems reasonable take loss function account performing parameter estimation following yuille let instead minimized posterior expected loss training set log log special case loss reduces assume write model following form exp exp exp also let deﬁne exp rewrite objective follows log log exp exp log log exp consider various bounds order simplify objective first note function max log exp log exp max log max example suppose log exp log exp exp exp log exp log ignore log term independent treat max lower upper bound hence see max amp max note violates fundamental bayesian distinction inference decision making however performing tasks separately result optimal decision compute exact posterior cases intractable need perform loss calibrated inference lacoste julien section perform loss calibrated map parameter estimation computationally simpler see also stoyanov structural svms means constants unfortunately objective convex however devise convex upper bound exploiting following looser lower bound log sum exp function log exp applying equation earlier example get log exp log exp exp exp applying bound get max amp set corresponding spherical gaussian prior get ssv max amp objective used ssvm approach tsochantaridis special case criterion reduces following considering two cases max standard binary svm objective see equation see ssvm criterion seen optimizing upper bound bayesian objective result ﬁrst shown yuille bound tight hence approximation good one large since case concentrate mass argmax unfortunately large corresponds model likely overﬁt unlikely working regime tune strength regularizer avoid situation alternative justiﬁcation svm criterion focusses effort ﬁtting parameters affect decision boundary better use computational resources ﬁtting full distribution especially model wrong 
[undirected, graphical, models, markov, random, ﬁelds, structural, svms, ssvms, non-probabilistic, view] present ssvms traditional non probabilistic way following tsochantaridis resulting objective one however derivation set stage algorithms discuss let argmax prediction function obtain zero loss training set using predictor max chapter undirected graphical models markov random ﬁelds one nonlinear inequalities equivalently replaced linear inequal ities resulting total linear constraints following form brevity introduce notation rewrite constraints achieve zero loss typically multiple solution vectors pick one maximizes margin deﬁned min max since margin made arbitrarily large rescaling norm resulting optimization problem max equivalently write min allow case zero loss cannot achieved equivalent data inseparable case binary classiﬁcation relax constraints introducing slack terms one per data case yields min case structured outputs want treat constraint violations equally example segmentation problem getting one position wrong punished less getting many positions wrong one way achieve divide slack variable size loss called slack scaling yields min alternatively deﬁne margin proportional loss called margin rescaling yields min fact write instead since using simpler notation exclude add extra redundant constraint latter approach used mnets structural svms future reference note solve terms follows max max max substituting dropping constraints get following equivalent problem min max empirical risk minimization let pause consider whether objective reasonable recall frequen tist approach machine learning section goal minimize regularized empirical risk deﬁned regularizer argmax prediction since objective hard optimize loss differentiable construct convex upper bound instead show max convex upper bound see note max using bound yields equation computational issues although objectives simple quadratic programs con straints intractable since usually exponentially large case margin rescaling formulation possible reduce exponential number constraints poly nomial number provided loss function feature vector decompose according graphical model approach used mnets taskar alternative approach work directly exponentially sized allows use general loss functions several possible methods make feasible one use cutting plane methods another use stochastic subgradient methods discuss chapter undirected graphical models markov random ﬁelds figure illustration cutting plane algorithm start estimate add ﬁrst constraint shaded region new feasible set new minimum norm solution add another constraint dark shaded region new feasible set add third constraint source figure altun used kind permission yasemin altun 
[undirected, graphical, models, markov, random, ﬁelds, structural, svms, cutting, plane, methods, ﬁtting, ssvms] section discuss efficient algorithm ﬁtting ssvms due joachims method handle general loss functions implemented popular svmstruct package method based cutting plane method convex optimization kelley basic idea follows start initial guess constraints iteration following example ﬁnd violated constraint involving loss augmented margin violation exceeds current value add working set constraints training case solve resulting new ﬁnd new see figure sketch algorithm pseudo code since step add one new constraint warm start solver easily modify algorithm optimize slack rescaling version replacing expression key efficiency method polynomially many constraints need added soon exponential number constraints guaranteed also satisﬁed within tolerance see tsochantaridis proof loss augmented decoding key efficiency ability ﬁnd violated constraint line algorithm compute argmax argmax http svmlight joachims org svm_struct html structural svms algorithm cutting plane algorithm ssvms margin rescaling slack version input repeat argmax argmin changed return call process loss augmented decoding joachims procedure called separation oracle loss function additive decomposition form features fold loss weight vector ﬁnd new set parameters use standard decoding algorithm viterbi model special case loss optimum either best solution argmax value second best solution argmax achieves overall value chain structured crfs use viterbi algorithm decoding second best path differ best path single position obtained changing variable whose max marginal closest decision boundary second best value generalize bit work ﬁnd best list schwarz chow nilsson goldberger hamming loss score deﬁned section devise dynamic programming algorithm compute equation see altun details models loss function combinations require different methods linear time algorithm although algorithm takes polynomial time better devise algorithm runs linear time assuming use linear kernel work original features apply kernel trick basic idea explained joachims single slack variable instead use constraints instead chapter undirected graphical models markov random ﬁelds speciﬁcally optimize following assuming margin rescaling formulation min compare original version min one show solution equation also solution equation vice versa algorithm cutting plane algorithm ssvms margin rescaling slack version input repeat argmin argmax return optimize equation using cutting plane algorithm algorithm implemented svmstruct inner line solved time using method joachims line make calls loss augmented decoder finally shown number iterations constant independent thus overall running time linear 
[undirected, graphical, models, markov, random, ﬁelds, structural, svms, online, algorithms, ﬁtting, ssvms] although cutting plane algorithm made run time linear number data points still slow large dataset cases preferable use online learning brieﬂy mention possible algorithms structured perceptron algorithm simple algorithm ﬁtting ssvms structured perceptron algorithm collins method extension regular perceptron algorithm section structural svms step compute argmax using viterbi algorithm current training sample nothing otherwise update weight vector using get good performance necessary average parameters last updates see section details rather using recent value stochastic subgradient descent disadvantage structured perceptron algorithm implicitly assumes loss enforce kind margin alternative approach perform stochastic subgradient descent speciﬁc instance pegasos algorithm shalev shwartz stands primal estimated sub gradient solver svm pegasos designed binary svms extended ssvms let start considering objective function max letting argmax max subgradient objective function stochastic subgradient descent approximate gradient single term perform update step size parameter satisfy robbins monro conditions sec tion notice perceptron algorithm special case ensure unit norm project onto ball update 
[undirected, graphical, models, markov, random, ﬁelds, structural, svms, latent, structural, svms] many applications interest latent hidden variables example object detections problems may told image contains object may know location object pose considered hidden variable machine translation may know source text say english target text say french typically know alignment words extend model follows get latent crf exp exp chapter undirected graphical models markov random ﬁelds addition introduce loss function measures loss action take predict using latent variables could use since usually nuisance variable direct interest however sometimes play useful role deﬁning loss function given loss function deﬁne objective log log exp exp using loose lower bound get max amp max set get objective optimized latent svms joachims unfortunately objective longer convex however difference convex functions hence solved efficiently using cccp concave convex procedure yuille rangarajan method minimizing functions form convex method alternates ﬁnding linear upper bound minimizing convex function see algorithm pseudocode cccp guaranteed decrease objective every iteration converge local minimum saddle point algorithm concave convex procedure cccp set initialize repeat find hyperplane solve argmin set converged applied latent ssvms cccp similar hard step compute example consider problem learning classify set documents relevant query given documents single query want produce labeling representing whether document relevant suppose goal maximize precision metric widely used ranking see section introduce latent variable document representing degree relevance corresponds latent total ordering consistent observed partial ordering given deﬁne following loss function min total number relevant documents loss essentially minus precision except replace loss minimum zero see joachims details structural svms linear upper bound setting argmax step estimate using techniques solving fully visible ssvms speciﬁcally minimize max 
[undirected, graphical, models, markov, random, ﬁelds, exercises] exercise derivative log partition function derive equation exercise properties gaussian graphical models source jordan question study relationship sparse matrices sparse graphs gaussian graphical models consider multivariate gaussian dimensions suppose throughout recall jointly gaussian random variables know independent iff uncorrelated true general even gaussian jointly gaussian also conditionally independent given variables iff suppose marginal independencies amongst conditional indepen dencies hint compute expand pairwise terms missing draw undirected graphical model captures many independence statements marginal conditional possible make false independence assertions suppose marginal independencies amongst conditional inde pendencies amongst draw undirected graphical model captures many independence statements marginal conditional possible make false independence assertions suppose distribution represented following dag let cpds follows multiply cpds together complete square bishop ﬁnd corresponding joint distribution may ﬁnd easier solve rather chapter undirected graphical models markov random ﬁelds dag model previous question marginal independencies amongst conditional independencies draw undirected graphical model captures many independence statements possible make false independence assertions either marginal conditional exercise independencies gaussian graphical models source mackay consider dag assume cpds linear gaussian following matrices could covariance matrix matrices could inverse covariance matrix consider dag assume cpds linear gaussian matrices could covariance matrix matrices could inverse covariance matrix let three variables covariance matrix precision matrix follows focus following statements covariance matrix precision matrix true exercise cost training mrfs crfs source koller consider process gradient ascent training log linear model features given data set training instances assume simplicity cost computing single feature single instance data set constant cost computing expected value feature compute marginal variables scope assume takes time compute marginals data case also assume need iterations gradient process converge using notation time required train mrf big notation using notation time required train crf big notation exercise full conditional ising model consider ising model ijgt exp exp denotes unique pairs edges coupling strength weight edge local evidence bias term parameters structural svms derive expression full conditional nodes except neighbors graph hint answer use sigmoid logistic function suppose derive related expression case result used applying gibbs sampling model 
[exact, inference, graphical, models, introduction] section discussed forwards backwards algorithm exactly compute posterior marginals chain structured graphical model hidden variables assumed discrete visible variables algorithm modiﬁed compute posterior mode posterior samples similar algorithm linear gaussian chains known kalman smoother discussed section goal chapter generalize exact inference algorithms arbitrary graphs resulting methods apply directed undirected graphical models describe variety algorithms omit derivations brevity see darwiche koller friedman detailed exposition exact inference techniques discrete directed graphical models 
[exact, inference, graphical, models, belief, propagation, trees] section generalize forwards backwards algorithm chains trees resulting algorithm known belief propagation pearl sum product algorithm 
[exact, inference, graphical, models, belief, propagation, trees, serial, protocol] initially assume notational simplicity model pairwise mrf crf local evidence node potential edge consider case models higher order cliques directed trees later one way implement undirected trees follows pick arbitrary node call root orient edges away intuitively imagine picking graph node letting edges dangle gives well deﬁned notion parent child send messages leaves root collect evidence phase back root distribute evidence phase manner analogous forwards backwards chains chapter exact inference graphical models root root figure message passing tree collect root phase distribute root phase explain process detail consider example figure suppose want compute belief state node initially condition belief evidence graph want compute bel call bottom belief state suppose induction computed messages two children summarizing think know evidence subtrees computed evidence downstream side edge see figure similarly computed compute bottom belief state follows bel local evidence node local normalization constant words multiply incoming messages children well incoming message local evidence normalize explained compute bottom belief states bottom messages compute messages consider computing one children assume recursion computed bel compute message follows bel essentially convert beliefs beliefs using edge potential continue way tree reach root root seen evidence tree compute local belief state root using bel completes end upwards pass analogous forwards pass hmm side effect compute probability evidence collecting belief propagation trees normalization constants pass messages root example consider node parent shown figure compute belief state need combine bottom belief together top message summarizes information rest graph evidence upstream root side edge shown figure bel bel compute downward messages example consider message suppose parent children shown figure want include information received except information sent bel rather dividing message sent plug equation bel get words multiply together messages coming nodes except recipient combine together pass edge potential case chain one child one parent simpliﬁes version use division called belief updating version multiply one messages called sum product belief updating version analogous formulated kalman smoother section top messages depend bottom messages means interpreted conditional posterior probabilities sum product version analogous formulated backwards algorithm section top messages completely independent bottom messages means interpreted conditional likelihoods see section detailed discussion subtle difference 
[exact, inference, graphical, models, belief, propagation, trees, parallel, protocol] far presented serial version algorithm send messages root back optimal approach tree natural extension forwards backwards chains however prelude handling general graphs loops consider parallel version gives equivalent results serial version less efficient implemented serial machine chapter exact inference graphical models basic idea nodes receive messages neighbors parallel updates belief states ﬁnally send new messages back neighbors process repeats convergence kind computing architecture called systolic array due resemblance beating heart precisely initialize messages vector parallel node absorbs messages neighbors using bel nbr parallel node sends messages neighbors nbr message computed multiplying together incoming messages except one sent recipient passing potential iteration algorithm bel represents posterior belief conditioned evidence steps away graph steps diameter graph largest distance two pairs nodes every node obtained information nodes local belief state correct posterior marginal since diameter tree algorithm converges linear number steps actually derive version algorithm imposing condition node send message received messages neighbors means must start leaf nodes one neighbor messages propagate root back also update nodes random order requirement node get updated times enough time information spread throughout whole tree similar parallel distributed algorithms solving linear systems equations discussed bertsekas particular gauss seidel algorithm analogous serial version jacobi algorithm analogous parallel version 
[exact, inference, graphical, models, belief, propagation, trees, gaussian] consider case jointly gaussian represented gaussian pairwise mrf section present belief propagation algorithm class models follow presentation bickson see also malioutov assume following node edge potentials exp exp overall model form exp belief propagation trees information form mvn see exercise precision matrix note completing square local evidence rewritten gaussian describe use compute posterior node marginals graph tree method exact graph loopy posterior means may still exact posterior variances often small weiss freeman although precision matrix often sparse computing posterior mean requires inverting since provides way exploit graph structure perform computation time instead related various methods linear algebra discussed bickson since model jointly gaussian marginals messages gaussian key operations need multiply together two gaussian factors marginalize variable joint gaussian factor multiplication use fact product two gaussians gaussian exp see exercise proof marginalization following result exp exp follows normalization constant gaussian exercise pieces need particular let message gaussian mean precision equation belief node given product incoming messages times local evidence equation hence bel nbr nbr nbr chapter exact inference graphical models compute messages use equation given nbr product local evidence incoming messages excluding message nbr nbr nbr returning equation exp exp exp const exp one generalize equations case node vector messages become small mvns instead scalar gaussians alag agogino apply resulting algorithm linear dynamical system recover kalman smoothing algorithm section perform message passing models non gaussian potentials one use sampling methods approximate relevant integrals called non parametric sudderth isard sudderth 
[exact, inference, graphical, models, belief, propagation, trees, variants] section brieﬂy discuss several variants main algorithm figure illustration compute two slice distribution hmm terms local evidence messages visible nodes hidde nodes respectively forwards message backwards message max product algorithm possible devise max product version algorithm replacing operator max operator compute local map marginal node however ties might globally consistent discussed section fortunately generalize viterbi algorithm trees use max argmax collect root phase perform traceback distribute root phase see dawid details sampling tree possible draw samples tree structured model generalizing forwards ﬁltering backwards sampling algorithm discussed section see dawid details computing posteriors sets variables section explained compute two slice distribution hmm namely using since forwards message think sending messages combining matrix shown figure like treating single mega node multiplying incoming messages well local factors belief propagation trees chapter exact inference graphical models coherence diﬃculty grade intelligence sat letter happy job coherence diﬃculty grade intelligence sat letter happy job figure left student dgm right equivalent ugm add moralization arcs based figure koller friedman 
[exact, inference, graphical, models, variable, elimination, algorithm] seen use compute exact marginals chains trees section discuss algorithm compute kind graph explain algorithm example consider dgm figure model koller friedman hypothetical model relating various variables pertaining typical student corresponding joint following form note forms cpds matter since calculations symbolic however illustration purposes assume variables binary proceeding convert model undirected form required makes uniﬁed presentation since resulting method applied dgms ugms see section variety problems nothing graphical models since computational complexity inference dgms ugms generally speaking nothing lost transformation computational point view convert dgm ugm simply deﬁne potential factor every cpd yielding tricks one exploit directed case cannot easily exploited undirected case one important example barren node removal explain consider naive bayes classiﬁer figure suppose want infer observe clear safely remove since similarly general removed hidden leaves apply process recursively since potential functions necessary sum one cannot use trick undirected case see koller friedman variety speedup tricks variable elimination algorithm since potentials locally normalized since cpds need global normalization constant corresponding undirected graph shown figure note edges dag particular unmarried nodes share child must get married adding edge process known moralization arrows dropped example added moralization arcs reason operation required ensure properties ugm match dgm explained section also ensures clique store cpds family suppose want compute marginal probability person get job since binary variables could simply enumerate possible assignments variables except adding probability joint instantiation however would take time smarter pushing sums inside products key idea behind variable elimination algorithm zhang poole also called bucket elimination dechter context genetic pedigree trees peeling algorithm cannings example get evaluate expression working right left shown table first multiply together terms scope operator create temporary factor marginalize get new factor next multiply together terms scope operator marginalize create chapter exact inference graphical models table eliminating variables figure order compute next multiply together terms scope operator marginalize create technique used compute marginal interest compute conditional take ratio two marginals visible variables clamped known values hence need summed example general write variable elimination algorithm normalization constant denominator called probability evi dence see variableelimination simple matlab implementation algorithm works arbitrary graphs arbitrary discrete factors crazy please read section points exponentially slow worst case 
[exact, inference, graphical, models, variable, elimination, algorithm, generalized, distributive, law] abstractly thought computing following expression understood visible variables clamped summed uses non serial dynamic programming bertele brioschi caching intermediate results avoid redundant computation however tasks might like solve given graphical model example might want map estimate argmax fortunately essentially algorithm also used solve task replace sum max also need traceback step actually recovers argmax opposed value max details explained section general applied commutative semi ring set together two binary operations called satisfy following three axioms operation associative commutative additive identity element called operation associative commutative multiplicative identity element called distributive law holds triples framework covers extremely wide range important applications including constraint satisfaction problems bistarelli dechter fast fourier transform aji mceliece etc see table examples 
[exact, inference, graphical, models, variable, elimination, algorithm, computational, complexity] running time clearly exponential size largest factor since sum corresponding variables factors come original model thus unavoidable new factors created process summing example chapter exact inference graphical models domain name sum product max max product min min sum boolean satisﬁability table commutative semirings table eliminating variables figure order equation created factor involving nodes originally present together factor order perform summation known elimination order large impact size intermediate factors created example consider ordering table largest created factor beyond original ones model size corresponding consider ordering table largest factors much bigger determine size largest factor graphically without worrying actual numerical values factors eliminate variable connect variables variable elimination algorithm coherence diﬃculty grade intelligence sat letter happy job coherence diﬃculty grade intelligence sat letter happy job coherence diﬃculty grade intelligence sat letter happy job figure example elimination process order etc eliminate ﬁgure add ﬁll edge since connected based figure koller friedman share factor reﬂect new temporary factor edges created process called ﬁll edges example figure shows ﬁll edges introduced eliminate order ﬁrst two steps introduce ﬁll ins eliminate connect since occur equation let undirected graph induced applying variable elimination using elimination ordering temporary factors generated correspond maximal cliques graph example ordering maximal cliques follows clear time complexity cliques created size clique assume notational simplicity variables states let deﬁne induced width graph given elimination ordering denoted size largest factor largest clique induced graph minus easy see complexity ordering obviously would like minimize running time hence induced width let deﬁne treewidth graph minimal induced width min max clearly best possible running time unfortunately one show arbitrary graphs ﬁnding elimination ordering minimizes hard arnborg practice greedy search techniques used ﬁnd reasonable orderings kjaerulff although people tried heuristic methods discrete optimization chapter exact inference graphical models genetic algorithms larranaga also possible derive approximate algorithms provable performance guarantees amir cases optimal elimination ordering clear example chains work forwards backwards time trees work leaves root orderings introduce ﬁll edges consequently inference chains trees takes time one reason markov chains markov trees widely used unfortunately graphs treewidth large example lattice treewidth min lipton tarjan ising model would take time course slow mean smarter algorithm discuss issue section 
[exact, inference, graphical, models, variable, elimination, algorithm, weakness] main disadvantage variable elimination algorithm apart exponential depen dence treewidth inefficient want compute multiple queries conditioned evidence example consider computing marginals chain structured graphical model hmm easily compute ﬁnal marginal elimi nating nodes order equivalent forwards algorithm takes time suppose want compute run cost time total cost compute marginals however know solve problem using forwards backwards difference caches messages computed forwards pass reuse later argument holds trees example consider node tree fig ure compute eliminating equivalent sending messages messages correspond factors created similarly compute see messages used compute marginal one node used compute marginals nodes storing messages later use compute marginals time collect distribute algorithm trees question combine efficiency trees generality answer given section 
[exact, inference, graphical, models, junction, tree, algorithm] junction tree algorithm jta generalizes trees arbitrary graphs sketch basic idea details see koller friedman 
[exact, inference, graphical, models, junction, tree, algorithm, creating, junction, tree] basic idea behind jta ﬁrst run algorithm symbolically adding ﬁll edges according given elimination ordering resulting graph chordal graph means every undirected cycle length junction tree algorithm xbd xbe figure sending multiple messages along tree root root root messages needed compute singleton marginals based figure jordan figure left graph triangulated despite appearances since contains chordless cycle right one possible triangulation adding ﬁll edges based armstrong chapter exact inference graphical models chord edge connects non adjacent nodes cycle created chordal graph extract maximal cliques general ﬁnding max cliques computationally hard turns done efficiently special kind graph figure gives example max cliques follows note original graphical model already chordal elimination process would add extra ﬁll edges assuming optimal elimination ordering used call models decomposable since break little pieces deﬁned cliques turns cliques chordal graph arranged special kind tree known junction tree enjoys running intersection property rip means subset nodes containing given variable forms connected component figure gives example tree see node occurs two adjacent tree nodes share information variable similar situation holds variables one show tree satisﬁes running intersection property applying tree explain return exact values node tree clique induced graph easily extract node edge marginals original model marginalizing clique distributions 
[exact, inference, graphical, models, junction, tree, algorithm, message, passing, junction, tree] constructed junction tree use inference process similar belief propagation tree section two versions sum product form also known shafer shenoy algorithm named shafer shenoy belief updating form involves division also known hugin named company lauritzen spiegelhalter algorithm named lauritzen spiegelhalter see lepar shenoy detailed comparison methods sketch hugin algorithm works assume original model following form cliques original graph hand tree deﬁnes distribution following form largest loop chordal graph length consequently chordal graphs sometimes called triangulated however enough graph look like made little triangles example figure chordal even though made little triangles since contains chordless cycle want joint distribution variables clique called clique query adapt technique described section follows create mega node containing query variables nuisance variables lie path multiply messages onto boundary mega node marginalize internal nuisance variables internal marginalization may require use see koller friedman details junction tree algorithm grade letter job happy coherence sat intelligence difﬁculty grade letter job happy coherence sat intelligence difﬁculty figure student graph ﬁll edges added maximal cliques junction tree edge nodes labeled intersection sets nodes called separating set figure koller friedman used kind permission daphne koller nodes junction tree cliques chordal graph separators tree make equal initialize deﬁning separators cliques clique original model ﬁnd clique tree contains multiply onto computing cliques original graph section send messages leaves root back sketched figure upwards pass also known collect root phase node sends parent following message marginalize variables node knows irrelevant send left node received messages children updates belief state using chapter exact inference graphical models root represents posterior nodes clique conditioned evidence normalization constant normalization constant unconditional prior original model dgm downwards pass also known distribute root phase node sends children following message divide sent avoid double counting evidence requires store messages upwards pass node received top message parent compute ﬁnal belief state using equivalent way present algorithm based storing messages inside separator potentials way sending compute separator potential update recipient potential recall initialize sometimes called passing ﬂow way compute separator potential update recipient potential process known junction tree calibration see figure illustration correctness follows fact edge partitions evidence two distinct groups plus fact tree satisﬁes rip ensures information lost performing local computations example jtree algorithm chain interesting see happens apply process chain structured graph hmm detailed discussion found smyth basic idea cliques edges separators nodes shown figure initialize potentials follows set separators set clique set clique junction tree algorithm figure junction tree derived hmm ssm length next send messages left right consider clique potential receives message clique via separator form combined clique potential becomes two slice predictive density clique also receives message via separator form corresponds local evidence combined updated clique potential becomes two slice ﬁltered posterior thus messages forwards pass ﬁltered belief states clique potentials two slice distributions backwards pass messages update factors multiplying message swap old message swap new message see backwards pass involves working posterior beliefs conditional likelihoods see section discussion difference 
[exact, inference, graphical, models, junction, tree, algorithm, computational, complexity, jta] nodes discrete states clear jta takes time space number cliques treewidth graph size largest clique minus unfortunately choosing triangulation minimize treewidth hard explained section jta modiﬁed handle case gaussian graphical models graph theoretic steps remain unchanged message computation differs need deﬁne multiply divide marginalize gaussian potential functions easily done information form see lauritzen murphy cemgil details algorithm takes time space applied chain structured graph algorithm equivalent kalman smoother section chapter exact inference graphical models 
[exact, inference, graphical, models] figure encoding sat problem variables clauses dgm variables binary random variables variables deterministic functions compute truth value clause nodes chain gates ensure cpt ﬁnal node bounded size double rings denote nodes deterministic cpds source figure koller friedman used kind permission daphne koller 
[exact, inference, graphical, models, jta, generalizations] seen use jta algorithm compute posterior marginals graphical model several possible generalizations algorithm mention exploit graph decomposition form differ terms deﬁne compute messages beliefs key requirement operators compute messages form commutative semiring see section computing map estimate replace sum product max product collect phase use traceback distribute phase viterbi algorithm sec tion see dawid details computing probable conﬁgurations nilsson computing posterior samples collect pass usual distribute pass sample variables given values higher tree thus generalizing forwards ﬁltering backwards sampling hmms described section see dawid details solving constraint satisfaction problems dechter solving logical reasoning problems amir mcilraith 
[exact, inference, graphical, models, computational, intractability, exact, inference, worst, case] saw sections jta take time exponential treewidth graph since treewidth number nodes worst case means algorithms exponential problem size course jta slow mean smarter algo rithm unfortunately seems unlikely since easy show exact inference hard dagum luby proof simple reduction satisﬁability prob computational intractability exact inference worst case method restriction section forwards backwards chains section belief propagation trees section variable elimination low treewidth single query section junction tree algorithm low treewidth section loopy belief propagation approximate section convex belief propagation approximate section mean ﬁeld approximate section gibbs sampling approximate section table summary methods used inference graphical models means hidden variables must discrete means factors must linear gaussian term single query refers restriction computes one marginal time see section discussion point stands conjugate exponential means variational mean ﬁeld applies models likelihood exponential family prior conjugate includes case many others well see section lem particular note encode sat problem dgm deterministic links shown figure clamp ﬁnal node arrange cpts iff satisfying assignment computing posterior marginal requires evaluating normalization constant represents probability evidence inference model implicitly solves sat problem fact exact inference hard roth even harder hard see arora barak deﬁnitions terms intuitive reason compute normalizing constant count many satisfying assignments contrast map estimation provably easier model classes greig since intuitively speaking requires ﬁnding one satisfying assignment counting 
[exact, inference, graphical, models, computational, intractability, exact, inference, worst, case, approximate, inference] many popular probabilistic models support efficient exact inference since based chains trees low treewidth graphs many models exact inference intractable fact even simple two node models form may support exact inference prior conjugate likelihood therefore need turn approximate inference methods see table summary coming attractions part methods come guarantee accuracy running time theoretical computer scientists would therefore describe heuristics rather approximation algorithms fact one prove sat problem logical expression form binary variables clause consists conjunction three variables negation goal ﬁnd satisfying assignment set values variables expression evaluates true discrete random variables conjugacy concern since discrete distributions always closed conditioning marginalization consequently graph theoretic considerations importance discussing inference models discrete hidden states chapter exact inference graphical models possible construct polynomial time approximation schemes inference general discrete gms dagum luby roth fortunately see many heuristic methods often perform well practice 
[exact, inference, graphical, models, exercises] exercise variable elimination consider mrf figure suppose want compute partition function using elimination ordering use variable elimination algorithm create new intermediate factors largest intermediate factor add edge original mrf every pair variables end factor called ﬁll edges draw resulting mrf size largest maximal clique graph consider elimination ordering use variable elimination algorithm create new intermediate factors largest intermediate factor add edge original mrf every pair variables end factor called ﬁll edges draw resulting mrf size largest maximal clique graph exercise gaussian times gaussian gaussian prove equation hint use completing square exercise message passing tree consider dgm figure represents following ﬁctitious biological model represents genotype person healthy gene unhealthy gene may inherit unhealthy gene parent continuous measure blood pressure low healthy high unhealthy deﬁne cpds follows meaning matrix etc computational intractability exact inference worst case figure simple dag representing inherited diseases suppose observe unobserved posterior belief suppose observe amd explain answer intuitively suppose explain answer intuitively suppose explain answer intuitively exercise inference lattice mrfs consider mrf lattice graph structure hidden node connected nearest neighbors ising model addition hidden node local evidence assume hidden nodes states general exact inference models intractable maximum cliques corresponding triangulated graph size max suppose lattice short fat one efficiently perform exact inference using deterministic algorithm models exact inference mean computing marginal probabilities exactly evidence give brief description method asymptotic complexity running time algorithm suppose lattice large square hidden states binary case one efficiently exactly compute using deterministic algorithm map estimate arg max joint assignment hidden nodes 
[variational, inference, introduction] seen several algorithms computing functions posterior distribution discrete graphical models use junction tree algorithm perform exact inference explained section however takes time exponential treewidth graph rendering exact inference often impractical case gaussian graphical models exact inference cubic treewidth however even slow many variables addition jta work continuous random variables outside gaussian case mixed discrete continuous variables outside conditionally gaussian case simple two node graphical models form compute exact posterior closed form provided prior conjugate likelihood means likelihood must exponential family see chapter examples note chapter represent unknown variables whereas chapter used represent unknowns general settings must use approximate inference methods section discussed gaussian approximation useful inference two node models form prior conjugate example section applied method logistic regression gaussian approximation simple however posteriors naturally modelled using gaussians example inferring multinomial parameters dirichlet distribution better choice inferring states discrete graphical model categorical distribution better choice chapter study general class deterministic approximate inference algorithms based variational inference jordan jaakkola jordan jaakkola wainwright jordan basic idea pick approximation distribution tractable family try make approximation close possible true posterior reduces inference optimization problem relaxing constraints approximating objective trade accuracy speed bottom line variational inference often gives speed beneﬁts map estimation statistical beneﬁts bayesian approach chapter variational inference 
[variational, inference, variational, inference] suppose true intractable distribution approximation chosen tractable family multivariate gaussian factored distribution assume free parameters want optimize make similar obvious cost function try minimize divergence log however hard compute since taking expectations wrt assumed intractable natural alternative reverse divergence log main advantage objective computing expectations wrt tractable choos ing suitable form discuss statistical differences two objectives section unfortunately equation still tractable written since even evaluating pointwise hard since requires evaluating intractable normalization constant however usually unnormalized distribution tractable compute therefore deﬁne new objective function follows slightly abusing notation since normalized distribution plugging deﬁnition get log log log log log since constant minimizing force become close since divergence always non negative see upper bound nll negative log likelihood log log log alternatively try maximize following quantity koller friedman referred energy functional lower bound log likelihood data log log log since bound tight see variational inference closely related see section variational inference 
[variational, inference, variational, inference, alternative, interpretations, variational, objective] several equivalent ways writing objective provide different insights one formulation follows log log expected energy recall log minus entropy system statistical physics called variational free energy helmholtz free energy another formulation objective follows log log log log log log expected nll plus penalty term measures far approximate posterior exact prior also interpret variational objective point view information theory called bits back argument see hinton camp honkela valpola details 
[variational, inference, variational, inference, forward, reverse, kl] since divergence symmetric arguments minimizing wrt give different behavior minimizing discuss two different methods first consider reverse also known projection information projection deﬁnition inﬁnite thus must ensure say reverse zero forcing hence typically estimate support consider forwards also known projection moment projection inﬁnite must ensure say forwards zero avoiding hence typically estimate support difference methods illustrated figure see true distribution multimodal using forwards bad idea assuming constrained unimodal since resulting posterior mode mean region low density right two peaks contexts reverse tractable compute also sensible statistically chapter variational inference figure illustrating forwards reverse bimodal distribution blue curves contours true distribution red curves contours unimodal approximation minimizing forwards tends cover minimizing reverse locks one two modes based figure bishop figure generated klfwdreversemixgauss figure illustrating forwards reverse symmetric gaussian blue curves contours true distribution red curves contours factorized approximation minimizing minimizing based figure bishop figure generated klpqgauss another example difference shown figure target distribution elongated gaussian approximating distribution product two gaussians figure show result minimizing simple example one show solution form called free variables free vary rather ﬁxed variational free energy function distribution whereas regular energy function state vector mean ﬁeld method figure shows correctly captured mean approximation compact variance controlled direction smallest variance fact often case although always turner minimizing factorized results approximation overconﬁdent figure show result minimizing show exercise optimal solution minimizing forward wrt factored approximation set product marginals thus solution form figure shows broad since estimate support rest chapter next focus minimizing section discuss expectation proagation discuss ways locally optimize one create family divergence measures indexed parameter deﬁning alpha divergence follows measure satisﬁes iff obviously symmetric hence metric corresponds limit whereas corresponds limit get symmetric divergence measure linearly related hellinger distance deﬁned note valid distance metric symmetric non negative satisﬁes triangle inequality see minka details 
[variational, inference, mean, ﬁeld, method] one popular forms variational inference called mean ﬁeld approxima tion opper saad approach assume posterior fully factorized approximation form goal solve optimization problem min optimize parameters marginal distribution section derive coordinate descent method step make following update log log const chapter variational inference model section ising model section factorial hmm section univariate gaussian section linear regression section logistic regression section mixtures gaussians section latent dirichlet allocation section table models book provide detailed derivations mean ﬁeld inference algorithm unnormalized posterior notation means take expectation respect variables except example three variables sums get replaced integrals necessary updating need reason variables share factor terms markov blanket see section terms get absorbed constant term since replacing neighboring values mean value method known mean ﬁeld similar gibbs sampling section except instead sending sampled values neighboring nodes send mean values nodes tends efficient since mean used proxy large number samples hand mean ﬁeld messages dense whereas samples sparse make sampling scalable large models course updating one distribution time slow since form coordinate descent several methods proposed speed basic approach including using pattern search honkela techniques based parameter expansion jaakkola however consider methods chapter important note mean ﬁeld method used infer discrete continuous latent quantities using variety parametric forms see contrast variational methods encounter later restricted applicability table lists examples mean ﬁeld cover book 
[variational, inference, mean, ﬁeld, method, derivation, mean, ﬁeld, update, equations] recall goal variational inference minimize upper bound log equivalently try maximize lower bound log log one term time mean ﬁeld method write objective singling terms involve regarding terms constants get log log log log log log log log const log log log average hidden variables except thus rewrite follows maximize minimizing setting follows exp log usually ignore local normalization constant since know must normalized distribution hence usually work form log log const functional form distributions determined type variables well form model sometimes called free form optimization discrete random variable discrete distribution continuous random variable kind pdf see examples 
[variational, inference, mean, ﬁeld, method, example, mean, ﬁeld, ising, model] consider image denoising example section hidden pixel values clean image joint model form chapter variational inference prior form exp nbr likelihood form exp therefore posterior form exp approximate fully factored approximation mean value node derive update variational parameter ﬁrst write log dropping terms involve log nbr const depends states neighboring nodes take expectations wrt get exp nbr thus replace states neighbors average values let nbr mean ﬁeld inﬂuence node also let approximate marginal posterior given sigm structured mean ﬁeld sample meanfieldh sample meanfieldh mean sweeps meanfieldh figure example image denoising using mean ﬁeld parallel updates damping factor use ising prior gaussian noise model show results iterations across image compare figure figure generated isingimagedenoisedemo similarly sigm compute new mean site tanh hence update equation becomes tanh nbr see also exercise alternative derivation equations turn equations ﬁxed point algorithm writing tanh nbr usually better use damped updates form tanh nbr update nodes parallel update asychronously figure shows method action applied ising model homogeneous attractive potentials use parallel updates damping factor use damping tend get checkerboard artefacts 
[variational, inference, structured, mean, ﬁeld] figure factorial hmm chains fully factorized approximation product chains approximation based figure ghahramani jordan efficiently handle kinds dependencies called structured mean ﬁeld approach saul jordan approach except group sets variables together update simultaneously follows simply treating variables group single mega variable repeating derivation section long perform efficient inference method tractable overall give example see bouchard cote jordan recent work area 
[variational, inference, structured, mean, ﬁeld, example, factorial, hmm] consider factorial hmm model ghahramani jordan introduced section suppose chains length suppose hidden node states model deﬁned follows mjk entry transition matrix chain initial state distribution chain observation model encoding matrix assuming figure illustrates model case even though chain priori independent become coupled posterior due observed common child junction tree algorithm applied graph takes time derive structured mean ﬁeld algorithm takes time number mean ﬁeld iterations typically suffices good performance structured mean ﬁeld write exact posterior following form exp log log interpreted elementwise approximate posterior product marginals figure better approximation use product chains figure chain tractably updated individually using forwards backwards algorithm precisely assume tmk mjk tmk see tmk parameters play role approximate local evidence averaging effects chains contrast exact local evidence couples chains together rewrite approximate posterior exp log see temporal factors exact posterior local evidence term different objective function given log log expectations taken wrt one show exercise update form exp diag chapter variational inference parameter plays role local evidence averaging neighboring chains computed chain perform forwards backwards parallel using approximate local evidence terms compute update cost full sweep variational parameters since run forwards backwards times chain independently cost fully factorized approximation much accurate 
[variational, inference, variational, bayes] consider latent variable models form includes mixtures models pca hmms etc two kinds unknowns parameters latent variables saw section common models using step infer posterior latent variables step compute point estimate parameters justiﬁcation two fold first results simple algorithms second posterior uncertainty usually less since informed data cases whereas informed makes map estimate chapter variational inference reasonable map estimate however provides way bayesian modeling uncertainty parameters well latent variables computational cost essentially method known variational bayes vbem basic idea use mean ﬁeld approximate posterior form ﬁrst factorization crucial assumption make algorithm tractable second factorization follows model since latent variables iid conditional vbem alternate updating variational step updating variational step recover standard vbem approximating param eter posterior using delta function variational step similar standard step except instead plugging map estimate parameters computing need average parameters roughly speaking computed plugging posterior mean parameters instead map estimate computing using standard algorithms forwards backwards unfortunately things quite simple basic idea details depend form model give examples variational step similar standard step except instead computing point estimate parameters update hyper parameters using expected sufficient statis tics process usually similar map estimation regular details depend form model principle advantage vbem regular marginalizing parameters compute lower bound marginal likelihood used model selection see example section vbem also egalitarian since treats parameters ﬁrst class citizens like unknown quantity whereas makes artiﬁcial distinction parameters latent variables 
[variational, inference, variational, bayes, example, univariate, gaussian] following mackay let consider apply infer posterior parameters gaussian precision convenience use conjugate prior form however use approximate factored posterior form need specify forms distributions optimal forms fall automatically derivation conveniently turn gaussian gamma respectively might wonder would want since know compute exact posterior model section two reasons first useful pedagogical exercise since compare quality approximation exact posterior second simple modify method handle semi conjugate prior form exact inference longer possible method originally called ensemble learning mackay since using ensemble parameters distribution instead point estimate however term ensemble learning also used describe methods boosting prefer term variational bayes target distribution unnormalized log posterior form log log log log log log log log const updating optimal form obtained averaging log log log const const completing square one show stage know hence cannot compute derive updating optimal form given log log log log const log log log const recognize log gamma distribution hence chapter variational inference computing expectations implement updates specify compute various expectations since since give explicit forms update equations see fact ﬁxed constants need updated iteratively fact one solve ﬁxed points analytically order illustrate iterative updating scheme illustration figure gives example method action green contours represent exact posterior gaussian gamma dotted red contours represent variational approximation several iterations see ﬁnal approximation reasonably close exact solution however compact true distribution often case mean ﬁeld inference underestimates posterior uncertainty see section discussion point lower bound maximizing lower bound log marginal likelihood log log dμdλ useful compute lower bound three reasons first used assess convergence algorithm second used assess correctness one variational bayes exact exact exact exact figure factored variational approximation red gaussian gamma distribution green initial guess updating updating convergence iterations based bishop figure generated unigaussvbdemo code bound increase monotonically must bug third bound used approximation marginal likelihood used bayesian model selection unfortunately computing lower bound involves fair amount tedious algebra work details example models state results without proof even omit discussion bound altogether brevity model computed follows log dμdλ log log log log log expectations wrt recognize last two terms entropy gaussian entropy gamma distribution given log log log log chapter variational inference digamma function compute terms need following facts log log expected log likelihood one show log log log log log empirical mean variance expected log prior log log log log log log log expected log prior one show log log log log log putting altogether one show log log log const quantity monotonically increases update 
[variational, inference, variational, bayes, example, linear, regression] section discussed empirical bayes approach setting hyper parameters ridge regression known evidence procedure particular assumed likelihood form prior form variational bayes computed type estimate approach extended section handle prior form diag allows one hyper parameter per feature technique known automatic relevancy determination section derive algorithm model follow presentation drugowitsch initially use following prior choose use following factorized approximation posterior given assumptions one show see drugowitsch optimal form posterior method extended ard case straightforward way using following priors diag posterior computed except use diag instead note drugowitsch uses hyper parameters hyper parameters whereas bishop sec uses hyper parameters treats ﬁxed hopefully avoid confusion use hyper parameters hyper parameters chapter variational inference posterior form algorithm alternates updating inferred posterior predictive student distribution shown equation speciﬁcally single data case exact marginal likelihood used model selection given dαdλ compute lower bound log follows log log log log log log log log log log ard case last line becomes log log log log figure compare model selection problem polynomial regression see gives similar results precise behavior depends sample size estimate posterior models diffuse since models uncertainty hyper parameters posterior estimate hyper parameters becomes well determined indeed compute uninformative prior get variational bayes method method method method figure plot posterior models polynomials degree assuming uniform prior approximate marginal likelihood using use data points shown figure use data points shown figure figure generated linregebmodelselvsn compare equation modulo terms hindsight perhaps surprising since trying maximize log trying maximize lower bound log 
[variational, inference, variational, bayes, example, vbem, mixtures, gaussians] let consider mixture gaussians using vbem use scare quotes since estimating model parameters inferring posterior follow presentation bishop sec unfortunately details rather complicated fortunately one gets used bit practice usual math simply reading equations help much really try deriving results try exercises want learn stuff depth variational posterior likelihood function usual one gaussian mixture models data point belongs cluster otherwise variational bayes assume following factored conjugate prior dir precision matrix cluster subscript means parameters prior assume prior parameters clusters mixing weights usually use symmetric prior exact posterior mixture distributions corresponding possible labelings try approximate volume around one modes use standard approximation posterior stage speciﬁed forms functions determined form likelihood prior show optimal form follows cat dir lack subscript means parameters posterior prior derive update equations variational parameters derivation variational step form obtained looking complete data log joint ignoring terms involve taking expectations left wrt hidden variables except log log const log const deﬁne log log log log using fact dir log log chapter variational inference digamma function see exercise detailed derivation next use fact get log log log log finally expected value quadratic form get putting altogether get posterior responsibility cluster datapoint exp compare expression used regular exp signiﬁcance difference discussed section derivation variational step using mean ﬁeld recipe log log log log log const see factorizes form term log log log const exponentiating recognize dirichlet distribution dir variational bayes iter lower bound log marginal likelihood variational bayes objective gmm old faithful data figure lower bound iterations algorithm figure steep parts curve correspond places algorithm ﬁgures increase bound killing unnecessary mixture components described section plateaus correspond slowly moving clusters around figure generated mixgaussvbdemofaithful terms similar step map estimation discussed section except computing parameters posterior rather map estimates lower bound marginal likelihood algorithm trying maximize following lower bound log log quantity increase monotonically iteration shown figure fortunately deriving bound bit messy need compute expectations unnormalized log posterior well entropies distribution leave details similar section exercise chapter variational inference posterior predictive distribution showed approximate posterior form dir consequently posterior predictive density approximated follows using results section weighted sum student distributions instead used plug approximation would get weighted sum gaussian distributions model selection using vbem simplest way select using several models use variational lower bound log marginal likelihood log approximate however lower bound needs modiﬁed somewhat take account lack identiﬁability parameters section particular although approximate volume occupied parameter posterior around one local modes components equivalent modes differ merely permuting labels therefore use log log automatic sparsity inducing effects vbem although provides reasonable approximation marginal likelihood better bic beal ghahramani method still requires ﬁtting multiple models one value considered faster alternative single model set large set small figure see resulting prior mixing weights spikes near corners simplex encouraging sparse mixing weight vector regular map estimate mixing weights form unforuntately negative figueiredo variational bayes iter iter figure visualize posterior mean parameters various stages vbem algorithm applied mixture gaussians model old faithful data shading intensity proportional mixing weight initialize means use dirichlet hyper parameter based figure bishop figure generated mixgaussvbdemofaithful based code emtiyaz khan iter iter figure visualize posterior values model figure see unnecessary components get killed figure generated mixgaussvbdemofaithful jain however vbem use exp exp exp compute like substract posterior counts hurt small clusters large clusters like regressive tax effect clusters weighted members become empty successive iterations whereas popular clusters get members called rich get richer phenomenon encounter section discuss dirichlet process mixture models automatic pruning method demonstrated figure mixture gaussians old faithful dataset data really needs clusters rest get killed details see liang chapter variational inference example used use larger get sparsity effect figure plot various iterations see unwanted components get extinguished provides efficient alternative performing discrete search number clusters 
[variational, inference, variational, message, passing, vibes] seen mean ﬁeld methods least fully factorized variety similar compute node full conditional average neighbors similar gibbs sampling section except derivation equations usually bit work fortunately possible derive general purpose set update equations work dgm cpds exponential family parent nodes conjugate distributions ghahramani beal see wand recent extension handle non conjugate priors one sweep graph updating nodes one time manner similar gibbs sampling known variational message passing vmp winn bishop implemented open source program vibes analog bugs popular generic program gibbs sampling discussed section vmp mean ﬁeld best suited inference one hidden nodes continuous performing bayesian learning models hidden nodes discrete accurate approximate inference algorithms used discuss chapter 
[variational, inference, local, variational, bounds] far focusing mean ﬁeld inference form variational inference based minimizing approximate posterior assumed factorized exact unnormalized posterior however another kind variational inference replace speciﬁc term joint distribution simpler function simplify computation posterior approach sometimes called local variational approximation since modifying one piece model unlike mean ﬁeld global approximation section study several examples method 
[variational, inference, local, variational, bounds, motivating, applications] explain derive local variational bounds give examples useful variational logistic regression consider problem approximate parameter posterior multiclass logistic regression model gaussian prior one approach use gaussian laplace approx imation discussed section however variational approach produce available http vibes sourceforge net local variational bounds accurate approximation posterior since tunable parameters another advantage variational approach monotonically optimizes lower bound likelihood data see see need bound note likelihood written follows exp lse since set identiﬁability deﬁne log sum exp lse function follows lse log main problem likelihood conjugate gaussian prior discuss compute gaussian like lower bounds likelihood give rise approximate gaussian posteriors multi task learning one important application bayesian inference logistic regression multiple related classiﬁers want case want share information parameters classiﬁer requires maintain posterior distibution parameters measure conﬁdence well estimate value embed variational method inside larger hierarchical model order perform multi task learning described braun mcauliffe discrete factor analysis another situation variational bounds useful arises factor analysis model discrete data model like multinomial logistic regression except input variables hidden factors need perform inference hidden variables well regression weights simplicity might perform point estimation weights integrate hidden variables using variational use variational bound step see section details correlated topic model topic model latent variable model text documents forms discrete data see section details often assume distribution topics dirichlet prior powerful model known correlated topic model uses gaussian prior model correlations easily see section details unfortunately also involves lse function however use variational bounds context variational algorithm see later chapter variational inference 
[variational, inference, local, variational, bounds, bohning’s, quadratic, bound, log-sum-exp, function] examples require dealing multiplying gaussian prior multinomial likelihood difficult log sum exp lse term section derive way derive gaussian like lower bound likelihood consider taylor series expansion lse function around lse lse exp lse diag gradient hessian lse chosen equality holds upper bound lse found replacing hessian matrix matrix bohning showed achieved use matrix recall number classes note independent however still write rather dropping subscript since bounds consider data dependent curvature term upper bound lse therefore becomes lse lse vector variational parameters use result get following lower bound softmax likelihood log simplify notation deﬁne pseudo measurement get gaussianized version observation model function depend given easy compute posterior using bayes rule gaussians explain update variational parameters local variational bounds applying bohning bound multinomial logistic regression let see apply bound multinomial logistic regression equation deﬁne goal variational inference maximizing log lse lse approximate posterior ﬁrst term divergence two gaussians given log dimensionality gaussian assume prior form typically block diagonal second term simply ﬁnal term lower bounded taking expectations quadratic upper bound lse follows lse putting altogether log lower bound combines jensen inequality mean ﬁeld inference plus quadratic lower bound due lse term write use coordinate ascent optimize lower bound update variational posterior parameters variational likelihood parameters leave chapter variational inference detailed derivation exercise state results exploit fact constant matrix plus fact block structure simplify ﬁrst two terms follows denotes kronecker product see algorithm pseudocode http www ubc emtiyaz software catlgm html matlab code algorithm variational inference multi class logistic regression using bohning bound input prior deﬁne dummy encode deﬁne blockdiag deﬁne initialize repeat reshape exp lse compute lower bound using equation converged return 
[variational, inference, local, variational, bounds, bounds, sigmoid, function] many models binary data case weight vector matrix case bohning bound local variational bounds bound figure quadratic lower bounds sigmoid logistic function solid red plot sigm dotted blue plot lower bound bohning bound tight bound tight figure generated sigmoidlowerbounds becomes log log possible derive alternative quadratic bound case shown jaakkola jordan following form log log tanh sigm shall refer bound inventors jaakkola jordan facilitate comparison bohning bound let rewrite bound quadratic form follows log log bound adaptive curvature term since depends addition tight two points evident figure contrast bohning bound constant curvature bound tight one point evident figure chapter variational inference wish use bound binary logistic regression make small modiﬁcations algorithm first use new deﬁnitions fact constant using bound unlike using bohning bound means cannot compute outside main loop making method constant factor slower next note updates posterior become finally compute update isolate terms depend get sigm const optimizing wrt gives equation monotonic need consider negative values symmetry bound around see figure hence way make expression hence update becomes new although bound tighter bohning bound sometimes tight enough order estimate posterior covariance accurately accurate approach uses piecewise quadratic upper bound lse described marlin increasing number pieces bound made arbitrarily tight 
[variational, inference, local, variational, bounds, bounds, approximations, log-sum-exp, function] several bounds approximations multiclass lse function use brieﬂy summarize note however require numerical optimization methods compute making complicated implement product sigmoids approach bouchard exploits fact log log applies bound term right local variational bounds jensen inequality approach blei lafferty uses jensen inequality follows lse log exp log exp log exp last term follows mean log normal distribution multivariate delta method approach ahmed xing braun mcauliffe uses multivariate delta method way approximate moments function using taylor series expansion detail let function interest using second order approximation around gradient hessian evaluated use lse get lse lse lse function deﬁned equations 
[variational, inference, local, variational, bounds, variational, inference, based, upper, bounds] far concentrating lower bounds however sometimes need use upper bound example saul derives mean ﬁeld algorithm sigmoid belief nets dgms cpd logistic regression function neal unlike case ising models resulting mrf pairwise contains higher order interactions makes standard mean ﬁeld updates intractable particular turn involve computing expression requires evaluating log log sigm notice minus sign front saul show derive upper bound sigmoid function make update tractable resulting monotonically convergent inference procedure chapter variational inference 
[variational, inference, exercises] exercise laplace approximation log univariate gaussian compute laplace approximation log gaussian using uninformative prior log exercise laplace approximation normal gamma consider estimating log gaussian using uniformative normal gamma prior log posterior log log show ﬁrst derivatives log log show hessian matrix given log log log log amp use derive laplace approximation posterior exercise variational lower bound univariate gaussian fill details derivation section exercise variational lower bound gmms consider vbem gmms section show lower bound following form local variational bounds dir dir normalization constant dirichlet wishart given dir multivariate gamma function finally entropy wishart given given equation exercise derivation log dirichlet distribution show exp log exp exp dir exercise alternative derivation mean ﬁeld updates ising model derive equation directly optimizing variational free energy one term time chapter variational inference exercise forwards reverse divergence source exercise mackay consider factored approximation joint distribution show minimize forwards set optimal approximation product marginals consider following joint distribution rows represent columns show reverse three distinct minima identify minima evaluate value set exercise derivation structured mean ﬁeld updates fhmm derive updates section exercise variational binary sigmoid link consider binary model ber sigm ber sigm derive algorithm model using jaakkola jordan bound hint answer tipping exercise asks derive equations exercise binary probit link section showed use probit regression using model form latent consider case inputs also unknown binary factor analysis show model using variational bayes making approximation posterior form hint gaussian truncated univariate gaussian 
[variational, inference, introduction] chapter discussed mean ﬁeld inference approximates posterior product marginal distributions allows use different parametric forms variable particularly useful performing bayesian inference parameters statistical models mean variance gaussian gmm regression weights glm saw discussed variational bayes chapter discuss slightly different kind variational inference basic idea minimize exact unnormalized posterior longer require factorized fact even require globally valid joint distribution instead require locally consistent meaning joint distribution two adjacent nodes agrees corresponding marginals deﬁne precisely addition new kind inference discuss approximate methods map state estimation discrete graphical models turns algorithms solving map problem similar approximate methods computing marginals see 
[variational, inference, loopy, belief, propagation, algorithmic, issues] figure simple ugm factor graph representation assuming one potential per maximal clique factor graph representation assuming one potential per edge figure simple dgm corresponding factor graph based figure yedidia also convert dgm factor graph create one factor per cpd connect factor variables use cpd example figure represents following factorization deﬁne etc node one parent hence graph chain simple tree one factor per edge root nodes prior cpds absorvbed children factors models equivalent pairwise mrfs loopy belief propagation algorithmic issues figure message passing bipartite factor graph square nodes represent factors circles represent variables source figure kschischang used kind permission brendan frey factor graph derive version sends messages factor graph proposed kschis chang speciﬁcally two kinds messages variables factors nbr factors variables nbr nbr factors connected variable nbr variables connected factor messages illustrated figure convergence compute ﬁnal beliefs product incoming messages bel nbr following sections focus lbp pairwise models rather factor graphs notational simplicity 
[variational, inference, loopy, belief, propagation, algorithmic, issues, brief, history] applied loopy graphs guaranteed give correct results may even converge indeed judea pearl invented belief propagation trees wrote following loopy loops present network longer singly connected local propagation chapter variational inference schemes invariably run trouble ignore existence loops permit nodes continue communicating network singly connected messages may circulate indeﬁnitely around loops process may converge stable equilibrium oscillations normally occur probabilistic networks tend bring messages stable equilibrium time goes however asymptotic equilibrium coherent sense represent posterior probabilities nodes network pearl despite reservations pearl advocated use belief propagation loopy networks approximation scheme pearl personal communication exercise pearl investigates quality approximation applied particular loopy belief network however main impetus behind interest arose mceliece showed popular algorithm error correcting codes known turbo codes berrou could viewed instance applied certain kind graph important observation since turbo codes gotten close theoretical lower bound coding efficiency proved shannon another approach known low density parity check ldpc codes achieved comparable performance also uses lbp decoding see figure example murphy lbp experimentally shown also work well inference kinds graphical models beyond error correcting code context since method widely used many different applications 
[variational, inference, loopy, belief, propagation, algorithmic, issues, lbp, pairwise, models] discuss apply lbp undirected graphical model pairwise factors discuss directed case involve higher order factors next section method simple continually apply equations convergence see algorithm pseudocode beliefpropagation matlab code discuss issues convergence accuracy method shortly algorithm loopy belief propagation pairwise mrf input node potentials edge potentials initialize messages edges initialize beliefs bel nodes repeat send message edge nbr update belief node bel nbr beliefs change signiﬁcantly return marginal beliefs bel loopy belief propagation algorithmic issues figure simple factor graph representation low density parity check code factor graphs deﬁned section message bit hollow round circle connected two parity factors solid black squares parity factor connected three bits parity factor form stu xor operator local evidence factors hidden node shown larger example random ldpc code see graph locally tree like meaning short cycles rather cycle length log number nodes gives hint loopy works well graphs note however error correcting code graphs short loops full explanation source figure wainwright jordan used kind permission martin wainwright 
[variational, inference, loopy, belief, propagation, algorithmic, issues, lbp, factor, graph] handle models higher order clique potentials includes directed models nodes one parent useful use representation known factor graph explain representation describe apply lbp models factor graphs factor graph kschischang frey graphical representation uniﬁes directed undirected models simpliﬁes certain message passing algorithms precisely factor graph undirected bipartite graph two kinds nodes round nodes represent variables square nodes represent factors edge variable every factor mentions example consider mrf figure assume one potential per maximal clique get factor graph figure represents function assume one potential per edge get factor graph figure represents function chapter variational inference 
[variational, inference, loopy, belief, propagation, algorithmic, issues, convergence] lbp always converge even may converge wrong answers raises several questions predict convergence occur increase probability convergence increase rate convergence brieﬂy discuss issues discuss issue accuracy results convergence chapter variational inference synchronous asynchronous smoothing true time seconds time seconds time seconds time seconds time seconds time seconds figure illustration behavior loopy belief propagation ising grid random potentials unif larger inference becomes harder percentage messasges converged time different update schedules dotted damped sychronous nodes converge dashed undamped asychnronous half nodes converge solid damped asychnronous nodes converge marginal beliefs certain nodes time solid straight line truth dashed sychronous solid damped asychronous source figure koller friedman used kind permission daphne koller lbp converge details analysis lbp converge beyond scope chapter brieﬂy sketch basic idea key analysis tool computation tree visualizes messages passed algorithm proceeds figure gives simple example ﬁrst iteration node receives messages nodes second iteration receives one message node via node one node via node two messages node via nodes key insight iterations lbp equivalent exact computation computation tree height strengths connections edges sufficiently weak inﬂuence leaves root diminish time convergence occur see wainwright jordan references therein information loopy belief propagation algorithmic issues figure simple loopy graph computation tree rooted node rounds message passing nodes occur often tree higher degree nodes source figure wainwright jordan used kind permission martin wainwright making lbp converge although theoretical convergence analysis interesting practice faced model lbp converging one simple way reduce chance oscillation use damping instead sending message send damped message form damping factor clearly reduces standard scheme partial updating scheme help improve convergence using value standard practice beneﬁts approach shown figure see damped updating results convergence much often undamped updating possible devise methods known double loop algorithms guaranteed converge local minimum objective lbp minimizing yuille welling teh unfortunately methods rather slow complicated accuracy resulting marginals usually much greater standard lbp indeed oscillating marginals sometimes sign lbp approximation poor one consequently techniques widely used section see different convergent version widely used increasing convergence rate message scheduling even lbp converges may take long time standard approach implementing lbp perform synchronous updates nodes absorb messages parallel send messages parallel new messages iteration computed parallel using number edges function computes message edge given old messages analogous jacobi method solving linear chapter variational inference systems equations well known bertsekas gauss seidel method performs asynchronous updates ﬁxed round robin fashion converges faster solving linear systems equations apply idea lbp using updates form message edge computed using new messages iteration edges earlier ordering using old messages iteration edges later ordering raises question order update messages one simple idea use ﬁxed random order beneﬁts approach shown figure see damped asynchronous updating results convergence much often synchronous updating smarter approach pick set spanning trees perform sweep one tree time keeping messages ﬁxed known tree reparameterization trp wainwright confused sophisticated tree reweighted often abbreviated trw discussed section however even better using adaptive ordering intuition focus computational efforts variables uncertain elidan proposed technique known residual belief propagation messages scheduled sent according norm difference previous value deﬁne residual new message iteration log log max log store messages priority queue always send one highest residual message sent messages depend messages form nbr need recomputed residual recomputed added back queue elidan showed experimentally method converges often much faster using sychronous updating asynchronous updating ﬁxed order trp approach reﬁnement residual presented sutton mccallum paper use upper bound residual message instead actual residual means messages computed going sent computed purposes evaluating residual observed ﬁve times faster residual although quality ﬁnal results similar 
[variational, inference, loopy, belief, propagation, algorithmic, issues, accuracy, lbp] graph single loop one show max product version lbp ﬁnd correct map estimate converges weiss general graphs one bound error approximate marginals computed lbp shown wainwright vinyals much stronger results available case gaussian models weiss freeman johnson bickson particular gaussian case method converges means exact although variances typically beliefs conﬁdent loopy belief propagation algorithmic issues 
[variational, inference, loopy, belief, propagation, algorithmic, issues, speedup, tricks, lbp] several tricks one use make run faster discuss fast message computation large state spaces cost computing message whether tree loopy graph number states size largest factor pairwise ugms many vision problems image denoising quite large say represents discretization underlying continuous space per message expensive fortunately certain kinds pairwise potential functions form one compute sum product messages log time using fast fourier transform fft explained felzenszwalb huttenlocher key insight message computation convolution nbr potential function gaussian like potential compute convolution time sequentially convolving small number box ﬁlters felzenszwalb huttenlocher max product case technique called distance transform used compute messages time however works exp one following forms quadratic truncated linear min potts model see felzenszwalb huttenlocher details multi scale methods method speciﬁc lattice structures commonly arise computer vision based multi grid techniques methods widely used numerical linear algebra one core problems fast solution linear systems equations equivalent map estimation gaussian mrf computer vision context felzenszwalb huttenlocher suggested using following heuristic signiﬁcantly speedup construct coarse ﬁne grid compute messages coarse level use initialize messages level reach bottom level iterations standard required since long range communication already achieved via initialization process beliefs coarse level computed small number large blocks local evidence computed average log probability possible block label assigns pixels block pairwise potential based discrepancy labels neighboring blocks taking account size run lbp coarse level use initialize messages one level note model still ﬂat grid however initialization process exploits multi scale nature problem see felzenszwalb huttenlocher details chapter variational inference cascades another trick handling high dimensional state spaces also used exact inference chain structured crfs prune improbable states based com putationally cheap ﬁltering step fact one create hierarchy models tradeoff speed accuracy called computational cascade case chains one guarantee cascade never ﬁlter true map solution weiss 
[variational, inference, loopy, belief, propagation, theoretical, issues] attempt understand lbp algorithm variational point view presen tation closely based excellent page review article wainwright jordan paper sometimes called monster authors view length technical difficulty section sketches main results simplify presentation focus special case pairwise ugms discrete variables tabular potentials many results generalize ugms higher order clique potentials includes dgms makes notation complex see koller friedman details general case 
[variational, inference, loopy, belief, propagation, theoretical, issues, ugms, represented, exponential, family, form] assume distribution following form exp graph nodes edges henceforth drop explicit conditioning brevity since assume known ﬁxed rewrite exponential family form follows exp node edge parameters canonical parameters node edge indicator functions sufficient statistics note use index nodes index states mean sufficient statistics known mean parameters model given vector length containing node edge marginals completely characterizes distribution sometimes treat distribution equation called standard overcomplete representation called overcom plete ignores sum one constraints cases convenient remove loopy belief propagation theoretical issues redundancy example consider ising model model written exp hence use following minimal parameterization corresponding mean parameters 
[variational, inference, loopy, belief, propagation, theoretical, issues, marginal, polytope] space allowable vectors called marginal polytope denoted structure graph deﬁning ugm deﬁned set mean parameters given model generated valid probability distribution example consider ising model two nodes connected one show following minimal set constraints write matrix vector form four constraints deﬁne series half planes whose intersection deﬁnes polytope shown figure since obtained taking convex combination vectors also written convex hull feature set conv example node mrf binary states conv four black dots figure see convex hull deﬁnes volume intersection half spaces marginal polytope play crucial role approximate inference algorithms discuss rest chapter chapter variational inference figure illustration marginal polytope ising model two variables cartoon illustration set nonconvex inner bound marginal polytope used mean ﬁeld cartoon illustration relationship used loopy set always outer bound inclusion strict whenever loops sets polytopes deﬁned intersection half planes deﬁned facets convex hull vertices actually fewer facets despite picture fact facets number states per variable number variables number edges contrast facets hand vertices despite picture since contains binary vector extreme points plus additional fractional extreme points source figures wainwright jordan used kind permission martin wainwright 
[variational, inference, loopy, belief, propagation, theoretical, issues, exact, inference, variational, optimization, problem] recall section goal variational inference ﬁnd distribution maximizes energy functional log log log unnormalized posterior write log let exact energy functional becomes max joint distribution state conﬁgurations valid write since divergence zero know max log way cast exact inference variational optimization problem equation seems easy optimize objective concave since sum linear function concave function see figure see entropy concave furthermore maximizing convex set however marginal polytope exponentially many facets cases structure polytope exploited dynamic programming saw chapter general exact inference takes exponential time existing deterministic approximate inference schemes proposed literature seen different approximations marginal polytope explain loopy belief propagation theoretical issues 
[variational, inference, loopy, belief, propagation, theoretical, issues, mean, ﬁeld, variational, optimization, problem] discussed mean ﬁeld length chapter let interpret mean ﬁeld inference new abstract framework help compare approximate methods discuss first let edge subgraph original graph let subset sufficient statistics associated cliques let set canonical parameters full model deﬁne canonical parameter space submodel follows words require natural parameters associated sufficient statistics outside chosen class zero example case fully factorized approximation remove edges graph giving case structured mean ﬁeld section set edges tractable subgraph next deﬁne mean parameter space restricted model follows called inner approximation marginal polytope since see figure sketch note non convex polytope results multiple local optima contrast approximations consider later convex deﬁne entropy approximation entropy distribution deﬁned submodel deﬁne mean ﬁeld energy functional optimization problem follows max log case fully factorized mean ﬁeld approximation pairwise ugms write objective follows max probability simplex mean ﬁeld involves concave objective maximized non convex set typically optimized using coordinate ascent since easy optimize scalar concave function example pairwise ugm get exp exp nbr 
[variational, inference, loopy, belief, propagation, theoretical, issues, lbp, variational, optimization, problem] section explain lbp viewed variational inference problem chapter variational inference figure illustration pairwise ugm binary nodes together set pseudo marginals globally consistent slice marginal polytope illustrating set feasible edge marginals assuming node marginals clamped source figure wainwright jordan used kind permission martin wainwright outer approximation marginal polytope want consider possible probability distributions markov wrt model need consider vectors since set exponentially large usually infeasible optimize standard strategy combinatorial optimization relax constraints case instead requiring probability vector live consider vector satisﬁes following local consistency constraints ﬁrst constraint called normalization constraint second called marginal ization constraint deﬁne set holds holds set also polytope constraints convex outer approximation shown figure call terms pseudo marginals since may correspond marginals valid probability distribution example consider figure picture shows set pseudo node edge marginals satisfy local consistency requirements however globally consistent see note implies implies implies possible see wainwright jordan formal proof indeed figure shows contains points claim equality iff tree see ﬁrst consider loopy belief propagation theoretical issues element vector must satisfy normalization marginalization constraints hence consider converse suppose tree let deﬁnition satisﬁes normalization marginalization constraints however tree represented form hence satsifying normalization local consistency enough deﬁne valid distribution tree hence well contrast graph loops see figure example fact entropy approximation equation write exact entropy tree structured distribution follows log log note rewrite mutual information term form hence get following alternative equivalent expression degree number neighbors node bethe approximation entropy simply use equation even tree bethe deﬁne bethe free energy bethe bethe deﬁne bethe energy functional negative bethe free energy hans bethe german american physicist chapter variational inference lbp objective combining outer approximation bethe approximation entropy get following bethe variational problem bvp min bethe max bethe space optimizing convex set objective concave since bethe concave thus multiple local optima bvp value obtained bvp approximation log case trees approximation exact case models attractive potentials approximation turns upper bound sudderth message passing lagrange multipliers subsection show ﬁxed point lbp algorithm deﬁnes stationary point constrained objective let deﬁne normalization constraint marginalization constraint edge write lagrangian bethe constraint explicitly enforced one show hold optimum since simple algebra shows yields log nbr log deﬁned using fact marginalization con straint implies get log nbr nbr make connection message passing deﬁne exp notation rewrite equations taking exponents sides follows exp nbr exp nbr nbr extensions belief propagation terms absorbed constant proportionality see equivalent usual expression node edge marginals lbp derive equation messages terms messages rather terms enforce marginalization condition one show exp nbr see equivalent usual expression messages lbp 
[variational, inference, loopy, belief, propagation, theoretical, issues, loopy, mean, ﬁeld] interesting compare naive mean ﬁeld lbp approximations several obvious differences first lbp exact trees whereas suggesting lbp general accurate see wainwright analysis second lbp optimizes node edge marginals whereas optimizes node marginals suggesting lbp accurate third case true edge marginals factorize free energy approximations cases less obvious nevertheless seems true objective many local optima lbp objective optimizing objective seems harder particular weiss shows empirically optimizing starting uniform random initial conditions often leads poor results whereas optimizing uniform initial messages often leads good results furthermore initializing marginals also leads good results although tends overconﬁdent indicating problem caused inaccuracy approximation rather severe non convexity objective weakness standard coordinate descent optimization method used however advantage gives lower bound partition function unlike useful using subroutine inside learning algorithm also easier extend distributions besides discrete gaussian saw chapter intuitively works marginal distributions single type rather needing deﬁne pairwise distributions may need two different types 
[variational, inference, extensions, belief, propagation] figure graph spanning trees source figure wainwright jordan used kind permission martin wainwright tree reweighted belief propagation consider speciﬁc case spanning trees graph given tree entropy given equation compute upper bound obtained averaging trees note terms single nodes since node appears every tree mutual information term receives weight known edge appearance probability hence following upper bound entropy edge appearance probabilities live space called spanning tree polytope constrained arise distribution trees figure gives example graph three spanning trees suppose tree equal weight edge occurs trees edge occurs trees edge appears trees ideally ﬁnd distribution equivalently edge probabilities spanning tree polytope make bound tight possible algorithm described wainwright simpler approach generate spanning trees random edges covered use single edges weight set optimizing require tree means enforcing normalization local consistency since every tree enforcing normalization local consistency every edge hence ﬁnal optimization problem follows max lbp objective except crucial weights long edges problem strictly concave unique maximum ﬁnd global optimum lbp several algorithms perhaps simplest modiﬁcation belief propagation known tree reweighted belief propagation expectation propagation also called trw trbp short message function messages sent neighbors also function message sent speciﬁcally exp nbr convergence node edge pseudo marginals given exp nbr nbr nbr exp algorithm derived using method similar described section edges algorithm reduces standard lbp algorithm however condition implies every edge present every spanning tree probability possible original graph tree hence method equivalent standard lbp trees method course exact general message passing scheme guaranteed converge unique global optimum one devise double loop methods guaranteed converge hazan shashua practice using damped updates equation often sufficient ensure convergence also possible produce convex version kikuchi free energy one optimize modiﬁed version generalized belief propagation see wainwright jordan sec details equation using fact trbp entropy approximation upper bound true entropy wee see trbp objective upper bound log using fact rewrite upper bound follows log log 
[variational, inference, extensions, belief, propagation, generalized, belief, propagation] improve accuracy loopy clustering together nodes form tight loop known cluster variational method result hyper graph graph honkela discusses use pattern search algorithm speedup mean ﬁeld inference case continuous random variables possible similar ideas could adapted discrete case although may reason given lbp already works well discrete case chapter variational inference figure kikuchi clusters superimposed lattice graph corresponding hyper graph source figure wainwright jordan used kind permission martin wainwright hyper edges sets vertices instead single vertices note junction tree section kind hyper graph represent hyper graph using poset partially ordered set diagram node represents hyper edge arrow see figure example let size largest hyper edge hyper graph allow large treewidth graph represent hyper graph tree method exact lbp exact regular trees treewidth way deﬁne continuum approximations lbp way exact inference deﬁne set pseudo marginals normalization marginaliza tion constraints hold hyper graph whose largest hyper edge size example figure impose constraints form furthermore approximate entropy follows kikuchi entropy joint pseudo distribution vertices set called overcounting number set related mobious numbers set theory rather giving precise deﬁnition give simple example graph figure kikuchi putting two approximations together deﬁne kikuchi free energy follows kikuchi kikuchi ryoichi kikuchi japanese physicist extensions belief propagation variational problem becomes min kikuchi max kikuchi bethe free energy concave objective several possible algorithms ﬁnding local optimum objective including message passing algorithm known generalized belief propagation however details beyond scope chapter see wainwright jordan sec koller friedman sec information suffice say method gives accurate results lbp increased computational cost need handle clusters nodes cost plus complexity approach precluded widespread use 
[variational, inference, extensions, belief, propagation, convex, belief, propagation] mean ﬁeld energy functional concave maximized non convex inner approximation marginal polytope bethe kikuchi energy functionals concave maximized convex outer approximation marginal polytope consequently lbp optimization problem multiple optima methods sensitive initial conditions given exact formulation equation concave objective maximized convex set natural try come appproximation involves concave objective maximized convex set describe one method known convex belief propagation involves working set tractable submodels trees planar graphs model entropy higher since fewer constraints consequently convex combination subgraphs higher entropy furthermore concave function deﬁne convex free energy convex deﬁne concave energy functional negative convex free energy discuss optimize deﬁned upper bound entropy consider convex outerbound marginal polytope mean parameters want ensure evaluate entropy vector set restrict projection onto subgraph lives projection onto convex set since projection convex set hence deﬁne problem min convex max concave objective maximized convex set hence unique maxi mum give speciﬁc example chapter variational inference 
[variational, inference, expectation, propagation] expectation propagation minka form belief propagation mes sages approximated generalization assumed density ﬁltering adf algorithm discussed section method approximated posterior step using assumed functional form gaussian posterior computed using ment matching locally optimizes single term derived message send next time step chapter variational inference adf works well sequential bayesian updating answer gives depends order data seen essentially corrects ﬂaw making multiple passes data thus offline batch inference algorithm 
[variational, inference, expectation, propagation, variational, inference, problem] explain view terms variational inference follow presentation wainwright jordan sec consulted details suppose joint distribution written exponential family form follows exp exp partitioned parameters sufficient statistics tractable term size intractable terms size example consider problem inferring unknown vector observation model mixture two gaussians one centered one centered used represent outliers example minka invented calls clutter problem formally assume observation model form known mixing weight fraction outliers variance background distribution assuming ﬁxed prior form write model required form follows exp exp log matches canonical form exp corresponds exp using set log exact inference problem corresponds max set mean parameters realizable probability distribution seen eyes sufficient statistics stands intractable perform inference distribution example clutter example posterior contains modes suppose incorporate one intractable terms say one call augmented distribution exp exp expectation propagation clutter example becomes exp tractable compute since mixture gaussians key idea behind work augmented distributions iterative fashion first approximate convex set another larger convex set next approximate entropy following term term approximation problem becomes max 
[variational, inference, expectation, propagation, optimizing, objective, using, moment, matching] discuss maximize objective equation let duplicate times yield augmented set parameters need optimize subject constraints let associate vector lagrange multipliers ﬁrst set constraints partial lagrangian becomes solving show corresponding distribution form exp terms represents approximation intractable term using sufficient statistics base distribution see similarly solving ﬁnd corresponding distribution form exp chapter variational inference corresponds removing approximation term base distribution adding correct term finally enforces constraints equal words get following moment matching constraints thus overall algorithm follows first initialize iterate following convergence pick term compute corresponding removing old approximation adding new one update term solving moment matching equation note particular optimization scheme guaranteed converge ﬁxed point equivalent way stating algorithm follows let assume true distribution given approximate set repeat following convergence choose factor reﬁne remove posterior dividing implemented substracting natural parameters compute new posterior new solving min new new done equating moments new corresponding normalization constant form compute new factor message implicitly used later removed new expectation propagation convergence approximate marginal likelihood using give examples make things clearer 
[variational, inference, expectation, propagation, clutter, problem] let return considering clutter problem presentation based bishop simplicity assume prior spherical gaussian also choose approximate posterior spherical gaussian set prior held ﬁxed factor approximations gaussian like terms form note however updates variances may negative thus terms interpreted functions necessarily probability distributions variance negative means curves upwards instead downwards first remove division yields normalization constant given next compute new computing mean variance follows dimensionality interpreted probability clutter finally compute new factor whose parameters given handy crib sheet containing many standard equations needed deriving gaussian algorithms see http research microsoft com people minka papers minka quickref pdf chapter variational inference convergence approximate marginal likelihood follows exp minka shown least example gives better accuracy per unit cpu time mcmc 
[variational, inference, expectation, propagation, lbp, special, case] show loopy belief propagation special case base distribution contains node marginals intractable terms correspond edge potentials assume model pairwise form shown equation nodes base distribution takes form exp entropy distribution simply add edge augmented distribution form exp exp since graph tree exact entropy distribution given mutual information thus approxi mation entropy full distribution given precisely bethe approximation entropy expectation propagation show convex set optimizing given equation one lbp optimizing given equation first let consider set consists marginal distributions realizable factored distribution therefore equivalent set distributions satisfy non negativity local normalization constraint consider set single edge equivalent marginal polytope graph single edge added since graph corresponds tree set also satisﬁes marginalization conditions since union sets sweep edges graph recover set shown bethe approximation equivalent approximation show algorithm reduces lbp associated intractable term pair lagrange multipliers recalling base distribution equation form exp exp exp similarly augmented distribution equation form exp need update enforce moment matching constraints shown done performing usual sum product message passing step along edge directions messages given exp exp updated derive corresponding messages analysis suggests natural extension make base distribution tree structure instead fully factored distribution add one edge time absorb effect approximate resulting distribution new tree known tree minka accurate lbp sometimes faster considering kinds structured base distributions derive algorothms outperform generalization belief propagation welling 
[variational, inference, expectation, propagation, ranking, players, using, trueskill] present interesting application problem ranking players compete games microsoft uses method known trueskill herbrich rank chapter variational inference figure dgm representing trueskill model players teams team player team players team player assume two games team team team team nodes double circles deterministic factor graph representation model assume players teams games player player player player numbers inside circles represent steps message passing algorithm expectation propagation players use xbox live online gaming system system process games per day making one largest application bayesian statistics date method also applied games tennis chess basic idea shown figure assume player latent true underlying skill level skill levels evolve time according simple dynamical model given game deﬁne performance player conditional distribution deﬁne performance team sum performance constituent players example figure assume team composed players deﬁne finally assume outcome game depends difference performance levels two teams example figure assume sign means team means team thus prior probability team wins simplify presentation algorithm ignore dynamical model assume common static factored gaussian prior skills also assume team consists player ties finally integrate performance variables assume leading ﬁnal model form sign ﬁrst player game second player represented factor graph form figure kinds factors prior factor game factor outcome factor sign since likelihood term conjugate gaussian priors perform approximate inference thus even graph tree need iterate additional game say player player graph would longer tree represent messages marginal beliefs gaussians use notation mean variance moment parameters precision precision adjusted mean natural parameters naive bayes classiﬁers widely used spam ﬁlters often described common application bayesian methods however parameters models usually using non bayesian methods penalized maximum likelihood presentation algorithm based part lecture notes carl rasmussen joaquin quinonero candela available http mlg eng cam teaching lect pdf note similar probit regression discussed section except inputs differences latent dimensional factors assume logistic noise model instead gaussian noise model recover bradley terry model ranking chapter variational inference initialize assuming iteration initial upward messages factors variables uniform similarly messages passing algorithm consists steps per game illustrated figure give details steps compute posterior skills variables compute message skills variables game factor division implemented subtracting natural parameters follows similarly compute message game factor difference variable compute posterior difference variables sign expectation propagation function function figure function function based figure herbrich figure generated trueskillplot note upward message factor constant ﬁnd parameters moment matching follows derivation equations left modiﬁcation exercise functions plotted figure let try understand equations suppose large positive number means expect based current estimate skills large positive consequently observe surprised winner reﬂected fact update factor mean small similarly update factor variance small however observe update factor mean variance becomes quite large compute upward message difference variable game factor compute upward messages game factor skill variables let assume chapter variational inference figure dag representing partial ordering players posterior mean plus minus standard deviation latent skills player based games figure generated trueskilldemo winner loser similarly compute next iteration combining prior factor see posterior mean goes similarly posterior mean goes straightforward combine adf perform online inference necessary practical applications let consider simple example method create partial ordering players shown figure sample game outcomes graph map state estimation parent always beats child pass data iterations algorithm infer posterior mean variance player skill level results shown figure see method correctly inferred rank ordering players 
[variational, inference, expectation, propagation, applications] trueskill model developed researchers microsoft others extended model variety interesting applications including personalized recommenda tion stern predicting click rate ads bing search engine graepel etc also developed general purpose bayesian inference toolbox based called infer net minka also used variety models gaussian process classiﬁcation nickisch rasmussen see http research microsoft com people minka papers roadmap html list applications 
[variational, inference, map, state, estimation, linear, programming, relaxation] rewrite objective terms variational parameters follows arg max arg max probability vector marginal polytope see equation true note set degenerate distribution optimal assigment node instead optimizing discrete assignments optimize probability distributions seems like easy problem solve since objective equation linear constraint set convex trouble general number facets exponential number nodes standard strategy combinatorial optimization relax constraints case instead requiring probability vector live marginal polytope allow chapter variational inference live inside convex outer bound deﬁned relaxed constraint set max max max solution integral exact fractional approximation called ﬁrst order linear programming relaxtion reason called ﬁrst order constraints enforced correspond consistency tree graph treewidth possible enforce higher order consistency using graphs larger treewidth see wainwright jordan sec details actually perform optimization use generic linear programming package often slow fortunately case graphical models possible devise specialised distributed message passing algorithms solving optimization problem explain 
[variational, inference, map, state, estimation, max-product, belief, propagation] map objective equation max almost identical inference objective equation max apart entropy term one heuristic way proceed would consider zero temperature limit probability distribution probability distribution mass centered mode see section setting entropy term becomes zero modify message passing methods used solve inference problem solve map estimation problem instead particular zero temperature limit sum operator becomes max operator results method called max product belief propagation detail let max consider inverse temperature going inﬁnity lim lim max max lim max concavity objective function allows interchange lim max operators see wainwright jordan details consider bethe approximation form max bethe showed loopy ﬁnds local optimum objective zero temperature limit objective equivalent relaxation map problem unfortunately max product loopy solve relaxation unless graph tree wainwright jordan reason bethe energy functional concave except trees licensed swap limit max operators zero temperature derivation however use tree reweighted trbp trw concave objective case map state estimation one show kolmogorov wainwright max product version trbp solve relaxation certain scheduling algorithm known sequential trbp trbp trw shown always converge kolmogorov furthermore typically faster standard parallel updates idea pick arbitrary node ordering consider set trees subsequence ordering iteration perform max product towards back along one trees shown monotonically minimizes lower bound energy thus guaranteed converge global optimum relaxation 
[variational, inference, map, state, estimation, graphcuts] section show ﬁnd map state estimates equivalently minimum energy conﬁgurations using max ﬂow min cut algorithm graphs class methods known graphcuts widely used especially computer vision applications start considering case mrfs binary nodes restricted class potentials case graphcuts ﬁnd exact global optimum consider case multiple states per node assumed underlying ordering approximately solve case solving series binary subproblems see graphcuts generalized ising model let start considering binary mrf edge energies following form edge cost encourages neighboring nodes value since trying minimize energy since free add constant like overall energy without affecting map state estimate let rescale local energy terms either let construct graph set nodes mrf plus two distin guished nodes source sink add edge cost ensures partition meaning assigned state pay cost cut similarly add edge cost finally every pair variables connected mrf add edges cost figure illustrates construction mrf nodes following non zero energy values constructed graph compute minimal cut partition nodes two sets nodes connected nodes connected variety ways implement algorithm see sedgewick wayne best take log time number edges number nodes chapter variational inference figure illustration graphcuts applied mrf nodes dashed lines ones contribute cost cut bidirected edges count one costs min cut cost source figure koller friedman used kind permission daphne koller pick partition minimizes sum cost edges nodes different sides partition cost cost figure see min cut cost minimizing cost graph equivalent minimizing energy mrf hence nodes assigned optimal state nodes assigned optimal state figure see optimal map estimate graphcuts binary mrfs submodular potentials discuss extend graphcuts construction binary mrfs general kinds potential functions particular suppose pairwise energy satisﬁes following condition words sum diagonal energies less sum diagonal energies case say energies submodular kolmogorov zabin example submodular energy ising model also known attractive mrf associative mrf since model wants neighboring states submodularity discrete analog convexity intuitively corresponds law diminishing returns extra value adding one element set reduced set already large formally say submodular submodular supermodular map state estimation apply graphcuts binary mrf submodular potentials construct pairwise edge weights follows guaranteed non negative virtue submodularity assumption addition construct new local edge weights follows ﬁrst initialize edge pair update values follows construct graph similar way speciﬁcally add edge cost otherwise add edge cost finally every mrf edge add graphcuts edge cost need add edge directions one show exercise min cut graph minimum energy conﬁguration thus use max ﬂow min cut ﬁnd globally optimal map estimate greig graphcuts nonbinary metric mrfs discuss use graphcuts approximate map estimation mrfs node multiple states boykov however require pairwise energies form metric call model metric mrf example suppose states natural ordering commonly arises discretization underlying continuous space case deﬁne metric form min semi metric form min constant energy encourages neighbors similar labels never punishes term prevents smoothing illustrate figure one version graphcuts alpha expansion step picks one available labels states calls solves binary subproblem variable choose remain current state become state see figure illustration precisely deﬁne new mrf binary nodes deﬁne energies new model relative current assignment follows optimize using graph cuts thus ﬁgure optimal alpha expansion move require energies submodular plugging deﬁnition get following constraint distance function remaining inequality follows triangle inequality thus apply alpha expansion move metric mrf chapter variational inference initial labeling standard move swap expansion figure image labels standard local move iterative conditional modes ﬂips label one pixel swap allows nodes currently labeled relabeled decreases energy expansion allows nodes currently labeled relabeled decreases energy source figure boykov used kind permission ramin zabih step alpha expansion ﬁnd optimal move amongst exponentially large set thus reach strong local optimum much lower energy local optima found standard greedy label ﬂipping methods iterative conditional modes fact one show algorithm converged energy resulting solution times optimal energy max max min see exercise proof case potts model approximation another version graphcuts alpha beta swap step two labels chosen call nodes currently labeled change vice versa reduces energy see figure illustration resulting binary subproblem solved exactly even energies semi metric triangle inequality need hold see exercise although swap version applied broader class models expansion version theoretically powerful indeed various low level vision problems szeliski show empirically expansion version usually better swap version see section 
[variational, inference, map, state, estimation, experimental, comparison, graphcuts] section described lattice structured crfs various low level vision problems szeliski performed extensive comparison different approximate optimization techniques class problems results problem stereo depth estimation shown figure see graphcut tree reweighted max product trw give best results regular max product much worse terms speed graphcuts fastest trw close second algorithms icm simulated annealing standard domain speciﬁc heuristic known normalize correlation map state estimation max product expansion swap trw max product expansion swap trw ener running time ener running time figure energy minimization crf stereo depth estimation top row two input images along ground truth depth values bottom row energy time different optimization algorithms bottom left results teddy image shown top row bottom right results tsukuba image shown figure source figure koller friedman used kind permission daphne koller even worse shown qualitatively figure since trw optimizing dual relaxed problem use value conver gence evaluate optimal energy turns many images stereo benchmark dataset ground truth higher energy lower probability globally timal estimate meltzer indicates optimizing wrong model surprising since pairwise crf ignores known long range constraints unfortunately add constraints model graph either becomes dense making slow potentials become non submodular making graphcuts inapplicable one way around generate diverse set local modes using repeated applications graph cuts described yadollahpour apply sophisticated model uses global features rerank solutions chapter variational inference left image labels ground truth swap algorithm expansion algorithm normalized correlation simulated annealing figure example stereo depth estimation using mrf left image size pixels university tsukuba corresponding right image similar shown ground truth depth map quantized levels map estimates using different methods swap expansion normalized cross correlation simulated annealing source figure boykov used kind permission ramin zabih 
[variational, inference, map, state, estimation, dual, decomposition] interested computing max represents set factors assume tractably optimize local factor combination factors makes problem intractable one way proceed optimize term independently introduce constraints force local estimates variables values agree explain detail following presentation sontag map state estimation 
[variational, inference] figure pairwise mrf different edge factors separate variables plus copy variable factor participates source figure sontag used kind permission david sontag basic idea let duplicate variables factor force equal speciﬁcally let set variables used factor construction illustrated figure reformulate objective follows max let introduce lagrange multipliers dual variables enforce constraints lagrangian becomes equivalent original problem following sense value max since constraints hold last term zero get upper bound dropping consistency constraints optimizing following upper bound max max max see figure illustration chapter variational inference figure illustration dual decomposition source figure sontag used kind permission david sontag objective tractable optimize since term decoupled furthermore see since relaxing consistency constraints optimizing larger space furthermore property min upper bound tight optimal value enforces original constraints minimizing upper bound known dual decomposition lagrangian relaxation komodakis sontag rush collins furthemore shown dual relaxation saw discuss several possible optimization algorithms main advantage dual decomposition practical point view allows one mix match different kinds optimization algorithms convenient way example combine grid structured graph local submodular factors perform image segmentation together tree structured model perform pose estimation see exercise analogous methods used natural language processing often mix local global constraints see koo rush collins theoretical guarantees say quality solutions obtained way understand let ﬁrst introduce notation map state estimation represents reparameterization original problem sense hence max max suppose set dual variables assignment maxi mizing assignments singleton terms agrees assignments factor terms argmax argmax case since conclude map assignment ﬁnd solution subproblems agree assured global optimum happens surprisingly often practical problems subgradient descent convex continuous objective non differentiable points multiple optima one approach use subgradient descent updates elements time follows subgradient step sizes set appropriately see sec tion method guaranteed converge global optimum dual see komodakis details one show gradient given following sparse vector first let argmax argmax next let elements finally factor disagrees local term set variable set effect decreasing increasing bringing closer agreement similarly subgradient update decrease value increasing value compute gradient need able solve subproblems following form argmax argmax chapter variational inference komodakis subproblems called slaves whereas called master obviously scope factor small simple example factor pairwise variable states cost however kinds global factors also support exact efficient maximization including following graphical models low tree width factors correspond bipartite graph matchings see duchi useful data association problems must match sensor reading unknown source ﬁnd maximal matching using called hungarian algorithm time see padadimitriou steiglitz supermodular functions discuss case detail section cardinality constraints example might factor large set binary variables enforces certain number bits turned useful problems image segmentation particular suppose otherwise ﬁnd maximizing assignment log time follows ﬁrst deﬁne sort ﬁnally set ﬁrst values rest tarlow factors constant small set distinguished values optimize factor time rother coordinate descent alternative updating entire vector albeit sparsely update using block coordinate descent choosing size blocks trade convergence speed ease local optimization problem one approach optimizes time ﬁxed factor known max product linear programming globerson jaakkola algorithmically similar belief propagation factor graph particular deﬁne messages sent factor variable deﬁne messages sent variable factor messages computed follows see globerson jaakkola derivation max set dual variables messages example consider grid mrf following pairwise factors outgoing message factor variable note denote map state estimation function messages coming local factor max similarly outgoing message variable factor function messages sent variable connected factors example factor local potential key computational bottleneck computing max marginals factor max variables except need able compute following max marginals efficiently max difference equation maxing one variables solve efficiently low treewidth graphical models using message passing also solve efficiently factors corresponding bipartite matchings duchi cardinality constraints tarlow however cases maximizing variables factor scope computationally easier maximizing one see sontag sec example cases may prefer use subgradient method coordinate descent simple algorithm often much faster minimizing dual gradient descent especially early iterations also reduces objective monotonically need step size parameters unfortunately guaranteed converge global optimum since convex strictly convex implies may one globally optimizing value one way ensure convergence replace max function deﬁnition soft max function makes objective strictly convex see hazan shashua details recovering map assignment far focussing ﬁnding optimal value really want optimal value general computing hard even relaxation tight map assignment unique sontag theorem troublesome cases arise fractional assignments optimal value map estimate however suppose unique maximum case say locally decodable one show case relaxation unique solution indeed many nodes uniquely decodable clamp uniquely decodable ones map value use exact inference algorithms ﬁgure optimal assignment remaining variables using method meltzer able optimally solve various stereo vision crf estimation problems yanover able optimally solve various protein side chain structure predicition problems another approach use upper bound provided dual branch bound search procedure geoffrion chapter variational inference 
[variational, inference, exercises] exercise graphcuts map estimation binary submodular mrfs source koller friedman show using graph construction described section cost cut equal energy corresponding assignment irrelevant constant warning exercise involves lot algebraic book keeping exercise graphcuts alpha beta swap source koller friedman show optimal alpha beta swap found running min cut appropriately constructed graph precisely deﬁne set binary variables means unchanged deﬁne energy function new variables const show submodular semimetric exercise constant factor optimality alpha expansion source daphne koller let pairwise metric markov random ﬁeld graph suppose variables nonbinary node potentials nonnegative let denote set labels though possible tractably ﬁnd globally optimal assignment general expansion algorithm provides method ﬁnding assignments locally optimal respect large set transformations possible expansion moves despite fact expansion produces locally optimal map assignment possible prove energy assignment within known factor energy globally optimal solution fact special case general principle applies wide variety algorithms including max product belief propagation general move making algorithms one prove solutions obtained algorithm strong local minima local minima respect large set potential moves possible derive bounds global suboptimality solutions quality bounds depend nature moves considered precise deﬁnition large set moves consider following approach proving suboptimality bound expansion let local minimum respect expansion moves let set nodes labelled global minimum let assignment equal equal elsewhere expansion verify building previous part show max max min denotes energy assignment hint think agrees agrees exercise dual decomposition pose segmentation source daphne koller two important problems computer vision parsing articulated objects human body called pose estimation segmenting foreground background called segmentation intuitively two problems linked solving either one would easier solution available consider solving problems simultaneously using joint model human poses foreground background labels using dual decomposition map inference model construct two level model high level handles pose estimation low level handles pixel level background segmentation let undirected grid pixels node represents pixel suppose one binary variable pixel means pixel foreground denote full set variables map state estimation addition suppose undirected tree structure parts body part discrete set candidate poses part pose characterized parameters specifying position orientation candidates generated procedure external algorithm described deﬁne binary variable indicating whether body part conﬁguration full set part variables given total number body parts number candidate poses part note order describe valid conﬁguration must satisfy constraint suppose following energy function pixels assume arises metric based differences pixel intensities viewed energy pairwise metric mrf respect following energy function parts since part candidate assumed come position orientation compute binary mask image plane mask assigns value pixel denoted pixel lies skeleton decreases move away use deﬁne energy function relating parts pixels words energy term penalizes case part candidate active pixel underneath labeled background formulate minimization integer program show use dual decomposition solve dual integer program solution describe decomposition slaves method solving one update rules overall algorithm brieﬂy justify design choices particularly choice inference algorithms slaves 
[monte, carlo, inference, introduction] far discussed various deterministic algorithms posterior inference meth ods enjoy many beneﬁts bayesian approach still fast optimization based point estimation methods trouble methods rather complicated derive somewhat limited domain applicabil ity usually assume conjugate priors exponential family likelihoods although see wand recent extensions mean ﬁeld complex distributions fur thermore although fast accuracy often limited form approximation choose chapter discuss alternative class algorithms based idea monte carlo approximation ﬁrst introduced section idea simple generate unweighted samples posterior use compute quantity interest posterior marginal posterior difference two quantities posterior predictive etc quantities approximated suitable function generating enough samples achieve desired level accuracy like main issue efficiently generate samples probability distribution particularly high dimensions chapter discuss non iterative methods generating independent samples next chapter discuss iterative method known markov chain monte carlo mcmc short produces dependent samples works well high dimensions note sampling large topic reader consult books liu robert casella information 
[monte, carlo, inference, sampling, standard, distributions] brieﬂy discuss ways sample dimensional distributions standard form methods often used subroutines complex methods 
[monte, carlo, inference, sampling, standard, distributions, using, cdf] simplest method sampling univariate distribution based inverse prob ability transform let cdf distribution want sample let chapter monte carlo inference figure sampling using inverse cdf figure generated samplecdf inverse following result theorem uniform proof applying sides ﬁrst line follows since monotonic function second line follows since uniform unit interval hence sample univariate distribution evaluate inverse cdf follows generate random number using pseudo random number generator see press details let represent height axis slide along axis intersect curve drop return corresponding value corresponds computing see figure illustration example consider exponential distribution expon cdf whose inverse quantile function theorem unif know expon furthermore since unif well sample exponential distribution ﬁrst sampling uniform transforming results using rejection sampling 
[monte, carlo, inference, sampling, standard, distributions, sampling, gaussian, box-muller, method] describe method sample gaussian idea sample uniformly unit radius circle use change variables formula derive samples spherical gaussian thought two samples gaussian detail sample uniformly discard pairs satisfy result points uniformly distributed inside unit circle inside circle deﬁne using multivariate change variables formula exp exp hence two independent samples univariate gaussian known box muller method sample multivariate gaussian ﬁrst compute cholesky decomposition covariance matrix lower triangular next sample using box muller method finally set valid since cov lcov 
[monte, carlo, inference, rejection, sampling] inverse cdf method cannot used one simple alternative use rejection sam pling explain 
[monte, carlo, inference, rejection, sampling, basic, idea] rejection sampling create proposal distribution satisifes constant unnormalized version possibly unknown constant function provides upper envelope sample corresponds picking random location sample corresponds picking random height location envelope reject sample otherwise accept see figure acceptance region shown shaded rejection region white region shaded zone upper envelope prove procedure correct let chapter monte carlo inference umq accept region reject region target comparison function figure schematic illustration rejection sampling source figure andrieu used kind permission nando freitas rejection sampling distribution solid blue using proposal form dotted red curves touch figure generated rejectionsamplingdemo cdf accepted points given accepted accepted accepted dudx dudx cdf desired efficient method since generate probability accept probability probability acceptance accept hence want choose small possible still satisfying 
[monte, carlo, inference, rejection, sampling, example] example suppose want sample gamma distribution exp one show iid expon non integer shape parameters cannot use trick however use rejection sampling section based notes ioana cosma available http users aims ioana pdf rejection sampling half gaussian samples ars figure idea behind adaptive rejection sampling place piecewise linear upper lower bounds log concave density based figure gilks wild figure generated arsenvelope using ars sample half gaussian figure generated arsdemo written daniel eaton using distribution proposal xcα ratio form exp exp exp ratio attains maximum hence see figure plot exercise asks devise better proposal distribution based cauchy distribution 
[monte, carlo, inference, rejection, sampling, application, bayesian, statistics] suppose want draw unweighted samples posterior use rejection sampling target distribution proposal arg max mle ﬁrst suggested smith gelfand accept points probability thus samples prior high likelihood likely retained posterior course big mismatch prior posterior case prior vague likelihood informative procedure inefficient discuss better algorithms later 
[monte, carlo, inference, rejection, sampling, adaptive, rejection, sampling] describe method automatically come tight upper envelope log concave density idea upper bound log density piecewise chapter monte carlo inference linear function illustrated figure choose initial locations pieces based ﬁxed grid support distribution evaluate gradient log density locations make lines tangent points since log envelope piecewise linear envelope piecewise exponential exp grid points relatively straightforward sample distribution sample rejected create new grid point thereby reﬁne envelope number grid points increased tightness envelope improves rejection rate goes known adaptive rejection sampling ars gilks wild figure gives example method action standard rejection sampling applied unnormalized distributions 
[monte, carlo, inference, rejection, sampling, rejection, sampling, high, dimensions] clear want make proposal close possible target distribution still upper bound quite hard achieve especially high dimensions see consider sampling using proposal obviously must order upper bound dimensions optimum value given acceptance rate since normalized decreases exponentially fast dimension example exceeds dimensions acceptance ratio fundamental weakness rejection sampling chapter describe mcmc sampling efficient way sample high dimensional distributions sometimes uses adaptive rejection sampling subroutine known adaptive rejection metropolis sampling gilks 
[monte, carlo, inference, importance, sampling] describe monte carlo method known importance sampling approximating integrals form 
[monte, carlo, inference, importance, sampling, basic, idea] idea draw samples regions high probability also large result super efficient meaning needs less samples sample exact distribution reason samples focussed important parts space example suppose want estimate probability rare event deﬁne set better sample proposal form sample importance sampling samples proposal uses samples estimate importance sampling integral follows importance weights note unlike rejection sampling use samples choose proposal natural criterion minimize variance estimate var since last term independent ignore jensen inequality following lower bound lower bound obtained use optimal importance distribution particular target function mind often try make close possible general difficult especially high dimensions possible adapt proposal distribution improve approximation known adaptive importance sampling berger 
[monte, carlo, inference, importance, sampling, handling, unnormalized, distributions] frequently case evaluate unnormalized target distribution normalization constant may also want use unnormalized proposal possibly unknown normlization constant follows first evaluate unnormalized importance weight use set samples evaluate ratio follows hence chapter monte carlo inference normalized importance weights resulting estimate ratio two estimates hence biased however weak assumptions see robert casella details 
[monte, carlo, inference, importance, sampling, importance, sampling, dgm, likelihood, weighting] describe way use importance sampling generate samples distribution represented directed graphical model chapter evidence sample unconditional joint distribution dgm follows ﬁrst sample root nodes sample children sample children etc known ancestral sampling works dag always topologically order nodes parents preceed children note equivalent easy method sampling unconditional undirected graphical model suppose evidence nodes clamped observed values want sample posterior variables discrete use following simple procedure perform ancestral sampling soon sample value inconsistent observed value reject whole sample start known logic sampling henrion needless say logic sampling inefficient cannot applied real valued evidence however modiﬁed follows sample unobserved variables conditional parents sample observed variables instead use observed values equivalent using proposal form set observed nodes observed value node therefore give overall sample importance weight follows technique known likelihood weighting fung chang shachter peot 
[monte, carlo, inference, importance, sampling, sampling, importance, resampling, sir] draw unweighted samples ﬁrst using importance sampling proposal generate distribution form particle ﬁltering normalized importance weights sample replacement equation probability pick let procedure induce distribution denoted see valid note known sampling importance resampling sir rubin result weighted approximation form note typically take algorithm used perform bayesian inference low dimensional settings smith gelfand suppose want draw unweighted samples posterior use importance sampling unnormalized posterior proposal normalized weights form use sir sample course big discrepancy proposal prior target posterior need huge number importance samples technique work reliably since otherwise variance importance weights large implying samples carry useful information issue come section discuss particle ﬁltering 
[monte, carlo, inference, particle, ﬁltering] particle ﬁltering monte carlo simulation based algorithm recursive bayesian inference approximates predict update cycle described section widely used many areas including tracking time series forecasting online parameter learning etc explain basic algorithm book length treatment see doucet good tutorial see arulampalam read chapter monte carlo inference 
[monte, carlo, inference, particle, ﬁltering, sequential, importance, sampling] basic idea appproximate belief state entire state trajectory using weighted set particles normalized weight sample time representation easily compute marginal distribution recent state simply ignoring previous parts trajectory fact samples space entire trajectories various implications discuss later update belief state using importance sampling proposal form importance weights given normalized follows rewrite numerator recursively follows made usual markov assumptions restrict attention proposal densities following form grow trajectory adding new state end case importance weights simplify assume need keep recent part trajectory observation sequence rather whole history order compute new sample case weight becomes particle ﬁltering hence approximate posterior ﬁltered density using one show approaches true posterior crisan basic algorithm simple old sample propose extension using give new particle weight using equation unfortunately basic algorithm work well discuss 
[monte, carlo, inference, particle, ﬁltering, degeneracy, problem] basic sequential importance sampling algorithm fails steps particles negligible weight called degeneracy problem occurs sampling high dimensional space fact space growing size time using myopic proposal distribution quantify degree degeneracy using effective sample size deﬁned eff var true weight particle quantity cannot computed exactly since know true posterior approximate using eff variance weights large wasting resources updating particles low weight contribute much posterior estimate two main solutions degeneracy problem adding resampling step using good proposal distribution discuss turn 
[monte, carlo, inference, particle, ﬁltering, resampling, step] main improvement basic sis algorithm monitor effective sampling size whenever drops threshold eliminate particles low weight create replicates surviving particles hence sometimes called survival ﬁttest kanazawa particular generate new set sampling replacement times weighted distribution probability choosing particle replication sometimes called rejuvenation result iid unweighted sample discrete density equation set new weights scheme illustrated figure chapter monte carlo inference sursrvdo zhljkwlqj uhvdpsoh xbw xdw xbw xdw xbw xbw xbw xdw figure illustration particle ﬁltering variety algorithms peforming resampling step simplest multi nomial resampling computes make copies various improvements exist systematic resampling residual resampling stratiﬁed sampling reduce variance weights methods take time see doucet details overall particle ﬁltering algorithm summarized algorithm note estimate state required computed resampling step since result lower variance algorithm one step generic particle ﬁlter draw compute weight normalize weights compute eff seff smin resample indices although resampling step helps degeneracy problem introduces problems particular since particles high weight selected many times loss diversity amongst population known sample impoverishment particle ﬁltering extreme case process noise static unknown parameters part state space particles collapse single point within iterations mitigate problem several solutions proposed resample necessary every time step original bootstrap ﬁlter gordon resampled every step suboptimal replicating old particles sample new values using mcmc step leaves posterior distribution invariant see resample move algorithm gilks berzuini create kernel density estimate top particles smoothing kernel sample smoothed distribution known regularized particle ﬁlter musso performing inference static parameters add artiﬁcial process noise undesirable algorithms must used online parameter estimation andrieu 
[monte, carlo, inference, particle, ﬁltering, proposal, distribution] simplest widely used proposal distribution sample prior case weight update simpliﬁes thought generate test approach sample values dynamic model evaluate good see data see figure approach used condensation algorithm stands conditional density propagation used visual tracking isard blake however likelihood narrower dynamical prior meaning sensor informative motion model often case inefficient approach since particles assigned low weight much better actually look data generating proposal fact optimal proposal distribution following form use proposal new weight given proposal optimal since given new weight takes value regardless value drawn hence conditional old values variance true weights var zero chapter monte carlo inference general intractable sample evaluate integral needed compute predictive density however two cases optimal proposal distribution used ﬁrst setting discrete integral becomes sum course entire state space discrete use hmm ﬁlter instead cases parts state discrete continuous second setting gaussian occurs dynamics nonlinear observations linear see exercise details cases model linear gaussian may still compute gaussian approxima tion using unscented transform section use proposal known unscented particle ﬁlter van der merwe general settings use kinds data driven proposals perhaps based discriminative models unlike mcmc need worry proposals reversible 
[monte, carlo, inference, particle, ﬁltering, application, robot, localization] consider mobile robot wandering around office environment assume already map world represented form occupancy grid speciﬁes whether grid cell empty space occupied something solid like wall goal robot estimate location solved optimally using hmm ﬁlter since assuming state space discrete however since number states often large time complexity per update prohibitive use particle ﬁlter sparse approximation belief state known monte carlo localization described detail thrun figure gives example method action robot uses sonar range ﬁnder sense distance obstacles starts uniform prior reﬂecting fact owner robot may turned arbitrary location figuring starting uniform prior called global localization ﬁrst scan indicates two walls either side belief state shown posterior still fairly broad since robot could location walls fairly close corridor narrow rooms moving location robot pretty sure must corridor shown moving location sensor able detect end corridor however due symmetry sure location true location location example perceptual aliasing refers fact different things may look moving locations ﬁnally able ﬁgure precisely whole process analogous someone getting lost office building wandering corridors see sign recognize section discuss estimate location map time 
[monte, carlo, inference, particle, ﬁltering, application, visual, object, tracking] next example concerned tracking object case remote controlled heli copter video sequence method uses simple linear motion model centroid object color histogram likelihood model using bhattacharya distance compare histograms proposal distribution obtained sampling likelihood see nummiaro details particle ﬁltering room room start room path reference poses belief reference pose belief reference pose belief reference pose belief reference pose belief reference pose figure illustration monte carlo localization source figure thrun used kind permission sebastian thrun figure shows example frames system uses particles effective sample size eff shows belief state frame system resample times keep effective sample size threshold shows belief state frame red lines show estimated location center object last frames shows system handle visual clutter long color target object shows system confused grey helicopter grey building posterior bimodal green ellipse representing posterior mean covariance two modes shows probability mass shifted wrong mode system lost track shows particles spread gray building recovery object unlikely state using chapter monte carlo inference figure example particle ﬁltering applied visual object tracking based color histograms succesful tracking green ellipse top helicopter tracker gets distracted gray clutter background see text details figure generated pfcolortrackerdemo written sebastien paris proposal see method able keep track fairly long time despite presence clutter however eventually loses track object note since algorithm stochastic simply running demo may problem real world option simplest way improve performance use particles alternative perform tracking detection running object detector image every frames see forsyth ponce szeliski prince details rao blackwellised particle ﬁltering rbpf 
[monte, carlo, inference, particle, ﬁltering, application, time, series, forecasting] section discussed use kalman ﬁlter perform time series forecasting assumes model linear gaussian state space model many models either non linear non gaussian example stochastic volatility models widely used ﬁnance assume variance system observation noise changes time particle ﬁltering widely used settings see doucet references therein details 
[monte, carlo, inference, rao-blackwellised, particle, ﬁltering, rbpf] models partition hidden variables two kinds analytically integrate provided know values means sample represent parametrically thus particle represents value distribution form hybrid particles sometimes called distributional particles collapsed particles koller friedman sec advantage approach reduce dimensionality space sampling reduces variance estimate hence technique known rao blackwellised particle ﬁltering rbpf short named theorem method best explained using speciﬁc example 
[monte, carlo, inference, rao-blackwellised, particle, ﬁltering, rbpf, rbpf, switching, lg-ssms] canonical example rbpf applied switching linear dynamical system slds model discussed section chen liu doucet represent using mean covariance matrix particle propose prior weight update becomes quantity predictive density new observation conditioned history case slds models computed using normalization constant kalman ﬁlter equation give pseudo code algorithm step marked kfupdate refers kalman ﬁlter update equations section known mixture kalman ﬁlters small compute optimal proposal distribution chapter monte carlo inference algorithm one step rbpf slds using prior proposal kfupdate normalize weights compute eff seff smin resample indices use following shorthand sample give resulting particle weight since weights particles equation independent new value actually sampled compute weights ﬁrst use decide particles propagate choose ﬁttest particles time using information time called look ahead rbpf freitas detail idea pass sample prior models get posteriors one per sample normalization constants process allow compute optimal weights equation resample indices finally old particle chosen sample one new state use corresponding posterior possible alternative already computed pseudo code shown algorithm method needs storage advantage particle chosen using latest information improvement obtained exploiting fact state space discrete hence use resampling method fearnhead avoids duplicating particles 
[monte, carlo, inference, rao-blackwellised, particle, ﬁltering, rbpf, application, tracking, maneuvering, target] one application slds track moving objects piecewise linear dynamics example suppose want track airplane missile specify object ﬂying normally taking evasive action called maneuvering target tracking figure gives example object moving setup essentially section except add three state discrete markov chain controls rao blackwellised particle ﬁltering rbpf algorithm one step look ahead rbpf slds using optimal proposal kfupdate normalize weights resample indices compute optimal proposal sample method misclassiﬁcation rate mse time seconds rbpf table comparison rbpf maneuvering target problem figure input system deﬁne set system turn different directions depending discrete state figure shows true state system sample run starting colored symbols denote discrete state location symbol denotes location small dots represent noisy observations figure shows estimate state computed using particle ﬁltering particles proposal sample prior colored symbols denote map estimate state location symbol denotes mmse minimum mean square error estimate location given posterior mean figure shows estimate computing using rbpf particles using optimal proposal distribution quantitative comparison shown table see rbpf slightly better performance although also slightly slower figure visualizes belief state system show distribution discrete states see particle ﬁlter estimate belief state second column accurate rbpf estimate third column beginning although ﬁrst observations performance similar methods plot posterior locations simplicity use estimate set weighted samples could also used rbpf estimate set weighted gaussians chapter monte carlo inference data mse rbpf mse figure maneuvering target colored symbols represent hidden discrete state particle ﬁlter estimate rbpf estimate figure generated rbpfmaneuverdemo based code nando freitas 
[monte, carlo, inference, rao-blackwellised, particle, ﬁltering, rbpf, application, fast, slam] section introduced problem simultaneous localization mapping slam mobile robotics main problem kalman ﬁlter implementation cubic number landmarks however looking dgm figure see conditional knowing robot path landmark locations independent assume landmarks move drop subscript consequently use rbpf sample robot trajectory run independent kalman ﬁlters inside particle takes time per particle fortunately number particles needed good performance quite small partly depends control exploration policy algorithm essentially linear number particles technique additional advantage rao blackwellised particle ﬁltering rbpf truth error rate rbpf error rate figure belief states corresponding figure discrete state system starts state red figure moves state black figure returns brieﬂy state switches state blue circle figure etc horizontal location estimate figure generated rbpfmaneuverdemo based code nando freitas easy use sampling handle data association ambiguity allows representations map occupancy grids idea ﬁrst suggested murphy subsequently extended made practical thrun christened technique fastslam see rbpfslamdemo simple demo discrete grid world 
[monte, carlo, inference, exercises] exercise sampling cauchy show use inverse probability transform sample standard cauchy exercise rejection sampling gamma using cauchy proposal show use cauchy proposal perform rejection sampling gamma distribution derive optimal constant plot density upper envelope exercise optimal proposal particle ﬁltering linear gaussian measurement model consider state space model following form derive expressions needed compute optimal minimum variance proposal distribution hint use bayes rule gaussians 
[markov, chain, monte, carlo, mcmc, inference, introduction] chapter introduced simple monte carlo methods including rejection sampling importance sampling trouble methods work well high dimensional spaces popular method sampling high dimensional distributions markov chain monte carlo mcmc survey siam news mcmc placed top important algorithms century basic idea behind mcmc construct markov chain section state space whose stationary distribution target density interest may prior posterior perform random walk state space way fraction time spend state proportional drawing correlated samples chain perform monte carlo integration wrt give details mcmc algorithm interesting history discovered physicists working atomic bomb los alamos world war ﬁrst published open literature metropolis chemistry journal extension published statistics literature hastings largely unnoticed special case gibbs sampling section independently invented context ising models published geman geman gelfand smith algorithm became well known wider statistical community since become wildly popular bayesian statistics becoming increasingly popular machine learning worth brieﬂy comparing mcmc variational inference chapter advantages variational inference small medium problems usually faster deterministic easy determine stop often provides lower bound log likelihood advantages sampling often easier implement applicable broader range models models whose size structure changes depending values certain variables happens matching problems models without nice conjugate priors sampling faster variational methods applied really huge models datasets source http www siam org pdf news pdf reason sampling passes speciﬁc values variables sets variables whereas variational inference pass around distributions thus sampling passes sparse messages whereas variational inference passes dense messages comparisons two approaches see yoshida west articles bekkerman chapter markov chain monte carlo mcmc inference 
[markov, chain, monte, carlo, mcmc, inference, gibbs, sampling] figure mixture model integrating parameters encounter rao blackwellisation section although reduce statistical variance worth integrating done quickly otherwise able produce many samples per second naive method give example example collapsed gibbs ﬁtting gmm consider gmm fully conjugate prior case analytically integrate model parameters sample indicators integrate nodes become inter dependent similarly integrate nodes become inter dependent shown figure nevertheless easily compute full conditionals follows hyper parameters class conditional densities ﬁrst term obtained integrating suppose use symmetric prior form dir equation gibbs sampling hence exploited fact obtain second term equation posterior predictive distribution given data assignments use fact data assigned cluster except use conjugate prior compute closed form furthermore efficiently update predictive likelihoods caching sufficient statistics cluster compute expression remove statistics current cluster namely evaluate cluster posterior predictive picked new cluster add statistics new cluster pseudo code one step algorithm shown algorithm based sud derth update nodes random order improve mixing time suggested roberts sahu initialize sample sequentially sampling see fmgibbs matlab code yee whye teh case gmms naive sampler collapsed sampler take time per step algorithm collapsed gibbs sampler mixture model random order remove sufficient statistics old cluster compute compute sample add sufficient statistics new cluster comparison method standard gibbs sampler shown figure vertical axis data log probability iteration computed using log log compute quantity using collapsed sampler sample given data current assignment figure see collapsed sampler indeed generally work better vanilla sampler occasionally however methods get stuck poor local modes note chapter markov chain monte carlo mcmc inference iteration log standard gibbs sampler rao blackwellized sampler iteration log standard gibbs sampler rao blackwellized sampler figure comparison collapsed red vanilla blue gibbs sampling mixture two dimensional gaussians applied data points shown figure plot log probability data iteration different random initializations logprob averaged different random initializations solid line median thick dashed quantiles thin dashed quintiles source figure sudderth used kind permission erik sudderth ses math score sample size slope ses math score figure least squares regression lines math scores socio economic status schools population mean pooled estimate bold plot slope sample size schools extreme slopes tend correspond schools smaller sample sizes predictions hierarchical model population mean bold based figure hoff figure generated multilevellinregdemo written emtiyaz khan error bars figure averaged starting values whereas theorem refers samples single run 
[markov, chain, monte, carlo, mcmc, inference, gibbs, sampling, basic, idea] idea behind gibbs sampling sample variable turn conditioned values variables distribution given joint sample variables generate new sample sampling component turn based recent values variables example variables use readily generalizes variables visible variable sample since value already known expression called full conditional variable general may depend variables represent graphical model infer dependencies looking markov blanket neighbors graph thus sample need know values neighbors sense gibbs sampling distributed algorithm however parallel algorithm since samples must generated sequentially reasons explain section necessary discard initial samples markov chain burned entered stationary distribution discuss estimate burnin occured section examples discard initial samples simplicity 
[markov, chain, monte, carlo, mcmc, inference, gibbs, sampling, example, gibbs, sampling, ising, model] section applied mean ﬁeld ising model apply gibbs sampling gibbs sampling pairwise mrf crf takes form nbr case ising model edge potentials exp josiah willard gibbs american physicist gibbs sampling sample gibbs sample gibbs mean sweeps gibbs figure example image denoising use ising prior gaussian noise model use gibbs sampling section perform approximate inference sample posterior one sweep image sample sweeps posterior mean computed averaging sweeps compare figure shows results using mean ﬁeld inference figure generated isingimagedenoisedemo full conditional becomes nbr nbr nbr exp nbr exp nbr exp nbr exp exp exp sigm coupling strength nbr sigm sigmoid function easy see number neighbors agree sign number neighbors disagree number equal forces cancel full conditional uniform combine ising prior local evidence term example gaussian observation model full conditional becomes exp exp exp sigm log probability entering state determined compatibility neighbors ising prior compatibility data local likelihood term see figure example algorithm applied simple image denoising problem results similar mean ﬁeld figure except ﬁnal estimate based averaging samples somewhat blurrier due fact mean ﬁeld tends conﬁdent chapter markov chain monte carlo mcmc inference 
[markov, chain, monte, carlo, mcmc, inference, gibbs, sampling, example, gibbs, sampling, inferring, parameters, gmm] straightforward derive gibbs sampling algorithm mixture model especially use conjugate priors focus case mixture gaussians although results easily extended kinds mixture models derivation follows results section much easier corresponding variational bayes algorithm section suppose use semi conjugate prior full joint distribution given dir use prior mixture component full conditionals follows discrete indicators mixing weights using results section dir means using results section covariances using results section see gaussmissingfitgibbs matlab code code also sample missing values necessary gibbs sampling label switching although simple implement gibbs sampling mixture models fundamental weakness problem parameters model indicator functions unidentiﬁable since arbitrarily permute hidden labels without affecting likelihood see section consequently cannot take monte carlo average samples compute posterior means since one sample considers parameters cluster may another sample considers parameters cluster indeed could average modes would ﬁnd assuming symmetric prior called label switching problem problem arise vbem lock single mode however arises method visits multiple modes problems one try prevent problem introducing constraints parameters ensure identiﬁability richardson green however always work since likelihood might overwhelm prior cause label switching anyway furthermore technique scale higher dimensions another approach post process samples searching global label permutation apply sample minimizes loss function stephens however slow perhaps best solution simply ask questions cannot uniquely identiﬁed example instead asking probability data point belongs cluster ask probability data points belong cluster latter question invariant labeling furthermore refers observable quantities grouped together rather referring unobservable quantities latent clusters approach advantage extends inﬁnite mixture models discussed section unbounded models notion hidden cluster well deﬁned notion partitioning data well deﬁned 
[markov, chain, monte, carlo, mcmc, inference, gibbs, sampling, collapsed, gibbs, sampling] cases analytically integrate unknown quantities sample rest called collapsed gibbs sampler tends much efficient since sampling lower dimensional space precisely suppose sample integrate thus parameters participate markov chain consequently draw conditionally independent samples much lower variance samples drawn joint state space liu process called rao blackwellisation named following theorem theorem rao blackwell let dependent random variables scalar function var var theorem guarantees variance estimate created analytically integrating always lower rather never higher variance direct estimate collapsed gibbs sample integrated rao blackwell theorem still applies case liu chapter markov chain monte carlo mcmc inference 
[markov, chain, monte, carlo, mcmc, inference, gibbs, sampling, gibbs, sampling, hierarchical, glms] often data multiple related sources sources reliable data rich others makes sense model data simultaneously enable borrowing statistical strength one natural way solve problems use hierarchical bayesian modeling also called multi level modeling section discussed way perform approximate inference models using variational methods discuss use gibbs sampling explain method consider following example suppose data students gibbs sampling figure multi level model linear regression different schools data naturally modeled two level hierarchy let response variable want predict student school prediction based school student speciﬁc covariates since quality schools varies want use separate parameter school model becomes illustrate model using dataset hoff socio economic status ses student school math score could separately give poor results sample size given school small illustrated figure plots least squares regression line estimated separately schools see slopes positive errant cases slope negative turns lines extreme slopes tend schools small sample size shown figure thus may necessarily trust ﬁts get better results construct hierarchical bayesian model assumed come common prior illustrated figure model schools small sample size borrow statistical strength schools larger sample size correlated via latent common parents crucial hyper parameters inferrred data ﬁxed constants would conditionally independent would information sharing complete model speciﬁcation must specify priors shared parameters fol lowing hoff use following semi conjugate forms convenience given simple show full conditionals needed gibbs sampling chapter markov chain monte carlo mcmc inference following forms group speciﬁc weights overall mean overall covariance noise variance ssr ssr applying gibbs sampling hierarchical model get results shown figure light gray lines plot mean posterior predictive distribution school dark gray line middle plots prediction using overall mean parameters see method regularized ﬁts quite nicely without enforcing much uniformity amount shrinkage controlled turns depends hyper parameters example used vague values 
[markov, chain, monte, carlo, mcmc, inference, gibbs, sampling, bugs, jags] one reason gibbs sampling popular possible design general purpose software work almost model software needs model speciﬁcation usually form directed graphical model speciﬁed ﬁle created graphical user interface library methods sampling different kinds full conditionals often done using adaptive rejection sampling described section example gibbs sampling package bugs lunn stands bayesian updating using gibbs sampling bugs widely used biostatistics social science another recent similar package jags plummer stands another gibbs sampler uses similar model speciﬁcation language bugs example describe model figure follows model dnorm hat tau hat inprod tau pow sigma sigma dunif dmnorm sigmainv sigmainv dwish eta dmnorm vinv pass model bugs jags generate samples see webpages details although approach appealing unfortunately much slower using hand written code especially complex models work automatically deriving model speciﬁc optimized inference code fischer schumann fast code still typically requires human expertise 
[markov, chain, monte, carlo, mcmc, inference, gibbs, sampling, imputation, posterior, ip, algorithm] imputation posterior algorithm tanner wong special case gibbs sampling group variables two classes hidden variables parameters sound familiar basically mcmc version step gets replaced step step gets replaced step example general strategy called data augmentation whereby introduce auxiliary variables order simplify posterior computations computation see tanner van dyk meng information 
[markov, chain, monte, carlo, mcmc, inference, gibbs, sampling, blocking, gibbs, sampling] gibbs sampling quite slow since updates one variable time called single site updating variables highly correlated take long time move away current state illustrated figure illustrate sampling gaussian see exercise details variables highly correlated algorithm chapter markov chain monte carlo mcmc inference figure illustration potentially slow sampling using gibbs sampling skewed gaus sian based figure bishop figure generated gibbsgaussdemo move slowly state space particular size moves controlled variance conditional distributions direction support distribution along dimension need steps obtain independent sample cases efficiently sample groups variables time called blocking gibbs sampling blocked gibbs sampling jensen wilkinson yeung make much bigger moves state space 
[markov, chain, monte, carlo, mcmc, inference, metropolis, hastings, algorithm] although gibbs sampling simple somewhat restricted set models applied example much help computing logistic regression model since corresponding graphical model useful markov structure addition gibbs sampling quite slow mentioned fortunately general algorithm used known metropolis hastings algorithm describe 
[markov, chain, monte, carlo, mcmc, inference, metropolis, hastings, algorithm, basic, idea] basic idea step propose move current state new state probability called proposal distribution also called kernel user free use kind proposal want subject conditions explain makes quite ﬂexible method commonly used proposal symmetric gaussian distribution centered current state called random walk metropolis algorithm discuss choose section use proposal form new state independent old state get method known independence sampler similar importance sampling section proposed move decide whether accept proposal according formula ensures fraction time spent state proportional proposal accepted new state otherwise new state metropolis hastings algorithm current state repeat sample proposal symmetric acceptance probability given following formula min see probable deﬁnitely move since less probable may still move anyway depending relative probabilities instead greedily moving probable states occasionally allow downhill moves less probable states section prove procedure ensures fraction time spend state proportional proposal asymmetric need hastings correction given following min correction needed compensate fact proposal distribution rather target distribution might favor certain states important reason useful algorithm evaluating need know target density normalization constant particular suppose unnormalized distribution normalization constant cancel hence sample even unknown particular evaluate pointwise overall algorithm summarized algorithm 
[markov, chain, monte, carlo, mcmc, inference, metropolis, hastings, algorithm, gibbs, sampling, special, case] turns gibbs sampling discussed section special case particular equivalent using sequence proposals form move new state sampled full conditional left unchanged prove acceptance rate proposal overall algorithm also acceptance rate chapter markov chain monte carlo mcmc inference algorithm metropolis hastings algorithm initialize deﬁne sample compute acceptance probability compute min sample set new sample exploited fact fact acceptance rate necessarily mean gibbs converge rapidly since updates one coordinate time see section fortunately many kinds proposals use discuss 
[markov, chain, monte, carlo, mcmc, inference, metropolis, hastings, algorithm, proposal, distributions] given target distribution proposal distribution valid admissible gives non zero probability moving states non zero probability target formally write supp supp example gaussian random walk proposal non zero probability density entire state space hence valid proposal continuous state space course practice important proposal spread probability mass right way figure shows example use sample mixture two gaussians using random walk proposal somewhat tricky target distribution since consists two well separated modes important set variance proposal correctly variance low chain explore one modes shown figure variance large moves rejected chain sticky stay state long time evident long stretches repeated values figure set proposal variance right get trace figure samples clearly explore support target distribution discuss tune proposal one big advantage gibbs sampling one need choose proposal metropolis hastings algorithm samples proposal iterations samples proposal iterations samples proposal iterations figure example metropolis hastings algorithm sampling mixture two gaussians using gaussian proposal variances chain gets trapped near starting state fails sample mode chain sticky effective sample size low reﬂected rough histogram approximation end using variance right leads good approximation true distribution shown red figure generated mcmcgmmdemo based code christophe andrieu nando freitas distribution furthermore acceptance rate course acceptance trivially achieved using proposal variance assuming start mode obviously exploring posterior high acceptance ultimate goal increase amount exploration increasing variance gaussian kernel often one experiments different parameters acceptance rate theory suggests optimal least gaussian target distributions short initial runs used tune proposal called pilot runs chapter markov chain monte carlo mcmc inference figure joint posterior parameters logistic regression applied sat data marginal offset marginal slope see marginals capture fact parameters highly correlated figure generated logregsatmhdemo gaussian proposals continuous state space hessian local mode used deﬁne covariance gaussian proposal distribution approach advantage hessian models local curvature length scales dimension approach therefore avoids slow mixing behavior gibbs sampling shown figure two obvious approaches independence proposal random walk proposal scale factor chosen facilitate rapid mixing roberts rosenthal prove posterior gaussian asymptotically optimal value use dimensionality results acceptance rate example consider binary logistic regression equation hessian log likelihood diag sigm assume gaussian prior asymptotically optimal gaussian proposal form see gamerman rossi fruhwirth schnatter fruhwirth details approach illustrated figure sample parameters logistic regression model sat data initialize chain mode computed using irls use random walk metropolis sampler cannot afford compute mode hessian xdx alternative approach suggested scott approximate proposal follows metropolis hastings algorithm mixture proposals one know kind proposal use one try mixture proposal convex combination base proposals mixing weights long individually valid overall proposal also valid data driven mcmc efficient proposals depend previous hidden state also visible data form called data driven mcmc see zhu create proposals one sample pairs forwards model train discriminative classiﬁer predict features extracted visible data typically high dimensional vector position orientation limbs person visual object detector hard predict entire state vector instead might train discriminative detector predict parts state space location face person use proposal form standard data independent proposal random walk updates component state space added efficiency discriminative proposals suggest joint changes multiple variables often hard overall procedure form generate test discriminative proposals generate new hypotheses tested computing posterior ratio see new hypothesis better worse adding annealing step one modify algorithm ﬁnd posterior modes called simulated annealing described section one advantage using mode seeking version algorithm need ensure proposal distribution reversible 
[markov, chain, monte, carlo, mcmc, inference, metropolis, hastings, algorithm, adaptive, mcmc] one change parameters proposal algorithm running increase efficiency called adaptive mcmc allows one start broad covariance say allowing large moves space mode found followed narrowing covariance ensure careful exploration region around mode however one must careful violate markov property thus parameters proposal depend entire history chain turns sufficient condition ensure adaption faded gradually time see andrieu thoms details chapter markov chain monte carlo mcmc inference 
[markov, chain, monte, carlo, mcmc, inference, metropolis, hastings, algorithm, initialization, mode, hopping] necessary start mcmc initial state non zero probability model deterministic constraints ﬁnding legal conﬁguration may hard problem therefore common initialize mcmc methods local mode found using optimizer domains especially discrete state spaces effective use computa tion time perform multiple restarts optimizer average modes rather exploring similar points around local mode however continuous state spaces mode contains negligible volume section necessary locally explore around mode order visit enough posterior probability mass 
[markov, chain, monte, carlo, mcmc, inference, metropolis, hastings, algorithm, works] prove procedure generates samples use bit markov chain theory sure read section ﬁrst algorithm deﬁnes markov chain following transition matrix otherwise follows case analysis move must proposed probability must accepted probability otherwise stay state either proposed probability proposed something else probability rejected probability let analyse markov chain recall section chain satisﬁes detailed balance also showed chain satisﬁes detailed balance stationary distribution goal show algorithm deﬁnes transition function satisﬁes detailed balance hence stationary distribution equation holds say invariant distribution wrt markov transition kernel theorem transition matrix deﬁned algorithm given equation ergodic irreducible unique limiting distribution proof consider two states either ignore ties occur probability zero continuous distributions without loss generality assume hence metropolis hastings algorithm hence move must ﬁrst propose accept hence hence backwards probability since inserting equation get detailed balance holds wrt hence theorem stationary distribution furthermore theorem distribution unique since chain ergodic irreducible 
[markov, chain, monte, carlo, mcmc, inference, metropolis, hastings, algorithm, reversible, jump, trans-dimensional, mcmc] suppose set models different numbers parameters mixture models number mixture components unknown let model denoted let unknowns parameters denoted dimensionality model sampling spaces differing dimensionality called trans dimensional mcmc green could sample model indicator sample parameters product space inefficient parsimonious sample union space worry parameters currently active model difficulty approach arises move models different dimen sionality trouble compute acceptance ratio comparing densities deﬁned different dimensionality spaces meaningless like trying compare sphere circle solution proposed green known reversible jump mcmc rjmcmc augment low dimensional space extra random variables two spaces common measure unfortunately space details suffice say method made work theory although bit tricky practice however continuous parameters integrated resulting method called collapsed rjmcmc much difficulty goes away since left discrete state space need worry change measure example denison includes many examples applications collapsed rjmcmc applied bayesian inference fro adaptive basis function models sample basis functions ﬁxed set candidates centered data points integrate parameters analytically provides bayesian alternative using rvms svms chapter markov chain monte carlo mcmc inference initial condition initial condition figure illustration convergence uniform distribution using symmetric random walk starting left state right state based figures mackay figure generated randomwalktodemo 
[markov, chain, monte, carlo, mcmc, inference, speed, accuracy, mcmc] figure markov chain low conductance dotted arcs represent transitions low probability source figure koller friedman used kind permission daphne koller ﬁrst second eigenvalues transition matrix particular one show log number states since computing transition matrix hard especially high dimensional continuous state spaces useful ﬁnd ways estimate mixing time alternative approach examine geometry state space example consider chain figure see state space consists two islands connected via narrow bottleneck completely disconnected chain would ergodic would longer unique stationary distribution deﬁne conductance chain minimum probability subsets states transitioning set complement min one show log hence chains low conductance high mixing time example distributions well separated modes usually high mixing time simple mcmc methods often work well cases advanced algorithms parallel tempering necessary see liu 
[markov, chain, monte, carlo, mcmc, inference, speed, accuracy, mcmc, burn-in, phase] start mcmc arbitrary initial state explained section chain forgotten started samples coming chain stationary distribution samples collected chain reached stationary distribution come usually thrown away initial period whose samples ignored called burn phase example consider uniform distribution integers suppose sample using symmetric random walk figure show two runs algorithm left start state right start state even small problem takes steps chain forgotten started difficult diagnose chain burned issue discuss detail one fundamental weaknesses mcmc interesting example happen start collecting samples early consider potts model figure shows sample iterations gibbs sampling suggests model likes speed accuracy mcmc figure illustration problems caused poor mixing one sample state potts model grid nearest neighbor connectivity geman geman iterations one sample model iterations used kind permission erik sudderth medium sized regions label implying model would make good prior image segmentation indeed suggested original gibbs sampling paper geman geman however turns run chain long enough get isolated speckles figure results depend coupling strength general hard ﬁnd setting produces nice medium sized blobs parameters result super clusters lots small fragments fact rapid phase transition two regimes led paper called ising potts model well suited segmentation tasks morris possible create priors suited image segmentation sudderth jordan main point sampling reaching convergence lead erroneous conclusions 
[markov, chain, monte, carlo, mcmc, inference, speed, accuracy, mcmc, mixing, rates, markov, chains] amount time takes markov chain converge stationary distribution forget initial state called mixing time formally say mixing time state minimal time constant min distribution mass state transition matrix chain depends target proposal distribution steps mixing time chain deﬁned max mixing time determined eigengap difference chapter markov chain monte carlo mcmc inference 
[markov, chain, monte, carlo, mcmc, inference, speed, accuracy, mcmc, practical, convergence, diagnostics] computing mixing time chain general quite difficult since transition matrix usually hard compute practice various heuristics proposed diagnose speed accuracy mcmc convergence see geyer cowles carlin brooks roberts review strictly speaking methods diagnose convergence rather non convergence method may claim chain converged fact ﬂaw common convergence diagnostics since diagnosing convergence computationally intractable general bhatnagar one simplest approaches assessing method converged run multiple chains different overdispersed starting points plot samples variables interest called trace plot chain mixed forgotten started trace plots converge distribution thus overlap figure gives example show traceplot sampled mixture two gaussians using four different methods symmetric gaussian proposal variance gibbs sampling see mixed also evident figure shows single chain never leaves area started results methods indicate chains rapidly converge stationary distribution matter started sticky nature proposal evident reduces computational efficiency discuss statistical validity estimated potential scale reduction epsr assess convergence quantitatively follows basic idea compare variance quantity within chain variance across chains precisely suppose collect samples burn chains variables isc let scalar quantity interest derived isc chosen deﬁne within sequence mean overall mean deﬁne sequence within sequence variance construct two estimates variance ﬁrst estimate underestimate var chains ranged full posterior second estimate estimate var unbiased stationarity overestimate starting points overdispersed gelman rubin deﬁne following convergence diagnostic statistic known estimated potential scale reduction epsr chapter markov chain monte carlo mcmc inference rhat rhat rhat gibbs rhat figure traceplots mcmc samplers color represents samples different starting point proposal corresponding figure gibbs sampling figure generated mcmcgmmdemo quantity ﬁrst proposed gelman rubin measures degree posterior variance would decrease continue sampling limit given quantity estimate reliable least unreliable values four samplers figure diagnostic correctly identiﬁed sampler using ﬁrst proposal untrustworthy 
[markov, chain, monte, carlo, mcmc, inference, speed, accuracy, mcmc, accuracy, mcmc] samples produced mcmc auto correlated reduces information content relative independent perfect samples quantify follows suppose want section based hoff sec speed accuracy mcmc gibbs figure autocorrelation functions corresponding figure figure generated mcmcgmmdemo estimate mean function denote true mean monte carlo estimate given chapter markov chain monte carlo mcmc inference mcmc estimate variance estimate given var var ﬁrst term monte carlo estimate variance samples correlated second term depends correlation samples measure follows deﬁne sample based auto correlation lag set samples follows called autocorrelation function acf plotted figure four samplers gaussian mixture model see acf gibbs sampler bottom right dies much rapidly samplers indicating gibbs sample worth sample simple method reduce autocorrelation use thinning keep every sample increase efficiency underlying sampler save space since avoids storing highly correlated samples estimate information content set samples computing effective sample size ess eff deﬁned eff var var figure clear effective sample size gibbs sampler higher samplers example 
[markov, chain, monte, carlo, mcmc, inference, speed, accuracy, mcmc, many, chains] natural question ask many chains run could either run one long chain ensure convergence collect samples spaced far apart could run many short chains wastes burnin time practice common run medium number chains say medium length say steps take samples discarding ﬁrst half samples initialize local mode may able use samples wait burn auxiliary variable mcmc model goal method reference probit map gradient section probit map section probit post nickisch rasmussen probit post gibbs exercise probit post gibbs ars dellaportas smith probit post using irls proposal gamerman logit map gradient section logit post gibbs student fruhwirth schnatter fruhwirth logit post gibbs holmes held table summary possible algorithms estimation inference binary classiﬁcation problems using gaussian priors abbreviations aux auxiliary variable sampling ars adaptive rejection sampling expectation propagation gibbs gibbs sampling auxiliary variables irls iterative reweighted least squares kolmogorov smirnov map maximum posteriori metropolis hastings post posterior 
[markov, chain, monte, carlo, mcmc, inference, auxiliary, variable, mcmc] sometimes dramatically improve efficiency sampling introducing dummy auxiliary variables order reduce correlation original variables original variables denoted auxiliary variables require easier sample meet two conditions sample enlarged model throw away sampled values thereby recovering samples give examples 
[markov, chain, monte, carlo, mcmc, inference, auxiliary, variable, mcmc, auxiliary, variable, sampling, logistic, regression] section discussed latent variable interpretation probit regression recall form exploited representation section used ﬁnd estimate straightforward convert auxiliary variable gibbs sampler exercise since gaussian truncated gaussian easy sample let discuss derive auxiliary variable gibbs sampler logistic regression let follow logistic distribution pdf logistic mean variance var cdf form sigm chapter markov chain monte carlo mcmc inference logistic function since iff symmetry sigm required derive auxiliary variable gibbs sampler sampling unfortunately sampling directly possible one approach deﬁne kolmogorov smirnov distribution sample holmes held simpler approach approximate logistic distribution student distribution albert chib speciﬁcally make approximation use scale mixture gaussians representation student simplify inference particular write full conditionals simple form see exercise details note set equivalent probit regression see section rather choosing probit logit regression simply estimate parameter convenient conjugate prior consider ﬁnite range possible values evaluate posterior follows furthermore deﬁne sample well example suppose use prior posterior given interleaved gibbs sampling steps provides appealing bayesian alternative cross validation setting strength regularizer see table summary various algorithms ﬁtting probit logit models many methods also extended multinomial logistic regression case details see scott fruhwirth schnatter fruhwirth 
[markov, chain, monte, carlo, mcmc, inference, auxiliary, variable, mcmc, slice, sampling] consider sampling univariate multimodal distribution sometimes improve ability make large moves adding auxiliary variable deﬁne joint distribution follows otherwise auxiliary variable mcmc figure illustration principle behind slice sampling given previous sample sample uniformly target density sample along slice source figure andrieu used kind permission nando freitas slice sampling action figure generated slicesamplingdemod intercept slope posterior density figure binomial regression data grid approximation posterior slice sampling approximation figure generated slicesamplingdemod marginal distribution given sample sampling ignoring full conditionals form set points chosen height corresponds slice distribution hence term slice sampling neal see figure practice difficult identify set use following approach construct interval min max around current point width chapter markov chain monte carlo mcmc inference test see end point lies within slice keep extending direction lies outside slice called stepping candidate value chosen uniformly region lies within slice kept otherwise shrink region forms one end region still contains another sample drawn continue way sample accepted apply method multivariate distributions sample one extra auxiliary variable dimension advantage slice sampling gibbs need speciﬁcation full conditionals unnormalized joint advantage slice sampling need user speciﬁed proposal distribution although require speciﬁcation width stepping interval figure illustrates algorithm action synthetic problem figure illustrates behavior slightly harder problem namely binomial logistic regression model form bin logit use vague gaussian prior figure shows grid based approximation posterior figure shows sample based approximation example grid faster compute problem dimensions grid approach infeasible 
[markov, chain, monte, carlo, mcmc, inference, auxiliary, variable, mcmc, swendsen, wang] consider ising model following form edge edge factor deﬁned edge strength gibbs sampling models slow large absolute value neighboring states highly correlated swendsen wang algorithm swendsen wang auxiliary variable mcmc sampler mixes much faster least case attractive ferromagnetic models suppose introduce auxiliary binary variables one per edge called bond variables denoted deﬁne extended model form deﬁne new factor follows clear presentation method based notes david mackay available http www inference phy cam mackay itila swendsen pdf auxiliary variable mcmc figure illustration swendsen wang algorithm grid used kind permission kevin tang hence sample extended model throw away samples get valid samples original distribution fortunately easy apply gibbs sampling extended model full conditional factorizes edges since bond variables conditionally independent given node variables furthermore full conditional simple compute nodes either end edge state set bond probability otherwise set figure top right bonds could turned corresponding nodes state represented dotted edges figure bottom right bonds randomly turned represented solid edges sample proceed follows find connected components deﬁned graph induced bonds turned note connected component may consist singleton node pick one components uniformly random nodes component must state since diagonal terms factor pick state uniformly random force variables component adopt new state illustrated figure bottom left green square chapter markov chain monte carlo mcmc inference denotes selected connected component choose force nodes within enter white state validity algorithm left exercise extension handle local evidence non stationary potentials intuitively clear swendsen wang makes much larger moves state space gibbs sampling fact mixes much faster gibbs sampling lattice ising models variety values coupling parameter provided precisely let edge strength parameterized computational temperature large nodes roughly independent methods work equally well however approaches critical temperature typical states system long correlation lengths gibbs sampling takes long time generate independent samples temperature continues drop typical states either frequency gibbs sampling moves two modes exponentiall small contrast mixes rapidly temperatures unfortunately edge weights negative system frustrated exponentially many modes even low temperature work well setting since tries force many neighboring variables state fact computation regime provably hard algorithm jerrum sinclair 
[markov, chain, monte, carlo, mcmc, inference, auxiliary, variable, mcmc, hybrid/hamiltonian, mcmc] section brieﬂy mention way perform mcmc sampling continuous state spaces compute gradient unnormalized log posterior case neural network models example basic idea think parameters particle space create auxiliary variables represent momentum particle update parameter momentum pair according certain rules see duane neal mackay neal details resulting method called hybrid mcmc hamiltonian mcmc two main parameters user must specify many leapfrog steps take updating position momentum big make steps performance quite sensitive parameters although see hoffman gelman recent way set automatically method combined stochastic gradient descent section order handle large datasets explained ahn recently powerful extension method developed exploits second order gradient information see girolami details 
[markov, chain, monte, carlo, mcmc, inference, annealing, methods] many distributions multimodal hence hard sample however analogy way metals heated cooled order make molecules align imagine using computational temperature parameter smooth distribution gradually cooling recover original bumpy distribution ﬁrst explain idea detail context algorithm map estimation discuss extensions sampling case annealing methods temp temp figure energy surface different temperatures note different vertical scales figure generated sademopeaks 
[markov, chain, monte, carlo, mcmc, inference, annealing, methods, simulated, annealing] simulated annealing kirkpatrick stochastic algorithm attempts ﬁnd global optimum black box function closely related metropolis hastings algorithm generating samples probability distribution discussed section used discrete continuous optimization method inspired statistical physics key quantity boltzmann distribution speciﬁes probability particular state given exp energy system computational temperature temperature approaches system cooled system spends time minimum energy probable state figure gives example function different temperatures high temperatures surface approximately ﬂat hence easy move around avoid local optima temperature cools largest peaks become larger smallest peaks disappear cooling slowly enough possible track largest peak thus ﬁnd global optimum example continuation method generate algorithm follows step sample new state according proposal distribution real valued parameters often simply random walk proposal discrete optimization kinds local moves must deﬁned proposed new state compute exp accept new state set probability min otherwise stay current state set means new state lower energy probable deﬁnitely accept higher energy less probable might still accept depending current temperature thus algorithm allows hill moves probability space hill energy space less frequently temperature drops chapter markov chain monte carlo mcmc inference temperature iteration energy iteration figure run simulated annealing energy surface figure temperature iteration energy iteration figure generated sademopeaks iter temp iter temp figure histogram samples annealed posterior different time points produced simulated annealing energy surface shown figure note cold temperatures samples concentrated near peak figure generated sademopeaks rate temperature changes time called cooling schedule shown kirkpatrick one cools sufficiently slowly algorithm provably ﬁnd global optimum however clear sufficient slowly means practice common use exponential cooling schedule following form initial temperature often cooling rate often see figure plot cooling schedule cooling quickly means one get stuck local maximum cooling slowly wastes time best cooling schedule difficult determine one main drawbacks simulated annealing figure shows example simulated annealing applied function figure using random walk proposal see method stochastically reduces energy time figures illustrate histogram samples drawn cooled probability distribution time see samples concentrated near global maximum algorithm converged return largest value found annealing methods 
[markov, chain, monte, carlo, mcmc, inference, annealing, methods, annealed, importance, sampling] describe method known annealed importance sampling neal com bines ideas simulated annealing importance sampling order draw independent samples difficult multimodal distributions suppose want sample cannot easily example might represent multimodal posterior suppose however easier distribution sample call example might prior construct sequence intermediate distributions move slowly follows inverse temperature contrast scheme used simulated annealing form makes hard sample furthermore suppose series markov chains leave invariant given sample ﬁrst sampling sequence follows sample sample sample finally set give weight shown correct viewing algorithm form importance sampling extended state space consider following distribution state space reversal clear safely use part sequences recover original ditribution consider proposal distribution deﬁned algorithm one show importance weights given equation 
[markov, chain, monte, carlo, mcmc, inference, annealing, methods, parallel, tempering] another way combine mcmc annealing run multiple chains parallel different temperatures allow one chain sample another chain neighboring temperature way high temperature chain make long distance moves state space inﬂuence lower temperature chains known parallel tempering see earl deem details chapter markov chain monte carlo mcmc inference 
[markov, chain, monte, carlo, mcmc, inference, approximating, marginal, likelihood] marginal likelihood key quantity bayesian model selection given unfortunately integral often intractable compute example non conjugate priors hidden variables section brieﬂy discuss ways approximate expression using monte carlo see gelman meng extensive review 
[markov, chain, monte, carlo, mcmc, inference, approximating, marginal, likelihood, candidate, method] simple method approximating marginal likelihood known candidate method chib exploits following identity holds value picked value evaluate quite easily estimate posterior near evaluate denominator well posterior often approximated using mcmc ﬂaw method relies assumption marginalized modes posterior practice rarely possible consequently method give inaccurate results practice neal 
[markov, chain, monte, carlo, mcmc, inference, approximating, marginal, likelihood, harmonic, mean, estimate] newton raftery proposed simple method approximating using output mcmc follows expression harmonic mean likelihood data sample theoretical correctness expression follows following identity unfortunately practice method works poorly indeed radford neal called worst monte carlo method ever reason bad depends samples drawn posterior posterior often insensitive prior whereas marginal likelihood mention method order warn use present better method source radfordneal wordpress com harmonic mean likelihood worst mon carlo method ever approximating marginal likelihood 
[markov, chain, monte, carlo, mcmc, inference, approximating, marginal, likelihood, annealed, importance, sampling] use annealed importance sampling section evaluate ratio partition functions notice hence prior posterior estimate using equation provided prior known normalization constant generally considered method choice evaluating difficult partition functions 
[markov, chain, monte, carlo, mcmc, inference, exercises] exercise gibbs sampling gaussian suppose derive full condition als implement algorithm plot marginals histograms superimpose plot exact marginals exercise gibbs sampling gaussian mixture model consider applying gibbs sampling univariate mixture gaussians section derive expressions full conditionals hint know say gets connected values structure graph simpliﬁes assigned values nodes hence given values posteriors independent conditional independent similarly exercise gibbs sampling potts model modify code gibbsdemoising draw samples potts prior different temperatures figure exercise full conditionals hierarchical model gaussian means let reconsider gaussian gaussian model section modelling multiple related mean parameters exercise derive gibbs sampler instead using suppose following hoff use following conjugate priors hyper parameters chapter markov chain monte carlo mcmc inference set uninformative values given model speciﬁcation show full conditionals follows exercise gibbs sampling robust linear regression student likelihood modify algorithm exercise perform gibbs sampling exercise gibbs sampling probit regression modify algorithm section perform gibbs sampling hint sample truncated gaussian two steps ﬁrst sample set robert exercise gibbs sampling logistic regression student approximation derive full conditionals joint model deﬁned equations 
[clustering, introduction] clustering process grouping similar objects together two kinds inputs might use similarity based clustering input algorithm dissimilarity matrix distance matrix feature based clustering input algorithm feature matrix design matrix similarity based clustering advantage allows easy inclusion domain speciﬁc similarity kernel functions section feature based clustering advantage applicable raw potentially noisy data see examples addition two types input two possible types output ﬂat cluster ing also called partitional clustering partition objects disjoint sets hierarchical clustering create nested tree partitions discuss surprisingly ﬂat clusterings usually faster create ﬂat log hierarchical hierarchical clusterings often useful furthermore hierarchical clustering algorithms deterministic require speciﬁcation number clusters whereas ﬂat clustering algorithms sensitive initial conditions require model selection method discuss choose detail ﬁnal distinction make chapter whether method based probabilistic model one might wonder even bother discussing non probabilistic methods clustering reason two fold ﬁrst widely used readers know second often contain good ideas used speed inference probabilistic models 
[clustering, introduction, measuring, dissimilarity] dissimilarity matrix matrix measure distance objects subjectively judged dissimilarities seldom distances strict sense since triangle inequality often hold algorithms require true distance matrix many similarity matrix convert dissimilarity matrix applying monotonically decreasing function max common way deﬁne dissimilarity objects terms dissimilarity chapter clustering attributes common attribute dissimilarity functions follows squared euclidean distance course makes sense attribute real valued squared distance strongly emphasizes large differences differences squared robust alternative use distance also called city block distance since distance computed counting many rows columns move horizontally vertically get vector time series real valued data common use correlation coefficient see section data standardized corr hence corr clustering based correlation similarity equivalent clustering based squared distance dissimilarity ordinal variables low medium high standard encode values real valued numbers say possible values one apply dissimilarity function quantitative variables squared distance categorical variables red green blue usually assign distance features different distance otherwise summing categorical features gives called hamming distance 
[clustering, introduction, evaluating, output, clustering, methods] validation clustering structures difficult frustrating part cluster analysis without strong effort direction cluster analysis remain black art accessible true believers experience great courage jain dubes jain dubes introduction xamp amp xamp xamp figure three clusters labeled objects inside based figure manning clustering unupervised learning technique hard evaluate quality output given method use probabilistic models always evaluate likelihood test set two drawbacks ﬁrst directly assess clustering discovered model second apply non probabilistic methods discuss performance measures based likelihood intuitively goal clustering assign points similar cluster ensure points dissimilar different clusters several ways measuring quantities see jain dubes kaufman rousseeuw however internal criteria may limited use alternative rely external form data validate method example suppose labels object figure equivalently reference clustering given clustering induce set labels vice versa compare clustering labels using various metrics describe use metrics later compare clustering methods purity let number objects cluster belong class let total number objects cluster deﬁne empirical distribution class labels cluster deﬁne purity cluster max overall purity clustering purity example figure purity purity ranges bad good however trivially achieve purity putting object cluster measure penalize number clusters rand index let two different partitions data points two different ﬂat clusterings example might estimated clustering reference clustering derived class labels deﬁne contingency table chapter clustering containing following numbers number pairs cluster true positives number pairs different clusters true negatives number pairs different clusters cluster false negatives number pairs cluster different clusters false positives common summary statistic rand index interpreted fraction clustering decisions correct clearly example consider figure three clusters contain points number positives pairs objects put cluster regardless label number true positives given last two terms come cluster pairs labeled pairs labeled similarly one show rand index rand index achieves lower bound rare event one deﬁne adjusted rand index hubert arabie follows index expected index max index expected index model randomness based using generalized hyper geometric distribution two partitions picked random subject original number classes objects expected value computed model used compute statistical signiﬁcance rand index rand index weights false positives false negatives equally various summary statistics binary decision problems score section also used one compute frequentist sampling distribution hence statistical signiﬁcance using methods bootstrap mutual information another way measure cluster quality compute mutual information vaithyanathan dom let probability randomly chosen object belongs cluster also let probability randomly chosen object belongs cluster deﬁne dirichlet process mixture models similarly log lies min unfortunately maximum value achieved using lots small clusters low entropy compensate use normalized mutual information lies version adjusted chance particular random data model described vinh another variant called variation information described meila 
[clustering, dirichlet, process, mixture, models] simplest approach ﬂat clustering use ﬁnite mixture model discussed section sometimes called model based clustering since deﬁne probabilistic model data optimize well deﬁned objective likelihood posterior opposed using heuristic algorithm principle problem ﬁnite mixture models choose number components discussed several techniques section however many cases well deﬁned number clusters even simple height weight data figure clear correct value would much better choose section discuss inﬁnite mixture models impose priori bound use non parametric prior based dirichlet process allows number clusters grow amount data increases also prove useful later discuss hiearchical clustering topic non parametric bayes currently active space details see hjort recent book topic instead give brief review application mixture modeling based presentation sudderth sec 
[clustering, dirichlet, process, mixture, models, ﬁnite, inﬁnite, mixture, models] consider ﬁnite mixture model shown figure usual representation follows dir form chosen conjugate write observation distribution similarly write prior chapter clustering figure two different representations ﬁnite mixture model left traditional representation right representation parameters samples discrete measure picture right illustrates case sample gaussian means gaussian prior height spikes reﬂects mixing weights weighted sum delta functions generate two parameters one per data point finally generate two data points source figure sudderth used kind permission erik sudderth equivalent representation model shown figure parameter used generate observation parameters sampled distribution form dir thus see ﬁnite mixture delta functions centered cluster parameters probability equal exactly prior probability cluster sample model always probability one get exactly clusters data points scattered around cluster centers would like ﬂexible model generate variable number clusters furthermore data generate likely see new cluster way replace discrete distribution random probability measure show dirichlet process denoted one way details show samples non parametric model figure see desired properties generating variable number clusters clusters amount data increases resulting samples look much like real data samples ﬁnite mixture model course working inﬁnite model sounds scary fortunately show even though model potentially inﬁnite perform inference using amount computation tractable often much less required set dirichlet process mixture models figure samples dirichlet process mixture model gaussians concentration parameter left right show samples row different run also show model parameters ellipses sampled vague niw base distribution based figure sudderth figure generated dpmsampledemo written yee whye teh ﬁnite mixture models different intuitive reason get evidence certain values appropriate high posterior support long able estimate parameters focus computational efforts models appropriate complexity thus going inﬁnite limit sometimes faster especially true multiple model selection problems solve chapter clustering figure base measure space one possible partition regions shading cell proportional reﬁned partition regions source figure sudderth used kind permission erik sudderth 
[clustering, dirichlet, process, mixture, models, dirichlet, process] recall chapter gaussian process distribution functions form deﬁned implicitly requirement jointly gaussian set points parameters gaussian computed using mean function covariance kernel function write fur thermore consistently deﬁned derived etc dirichlet process distribution probability measures require deﬁned implicitly requirement joint dirichlet distribution dir ﬁnite partition case write called concentration parameter called base measure example shown figure base measure gaussian distribution cells dirichlet marginals cell beta distributed beta consistently deﬁned sense form partition follow beta distribution recall dir cat integrate get predictive distribution dirichlet multinoulli model cat unlike knowing something tell anything beyond sum one constraint say neutral process stochastic processes deﬁned property computationally convenient dirichlet process mixture models figure illustration stick breaking construction unit length stick break random point length piece keep called recursively break pieces remaining stick generate source figure sudderth used kind permission erik sudderth samples process top row bottom row figure generated stickbreakingdemo written yee whye teh words also updated posterior given one observation given dir generalizes arbitrary partitions posterior dir holds set partitions hence observe multiple samples new posterior given thus see effectively deﬁnes conjugate prior arbitrary measurable spaces concentration parameter like effective sample size base measure stick breaking construction discussion far abstract give constructive deﬁnition known stick breaking construction let inﬁnite sequence mixture weights derived following process beta chapter clustering often denoted gem gem stands griffiths engen mccloskey term due ewens samples process shown figure one show process process terminate probability although number elements generates increases furthermore size components decreases average deﬁne gem one show consequence construction see samples discrete probability one words keep sampling get repetitions previously generated values sample see repeated values let number unique values etc data sampled therefore cluster around evident figure data comes gaussians large values represented ellipses thick borders ﬁrst indication might useful clustering chinese restaurant process crp working inﬁnite dimensional sticks problematic however exploit clustering property draw samples form show key result observations taking distinct values predictive distribution next observation given number previous observations equal called polya urn blackwell macqueen sampling scheme provides constructive way sample much convenient work discrete variables specify value use deﬁne based expression represents new cluster index yet used called chinese restaurant process crp based seemingly inﬁnite supply tables certain chinese restaurants analogy follows tables like clusters customers like observations person enters restaurant may choose join existing table probability proportional number people already sitting table otherwise probability diminishes people enter room due term dirichlet process mixture models figure two views mixture model left inﬁnite number clusters parameters gem right drawn compare figure source figure sudderth used kind permission erik sudderth may choose sit new table result distribution partitions integers like distribution customers tables fact currently occupied tables likely get new customers sometimes called rich get richer phenomenon indeed one derive expression distri bution cluster sizes induced prior process basically power law number occupied tables almost surely approaches log showing model complexity indeed grow logarithmically dataset size ﬂexible priors cluster sizes also deﬁned two parameter pitman yor process 
[clustering, dirichlet, process, mixture, models, applying, dirichlet, processes, mixture, modeling] particularly useful model data directly since data vectors rarely repeat exactly however useful prior parameters stochastic data generating mechanism mixture model create model follow exactly setup section deﬁne equivalently write model follows gem illustrated figure see random draw unbounded number parameters base distribution weight data point generated sampling private parameter get data becomes increasingly likely equal one seen thus generated close existing datapoint chapter clustering 
[clustering, dirichlet, process, mixture, models, fitting, mixture, model] simplest way dpmm modify collapsed gibbs sampler section equation exchangeability assume last customer enter restaurant hence ﬁrst term given number clusters used new cluster another way write follows seen new cluster interestingly equivalent equation form limit rasmussen neal compute second term let partition data clusters based let data assigned cluster conditionally independent data points except assigned cluster hence marginal likelihood data assigned cluster including analogous expression excluding thus see term posterior preditive distribution cluster evaluated corresponding new cluster prior predictive distribution new cluster evaluated see algorithm pseudocode called algorithm neal similar collapsed gibbs ﬁnite mixtures except consider case example procedure action shown figure sample clusterings induced posterior seems reasonable method tends rapidly discover good clustering contrast gibbs sampling ﬁnite mixture model often gets stuck affinity propagation algorithm collapsed gibbs sampler mixtures random order remove sufficient statistics old cluster compute set dim compute compute compute normalize sample add sufficient statistics new cluster cluster empty remove decrease poor local optima shown dpmm able create extra redundant clusters early use escape local optima figure shows time dpmm converges rapidly ﬁnite mixture model variety ﬁtting methods proposed daume shows one use star search beam search quickly ﬁnd approximate map estimate mansinghka discusses dpmm online using particle ﬁltering like stochastic version beam search efficient gibbs sampling particularly large datasets kurihara develops variational approximation even faster see also zobay extensions case non conjugate priors discussed neal another important issue set hyper parameters value much impact predictive accuracy affect number clusters one approach put prior posterior using auxiliary variable methods escobar west alternatively one use empirical bayes mcauliffe similarly base distribution either sample hyper parameters rasmussen use empirical bayes mcauliffe 
[clustering, affinity, propagation] mixture models whether ﬁnite inﬁnite require access raw data matrix need specify generative model data alternative approach takes input similarity matrix tries identify examplars act cluster centers medoids centers algorithm section one approach suffer local minima describe alternative approach called affinity propagation frey dueck works substantially better practice idea data point must choose another data point exemplar centroid data points choose centroids automatically determine number clusters precisely let represent centroid datapoint chapter clustering iter iter iter figure data points clustered using mixture collapsed gibbs sampling show samples posterior samples also show posterior based samples discarding ﬁrst burnin figure generated dpmgaussddemo written yee whye teh goal maximize following function ﬁrst term measures similarity point centroid second term penalty term data point chosen exemplar chosen exemplar formally otherwise objective function represented factor graph either use nodes affinity propagation iteration log dirichlet process mixture finite mixture iteration log dirichlet process mixture finite mixture figure comparison collapsed gibbs samplers mixture dark blue ﬁnite mixture light red applied data points shown figure left logprob iteration different starting values right median thick line quantiles dashed lines different starting values source figure sudderth used kind permission erik sudderth figure factor graphs affinity propagation circles variables squares factors node possible states figure frey dueck used kind permission brendan frey possible values shown figure use binary nodes see givoni frey details assume former representation ﬁnd strong local maximum objective using max product loopy belief propagation section referring model figure variable nodes sends message factor node turns vector numbers reduced scalar message denote known responsibility measure much thinks would make good exemplar compared exemplars looked addition factor node sends message variable node reduced scalar message known availability measure strongly believes exemplar based data points looked usual loopy method might oscillate convergence guaranteed chapter clustering figure example affinity propagation point colored coded much wants exemplar red green least computed summing incoming availability messages self similarity term darkness point wants belong exemplar figure frey dueck used kind permission brendan frey however using damping method reliable practice graph densely connected message passing takes time sparse similarity matrices takes time number edges non zero entries number clusters controlled scaling diagonal terms reﬂect much data point wants exemplar figure gives simple example data negative euclidean distance used measured similarity values set median pairwise similarities result clusters many results reported frey dueck show method signiﬁcantly outperforms medoids spectral clustering alternative view clustering terms graph cuts idea create weighted undirected graph similarity matrix typically using nearest neighbors point ensures graph sparse speeds computation want ﬁnd partition clusters say one natural criterion minimize cut spectral clustering complement problem easy solve unfortunately optimal solution often partitions single data point rest ensure sets reasonably large deﬁne normalized cut ncut cut vol vol weighted degree node splits graph clusters nodes within cluster similar different nodes clusters formulate ncut problem terms searching binary vectors point belongs cluster minimize objective unfortunately hard wagner wagner affinity propagation one way solve problem another relax constraints binary allow real valued result turns eigenvector problem known spectral clustering see shi malik general technique performing eigenalysis graphs called spectral graph theory chung going details would take far aﬁeld give brief summary based von luxburg since encounter ideas later 
[clustering, affinity, propagation, graph, laplacian] let symmetric weight matrix graph let diag diaogonal matrix containing weighted degree node deﬁne graph laplacian follows matrix various important properties row sums zero eigenvector eigenvalue furthermore matrix symmetric positive semi deﬁnite see note hence consequently see non negative real valued eigenvalues get intuition might useful graph based clustering note following result theorem set eigenvectors eigenvalue spanned indicator vectors connected components graph chapter clustering proof let start case eigenvector eigenvalue two nodes connected must hence constant vertices connected path graph suppose case block diagonal similar argument shows indicator functions select connected components suggests following algorithm compute ﬁrst eigenvectors let matrix eigenvectors columns let row since piecewise constant apply means clustering recover connected components assign point cluster iff row assigned cluster reality expect graph derived real similarity matrix isolated connected components would easy reasonable suppose graph small perturbation ideal case one use results perturbation theory show eigenvectors perturbed laplacian close ideal indicator functions note approach related kernel pca section particular kpca uses largest eigenvectors equivalent smallest eigenvectors similar method computes smallest eigenvectors see bengio details practice spectral clustering gives much better results kpca 
[clustering, affinity, propagation, normalized, graph, laplacian] practice important normalize graph laplacian account fact nodes highly connected others two comon ways one method used shi malik meila creates stochastic matrix row sums one eigenvalues eigenvectors closely related see von luxburg details furthemore one show eigenspace spanned indicator vectors suggests following algorithm ﬁnd smallest eigenvectors create cluster rows using means infer partitioning original points shi malik note eigenvectors values equivalent generalized eigenvectors values solve another method used creates symmetric matrix sym time eigenspace spanned suggest following algorithm ﬁnd smallest eigenvectors sym create normalize row unit norm creating cluster rows using means infer partitioning original points interesting connection ncuts random walks graph meila first note stochastic matrix hierarchical clustering means clustering spectral clustering figure clustering data consisting spirals means spectral clustering figure generated spectralclusteringdemo written wei lwun interpreted probability going graph connected non bipartite possesses unique stationary distribution vol furthermore one show ncut means looking cut random walk rarely makes transitions vice versa 
[clustering, affinity, propagation, example] figure illustrates method action figure see means poor job clustering since implicitly assumes cluster corresponds spherical gaussian next try spectral clustering deﬁne similarity matrix using gaussian kernel compute ﬁrst two eigenvectors laplacian infer clustering figure since method based ﬁnding smallest eigenvectors sparse matrix takes time however variety methods used scale large datasets see yan 
[clustering, hierarchical, clustering] mixture models whether ﬁnite inﬁnite produce ﬂat clustering often want learn hierarchical clustering clusters nested inside two main approaches hierarchical clustering bottom agglomerative clus tering top divisive clustering methods take input dissimilarity matrix objects bottom approach similar groups merged chapter clustering figure example single link clustering using city block distance pairs distance apart get merged ﬁrst resulting dendrogram based figure alpaydin figure generated agglomdemo hierarchical clustering profiles figure hierarchical clustering applied yeast gene expression data rows permuted according hierarchical clustering scheme average link agglomerative clustering order bring similar rows close together clusters induced cutting average linkage tree certain height figure generated hclustyeastdemo step top approach groups split using various different criteria give details note agglomerative divisive clustering heuristics optimize well deﬁned objective function thus hard assess quality clustering produce formal sense furthermore always produce clustering input data even data structure random noise later section discuss probabilistic version hierarchical clustering solves problems hierarchical clustering algorithm agglomerative clustering initialize clusters singletons initialize set clusters available merging repeat pick similar clusters merge arg min create new cluster mark unavailable mark available foreach update dissimilarity matrix clusters available merging figure illustration single linkage complete linkage average linkage 
[clustering, hierarchical, clustering, agglomerative, clustering] agglomerative clustering starts groups initially containing one object step merges two similar groups single group containing data see algorithm pseudocode since picking two similar clusters merge takes time steps algorithm total running time however using priority queue reduced log see manning details large common heuristic ﬁrst run means takes time apply hierarchical clustering estimated cluster centers merging process represented binary tree called dendrogram shown figure initial groups objects leaves bottom ﬁgure every time two groups merged join tree height branches represents dissimilarity groups joined root tree top represents group containing data cut tree given height induce clustering given size example cut tree figure height get clustering discuss issue choose height number clusters complex example shown figure show gene expression data cut tree figure certain height get clusters shown figure actually three variants agglomerative clustering depending deﬁne dissimilarity groups objects give quite different results shown chapter clustering single link complete link average link figure hierarchical clustering yeast gene expression data single linkage complete linkage average linkage figure generated hclustyeastdemo hierarchical clustering figure give details single link single link clustering also called nearest neighbor clustering distance two groups deﬁned distance two closest members group min see figure tree built using single link clustering minimum spanning tree data tree connects objects way minimizes sum edge weights distances see note merge two clusters connect together two closest members clusters adds edge corresponding nodes guaranteed lightest weight edge joining two clusters two clusters merged never considered cannot create cycles consequence actually implement single link clustering time whereas variants take time complete link complete link clustering also called furthest neighbor clustering distance two groups deﬁned distance two distant pairs max see figure single linkage requires single pair objects close two groups considered close together regardless similarity members group thus clusters formed violate compactness property says observations within group similar particular deﬁne diameter group largest dissimilarity members max see single linkage produce clusters large diameters complete linkage represents opposite extreme two groups considered close observations union relatively similar tend produce clusterings small diameter compact clusters average link practice preferred method average link clustering measures average distance pairs avg number elements groups see figure average link clustering represents compromise single complete link clustering tends produce relatively compact clusters relatively far apart however since chapter clustering involves averaging change measurement scale change result contrast single linkage complete linkage invariant monotonic transformations since leave relative ordering 
[clustering, hierarchical, clustering, divisive, clustering] divisive clustering starts data single cluster recursively divides cluster two daughter clusters top fashion since ways split group items groups hard compute optimal split various heuristics used one approach pick cluster largest diameter split two using means medoids algorithm called bisecting means algorithm steinbach repeat desired number clusters used alternative regular means also induces hierarchical clustering another method build minimum spanning tree dissimilarity graph make new clusters breaking link corresponding largest dissimilarity actually gives results single link agglomerative clustering yet another method called dissimilarity analysis macnaughton smith follows start single cluster containing data measure average dissimilarity remove dissimilar object put cluster arg max continue move objects stopping criterion met speciﬁcally pick point move maximizes average dissimilarity minimizes average dissimilarity arg max continue negative ﬁnal result split two daughter clusters recursively call algorithm node tree example might choose split node whose average dissimilarity highest whose maximum dissimilarity diameter highest continue process average dissimilarity within cluster threshold clusters singletons divisive clustering less popular agglomerative clustering two advantages first faster since split constant number levels takes time second splitting decisions made context seeing data whereas bottom methods make myopic merge decisions hierarchical clustering 
[clustering, hierarchical, clustering, choosing, number, clusters] difficult choose right number clusters since hierarchical clustering algorithm always create hierarchy even data completely random choosing means hope visible gap lengths links dendrogram represent dissimilarity merged groups natural clusters unnatural clusters course real data gap might hard detect section present bayesian approach hierarchical clustering nicely solves problem 
[clustering, hierarchical, clustering, bayesian, hierarchical, clustering] several ways make probabilistic models produce results similar hierarchical clustering williams neal castro lau green present one particular approach called bayesian hierarchical clustering heller ghahra mani algorithmically similar standard bottom agglomerative clustering takes comparable time whereas several techniques referenced much slower however uses bayesian hypothesis tests decide clusters merge rather computing similarity groups points hoc way hypothesis tests closely related calculations required inference dirichlet process mixture model see furthermore input model data matrix dissimilarity matrix algorithm let represent data let set datapoints leaves substree step compare two trees see merged new tree deﬁne merged data let merged otherwise probability merge given prior probability merge computed using bottom algorithm described turn likelihood terms data assumed come model hence data assumed generated tree independently two terms already computed bottom process consequently quantities need decide trees merge see algorithm pseudocode assuming uniform ﬁnished cut tree points chapter clustering algorithm bayesian hierarchical clustering initialize compute repeat pair clusters compute find pair highest merge probability merge delete clusters merged connection dirichlet process mixture models section establish connection bhc dpmms turn give algorithm compute prior probabilities note marginal likelihood dpmm summing partitions given set possible partitions probability partition number clusters partition number points cluster partition points cluster partition number points one show heller ghahramani computed bhc algorithm similar given except fact sums partitions consistent tree number tree consistent partitions exponential number data points balanced binary trees obviously subset possible partitions way use bhc algorithm compute lower bound marginal likelihood data dpmm furthermore interpret algorithm greedily searching exponentially large space tree consistent partitions ﬁnd best ones given size step position compute node children equal probability cluster coming dpmm relative partitions consistent current tree computed follows initialize leaf build tree internal node compute left right children clustering datapoints features data set single linkage complete linkage average linkage bhc synthetic newsgroups spambase digits fglass table purity scores various hierarchical clustering schemes applied various data sets synthetic data real features newsgroups extracted newsgroups dataset binary features spambase binary features digits cedar buffalo digits binarized features fglass forensic glass dataset real features source table heller ghahramani used kind permission katherine heller learning hyper parameters model two free parameters hyper parameters prior parameters heller ghahramani show one back propagate gradients form tree thus perform empirical bayes estimate hyper parameters experimental results heller ghahramani compared bhc traditional agglomerative clustering algo rithms various data sets terms purity scores results shown table see bhc much better methods datasets except forensic glass one figure visualizes tree structure estimated bhc agglomerative hierarchical clustering ahc newsgroup data using beta bernoulli model bhc tree clearly superior look colors leaves represent class labels figure zoom top nodes two trees bhc splits clusters concerning sports clusters concerning cars space ahc keeps sports cars merged together although sports cars fall rec newsgroup heading opposed space comes sci newsgroup heading bhc clustering still seems reasonable borne quantitative purity scores bhc also applied gene expression data good results savage 
[clustering, clustering, datapoints, features] far concentrating clustering datapoints datapoint often described multiple features might interested clustering well describe methods chapter clustering 
[clustering, clustering, datapoints, features, newsgroups, bayesian, hierarchical, clustering] figure hierarchical clustering applied documents newsgroups red rec autos blue rec sport baseball green rec sport hockey magenta sci space top average linkage hierarchical clustering bottom bayesian hierarchical clustering leaves labeled color according newsgroup document came see bayesian method results clustering consistent labels used model ﬁtting source figure heller ghahramani used kind permission katherine heller clustering datapoints features data game team play car space nasa baseball pitch hit nhl hockey round car dealer drive space nasa orbit data car baseball engine pitcher boston ball car player space quebec jet boston vehicle dealer driver team game hockey figure zoom top nodes trees figure bayesian method average linkage show probable words per cluster number documents cluster also given source figure heller ghahramani used kind permission katherine heller 
[clustering, clustering, datapoints, features, biclustering] clustering rows columns known biclustering coclustering widely used bioinformatics rows often represent genes columns represent conditions also used collaborative ﬁltering rows represent users columns represent movies variety hoc methods biclustering proposed see madeira oliveira review present simple probabilistic generative model based kemp see also sheng related approach idea associate row column latent indicator assume data iid across samples across features within block parameters row cluster column cluster rather using ﬁnite number clusters rows columns use dirchlet process inﬁnite relational model discuss section model using collapsed gibbs sampling behavior model illustrated figure data form iff animal feature animals represent whales bears horses etc features represent properties habitat jungle tree coastal anatomical properties teeth quadrapedal behavioral properties swims eats meat etc model using bernoulli likelihood data discovered animal clusters feature clusters example discovered bicluster represents fact mammals tend aquatic features 
[clustering, clustering, datapoints, features, multi-view, clustering] problem biclustering object row belong one cluster intuitively object multiple roles assigned different clusters depending chapter clustering killer whale blue whale humpback seal walrus dolphin antelope horse giraffe zebra deer monkey gorilla chimp hippo elephant rhino grizzly bear polar bear ﬂippers strain teeth swims arctic coastal ocean water hooves long neck horns hands bipedal jungle tree bulbous body shape slow inactive meat teeth eats meat hunter ﬁerce walks quadrapedal ground figure illustration biclustering show animal clusters feature clusters original data matrix shown partitioned according discovered clusters figure kemp used kind permission charles kemp figure illustration multi view clustering views column partitions ﬁrst view clusters row partitions second view clusters third view clusters number views partitions inferred data rows within colored block assumed generated iid however column different distributional form useful modeling discrete continuous data figure guan used kind permission jennifer corresponding dgm subset features use example animal dataset may want group animals basis anatomical features mammals warm blooded reptiles basis behavioral features predators prey present model capture phenomenon model indepen dently proposed shafto mansinghka call crosscat cross categorization guan cui call non parametric multi clust see also rodriguez ghosh similar model idea partition columns features groups views indexes clustering datapoints features features use dirichlet process prior allows grow automatically partition columns view call partition rows using illustrated figure let cluster row belongs view finally partitioned rows columns generate data assume rows columns within block iid deﬁne model precisely follows see figure dgm data binary use beta prior likelihood reduces beta beta counts number features column view row cluster similarly counts many features model easily extended kinds data replacing beta bernoulli say gaussian gamma gaussian model discussed guan mansinghka approximate map estimation done using stochastic search shafto approximate inference done using variational bayes guan gibbs sampling mansinghka hyper parameter likelihood usually set non informative way results sensitive two parameters since controls number column partitions controls number row partitions hence robust technique infer hyper parameters using also speeds convergence mansinghka figure illustrates model applied binary data containing animals features ﬁgures shows approximate map partition ﬁrst partition columns contains taxonomic features bones warm blooded lays eggs etc divides animals birds reptiles amphibians mammals invertebrates second partition columns contains features treated noise apparent structure except single row labeled frog third partition columns contains ecological features like dangerous carnivorous lives water etc divides animals prey land predators sea predators air predators thus animal row belong different dependence shown since dependence values cardinality words number row partitions need specify number views indexed depends number column partitions clusters chapter clustering lives lakes amphibian rodent tall fish slimy horns hooves feline roars fins webbed feet eats nuts smooth lives trees large lives cold climates ferocious dangerous carnivore predator lives water flies long eats leaves eats animals lives grass eats fish lives hot climates leopard alligator python seal dolphin frog jellyfish octopus penguin finch seagull owl eagle dragonfly bat grasshopper ant bee sheep monkey iguana ostrich bones lays eggs warm blooded mammal squawks beak tongue green spinal cord lizard antennae flippers paws large brain tail furry eats mice eats rodents snout brown makes loud noises teeth feet smart travels groups leopard sheep seal dolphin monkey bat alligator iguana frog python finch ostrich seagull owl penguin eagle grasshopper ant bee jellyfish octopus dragonfly frog figure map estimate produced crosscat system applied binary data matrix animals rows features columns see text details source figure shafto used kind permission vikash mansingkha cluster depending set features considered uncertainty partitions handled sampling interesting compare model standard inﬁnite mixture model standard model represent density ﬁxed sized vectors cannot cope since way handle irrelevant noisy redundant features contrast crosscat multi clust system robust irrelevant features partition cluster rows using relevant features note however need separate background model since everything modelled using mechanism useful since one person noise another person signal indeed symmetry may explain multi clust outperformed sparse mixture model approach law experiments reported guan 
[graphical, model, structure, learning, introduction] seen graphical models used express conditional independence assump tions variables chapter discuss learn structure graphical model want compute graph structure represented adjacency matrix discussed section two main applications structure learning knowl edge discovery density estimation former requires graph topology whereas latter requires fully speciﬁed model main obstacle structure learning number possible graphs exponential number nodes simple upper bound thus full posterior prohibitively large even could afford compute could even store seek appropriate summaries posterior summary statistics depend task goal knowledge discovery may want compute posterior edge marginals plot corresponding graph thickness edge represents conﬁdence presence setting threshold generate sparse graph useful visualization purposes see figure goal density estimation may want compute map graph argmax cases ﬁnding globally optimal graph take exponential time use dis crete optimization methods heuristic search however case trees ﬁnd globally optimal graph structure quite efficiently using exact methods discuss section density estimation goal worth considering whether would appropriate learn latent variable model capture correlation visible variables via set latent common causes see chapters models often easier learn perhaps importantly applied prediction purposes much efficiently since require performing inference learned graph potentially high treewidth downside models latent factors often unidentiﬁable hence hard interpret course combine graphical model structure learning latent variable learning show later chapter cases want model observed correlation variables instead want model causal structure behind data predict effects manipulating variables much challenging task brieﬂy discuss chapter graphical model structure learning bible christian god jesus case fact children government religion computer science university course earth orbit evidence human world jews law president rights state war gun israel launch nasa shuttle space lunar moon mission solar figure part relevance network constructed news data shown figure show edges whose mutual information greater equal maximum pairwise clarity graph cropped show subset nodes edges figure generated relevancenetworknewsgroupdemo section 
[graphical, model, structure, learning, structure, learning, knowledge, discovery] since computing map graph exact posterior edge marginals general computa tionally intractable chickering section discuss quick dirty methods learning graph structures used visualize one data resulting models constitute consistent joint probability distributions cannot used prediction cannot even formally evaluated terms goodness nevertheless methods useful hoc tool one data visualization toolbox view speed simplicity 
[graphical, model, structure, learning, structure, learning, knowledge, discovery, relevance, networks] relevance network way visualizing pairwise mutual information multiple random variables simply choose threshold draw edge node node threshold gaussian case log correlation coefficient see exercise essentially visualizing known covariance graph section method quite popular systems biology margolin used visualize interaction genes trouble biological examples hard non biologists understand let instead illustrate idea using natural language text figure gives example visualize words newsgroup dataset figure results seem intuitively reasonable however relevance networks suffer major problem graphs usually dense since variables dependent variables even thresholding mis example suppose directly inﬂuences directly inﬂuences form components signalling cascade non zero vice versa edge relevance network indeed pairs structure learning knowledge discovery aids health baseball fans games hit league players bible god jesus orbit bmw engine honda cancer disease patients car dealer drive driver gun insurance oil card graphics video case children food christian mission religion science computer course data solar problem evidence medicine disk files memory scsi space display image server doctor help dos windows format earth mars moon email phone power msg fact hockey team ftp human vitamin water number version nhl puck season win government jews president rights war law system israel research state launch shuttle lunar mac studies nasa satellite program question university software technology world figure dependency network constructed news data show edges regres sion weight markov blankets estimated penalized logistic regression undirected edges represent cases directed edge found directions figure schmidt used kind permission mark schmidt connected better approach use graphical models represent conditional independence rather dependence example conditionally independent given edge consequently graphical models usually much sparser relevance networks hence useful way visualizing interactions multiple variables 
[graphical, model, structure, learning, structure, learning, knowledge, discovery, dependency, networks] simple efficient way learn graphical model structure independently sparse full conditional distributions called dependency network heckerman chosen variables constitute inputs node markov blanket visualize resulting sparse graph advantage relevance networks redundant variables selected inputs use kind sparse regression classiﬁcation method cpd heckerman uses classiﬁcation regression trees meinshausen buhlmann use regularized linear regression wainwright use regularized logistic regression see depnetfit code dobra uses bayesian variable selection etc meinshausen chapter graphical model structure learning buhlmann discuss theoretical conditions regularized linear regression recover true graph structure assuming data generated sparse gaussian graphical model figure shows dependency network learned newsgroup data using regularized logistic regression penalty parameter chosen bic many words present estimated markov blankets represent fairly natural associations aids disease baseball fans bible god bmw car cancer patients etc however esti mated statistical dependencies seem less intuitive baseball windows bmw christian gain insight look sparsity pattern also values regression weights example incoming weights ﬁrst words aids children disease fact health president research baseball christian drive games god government hit memory players season software windows bible car card christian fact god jesus orbit program religion version bmw car christian engine god government help windows cancer disease medicine patients research studies words italic red negative weights represents dissociative relationship example model reﬂects baseball windows unlikely combination turns weights negative negative positive zero model addition visualizing data dependency network used inference however algorithm use gibbs sampling repeatedly sample nodes missing values full conditionals unfortunately product full conditionals general constitute representation valid joint distribution heckerman output gibbs sampler may meaningful nevertheless method sometimes give reasonable results much missing data useful method data imputation gelman raghunathan addition method used initialization technique complex structure learning methods discuss 
[graphical, model, structure, learning, learning, tree, structures] rest chapter focus learning fully speciﬁed joint probability models used density estimation prediction knowledge discovery since problem structure learning general graphs hard chickering start considering special case trees trees special learn structure efficiently disuscs learned tree use efficient exact inference discussed section learning tree structures 
[graphical, model, structure, learning] figure surgical intervention based happens value use pearl calculus notation verb write denote event set causal model used make inferences form different making inferences form understand difference conditioning interventions conditioning observations difference seeing consider node dgm smoke otherwise yellow stained ﬁngers otherwise observe yellow ﬁngers licensed infer probably smoker since nicotine causes yellow stains however intervene paint ﬁngers yellow longer licensed infer since disrupted normal causal mechanism thus one way model perfect interventions use graph surgery represent joint distri bution dgm cut arcs coming nodes set intervention see figure example prevents information ﬂow nodes intervened sent back parents perform surgery perform probabilistic inference resulting mutilated graph usual way reason effects interventions state formally follows theorem manipulation theorem pearl spirtes compute sets nodes perform surgical intervention nodes use standard probabilistic inference mutilated graph generalize notion perfect intervention adding interventions explicit action nodes graph result like inﬂuence diagram except utility nodes lauritzen dawid called augmented dag pearl learning causal dags figure illustration simpson paradox figure generated simpsonsparadoxgraph deﬁne cpd anything want also allow action affect multiple nodes called fat hand intervention reference someone trying change single component system electronic circuit accidently touching multiple components thereby causing various side effects see eaton murphy way model using augmented dags 
[graphical, model, structure, learning, directed, undirected, tree] continuing need discuss issue whether use directed undirected trees directed tree single root node deﬁnes joint distribution follows deﬁne example figure see choice root matter models equivalent make model symmetric preferable use undirected tree represented follows edge marginal node marginal example figure see equivalence directed representation let cancel terms get thus tree represented either undirected directed graph number parameters hence complexity learning course inference representations undirected representation symmetric useful structure learning directed representation convenient parameter learning chapter graphical model structure learning 
[graphical, model, structure, learning, chow-liu, algorithm, ﬁnding, tree, structure] using equation write log likelihood tree follows log log stjk log stjk number times node state node state number times node state rewrite counts terms empirical distribution stjk emp emp setting mles becomes log emp log emp mutual information given empirical distribution emp log emp emp emp since ﬁrst term equation independent topology ignore learning structure thus tree topology maximizes likelihood found computing maximum weight spanning tree edge weights pairwise mutual informations called chow liu algorithm chow liu several algorithms ﬁnding max spanning tree mst two best known prim algorithm kruskal algorithm implemented run log time number edges number nodes see sedgewick wayne details thus overall running time log ﬁrst term cost computing sufficient statistics figure gives example method action applied binary newsgroups data shown figure tree arbitrarily rooted node representing email connections learned seem intuitively reasonable 
[graphical, model, structure, learning, finding, map, forest] since trees number parameters safely used maximum likelihood score model selection criterion without worrying overﬁtting however sometimes may want forest rather single tree since inference forest much faster tree run belief propagation tree forest parallel mle criterion never choose omit edge however use marginal likelihood penalized likelihood bic optimal solution may forest give details marginal likelihood case learning tree structures aids baseball hit bible bmw cancer car dealer engine honda card graphics video case children christian computer course data disease disk drive memory system display server doctor dos scsi driver earth god email ftp phone oil evidence fact question fans files format windows food msg water image games jesus religion government power president rights state war gun health insurance medicine help hockey nhl human israel jews launch law league lunar mac mars patients studies mission moon nasa number orbit satellite solar vitamin software players problem program space puck research science season shuttle technology university team version world win figure mle tree newsgroup data figure schmidt used kind permission mark schmidt topologically equivalent tree produced using chowliutreedemo section explain compute marginal likelihood dag using dirichlet prior cpts resulting expression written follows log log score counts sufficient statistics node parents score deﬁned equation suppose allow dags one parent following heckerman let associate weight edge score score score score parents note weights might negative unlike mle case edge weights aways non negative correspond mutual information rewrite objective follows log score score last term trees ignore thus ﬁnding probable tree amounts ﬁnding maximal branching corresponding weighted directed graph found using algorithm gabow chapter graphical model structure learning scoring function prior likelihood equivalent terms explained sec tion score score score score hence weight matrix symmetric case maximal branching maximal weight forest apply slightly modiﬁed version mst algorithm ﬁnd edwards see let graph positive negative edge weights let graph obtained omitting negative edges cannot reduce total weight ﬁnd maximum weight forest ﬁnding mst connected component running kruskal algorithm directly need ﬁnd connected components explicitly 
[graphical, model, structure, learning, mixtures, trees] single tree rather limited expressive power later chapter discuss ways learn general graphs however resulting graphs expensive inference interesting alternative learn mixture trees meila jordan mixture component may different tree topology like unsupervised version tan classiﬁer discussed section mixture trees using step compute responsibilities cluster data point step use weighted version chow liu algorithm see meila jordan details fact possible create inﬁnite mixture trees integrating possible trees remarkably done time using matrix tree theorem allows perform exact bayesian inference posterior edge marginals etc however tractable use inﬁnite mixture inference hidden nodes see meila jaakkola details 
[graphical, model, structure, learning, learning, dag, structures] section discuss compute functions constrained dag often called bayesian network structure learning section assume missing data hidden variables called complete data assumption simplicity focus case variables categorical cpds tables although results generalize real valued data kinds cpds linear gaussian cpds presentation based part heckerman although follow notation section particular let value node case number states node let tck number parent combinations possible conditioning cases notational simplicity often assume nodes number states also let dim degree fan node 
[graphical, model, structure, learning, learning, dag, structures, markov, equivalence] section discuss fundamental limits ability learn dag structures data learning dag structures 
[graphical, model, structure, learning, exact, structural, inference] section discuss compute exact posterior graphs ignoring issue computational tractability deriving likelihood assuming missing data cpds tabular likelihood written follows cat cat tck tck tck tck number times node state parents state technically counts depend graph structure drop notation deriving marginal likelihood course choosing graph maximum likelihood always pick fully connected graph subject acyclicity constraint since maximizes number parameters avoid overﬁtting choose graph maximum marginal likelihood magic bayesian occam razor penalize overly complex graphs compute marginal likelihood need specify priors parameters make two standard assumptions first assume global prior parameter independence means learning dag structures second assume local prior parameter independence means turns assumtions imply prior row cpt must dirichlet geiger heckerman dir given assumptions using results section write marginal likelihood dag follows cat dir tck tck ijk score tck tck vector counts sufficient statistics node parents score local scoring function deﬁned score say marginal likelihood decomposes factorizes according graph structure setting prior set hyper parameters tck tempting use jeffreys prior form tck equation however turns violates property called likelihood equivalence sometimes considered desirable property says markov equivalent section marginal likelihood since essentially equivalent models geiger heckerman proved complete graphs prior satisﬁes likelihood equivalence parameter independence dirichlet prior pseudo counts form tck called equivalent sample size prior joint probability dis tribution called bde prior stands bayesian dirichlet likelihood equivalent chapter graphical model structure learning derive hyper parameters graph structures geiger heckerman invoked additional assumption called parameter modularity says node parents assumption always derive node graph marginalizing pseudo counts equation typically prior distribution assumed uniform possible joint conﬁgura tions case tck since thus sum pseudo counts entries cpt get total equivalent sample size called bdeu prior stands uniform widely used prior learning bayes net structures advice setting global tuning parameter see silander simple worked example give simple worked example neapolitan suppose binary nodes following data cases suppose interested two possible graphs disconnected graph empirical counts node node bdeu prior prior set use bdeu prior ﬁnd hence posterior probabilites uniform graph prior example analysis college plans dataset consider interesting example heckerman consider data set collected sewell shah measured variables might inﬂuence decision high school students whether attend college speciﬁcally variables follows learning dag structures figure two probable dags learned sewell shah data source heckerman used kind permission david heckerman sex male female ses socio economic status low lower middle upper middle high intelligence quotient discretized low lower middle upper middle high parental encouragment low high college plans yes variables measured wisconsin high school seniors possible joint conﬁgurations heckerman computed exact posterior possible node dags except ones sex ses parents children prior probability graphs set based domain knowledge used bdeu score although said results robust range top two graphs shown figure see probable one approximately probability mass posterior extremely peaked tempting interpret graph terms causality see section particular seems socio economic status parental encouragment causally inﬂuence decision whether college makes sense also sex inﬂuences college plans indirectly parental encouragement also makes sense however direct link socio economic status seems surprising may due hidden common cause section examine dataset allowing presence hidden variables algorithm suppose know total ordering nodes compute distribution parents node independently without risk introducing directed cycles chapter graphical model structure learning simply enumerate possible subsets ancestors compute marginal likelihoods return best set parents node get algorithm cooper herskovits handling non tabular cpds cpds linear gaussian replace dirichlet multinomial model normal gamma model thus derive different exact expression marginal likelihood see geiger heckerman details fact easily combine discrete nodes gaussian nodes long discrete nodes always discrete parents called conditional gaussian dag compute marginal likelihood closed form see bottcher dethlefsen details general case everything except gaussians cpts need approximate marginal likelihood simplest approach use bic approximation form log log 
[graphical, model, structure, learning, scaling, larger, graphs] main challenge computing posterior dags many possible graphs precisely robinson showed number dags nodes satisﬁes following recurrence base case solving recurrence yields following sequence etc view enormous size hypothesis space generally forced use approximate methods review approximating mode posterior use dynamic programming ﬁnd globally optimal map dag markov equiv alence koivisto sood silander myllmaki unfortunately method takes time space making intractable beyond nodes indeed general problem ﬁnding globally optimal map dag provably complete chickering consequently must settle ﬁnding locally optimal map dag common method greedy hill climbing step algorithm proposes small changes current graph adding deleting reversing single edge moves neigh boring graph increases posterior method stops reaches cal maximum important method proposes local changes graph make method efficient using regularization select parents schmidt case need approximate marginal likelhood discuss longer list values found http www research att com njas sequences interest ingly number dags equal number matrices whose eigenvalues positive real numbers mckay learning dag structures aids baseball fans league bible bmw cancer health car dealer driver engine honda insurance oil card graphics case children food water christian jesus religion computer science course data problem disease medicine patients disk dos files mac memory program space display doctor help version windows drive format scsi earth image mars moon email phone power evidence fact msg studies god ftp vitamin number games hockey nhl puck season team win government jews law president rights war gun hit human israel research state launch lunar software server video mission orbit solar nasa satellite shuttle players question university system technology world figure locally optimal dag learned newsgroup data figure schmidt used kind permission mark schmidt since enables change marginal likelihood hence posterior computed constant time assuming cache sufficient statistics one two terms equation cancel computing log bayes factor log log initialize search best tree found using exact methods discussed section speed restrict search adds edges part markov blankets estimated dependency network schmidt figure gives example dag learned way newsgroup data use techniques multiple random restarts increase chance ﬁnding good local maximum also use sophisticated local search methods genetic algorithms simulated annealing structure learning approximating functions posterior goal knowledge discovery map dag misleading reasons discussed section better approach compute probability edge present probability path exactly using dynamic programming koivisto parviainen koivisto unfortunately methods take time general case making intractable graphs chapter graphical model structure learning nodes approximate method sample dags posterior compute fraction times edge path pair standard way draw samples use metropolis hastings algorithm section use local proposal greedy search madigan raftery faster mixing method use collapsed sampler suggested friedman koller exploits fact total ordering nodes known select parents node independently without worrying cycles discussed section summing possible choice parents marginalize part problem sample total orders ellis wong also use order space collapsed mcmc time parallel tempering mcmc algorithm 
[graphical, model, structure, learning, learning, dag, structure, latent, variables, approximating, marginal, likelihood, missing, data] simplest approach use standard structure learning methods fully visible dags approximate marginal likelihood section discussed monte carlo methods approximating marginal likelihood however usually slow use inside search models mention faster deterministic approximations bic approximation simple approximation use bic score given bic log log dim dim number degrees freedom model map estimate however bic score often severely underestimates true marginal likelihood chickering heckerman resulting selecting overly simple models learning dag structure latent variables cheeseman stutz approximation present better method known cheeseman stutz approximation cheese man stutz ﬁrst compute map estimate parameters using denote expected sufficient statistics data case discrete variables ﬁll hidden variables expectation use exact marginal likelihood equation ﬁlled data however comparing equation see value exponentially smaller since sum values correct ﬁrst write log log log log apply bic approximation last two terms log log log dim log dim log log putting altogether get log log log log ﬁrst term computed plugging ﬁlled data exact marginal likelihood second term involves exponential sum thus matching dimensionality left hand side computed using inference algorithm ﬁnal term computed plugging ﬁlled data regular likelihood variational bayes even accurate approach use variational bayes algorithm recall section key idea make following factorization assumption hidden variables case step update step update corresponding variational free energy provides lower bound log marginal likelihood beal ghahramani shown bound much better approximation true log marginal likelihood estimated slow annealed importance sampling procedure either bic fact one prove variational bound always accurate turn always accurate bic chapter graphical model structure learning male ses sex low low high high high ses high sex male female male female ses low low high high high ses sex ses low low low low high high high high low high low high low high low high low low high high low low high high yes ses figure probable dag single binary hidden variable learned sewell shah data map estimates cpt entries shown nodes source heckerman used kind permission david heckerman example college plans revisited let revisit college plans dataset section recall ignore possibility hidden variables direct link socio economic status map dag heckerman decided see would happen introduced hidden variable made parent ses representing hidden common cause also considered variant points ses cases considered dropping none one ses edges varied number states hidden node thus computed approximate posterior different models using approximation probable model found shown figure times likely best model containing hidden variable also times likely second probable model hidden variable posterior peaked results suggests indeed hidden common cause underlying socio economic status parents children examining cpt entries see ses likely high takes value interpret mean hidden variable represents parent quality possibly genetic factor note however arc ses reversed without changing structures graph thus without affecting likelihood underscores difficulty interpreting hidden variables interestingly hidden variable model conditional independence assumptions amongst visible variables probable visible variable model pos sible distinguish hypotheses merely looking empirical conditional independencies data basis constraint based approach structure learning pearl verma spirtes instead adopting bayesian approach takes parsimony account conditional independence discover learning dag structure latent variables possible existence hidden factors basis much scientiﬁc everday human reasoning see griffiths tenenbaum discussion 
[graphical, model, structure, learning, learning, dag, structure, latent, variables, structural] one way perform structural inference presence missing data use standard search procedure deterministic stochastic use methods section estimate marginal likelihood however approach efficient marginal likelihood decompose missing data approximations example use approximation vbem approximation perform inference every neighboring model evaluate quality single move friedman thiesson presents much efficient approach called structural algorithm basic idea instead ﬁtting candidate neighboring graph ﬁlling data ﬁll data use ﬁlled data evaluate score neighbors although might bad approximation marginal likelihood good enough approximation difference marginal likelihoods different models need order pick best neighbor precisely deﬁne data ﬁlled using model map parameters deﬁne modiﬁed bic score follows scorebic log log dim log log included log prior graph parameters one show friedman pick graph increases bic score relative expected data also increase score actual data scorebic scorebic scorebic scorebic convert algorithm proceed follows first initialize graph set parameters ﬁll data using current parameters practice means ask expected counts particular family perform inference using current model know counts need precompute much faster evaluate bic score neighbors using ﬁlled data pick best neighbor reﬁt model parameters ﬁll data repeat increased speed may choose reﬁt model every steps since small changes structure hopefully invalidate parameter estimates ﬁlled data much one interesting application learn phylogenetic tree structure observed leaves dna protein sequences currently alive species goal infer topology tree values missing internal nodes many classical algorithms task see durbin one uses sem discussed friedman another interesting application method learn sparse mixture models barash friedman idea one hidden variable specifying cluster choose whether add edges possible feature thus features dependent cluster independent see also law chapter graphical model structure learning figure part hierarchical latent tree learned newsgroup data figure harmeling williams used kind permission stefan harmeling different way perform task using regular set bits one per feature free change across data cases 
[graphical, model, structure, learning, learning, dag, structure, latent, variables, discovering, hidden, variables] section introduced hidden variable hand ﬁgured local topology ﬁtting series different models computing one best marginal likelihood automate process figure provides one useful intuition hidden variable true model children likely densely connected suggest following heuristic elidan perform structure learning visible domain look structural signatures sets densely connected nodes near cliques introduce hidden variable connect nodes near clique let structural sort details unfortunately technique work well since structure learning algorithms biased ﬁtting models densely connected cliques another useful intuition comes clustering ﬂat mixture model also called latent class model discrete latent variable provides compressed representation children thus want create hidden variables high mutual information children one way create tree structured hierarchy latent variables explain small set children zhang calls hierarchical latent class model propose greedy local search algorithm learn structures based adding deleting hidden nodes adding deleting edges etc note learning optimal latent learning dag structure latent variables program earth launch lunar mars mission moon nasa orbit satellite shuttle solar space technology card computer data disk display dos drive driver email files format ftp graphics help image mac memory number phone problem research science scsi server software system university version video windows case children course evidence fact government gun human israel law power president question rights state war world baseball fans games hit hockey league nhl players puck season team win aids cancer disease doctor food health medicine msg patients studies vitamin water bmw car dealer engine honda insurance oil bible christian jesus religion god jews figure partially latent tree learned newsgroup data note words multiple meanings get connected different latent variables representing different topics example word win refer sports context represented microsoft windows context represented figure choi used kind permission jin choi tree hard roch recently harmeling williams proposed faster greedy algorithm learning models based agglomerative hierarchical clustering rather details give example system learn figure shows part latent forest learned newsgroup data algorithm imposes constraint latent node exactly two children speed reasons nevertheless see interpretable clusters arising example figure shows separate clusters concerning medicine sports religion provides alternative lda topic models section added advantage inference latent trees exact takes time linear number nodes alternative approach proposed choi observed data constrained leaves method starts chow liu tree observed data adds hidden variables capture higher order dependencies internal nodes results much compact models shown figure model also better predictive accuracy approaches mixture models trees observed data forced leaves interestingly one show method recover exact latent tree structure providing data generated tree see chapter graphical model structure learning figure google rephil model leaves represent presence absence words internal nodes represent clusters occuring words concepts nodes binary cpds noisy model contains million word nodes million latent cluster nodes million edges used kind permission brian milch choi details note however approach unlike zhang harmeling williams requires cardinality variables hidden observed furthermore observed variables gaussian hidden variables must gaussian also 
[graphical, model, structure, learning, learning, dag, structure, latent, variables, case, study, google’s, rephil] section describe huge dgm called rephil automatically learned data model widely used inside google various purposes including famous adsense system model structure shown figure leaves binary nodes represent presence absence words compounds new york city text document query latent variables also binary represent clusters occuring words cpds noisy since leaf nodes representing words many parents means edge augmented hidden variable specifying link activated link active parent cannot turn child similar model proposed independently singliar hauskrecht parameter learning based hidden activation status edge needs inferred meek heckerman structure learning based old neuroscience original system called phil developed georges harik noam shazeer published patent method apparatus learning probabilistic generative model text ﬁled rephil probabilistically sound version method developed uri lerner summary based notes brian milch also works google adsense google system matching web pages content appropriate ads automatic way extracting semantic keywords web pages keywords play role analogous words users type searching latter form information used google adwords system details secret levy gives overview learning dag structure latent variables idea nodes ﬁre together wire together implement run inference check cluster word cluster cluster pairs frequently turn together add edge parent child link signiﬁcantly increase probability child links activated often pruned initialize one cluster per document corresponding set semantically related phrases merge clusters explains top words vice versa also discard clusters used rarely model trained billion text snippets search queries takes several weeks even parallel distributed computing architecture resulting model contains million word nodes million latent cluster nodes million links model including many cluster cluster dependencies longest path graph length model quite deep exact inference model obviously infeasible however note leaves since words occur given query leaves analytically removed shown exercise also prune unlikely hidden nodes following strongest links words parents get candidate set concepts perform iterative conditional modes ﬁnd good set local maxima step icm node sets value probable state given values neighbors markov blanket continues reaches local maximum repeat process times random starting conﬁgurations google made run milliseconds 
[graphical, model, structure, learning, learning, dag, structure, latent, variables, structural, equation, models] structural equation model bollen special kind directed mixed graph sec tion possibly cyclic cpds linear gaussian bidirected edges represent correlated gaussian noise models also called path diagrams sems widely used especially economics social science common interpret edge directions terms causality directed cycles interpreted terms feedback loops see pearl however model really way specifying joint gaussian show nothing inherently causal discuss causality section deﬁne sem series full conditionals follows rewrite model matrix form follows hence joint distribution given draw arc lower triangular graph acyclic addition diagonal model equivalent gaussian dgm discussed section models called recursive diagonal draw bidirected chapter graphical model structure learning 
[graphical, model, structure, learning, learning, causal, dags] causal models models predict effects interventions manipulations system example electronic circuit diagram implicitly provides compact encoding happen one removes given component cuts wire causal medical model might predict continue smoke likely get lung cancer hence cease smoking less likely get lung cancer causal claims inherently stronger yet useful purely associative claims people smoke often lung cancer causal models often represented dags pearl although somewhat contro versial dawid explain causal interpretation dags show use dag causal reasoning finally brieﬂy discuss learn structure causal dags detailed description topic found pearl koller friedman 
[graphical, model, structure, learning, learning, causal, dags, causal, interpretation, dags] section deﬁne directed edge dag mean directly causes manipulate change known causal markov assumption course deﬁned word causes cannot appealing dag lest end cyclic deﬁnition see dawid disussion point also assume relevant variables included model unknown confounders reﬂecting hidden common causes called causal sufficiency assumption known confounders added model although one sometimes use mixed directed graphs section way avoid model confounders explicitly assuming willing make causal markov causal sufficiency assumptions use dags answer causal questions key abstraction perfect intervention represents act setting variable known value say setting real world example perfect intervention gene knockout experiment gene silenced need notational convention distinguish observing chapter graphical model structure learning 
[graphical, model, structure, learning, using, causal, dags, resolve, simpson’s, paradox] section assume know causal dag causal reasoning applying separation mutilated graph section give example show causal reasoning help resolve famous paradox known simpon paradox simpson paradox says statistical relationship two variables reversed including additional factors analysis example suppose cause say taking drug makes effect say getting better likely yet condition gender patient ﬁnd taking drug makes effect less likely females males seems impossible rules probability perfectly possible event space condition completely different event space condition table numbers shows concrete example pearl combined male female total rate total rate total rate total chapter graphical model structure learning table numbers see visual representation paradox given figure line goes right shows effect axis increases cause axis increases however dots represent data females crosses represent data males within subgroup see effect decreases increase cause clear effect real still counter intuitive reason paradox arises interpreting statements causally using proper causal reasoning performing calculations statement drug causes recovery whereas data merely tell contradiction observing positive evidence since males females take drug male recovery rate higher regardless drug thus equation imply equation nevertheless left practical question use drug seems like know patient gender use drug soon discover male female stop using obviously conclusion ridiculous answer question need make assumptions explicit suppose reality modeled causal dag figure compute causal effect need adjust condition confounding variable necessary backdoor path via need check relationship value separately make sure relationship affected value suppose value taking drug harmful show taking drug harmful overall proof follows pearl first assumptions figure see drugs effect gender using law total probability learning causal dags treatment gender recovery treatment blood pressure recovery figure two different models uses illustrate simpson paradox gender confounder blood pressure caused similarly since every term equation less corresponding term equation conclude model figure correct administer drug since reduces probability effect consider different version example suppose keep data interpret something affected blood pressure see figure case longer assume proof breaks may positive negaitve true model figure condition assessing effect since backdoor path case structure conditioning might block one causal pathways words comparing patients post treatment blood pressure value may mask effect one two pathways drug operates bring recovery thus see different causal assumptions lead different causal conclusions hence different courses action raises question whether learn causal model data discuss issue 
[graphical, model, structure, learning, learning, causal, dag, structures] section discuss ways learn causal dag structures chapter graphical model structure learning learning observational data section discussed various methods learning dag structures observational data natural ask whether methods recover true dag structure used generate data clearly even inﬁnite data optimal method identify dag markov equivalence section identify pdag partially directed acylic graph complete dag structure dags markov equivalent likelihood several algorithms greedy equivalence search method chickering consistent estimators pdag structure sense identify true markov equivalence class sample size goes inﬁnity assuming observe variables however also assume generating distribution faithful generating dag means conditional indepence properties exactly captured graphical structure means cannot properties due particular settings parameters zeros regression matrix graphically explicit reason faithful distribution also called stable distribution suppose assumptions hold learn pdag instead recovering full graph focus causal analog edge marginals computing magnitude causal effect one node another say know dag using techniques described pearl dag unknown compute lower bound effect follows maathuis learn equivalence class pdag data enumerate dags equivalence class apply pearl calculus compute magnitude causal effect dag ﬁnally take minimum effects lower bound usually computationally infeasible compute dags equivalence class fortunately one needs able identify local neighborhood esimated efficiently described maathuis technique called ida short intervention calculus dag absent maathuis technique applied yeast gene expression data gene knockout data used estimate ground truth effect single gene deletions remaining genes algorithm applied unperturbed wild type samples used rank order likely targets genes method precision recall set low substantially rival variable selection methods lasso elastic net slightly chance learning interventional data want distinguish dags within equivalence class need use interven tional data certain variables set consequences measured example dataset figure proteins signalling pathway perturbed phosphorylation status measured using technique called ﬂow cytometry sachs straightforward modify standard bayesian scoring criteria marginal likelihood bic score handle learning mixed observational experimental data learning causal dags bcamp erk akt jnk psitect akt inh pma mek raf pkc pip plcy pip pka present missing int edge figure design matrix consisting data points rows measuring status using ﬂow cytometry proteins columns different experimental conditions data discretized states low black medium grey high white proteins explicitly controlled using activating inhibiting chemicals directed graphical model representing dependencies various proteins blue circles various experimental interventions pink ovals inferred data plot edges dotted edges believed exist nature discovered algorithm false negative solid edges true positives light colored edges represent effects intervention source figure eaton murphy ﬁgure reproduced using code http www ubc murphyk software bdagl index html chapter graphical model structure learning compute sufficient statistics cpd parameter skipping cases node set intervention cooper yoo example using tabular cpds modify counts follows tck set justiﬁcation cases node set force sampled usual mechanism cases ignored inferring parameter mod iﬁed scoring criterion combined standard structure learning algorithms geng discusses methods choosing interventions perform reduce posterior uncertainty quickly possible form active learning preceeding method assumes interventions perfect reality experimenters rarely control state individual molecules instead inject various stimulant inhibitor chemicals designed target speciﬁc molecules may side effects model quite simply adding intervention nodes dag learning larger augmented dag structure constraint edges intervention nodes edges regular nodes back intervention nodes figure shows augmented dag learned interventional ﬂow cytometry data depicted figure particular plot median graph includes edges computed using exact algorithm koivisto turns example median model exactly structure optimal map model argmax computed using algorithm koivisto sood silander myllmaki 
[graphical, model, structure, learning, learning, undirected, gaussian, graphical, models] learning structured undirected graphical models easier learning dag structure need worry acyclicity hand harder learning dag structure since likelihood decompose see section precludes kind local search methods greedy search mcmc sampling used learn dag structures cost evaluating neighboring graph high since reﬁt model scratch way incrementally update score model section discuss several solutions problem context gaussian random ﬁelds undirected gaussian graphical models ggm consider structure learning discrete undirected models section 
[graphical, model, structure, learning, learning, undirected, gaussian, graphical, models, mle, ggm] discussing structure learning need discuss parameter estimation task computing mle non decomposable ggm called covariance selection dempster equation log likelihood written log det learning undirected gaussian graphical models precision matrix empirical covariance matrix notational simplicity assume already estimated one show gradient given however enforce constraints structural zeros positive deﬁnite former constraint easy enforce latter somewhat challenging albeit still convex constraint one approach add penalty term objective leaves positive deﬁnite cone approach used ggmfitminfunc see also dahl another approach use coordinate descent method described hastie implemented ggmfithtf yet another approach use iterative proportional ﬁtting described section however ipf requires identifying cliques graph hard general interestingly one show mle must satisfy following property covariance pair connected edge must match empirical covariance addition deﬁnition ggm precision pair connected must say positive deﬁnite matrix completion since retains many entries possible corresponding edges graph subject required sparsity pattern corresponding absent edges remaining entries ﬁlled maximize likelihood let consider worked example hastie use following adjacency matrix representing cyclic structure following empirical covariance matrix mle given see ggmfitdemo code reproduce numbers constrained elements free elements correspond absent edges highlighted 
[graphical, model, structure, learning, learning, undirected, gaussian, graphical, models, graphical, lasso] discuss one way learn sparse grf structure exploits fact correspondence zeros precision matrix absent edges graph suggests learn sparse graph structure using objective encourages zeros precision matrix analogy lasso see section one deﬁne following penalized nll log det chapter graphical model structure learning lambda nedges lambda nedges lambda nedges lambda nedges figure sparse ggms learned using graphical lasso applied ﬂow cytometry data figure generated ggmlassodemo norm matrix called graphical lasso glasso although objective convex non smooth non differentiable penalty constrained must positive deﬁnite matrix several algorithms proposed optimizing objective yuan lin banerjee duchi although arguably simplest one friedman uses coordinate descent algorithm similar shooting algorithm lasso see ggmlassohtf implementation see also mazumder hastie recent version algorithm example let apply method ﬂow cytometry dataset sachs discretized version data shown figure use original continuous data however ignoring fact data sampled intervention figure illustrate graph structures learned sweep large value represent range plausible hypotheses connectivity proteins worth comparing dag learned figure dag advantage easily model interventional nature data disadvantage cannot model feedback loops known exist biological pathway see discussion schmidt murphy note fact show many ugms one dag incidental could easily use bic pick best ugm conversely learning undirected gaussian graphical models could easily display several dag structures sampled posterior 
[graphical, model, structure, learning, learning, undirected, gaussian, graphical, models, bayesian, inference, ggm, structure] although graphical lasso reasonably fast gives point estimate structure furthermore model selection consistent meinshausen meaning cannot recover true graph even would preferable integrate parameters perform posterior inference space graphs compute extract summaries posterior posterior edge marginals dags section discuss note situation analogous chapter discussed variable selection section discussed bayesian variable selection integrated regression weights computed marginal inclusion probabilities section discussed methods based regularization dichotomy presenting opposite order graph decomposable use conjugate priors compute marginal likelihood closed form dawid lauritzen furthermore efficiently identify decomposable neighbors graph thomas green set legal edge additions removals means perform relatively efficient stochastic local search approximate posterior see giudici green armstrong scott carvalho however restriction decomposable graphs rather limiting one goal knowledge discovery since number decomposable graphs much less number general undirected graphs authors looked bayesian inference ggm structure non decomposable case dellaportas wong jones methods cannot scale large models use expensive monte carlo approximation marginal likelihood atay kayis massam lenkoski dobra suggested using laplace approxmation requires computing map estimate parameters wishart prior roverato lenkoski dobra used iterative proportional scaling algorithm speed kiiveri hara takimura ﬁnd mode however slow since requires knowing maximal cliques graph hard general moghaddam much faster method proposed particular modify gradient based methods section ﬁnd map estimate algorithms need know cliques graph speedup obtained using diagonal laplace approximation accurate bic essentially cost plus lack restriction decomposable graphs enables fairly fast stochastic search methods used approximate mode approach signiﬁcantly outperfomed graphical lasso terms predictive accuracy structural recovery comparable computational cost number decomposable graphs nodes follows armstrong divide numbers number undirected graphs ﬁnd ratios see decomposable graphs form vanishing fraction total hypothesis space chapter graphical model structure learning 
[graphical, model, structure, learning, learning, undirected, gaussian, graphical, models, handling, non-gaussian, data, using, copulas] graphical lasso variants inhertently limited data jointly gaussian rather severe restriction fortunately method generalized handle non gaussian still continuous data fairly simple fashion basic idea estimate set univariate monotonic transformations one per variable resulting transformed data jointly gaussian possible say data belongs nonparametric normal distribution nonparanormal distribution liu equivalent family gaussian copulas klaassen wellner details estimate transformations empirical cdf variable found liu transforming data compute correlation matrix apply glasso usual way one show various assumptions consistent estimator graph structure representing assumptions original distribution liu 
[graphical, model, structure, learning, learning, undirected, discrete, graphical, models] problem learning structure ugms discrete variables harder gaussian case computing partition function needed parameter estimation complexity comparable computing permanent matrix general intractable jerrum contrast gaussian case computing requires computing matrix determinant since stochastic local search tractable general discrete ugms mention possible alternative approaches tried 
[graphical, model, structure, learning, learning, undirected, discrete, graphical, models, graphical, lasso, mrfs/crfs] possible extend graphical lasso idea discrete mrf crf case however set parameters associated edge graph use graph analog group lasso see section example consider pairwise crf ternary nodes node edge potentials given assume begins constant term account offset contains crf reduces mrf note may choose set stjk weights ensure identiﬁability although also taken care prior shown exercise learn sparse structure minimize following objective log log learning undirected discrete graphical models baseball games league players bible christian god jesus question car dealer drive engine card driver graphics problem system video windows case course evidence fact government human law number power rights state world children president religion war computer data email program science software university memory research space disk files display image dos mac scsi earth orbit format ftp help phone jews fans hockey team version nhl season win gun health insurance israel launch moon nasa shuttle technology figure mrf estimated newsgroup data using group regularization isolated nodes plotted figure schmidt used kind permission mark schmidt norm common choices explained sec tion method crf structure learning ﬁrst suggested schmidt use regularization learning structure binary mrfs proposed lee although objective convex costly evaluate since need perform inference compute gradient explained section true also mrfs therefore use optimizer make many calls objective function gradient projected quasi newton method schmidt addition use approximate inference convex belief propagation section compute approximate objective gradient quickly another approach apply group lasso penalty pseudo likelihood discussed section much faster since inference longer required hoeﬂing tibshirani figure shows result applying procedure newsgroup data indicates presence word document model mrf chapter graphical model structure learning xbamp xbamp amp amp amp orxg sulqnohu dlq udvv figure water sprinkler dgm corresponding binary cpts stand true false 
[graphical, model, structure, learning, learning, undirected, discrete, graphical, models, thin, junction, trees] far concerned learning sparse graphs necessarily low treewidth example grid sparse treewidth means models learn may intractable use inference purposes defeats one two main reasons learn graph structure ﬁrst place reason knowledge discovery various attempts learn graphical models bounded treewidth bach jordan srebro elidan gould shahaf also known thin junction trees exact problem general hard alternative approach learn model low circuit complexity gogate poon domingos models may high treewidth exploit context speciﬁc independence determinism enable fast exact inference see darwiche 
[graphical, model, structure, learning, exercises] exercise causal reasoning sprinkler network consider causal network figure let represent true represent false suppose perform perfect intervention make grass wet probability sprinkler suppose perform perfect intervention make grass dry probability sprinkler suppose perform perfect intervention make clouds turn seeding probability sprinkler 
[latent, variable, models, discrete, data, introduction] chapter concerned latent variable models discrete data bit vectors sequences categorical variables count vectors graph structures relational data etc models used analyze voting records text document collections low intensity images movie ratings etc however mostly focus text analysis reﬂected terminology since dealing many different kinds data need precise notation keep things clear modeling variable length sequences categorical variables symbols tokens words document let represent identity word document number possible words vocabulary assume known length document number documents often ignore word order resulting bag words reduced ﬁxed length vector counts histogram use denote number times word occurs document note count matrix often large sparse since typically many documents words occur given document cases might multiple different bags words bags text words bags visual words correspond different channels types features denote irl number responses means single token bag length case write brevity every channel single token write ﬁxed size response vector case design matrix sparse example social science surveys could response person multi choice question goal build joint probability models using latent variables capture correlations try interpret latent variables provide compressed representation data provide overview approaches section going detail later sections towards end chapter consider modeling graphs relations also represented sparse discrete matrices example might want model graph papers mycite papers denote relations reserving symbol categorical data text associated nodes chapter latent variable models discrete data 
[latent, variable, models, discrete, data, distributed, state, lvms, discrete, data] section summarize variety possible approaches constructing models form bags tokens vectors tokens vectors integer counts 
[latent, variable, models, discrete, data, distributed, state, lvms, discrete, data, mixture, models] simplest approach use ﬁnite mixture model chapter associates single discrete latent variable every document number clusters use discrete prior cat variable length documents deﬁne probability cluster generates word value known topic vector topic word distribution likelihood form cat induced distribution visible data given cat generative story encodes follows document pick topic call word pick word consider sophisticated generative models later chapter ﬁxed set categorical observations use different topic matrix output variable cat unsupervised analog naive bayes classiﬁcation also model count vectors sum known use multinomial sum unknown use poisson class conditional density give poi case poi distributed state lvms discrete data 
[latent, variable, models, discrete, data, distributed, state, lvms, discrete, data, exponential, family, pca] unfortunately ﬁnite mixture models limited expressive power ﬂexible model use vector real valued continuous latent variables similar factor analysis pca models chapter pca use gaussian prior form gaussian likelihood form method certainly applied discrete count data indeed method known latent semantic analysis lsa latent semantic indexing lsi deerwester dumais landauer exactly equivalent applying pca term document count matrix better method modeling categorical data use multinoulli multinomial distribu tion change likelihood cat weight matrix softmax function ﬁxed number categorical responses use cat weight matrix response variable model called categorical pca illustrated figure see section discussion counts use multinomial model poisson model poi exp models examples exponential family pca epca collins mohamed unsupervised analog glms corresponding induced distribution visible variables form fitting model tricky due lack conjugacy collins proposed coordinate ascent method alternates estimating regarded degenerate version computes point estimate step problem degenerate approach prone overﬁtting since number latent variables proportional number datacases welling true algorithm would marginalize latent variables way categorical pca using variational discussed section general models one use mcmc mohamed chapter latent variable models discrete data figure two lvms discrete data circles scalar nodes ellipses vector nodes squares matrix nodes categorical pca multinomial pca 
[latent, variable, models, discrete, data, distributed, state, lvms, discrete, data, lda, mpca] epca quantity represents natural parameters exponential family times convenient use dual parameters example multinomial dual parameter probability vector whereas natural parameter vector log odds want use dual parameters need constrain latent variables live appropriate parameter space case categorical data need ensure latent vector lives dimensional probability simplex avoid confusion epca denote latent vector case natural prior latent variables dirichlet dir typically set set encourage sparse shown figure count vector whose total sum known likelihood given model called multinomial pca mpca buntine buntine jakulin see figure since assuming seen form matrix factorization count matrix note use denote parameter vector rather since impose constraints corresponding marginal distribution form dir unfortunately integral cannot computed analytically variable length sequence known length use cat distributed state lvms discrete data called latent dirichlet allocation lda blei described much greater detail lda thought probabilistic extension lsa latent quantities non negative sum one contrast lsa negative makes interpetation difficult predecessor lda known probabilistic latent semantic indexing plsi hofmann uses model computes point estimate document similar epca rather integrating thus plsi prior modify lda handle ﬁxed number different categorical responses follows cat called user rating proﬁle urp model marlin simplex factor model bhattacharya dunson 
[latent, variable, models, discrete, data, distributed, state, lvms, discrete, data, gap, model, non-negative, matrix, factorization] consider modeling count vectors constrain sum observed case latent variables need non negative denote ensured using prior form likelihood given poi called gap gamma poisson model canny see figure buntine jakulin shown gap model conditioned ﬁxed reduces mpca model follows since set poisson random variables conditioned sum becomes multinomial distribution see ross set gap model recover method known non negative matrix factorization nmf lee seung shown buntine jakulin nmf probabilistic generative model since specify proper prior furthermore algorithm proposed lee seung another degenerate algo rithm suffers overﬁtting procedures gap model overcome problems given buntine jakulin encourage sparse modify prior spike gamma type prior follows probability spike called conditional gamma poisson model buntine jakulin simple modify gibbs sampling handle kind prior although detail chapter latent variable models discrete data figure gaussian poisson gap model latent dirichlet allocation lda model 
[latent, variable, models, discrete, data, latent, dirichlet, allocation, lda, basics] mixture multinoullis every document assigned single topic drawn global distribution lda every word assigned topic drawn document speciﬁc distribution since document belongs adding conjugate priors parameters full model follows dir cat dir cat illustrated figure marginalize variables thereby creating identity word document represent discrete indicator continuous latent vector document topic vector latent dirichlet allocation lda word word word observed document generated document topic figure geometric interpretation lda topics words document white dots topic black dots point simplex source figure steyvers griffiths used kind permission tom griffiths direct arc following cpd mentioned introduction similar multinomial pca model proposed buntine turn closely related categorical pca gap nmf etc lda interesting geometric interpretation vector deﬁnes distribution words known topic document vector deﬁnes distribution topics model document admixture topics equivalently think lda form dimensionality reduction assuming usually case project point dimensional simplex normalized document count vector onto dimensional simplex illustrated figure words topics observed documents live simplex approximated living simplex spanned topic vectors lives simplex one advantage using simplex latent space rather euclidean space simplex handle ambiguity importance since natural language words often multiple meanings phenomomen known polysemy example play might refer verb play ball play coronet noun shakespeare play lda multiple topics generate word play shown figure reﬂecting ambiguity given word document compute thus infer likely topic looking word isolation might hard know sense word meant disambiguate looking words document particular given infer topic distribution document acts prior disambiguating illustrated figure show three documents tasa corpus ﬁrst document variety music related words suggest tasa corpus collection high school level english documents comprising million words chapter latent variable models discrete data word prob word prob word prob music literature play dance poem ball song poetry game play poet playing sing plays hit singing poems played band play baseball played literary games sang writers bat songs drama run dancing wrote throw piano poets balls playing writer tennis rhythm shakespeare home albert written catch musical stage field topic topic topic figure three topics related word play source figure steyvers griffiths used kind permission tom griffiths document bix beiderbecke age fifteen sat slope bluff overlooking mississippi river listening music coming passing riverboat music already captured heart well ear jazz bix beiderbecke already music lessons showed promise piano parents hoped might consider becoming concert pianist bix interested another kind music wanted play cornet wanted play jazz document simple reason periods really great theater whole western world many things come right time dramatists must right actors actors must right playhouses playhouses must right audiences must remember plays exist performed merely read even read play try perform put stage along soon play performed kind theatrical document jim game book jim reads book jim sees game one jim plays game jim likes game one game book helps jim comes house jim read game book boys see game two two boys play game boys play game two boys like game meg comes house meg jim read book see game three meg jim play game play figure three documents tasa corpus containing different senses word play grayed words ignored model correspond uninteresting stop words etc low frequency words source figure steyvers griffiths used kind permission tom griffiths latent dirichlet allocation lda put mass music topic number turn makes music interpretation play likely shown superscript second document interprets play theatrical sense third sports sense note crucial latent variable information ﬂow thus enabling local disambiguation use full set words 
[latent, variable, models, discrete, data, latent, dirichlet, allocation, lda, unsupervised, discovery, topics] one main purposes lda discover topics large collection corpus docu ments see figure example unfortunately since model unidentiﬁable interpertation topics difficult chang one approach known beled lda ramage exploits existence tags documents way ensure identiﬁability particular forces topics correspond tags learns distribution words tag make results easier interpret 
[latent, variable, models, discrete, data, latent, dirichlet, allocation, lda, quantitatively, evaluating, lda, language, model] order evaluate lda quantitatively treat language model probability distribution sequences words course good language model since ignores word order looks single words unigrams interesting compare lda unigram based models mixtures multinoullis plsi simple language models sometimes useful information retrieval purposes standard way measure quality language model use perplexity deﬁne perplexity perplexity language model given stochastic process deﬁned perplexity cross entropy two stochastic processes deﬁned lim log cross entropy hence perplexity minimized case model predict well true distribution approximate stochastic process using single long test sequence composed multiple documents multiple sentences complete end sentence markers call approximation becomes accurate sequence gets longer provided process stationary ergodic cover thomas deﬁne empirical distribution approximation stochastic process emp collated company formerly known touchstone applied science associates known questar assessment inc www questarai com stochastic process one deﬁne joint distribution arbitrary number random variables think natural language stochastic process since generate inﬁnite stream words chapter latent variable models discrete data case cross entropy becomes emp log perplexity becomes perplexity emp emp see geometric mean inverse predictive probabilities usual deﬁnition perplexity jurafsky martin case unigram models cross entropy term given log number documents number words document hence perplexity model given perplexity emp exp log intuitively perplexity mesures weighted average branching factor model predic tive distribution suppose model predicts symbol letter word whatever equally likely perplexity symbols likely others model correctly reﬂects perplexity lower course never reduce perplexity entropy underlying stochastic process perplexity lda key quantity predictive distribution model possible words implicitly conditioned training set lda approximated plugging posterior mean estimate approximately integrating using mean ﬁeld inference see wallach accurate way approximate predictive likelihood figure compare lda several simple unigram models namely map estima tion multinoulli map estimation mixture multinoullis plsi performing map estimation dirichlet prior used lda model metric perplexity equation data subset trec corpus containing newswire articles unique terms see lda signiﬁcantly outperforms methods latent dirichlet allocation lda number topics perplexity figure perplexity number topics trec corpus various language models based figure blei figure generated bleildaperplexityplot figure lda unrolled documents collapsed lda integrate 
[latent, variable, models, discrete, data, latent, dirichlet, allocation, lda, fitting, using, collapsed, gibbs, sampling] straightforward derive gibbs sampling algorithm lda full conditionals follows exp log log dir dir however one get better performance analytically integrating chapter latent variable models discrete data dirichlet distribution sampling discrete approach ﬁrst suggested griffiths steyvers example collapsed gibbs sampling figure shows variables fully correlated however sample one time explain first need notation let ivk number times word assigned topic document let ivk number times word document assigned topic let ivk number times word assigned topic document let ivk number times word occurs document observed let number words assigned topic finally let number words document observed derive marginal prior applying equation one show cat dir similar reasoning one show cat dir equations using fact derive full conditional deﬁne ivk ivk except compute summing locations document except also let see word document assigned topic based often word generated topic ﬁrst term also often topic used document second term given equation implement collapsed gibbs sampler follows randomly assign topic word sample new topic follows given word corpus decrement relevant counts based topic assigned current word draw new topic equation update count matrices repeat algorithm made efficient since count matrices sparse 
[latent, variable, models, discrete, data, latent, dirichlet, allocation, lda, example] process illustrated figure small example two topics ﬁve words left part ﬁgure illustrates documents sampled lda model using latent dirichlet allocation lda river stream bank money loan river stream bank money loan figure illustration collapsed gibbs sampling applied small lda example documents containing variable number words drawn vocabulary words two topics white dot means word word assigned topic black dot means word assigned topic initial random assignment states sample posterior steps gibbs sampling source figure steyvers griffiths used kind permission tom griffiths money loan bank river stream bank example see ﬁrst document contains word bank times indicated four dots row bank column well various ﬁnancial terms right part ﬁgure shows state gibbs sampler iterations correct topic assigned token cases example document see word bank correctly assigned ﬁnancial topic based presence words money loan posterior mean estimate parameters given money loan bank river stream bank impressively accurate given training examples 
[latent, variable, models, discrete, data, latent, dirichlet, allocation, lda, fitting, using, batch, variational, inference] faster alternative mcmc use variational cannot use exact since exact inference intractable give details sequence version following blei use fully factorized mean ﬁeld approximation form dir cat follow usual mean ﬁeld recipe use bayes rule need take expectations prior ilk exp log log chapter latent variable models discrete data digamma function update obtained adding expected counts ilk step obtained adding expected counts normalizing ilk count version note step takes space store ilk much space efficient perform inference mpca version model works counts take space big savings documents long contrast collapsed gibbs sampler must work explicitly variables focus approximating write shorthand use fully factorized mean ﬁeld approximation form dir new step becomes ivk ivk exp log new step becomes ivk version modify algorithm use instead infer parameters well latent variables two advantages first setting encourage sparse section second able generalize online learning setting discuss new posterior approximation becomes dir dir update ivk changes following ivk exp log log latent dirichlet allocation lda algorithm batch lda input estimate using multinomial mixtures initialize counts converged step expected sufficient statistics document estep ivk step topic function estep initialize repeat old word topic ivk exp old normalize ivk old thresh also step becomes ivk normalization required since updating pseudcounts overall algorithm summarized algorithm 
[latent, variable, models, discrete, data, latent, dirichlet, allocation, lda, fitting, using, online, variational, inference] bathc version step clearly takes time number mean ﬁeld updates typically slow many documents reduced using stochastic gradient descent section perform online variational inference explain derive online version following hoffman perform step usual way compute variational parameters treating expected sufficient statistics single data case whole data set statistics finally make chapter latent variable models discrete data algorithm online variational bayes lda input initialize randomly set step size pick document estep new ivk new documents seen log scale perplexity batch online online figure test perplexity number training documents batch online lda figure hoffman used kind permission david blei partial update variational parameters putting weight new estimate weight old estimate step size decays time equation overall algorithm summarized algorithm practice use mini batches explained section hoffman used batch size figure plots perplexity test set size number analyzed documents steps data drawn english wikipedia ﬁgure shows online variational inference much faster offline inference yet produces similar results 
[latent, variable, models, discrete, data, latent, dirichlet, allocation, lda, determining, number, topics] choosing number topics standard model selection problem approaches taken use annealed importance sampling section approximate evidence wallach cross validation using log likelihood test set extensions lda use variational lower bound proxy log use non parametric bayesian methods teh 
[latent, variable, models, discrete, data, extensions, lda] figure categorical pca inputs exchangeable outputs vector nodes expanded key difference neural net information ﬂow via latent bottleneck layer work better conventional neural net output labels highly correlated even conditioning features problem frequently arises multi label classiﬁcation note could allow direct arc would require many parameters number labels large model small modiﬁcation variational algorithm section use model regression rather classiﬁcation perform step exactly modifying algorithm factor analysis reports method converges faster standard backpropagation also extend model prior mixture gaussians using input dependent means output gaussian corresponds mixture discriminative factor analysers fokoue zhou liu output categorical would yet unpublished model could call discriminative mixtures categorical factor analyzers 
[latent, variable, models, discrete, data, extensions, lda, correlated, topic, model] one weakness lda cannot capture correlation topics example document business topic reasonable expect ﬁnance topic occcur source problem use dirichlet prior problem dirichelt characterized mean vector strength parameter covariance ﬁxed rather free parameter one way around replace dirichlet prior logistic normal distribution categorical pca section model becomes dir cat cat known correlated topic model blei lafferty similar categorical pca slightly different see difference let marginalize ctm cat stochastic matrix contrast catpca cat unconstrained matrix fitting model tricky since prior longer conjugate multinomial likelihood however use variational methods section discussed bayesian multiclass logistic regression ctm case things even harder since categorical response variables hidden handle using additional mean ﬁeld approximation see blei lafferty details model one convert sparse precision matrix pruning low strength edges get sparse gaussian graphical model allows visualize correlation topics figure shows result applying procedure articles science magazine corpus contains documents words unique stop word low frequency removal nodes represent topics top words per topic listed inside font size reﬂects overall prevalence topic corpus edges represent signiﬁcant elements precision matrix chapter latent variable models discrete data wild type mutant mutations mutants mutation plants plant gene genes arabidopsis cell cycle activity cyclin regulation amino acids cdna sequence isolated protein gene disease mutations families mutation rna dna rna polymerase cleavage site cells cell expression cell lines bone marrow united states women universities students education science scientists says research people research funding support nih program surface tip image sample device laser optical light electrons quantum materials organic polymer polymers molecules volcanic deposits magma eruption volcanism mantle crust upper mantle meteorites ratios earthquake earthquakes fault images data ancient found impact million years ago africa climate ocean ice changes climate change cells proteins researchers protein found patients disease treatment drugs clinical genetic population populations differences variation fossil record birds fossils dinosaurs fossil sequence sequences genome dna sequencing bacteria bacterial host resistance parasite development embryos drosophila genes expression species forest forests populations ecosystems synapses ltp glutamate synaptic neurons neurons stimulus motor visual cortical ozone atmospheric measurements stratosphere concentrations sun solar wind earth planets planet carbon carbon dioxide methane water receptor receptors ligand ligands apoptosis proteins protein binding domain domains activated tyrosine phosphorylation activation phosphorylation kinase magnetic magnetic ﬁeld spin superconductivity superconducting physicists particles physics particle experiment surface liquid surfaces ﬂuid model reaction reactions molecule molecules transition state enzyme enzymes iron active site reduction pressure high pressure pressures core inner core brain memory subjects left task computer problem information computers problems stars astronomers universe galaxies galaxy virus hiv aids infection viruses mice antigen cells antigens immune response figure output correlated topic model topics applied articles science nodes represent topics probable phrases topic shown inside font size reﬂects overall prevalence topic see http www cmu edu lemur science interactive version model topics source figure blei lafferty used kind permission david blei 
[latent, variable, models, discrete, data, extensions, lda, dynamic, topic, model] lda topics distributions words assumed static cases makes sense allow distributions evolve smoothly time example article might use topic neuroscience written likely use words like nerve whereas written likely use words like calcium receptor reﬂects general trend neuroscience towards molecular biology one way model use dynamic logistic normal model illustrated figure particular assume topic distributions evolve according gaussian random walk map gaussian vectors probabilities via softmax function dir cat cat known dynamic topic model blei lafferty extensions lda figure dynamic topic model one perform approximate infernece model using structured mean ﬁeld method section exploits kalman smoothing algorithm section perform exact inference linear gaussian chain nodes see blei lafferty details figure illustrates typical output system applied years articles science top visualize top words speciﬁc topic seems related neuroscience year intervals bottom left plot probability speciﬁc words belonging topic bottom right list titles articles contained topic one interesting application model perform temporally corrected document trieval suppose look documents inheritance disease modern articles use words like dna older articles discovery dna may use terms heritable unit articles likely use topics similar ideas used perform cross language information retrieval see cimiano 
[latent, variable, models, discrete, data, extensions, lda, lda-hmm] lda model assumes words exchangeable clearly true simple way model sequential dependence words use hidden markov model hmm trouble hmms model short range dependencies cannot capture overall gist document hence generate syntactically correct sentences see table semantically plausible ones possible combine lda hmm create model called lda hmm griffiths chapter latent variable models discrete data neuroscience brain movement action right eye hand left muscle nerve sound movement eye right hand brain left action muscle sound experiment brain eye movement right left hand nerve vision sound muscle movement brain sound nerve active muscle left eye right nervous movement sound muscle active nerve stimulate ﬁber reaction brain response stimulate muscle sound movement response nerve frequency ﬁber active brain record nerve stimulate response muscle electrode active brain ﬁber potential respons record stimulate nerve muscle active frequency electrode potential study response stimulate record condition active potential stimulus nerve subject eye respons cell potential stimul neuron active nerve eye record abstract cell neuron response active brain stimul muscle system nerve receptor cell channel neuron active brain receptor muscle respons current neuron active brain cell response channel receptor synapse signal neuron nerve mental science hemianopsia migraine defence new phrenology synchronal flashing fireﬂies myoesthesis imageless thought acetylcholine physiology nervous system brain waves unit discharge cerebral cortex errorless discrimination learning pigeon temporal summation light vertebrate visual receptor hysteresis force calcium relation muscle gaba activated chloride channels secretory nerve endings figure part output dynamic topic model applied articles science show top words neuroscience topic time also show probability three words within topic time articles contained topic source figure blei lafferty used kind permission david blei model uses hmm states model function syntactic words however uses lda model content semantic words harder predict distinguished hmm state speciﬁes lda model used generate word rest time hmm generates word formally document model deﬁnes hmm states addition document lda model associated generate word semantic lda model topic speciﬁed otherwise generate word syntactic hmm model dgm shown figure cpds follows dir lda lda usual topic word matrix state word hmm emission matrix state state hmm transition matrix inference model done collapsed gibbs sampling analytically integrating continuous quantities see griffiths details results applying model lda topics hmm states combined brown tasa corpora shown table see hmm generally brown corpus consists documents word tokens part speech tags token extensions lda hmm hmm lda figure lda hmm model contrast approach study overall network activity control single cell parameters input resistance well time space constants parameters crucial excitability spariotemporal sic integration integrated architecture paper combines feed forward control error feedback adaptive control using neural networks words proof convergence require softassign algorithm return doubly stochastic matrix sinkhorn theorem guarantees instead matrix merely close doubly stochastic based reasonable metric aim construct portfolio maximal expected return given risk level time horizon simultaneously obeying institutional legally required constraints left graph standard experiment right training samples graph called guest graph called host graph figure function content words nips corpus distinguished lda hmm model graylevel indicates posterior probability assignment lda component black highest boxed word appears function word one sentence content word another sentence asterisked words low frequency treated single word type model source figure griffiths used kind permission tom griffiths chapter latent variable models discrete data blood body game heart land water drink ball trees classes picture story alcohol tree farmers government ﬁlm team image matter bottle farm state lens play blood forest farmers government light water story drugs ball heart trees land state eye matter stories drug game pressure forests crops federal lens molecules poem alcohol team body land farm public image liquid characters people lungs soil food local mirror particles poetry drinking baseball oxygen areas people act eyes gas character person players vessels park farming states glass solid author effects football arteries wildlife wheat national object substance poems marijuana player area farms laws objects temperature life body ﬁeld breathing rain corn department lenses changes poet use basketball said time new made would way see used years ﬁrst make came could day went may part great know found number good get called must kind small place little take old ﬁnd table upper row topics extracted lda model trained combined brown tasa corpora middle row topics extracted lda part lda hmm model bottom row topics extracted hmm part lda hmm model column represents single topic class words appear order probability topic class since classes give almost probability words list terminated words account probability mass source figure griffiths used kind permission tom griffiths responsible syntactic words lda semantics words hmm lda topics would get polluted function words see top ﬁgure words normally removed preprocessing model also help disambiguate word used syntactically semantically figure shows examples model applied nips corpus see roles words distinguished require algorithm return matrix verb maximal expected return noun principle part speech tagger could disambiguate two uses note lda hmm method fully unsupervised pos tags used sometimes word pos tag different senses left graph synactic role graph semantic role topic probabilistic models syntax semantics vast one tasa corpus untagged collection educational materials consisting documents word tokens words appearing fewer documents replaced asterisk punctuation included combined vocabulary size unique words nips stands neural information processing systems one top machine learning conferences nips corpus volumes contains documents extensions lda figure supervised lda discriminative lda space delve see jurafsky martin information 
[latent, variable, models, discrete, data, extensions, lda, supervised, lda] section discuss extensions lda handle side information various kinds beyond words generative supervised lda suppose variable length sequence words usual also class label predict many possible approaches direct mappings words class cases sentiment analysis get better performance ﬁrst performing inference try disambiguate meaning words example suppose goal determine document favorable review movie encounter phrase brad pitt excellent middle movie word excellent may lead think review positive clearly overall sentiment negative one way tackle problems build joint model form blei mcauliffe proposes approach called supervised lda class label generated topics follows ber sigm empirical topic distribution document ilk see figure illustration chapter latent variable models discrete data figure discriminative variants lda mixture experts aka lda double ring denotes node deterministic function parents mixture experts random effects dmr lda model using monte carlo run collapsed gibbs sampler step compute use input feature standard logistic regression package discriminative supervised lda alternative approach known discriminative lda lacoste julien shown figure discriminative model form change regular lda topic prior becomes input dependent follows cat stochastic matrix far assumed side information single categorical variable often high dimensional covariates example consider task image tagging idea represent correlated tags labels want predict given discuss several attempts extend lda generate tags given inputs simplest approach use mixture experts section multiple outputs like lda except replace dirichlet prior deterministic function input law called multinomial regression lda see figure eliminating deterministic cat usual way however law suggest alternative first unsupervised lda model based treat inferred data extensions lda multinomial logistic regression model mapping although fast ﬁtting lda unsupervised fashion necessarily result discriminative set latent variables discussed blei mcauliffe subtle problem model since deterministic function inputs effectively observed rendering hence tags independent words means observe value one tag inﬂuence others may explain results law show negligible improvement predicting tag independently one way induce correlations make random variable resulting model shown figure call random effects mixture experts typically assume gaussian prior cat recover correlated topic model possible extend model adding markovian dynamics variables called conditional topic random ﬁeld zhu xing closely related approach known dirichlet multinomial regression lda mimno mccallum shown figure identical standard lda except make function input exp matrix eliminating deterministic dir exp unlike law model allows information ﬂow tags via latent variant model corresponds bag discrete labels dir known labeled lda ramage case labels correspondence latent topics makes resulting topics much interpretable extension known partially labeled lda ramage allows label multiple latent sub topics model includes lda labeled lda multinomial mixture model special cases discriminative categorical pca alternative using lda expand categorical pca model inputs shown figure since latent space real valued use simple linear regression input hidden mapping hidden output mapping use traditional catpca cat model essentially probabilistic neural network one hidden layer shown figure exchangeable output handle variable numbers tags chapter latent variable models discrete data 
[latent, variable, models, discrete, data, lvms, graph-structured, data] figure adjacency matrix graph figure rows columns shown permuted show block structure also sketch stochastic block model generate graph figure kemp used kind permission charles kemp structure graph clusters communities second try predict links might occur future make friends summarize models proposed tasks related lda futher details approaches found goldenberg references therein 
[latent, variable, models, discrete, data, lvms, graph-structured, data, stochastic, block, model] figure show directed graph nodes apparent structure however look deeply see possible partition nodes three groups blocks connections nodes illustrated figure chapter latent variable models discrete data relational system sorted matrix figure examples graphs generated using stochastic block model different kinds connectivity patterns blocks abstract graph blocks represent ring dominance hierarchy common cause structure common effect structure figure kemp used kind permission charles kemp problem easier understand plot adjacency matrices figure shows matrix graph nodes original ordering figure shows matrix graph nodes permtuted ordering clear block structure make generative model block structured graphs follows first every node sample latent block cat probability choosing block second choose probability connecting group group pairs groups let denote probability come beta prior finally generate edge using following model ber called stochastic block model nowicki snijders figure illustrates model dgm figure illustrates model used cluster nodes example note quite different conventional clustering problem example see nodes block grouped together even though connections share property like connect nodes block receive connections nodes block figure illustrates power model generating many different kinds graph structure example social networks hierarchical structure modeled clustering people different social strata whereas others consist set cliques unlike standard mixture model possible model using exact latent variables become correlated however one use variational airoldi lvms graph structured data figure stochastic block model mixed membership stochastic block model collapsed gibbs sampling kemp etc omit details similar lda case kemp lifted restriction number blocks ﬁxed replacing dirichlet prior dirichlet process see section known inﬁnite relational model see section details features associated node make discriminative version model example deﬁning ber way combining feature vectors example could use concatenation elementwise product supervised lda overall model like relational extension mixture experts model 
[latent, variable, models, discrete, data, lvms, graph-structured, data, mixed, membership, stochastic, block, model] airoldi lifted restriction node belong one cluster replaced known mixed membership stochastic block model similar spirit fuzzy clustering soft clustering note former represents ontological uncertainty degree object belong cluster wheras latter represents epistemological uncertainty cluster object belong want combine epistemological ontological uncertainty compute detail generative process follows first node picks distribution blocks dir second choose probability connecting group group pairs groups third edge sample two discrete variables one direction cat cat finally generate edge using following model chapter latent variable models discrete data outcasts loyal opposition young turks waverers ambrose boniface mark winfrid elias basil simplicius berthold john bosco victor bonaventure amand louis albert ramuald peter gregory hugh figure likes graph sampson monks mixed membership monk one three groups figures airoldi used kind permission edo airoldi see figure dgm unlike regular stochastic block model node play different role depending connecting illustration consider data set widely used social networks analysis literature data concerns likes amongst group monks collected hand sampson sampson period months days era social media facebook social network people trivially small methods discussing made scale figure plots raw data figure plots monk see monk belong one three clusters known young turks outcasts loyal opposition however individuals notably monk belong two clusters sampson called monks waverers interesting see model recover kinds insights sampson derived hand one prevalent problem social network analysis missing data example may due fact person opportunity interact data available interaction opposed fact people want interact words absence evidence evidence absence model modifying observation model probability generate background model force model explain observed probability words robustify observation model allow outliers follows ber see airoldi details 
[latent, variable, models, discrete, data, lvms, graph-structured, data, relational, topic, model] many cases nodes network atttributes example nodes represent academic papers edges represent citations attributes include text document therefore desirable create model explain text link structure concurrently model predict links given text even vice versa relational topic model rtm chang blei one way lvms relational data figure dgm relational topic model simple extension supervised lda section response variable represents whether edge nodes modeled follows sigm recall empirical topic distribution document ilk see figure note important depend actual topics chosen topic distributions otherwise predictive performance good intuitive reason follows child treated another word similar since many words edges graph structure information get washed making child graph information inﬂuence choice topics directly one model manner similar slda see chang blei details method better predicting missing links simpler approach ﬁrst ﬁtting lda model using inputs logistic regression problem reason analogous superiority partial least squares section pca linear regression namely rtm learns latent space forced predictive graph structure words whereas lda might learn latent space useful predicting graph 
[latent, variable, models, discrete, data, lvms, relational, data] graphs used represent data represents relation amongst variables certain type friendship relationships people often multiple types objects multiple types relations example figure illustrates two relations one people people one people movies general deﬁne ary relation subset tuples appropriate types chapter latent variable models discrete data figure example relational data two types objects people movies one ary relation friends people people one ary function rates people movie age sex attributes unary functions people class sets types binary pairwise dyadic relation relation deﬁned pairs objects example seen relation people movies might represented set movies people seen either represent explicitly set seen bob starwars bob tombraider alice jaws implicitly using indicator function set seen bob starwars seen bob tombraider seen alice jaws relation two entities types represented binary function hence binary matrix also represented bipartite graph nodes two types becomes regular directed graph section however situations easily modelled graphs still modelled relations example might ternary relation say iff protein interacts protein chemical present modelled binary matrix give examples section making probabilistic models relational data called statistical relational learning getoor taskar one approach directly model relationship variables using graphical models known probabilistic relational modeling another approach use latent variable models discuss 
[latent, variable, models, discrete, data, lvms, relational, data, inﬁnite, relational, model] straightforward extend stochastic block model model relational data associate latent variable entity type deﬁne probability relation holding speciﬁc entities looking probability relation holding entities type example allow number clusters type unbounded using dirichlet pro cess model called inﬁnite relational model irm kemp essentially lvms relational data causes result affects complicates disrupts result manifestation affects process affects process associated causes manifestation affects affects process result affects result complicates manifestation interact process result affects process causes manifestation associated complicates affects causes manifestation complicates affects figure illustration ontology learned irm applied uniﬁed medical language system boxes represent concept clusters predicates belong cluster grouped together associated edges pertain links weight included figure kemp used kind permission charles kemp identical model name inﬁnite hidden relational model ihrm concurrently proposed model variational bayes collapsed gibbs sampling kemp rather algorithmic detail sketch interesting applications learning ontologies ontology refers organisation knowledge ontologies often built hand see russell norvig interesting try learn data kemp show done using irm data comes uniﬁed medical language system mccray deﬁnes semantic network concepts disease syndrome diagnostic procedure animal binary predicates affects prevents represent ternary relation set concepts set binary predicates result cube apply irm partition cube regions roughly homogoneous response system found concept clusters predicate clusters shown figure system learns example biological functions affect organisms since represents biological function cluster represents organism cluster represents affects cluster clustering based relations features also use irm cluster objects based relations features example kemp consider political dataset consisting countries binary chapter latent variable models discrete data joint membership igos joint membership ngos negative behavior negative communications accusations protests treaties conferences common bloc membership economic aid emigration military alliance sends tourists exports books exports brazil netherlands usa burma indonesia jordan egypt india israel china cuba poland ussr noncommunist western bloc constitutional govt free elections communist bloc communists totalitarian elitist high censorship free elections illiteracy domestic violence purges far rainfall religious books exports gnp govt education military personnel seaborne goods govt crisis delinquent neutral bloc assassinations govt revolution num religions intervening military censorship energy consumed telephone population defense threats gnp protests catholics aid taken popn density land area railroad length foreign students age country law ngos num languages aid taken female workers foreign mail sent protein diet investments arts ngos monarchy road length arable emigrants unemployed calories diet book translations figure illustration irm applied political data containing features pairwise interac tions top row partition countries clusters features clusters every second column labelled name corresponding feature small squares bottom clusters interaction types figure kemp used kind permission charles kemp predicates representing interaction types countries sends tourists economic aid features communist monarchy create binary dataset real valued features thresholded mean categorical variables dummy encoded data types represents countries represents interactions represents features two relations problem therefore combines aspects biclustering model ontology discovery model given multiple relations irm treats conditionally independent case results shown figure irm divides features clusters ﬁrst contains noncommunist captures one important aspects cold war era dataset also clusters countries clusters reﬂecting natural geo political groupings communist bloc predicates clusters reﬂecting similar relationships negative behavior accusations lvms relational data 
[latent, variable, models, discrete, data, lvms, relational, data, probabilistic, matrix, factorization, collaborative, ﬁltering] discussed section collaborative ﬁltering requires predicting entries matrix example rating user gave movie thus see kind relational learning problem one particular commercial importance much work area makes use data netﬂix made available competition particular large movie user ratings matrix provided full matrix would entries entries observed matrix extremely sparse addition data quite imbalanced many users rating fewer movies users rating movies validation set movie user pairs finally separate test set movie user pairs ranking known withheld contestants performance measure root mean square error true rating user movie prediction baseline system known cinematch rmse training set test set qualify grand prize teams needed reduce test rmse get test rmse less discuss basic methods used byt winning team since ratings drawn set tempting use categorical observation model however capture fact ratings ordered although could use ordinal observation model practice people use gaussian observation model simplicity one way make model better match data pass model predicted mean response sigmoid map interval salakhutdinov mnih alternatively make data better match gaussian model transforming data using aggarwal merugu could use irm task associating discrete latent variable user movie video deﬁning another example clustering also extend model generate side information attributes user movie see figure illustration another possibility replace discrete latent variables continuous latent variables however found see banerjee one obtains much better results using unconstrained real valued latent factors user movie use likelihood form good results discrete latent variables obtained datasets smaller netﬂix movielens eachmovie however datasets much easier predict less imbalance number reviews performed different users netﬂix users rated movies whereas others rated less chapter latent variable models discrete data figure visualization small relational dataset one relation likes user movie features movies genre users occupation figure used kind permission zhao factor vector factor vector freddy got fingered freddy jason half baked road trip sound music sophie choice moonstruck maid manhattan way runaway bride coyote ugly royal tenenbaums punch drunk love heart huckabees armageddon citizen kane waltons season stepmom julien donkey boy sister act fast furious wizard kill bill vol scarface natural born killers annie hall belle jour lost translation longest yard john malkovich catwoman figure dgm probabilistic matrix factorization visualization ﬁrst two factors pmf model estimated netﬂix challenge data movie plotted location speciﬁed left low brow humor horror movies half baked freddy jason right serious dramas sophie choice moonstruck top critically acclaimed independent movies punch drunk love heart huckabees bottom mainstream hollywood blockbusters armageddon runway bride wizard right middle axes figure koren used kind permission yehuda koren called probabilistic matrix factorization pmf salakhutdinov mnih see figure dgm intuition behind method user movie get embedded low dimensional continuous space see figure user close movie space likely rate highly best entries netﬂix competition used approach one form another pmf closely related svd particular missing data computing mle equivalent ﬁnding rank approximation however soon missing data problem becomes non convex shown winning entry actually ensemble different methods including pmf nearest neighbor methods etc lvms relational data epochs rmse pmf constrained pmf netflix baseline score svd millions parameters rmse plain biases implicit feedback temporal dynamics figure rmse validation set different pmf variants number passes data svd unregularized version pmf corresponds pmf corresponds pmfa corresponds version mean diagonal covariance gaussian prior learned data figure salakhutdinov mnih used kind permission ruslan salakhutdinov rmse test set quiz portion number parameters several different models plain baseline pmf suitably chosen biases adds offset terms implicit feedback temporal dynamics allows offset terms change time netﬂix baseline system achieves rmse grand prize required accuracy obtained september figure generated netflixresultsplot figure koren used kind permission yehuda koren srebro jaakkola standard svd methods cannot applied recall netﬂix challenge matrix observed straightforward way pmf model minimize overall nll log log user seen movie since non convex ﬁnd locally optimal mle since netﬂix data large million observed entries common use stochastic gradient descent section task gradient given error term stochastically sampling single movie user watched update takes following simple form learning rate update similar chapter latent variable models discrete data course maximizing likelihood results overﬁtting shown figure regularize imposing gaussian priors use new objective becomes log const deﬁned varying regularizers reduce effect overﬁtting shown figure ﬁnd map estimates using stochastic gradient descent also compute approximate posteriors using variational bayes ilin raiko use diagonal covariances priors penalize latent dimension different amount also use non zero means priors account offset terms optimizing prior parameters time model parameters way create adaptive prior avoids need search optimal values gives even better results shown figure turns much variation data explained movie speciﬁc user speciﬁc effects example movies popular types users users give low scores types movies model allowing user movie speciﬁc offset bias terms follows overall mean user bias movie bias interaction term equivalent applying pmf residual matrix gives much better results shown figure estimate terms using stochastic gradient descent estimated also allow bias terms evolve time reﬂect changing preferences users koren important since netﬂix competition test data recent training data figure shows allowing temporal dynamics help lot often also side information various kinds netﬂix competition entrants knew movies user rated test set even though know values ratings knew value dense matrix even test set user chooses rate movie likely seen turns means thought would like thus act rating reveals information conversely user chooses rate movie suggests knew would like data missing random see marlin zemel exploiting improve performance shown figure real problems information test set available however often know movies user watched declined restricted boltzmann machines rbms watch even rate called implicit feedback used useful side information another source side information concerns content movie movie genre list actors synopsis plot denoted features video case video treat dimensional bit vector one bit turned may also know features user denote cases know user clicked video may numerical rating modify model follows ber matrix matrix incorporate offset term appending usual way method computing approximate posterior online fashion using adf described stern implemented microsoft deployed predict click rates ads used bing unfortunately ﬁtting model positive binary data result prediction links since negative examples included better performance obtained one access set videos shown user one picked data form known impression log case use multinomial model instead binary model yang shown work much better binary model understand suppose presented choice action movie starring arnold schwarzenegger action movie starring vin diesel comedy starring hugh grant user picks arnold schwarzenegger learn like prefer action movies comedies also prefer schwarzenegger diesel informative knowing like schwarzenegger action movies 
[latent, variable, models, discrete, data, restricted, boltzmann, machines, rbms] far models proposed chapter representable directed graphical models models better represented using undirected graphs example boltzmann machine ackley pairwise mrf hidden nodes visible nodes shown figure main problem boltzmann machine exact inference intractable even approximate inference using gibbs sampling slow however suppose restrict architecture nodes arranged layers connections nodes within layer see figure model form number visible response variables number hidden variables plays role earlier chapter model known restricted boltzmann machine rbm hinton harmonium smolensky rbm special case product experts poe hinton called multiplying together set experts potential functions edge chapter latent variable models discrete data figure general boltzmann machine arbitrary graph structure shaded visible nodes partitioned input output although model actually symmetric deﬁnes joint density nodes restricted boltzmann machine bipartite structure note lack intra layer connections normalizing whereas mixture experts take convex combination normalized distributions intuitive reason poe models might work better mixture expert enforce constraint expert value care condition expert value multiplying experts together different ways create sharp distributions predict data satisﬁes speciﬁed constraints hinton teh example consider distributed model text given document might topics government maﬁa playboy multiply predictions topic together model may give high probability word berlusconi salakhutdinov hinton contrast adding together experts make distribution broader see figure typically hidden nodes rbm binary speciﬁes constraints active worth comparing directed models discussed mixture model one hidden variable represent using set bits restriction exactly one bit time called localist encoding since one hidden unit used generate response vector analogous hypothetical notion grandmother cells brain able recognize one kind object contrast rbm uses distributed encoding many units involved generating output models used vector valued hidden variables mpca lda epca also use distributed encodings main difference rbm directed two layer models hidden variables conditionally independent given visible variables posterior factorizes makes inference much simpler directed model since estimate silvio berlusconi current prime minister italy restricted boltzmann machines rbms visible hidden name reference binary binary binary rbm hinton gaussian binary gaussian rbm welling sutton categorical binary categorical rbm salakhutdinov multiple categorical binary replicated softmax undirected lda salakhutdinov hinton gaussian gaussian undirected pca marks movellan binary gaussian undirected binary pca welling sutton table summary different kinds rbm independently parallel feedforward neural network disadvantage training undirected models much harder discuss 
[latent, variable, models, discrete, data, restricted, boltzmann, machines, rbms, varieties, rbms] section describe various forms rbms deﬁning different pairwise potential functions see table summary special cases exponential family harmonium welling binary rbms common form rbm binary hidden nodes binary visible nodes joint distribution following form exp exp energy function weight matrix visible bias terms hidden bias terms parameters notational simplicity absorb bias terms weight matrix clamping dummy units setting note naively computing takes time reduce min time exercise using binary rbm posterior computed follows ber sigm symmetry one show generate data given hidden variables follows ber sigm chapter latent variable models discrete data write matrix vetor notation follows sigm sigm weights called generative weights since used generate observations weights called recognition weights since used recognize input equation see activate hidden node proportion much input vector looks like weight vector scaling factors thus hidden node captures certain features input encoded weight vector similar feedforward neural network categorical rbm extend binary rbm categorical visible variables using encoding number states deﬁne new energy function follows salakhutdinov salakhutdinov hinton full conditionals given cat sigm gaussian rbm generalize model handle real valued data particular gaussian rbm following energy function parameters model assumed data standardized variance compare gaussian information form exp see set thus mean given full conditionals needed inference restricted boltzmann machines rbms learning given sigm see visible unit gaussian distribution whose mean function hidden bit vector powerful models make variance depend hidden state also developed ranzato hinton rbms gaussian hidden units use gaussian latent variables gaussian visible variables get undirected version factor analysis however turns identical standard directed version marks movellan use gaussian latent variables categorical observed variables get undirected version categorical pca section salakhutdinov applied netﬂix collaborative ﬁltering problem found signiﬁcantly inferior using binary latent variables expressive power 
[latent, variable, models, discrete, data, restricted, boltzmann, machines, rbms, learning, rbms] section discuss ways compute parameter estimates rbms using gradient based optimizers common use stochastic gradient descent since rbms often many parameters therefore need trained large datasets addition standard use regularization technique often called weight decay context requires small change objective gradient discussed section deriving gradient using compute gradient modify equations section show generic latent variable maxent model context boltzmann machine one feature per edge gradient given write matrix vector form follows emp emp emp emp empirical distribution derive similar expression bias terms setting ﬁrst term gradient ﬁxed data case sometimes called clamped phase second term free sometimes called unclamped chapter latent variable models discrete data phase model expectations match empirical expectations two terms cancel gradient becomes zero learning stops algorithm ﬁrst proposed ackley main problem efficiently computing expectations discuss ways deriving gradient using present alternative way derive equation also applies energy based models first marginalize hidden variables write rbm form exp free energy exp exp exp exp next write scaled log likelihood following form log log using fact exp exp plugging free energy equation one show hence matches equation restricted boltzmann machines rbms lqilqlw xgdwd xvwhs uhfrqvwuxfwlrqv xlqilqlw xhtxloleulxp vdpsohv figure illustration gibbs sampling rbm visible nodes initialized datavector sample hidden vector another visible vector etc eventually inﬁnity producing samples joint distribution approximating expectations approximate expectations needed evaluate gradient performing block gibbs sampling using equations detail sample joint distribution follows initialize chain setting data vector sample etc see figure illustration note however wait markov chain reaches equilibrium burned interpret samples coming joint distribution interest might take long time faster alternative use mean ﬁeld make approximation however since typically multimodal usually poor approx imation since average different modes see section furthermore subtle reason use mean ﬁeld since gradient form see negative sign front means method try make variational bound loose possible salakhutdinov hinton explains earlier attempts use mean ﬁeld learn boltzmann machines kappen rodriguez work contrastive divergence problem using gibbs sampling compute gradient slow present faster method known contrastive divergence hinton originally derived approximating objective function deﬁned difference two divergences rather trying maximize likelihood however algorithmic point view thought similar stochastic gradient descent except approxi mates unclamped expectations brief gibbs sampling initialize markov chain data vectors approximate gradient one datavector follows corresponds distribution generated gibbs sweeps started figure known detail procedure chapter latent variable models discrete data follows make approximation samples sometimes called fantasy data think model best attempt reconstructing coded decoded model similar way train auto encoders models try squeeze data restricted parametric bottleneck see section practice common use instead sampled value ﬁnal upwards pass since reduces variance however valid use instead sampling earlier upwards passes hidden unit would able pass bit information would act much bottleneck whole procedure summarized algorithm note follow positive gradient since maximizing likelihood various tricks used speed algorithm using momentum term section using mini batches averaging updates etc details found hinton swersky algorithm training rbm binary hidden visible units initialize weights randomly epoch minibatch size set minibatch gradient zero case minibatch compute sample sample compute compute gradient accumulate update parameters persistent section presented technique called stochastic maximum likelihood sml ﬁtting maxent models avoids need run mcmc convergence iteration restricted boltzmann machines rbms exploiting fact parameters changing slowly markov chains pushed far equilibrium update younes words two dynamical processes running different time scales states change quickly parameters change slowly algorithm independently rediscovered tieleman called persistent see algorithm pseudocode pcd often works better see tieleman marlin swersky although faster early stages learning algorithm persistent training rbm binary hidden visible units initialize weights randomly initialize chains randomly mean ﬁeld updates case sigm mcmc updates sample generate brief gibbs sampling old parameter updates decrease 
[latent, variable, models, discrete, data, restricted, boltzmann, machines, rbms, applications, rbms] main application rbms building block deep generative models discuss section also used substitutes directed two layer models particularly useful cases inference hidden states test time must fast give examples language modeling document retrieval use categorical rbm deﬁne generative model bag words alternative lda one subtlety partition function undirected models depends big graph therefore long document solution proposed salakhutdinov hinton use categorical rbm tied weights multiply hidden activation bias terms document length compensate form fact observed word count vector larger magnitude chapter latent variable models discrete data data set number docs dev avg test perplexity per word nats train test lda lda soft unigram nips news reuters figure comparison rbm replicated softmax lda three corpora number words vocabulary average document length dev standard deviation document length source salakhutdinov hinton recall precision replicated softmax lda recall precision replicated softmax lda newsgroups reuters figure precision recall curves rbm replicated softmax lda two corpora figure salakhutdinov hinton used kind permission ruslan salakhutdinov like single multinomial node dropped subscript states number words vocabulary called replicated softmax model salakhutdinov hinton undirected alternative mpca lda compare modeling power rbms lda measuring perplexity test set approximated using annealing importance sampling section results shown figure see lda signiﬁcantly better unigram model rbm signiﬁcantly better lda another advantage lda inference fast exact single matrix vector multiply followed sigmoid nonlinearity equation addition faster rbm accurate illustrated figure shows precision recall curves rbms lda two different corpora curves generated follows query document test set taken similarity training documents computed similarity deﬁned cosine angle two topic vectors top documents returned varying retrieved document considered relevant class label query place labels used restricted boltzmann machines rbms rbms collaborative ﬁltering rbms applied netﬂix collaborative ﬁltering competition salakhutdinov fact rbm binary hidden nodes categorical visible nodes slightly outperform svd combining two methods performance improved winning entry challenge ensemble many different types model koren 
[latent, variable, models, discrete, data, exercises] exercise partition function rbm show compute rbm binary hidden nodes binary observed nodes time assuming 
[deep, learning, introduction] many models looked book simple two layer architecture form unsupervised latent variable models supervised models however look brain seem many levels processing believed level learning features representations increasing levels abstraction example standard model visual cortex hubel wiesel serre ranzato suggests roughly speaking brain ﬁrst extracts edges patches surfaces objects etc see palmer kandel information brain might perform vision observation inspired recent trend machine learning known deep learning see bengio deeplearning net references therein attempts replicate kind architecture computer note idea applied non vision problems well speech language chapter give brief overview new ﬁeld however caution reader topic deep learning currently evolving quickly material chapter may soon outdated 
[deep, learning, deep, generative, models] deep models often millions parameters acquiring enough labeled data train models diffcult despite crowd sourcing sites mechanical turk simple settings hand written character recognition possible generate lots labeled data making modiﬁed copies small manually labeled training set see figure seems unlikely approach scale complex scenes overcome problem needing labeled training data focus unsupervised learning natural way perform use generative models section discuss three different kinds deep generative models directed undirected mixed attempts use computer graphics video games generate realistic looking images complex scenes use training data computer vision systems however often graphics programs cut corners order make perceptually appealing images reﬂective natural statistics real world images chapter deep learning figure deep multi layer graphical models observed variables bottom directed model undirected model deep boltzmann machine mixed directed undirected model deep belief net 
[deep, learning, deep, generative, models, deep, directed, networks] perhaps natural way build deep generative model construct deep directed graphical model shown figure bottom level contains observed pixels whatever data remaining layers hidden assumed layers notational simplicity number size layers usually chosen hand although one also use non parametric bayesian methods adams boosting chen infer model structure shall call models form deep directed networks ddns nodes binary cpds logistic functions called sigmoid belief net neal case model deﬁnes following joint distribution ber sigm ber sigm ber sigm ber unfortunately inference directed models intractable posterior hidden nodes correlated due explaining away one use fast mean ﬁeld approxi mations jaakkola jordan saul jordan may accurate since approximate correlated posterior factorial posterior one also use mcmc inference neal adams quite slow variables highly correlated slow inference also results slow learning 
[deep, learning, deep, generative, models, deep, boltzmann, machines] natural alternative directed model construct deep undirected model example stack series rbms top shown figure known deep boltzmann machine dbm salakhutdinov hinton hidden layers model deﬁned follows exp deep generative models ignoring constant offset bias terms main advantage directed model one perform efficient block layer wise gibbs sampling block mean ﬁeld since nodes layer conditionally independent given layers salakhutdinov larochelle main disadvantage training undirected models difficult partition function however see greedy layer wise strategy learning deep undirected models 
[deep, learning, deep, generative, models, deep, belief, networks] interesting compromise use model partially directed partially undirected particular suppose construct layered model directed arrows except top undirected bipartite graph shown figure model known deep belief network hinton dbn hidden layers model deﬁned follows ber sigm ber sigm exp essentially top two layers act associative memory remaining layers generate output advantage peculiar architecture infer hidden states fast bottom fashion see suppose two hidden layers second level weights tied ﬁrst level weights see figure deﬁnes model form one show distribution form exp equivalent rbm since dbn equivalent rbm far concerned infer posterior dbn exactly rbm posterior exact even though fully factorized way get factored posterior prior complementary prior prior multiplied likelihood results perfectly factored posterior thus see top level rbm dbn acts complementary prior bottom level directed sigmoidal likelihood function multiple hidden levels weights tied correspondence dbn rbm hold exactly still use factored inference rule form approximate bottom inference show valid variational lower bound bound also suggests layer wise training strategy explain detail later note however top inference dbn tractable dbns usually used feedforward manner unforuntately acronym dbn also stands dynamic bayes net section geoff hinton invented deep belief networks suggested acronyms deebns dybns two different meanings however terminology non standard chapter deep learning figure dbn two hidden layers tied weights equivalent rbm source figure salakhutdinov stack rbms trained greedily corresponding dbn source figure salakhutdinov used kind permission ruslan salakhutdinov 
[deep, learning, deep, generative, models, greedy, layer-wise, learning, dbns] equivalence dbns rbms suggests following strategy learning dbn fit rbm learn using methods described section unroll rbm dbn hidden layers figure freeze directed weights let untied longer forced equal learn better prior ﬁtting second rbm input data new rbm activation hidden units computed using factorial approximation continue add hidden layers stopping criterion satisiﬁed run time memory start overﬁt validation set construct dbn rbms illustrated figure one show hinton procedure always increases lower bound observed data likelihood course procedure might result overﬁtting different matter practice want able use number hidden units level means able initialize weights voids theoretical guarantee nevertheless method works well practice see method also extended train dbms greedy way salakhutdinov larochelle using greedy layer wise training strategy standard ﬁne tune weights using technique called backﬁtting works follows perform upwards sampling pass top perform brief gibbs sampling top level rbm perform update rbm parameters finally perform downwards ancestral sampling pass approximate sample posterior update logistic cpd parameters using small gradient step called procedure hinton unfortunately procedure slow deep neural networks 
[deep, learning, deep, neural, networks] given dbns often used feed forward bottom mode effectively acting like neural networks view natural dispense generative story try deep neural networks directly discuss resulting training methods often simpler implement faster note however performance deep neural nets sometimes good probabilistic models bengio one reason probabilistic models support top inference well bottom inference dbns support efficient top inference dbms shown help salakhutdinov larochelle top inference useful lot ambiguity correct interpretation signal interesting note mammalian visual cortex many feedback connections feedforward connections see palmer kandel role feedback connections precisely understood presumably provide contextual prior information coming previous frame retinal glance used disambiguate current bottom signals lee mumford course simulate effect top inference using neural network however models discuss 
[deep, learning, deep, neural, networks, deep, multi-layer, perceptrons] many decision problems reduced classiﬁcation predict object present image patch predict phoneme present given acoustic feature vector solve problems creating deep feedforward neural network multi layer perceptron mlp section ﬁtting parameters using gradient descent aka back propagation unfortunately method work well one problem gradient becomes weaker move away data known vanishing gradient problem bengio frasconi related problem large plateaus error surface cause simple ﬁrst order gadient based methods get stuck glorot bengio consequently early attempts learn deep neural networks proved unsuccesful recently progress due adoption gpus ciresan second order optimization algorithms martens nevertheless models remain difficult train discuss way initialize parameters using unsupervised learning called generative pre training advantage performing unsupervised learning ﬁrst model forced model high dimensional response namely input feature vector rather predicting scalar response acts like data induced regularizer helps backpropagation ﬁnd local minima good generalization properties erhan glorot bengio chapter deep learning figure training deep autoencoder first greedily train rbms construct auto encoder replicating weights finally ﬁne tune weights using back propagation figure hinton salakhutdinov used kind permission ruslan salakhutdinov 
[deep, learning, deep, neural, networks, deep, auto-encoders] auto encoder kind unsupervised neural network used dimensionality reduction feature discovery precisely auto encoder feedforward neural network trained predict input prevent system learning trivial identity mapping hidden layer middle usually constrained narrow bottleneck system minimize reconstruction error ensuring hidden units capture relevant aspects data suppose system one hidden layer model form suppose functions linear case one show weights hidden units span subspace ﬁrst principal components data karhunen joutsensalo japkowicz words linear auto encoders equivalent pca however using nonlinear activation functions one discover nonlinear representations data powerful representations learned using deep auto encoders unfortunately training models using back propagation work well gradient signal becomes small passes back multiple layers learning algorithm often gets stuck poor local minima one solution problem greedily train series rbms use initialize auto encoder illustrated figure whole system ﬁne tuned using backprop usual fashion approach ﬁrst suggested hinton salakhutdinov applications deep networks top level units units units pixel image label units could top level another sensory pathway figure dbn architecture classifying mnist digits source figure hinton used kind permission geoff hinton errors made dbn test cases mnist image estimated label source figure hinton used kind permission geoff hinton compare figure works much better trying deep auto encoder directly starting random weights 
[deep, learning, deep, neural, networks, stacked, denoising, auto-encoders] standard way train auto encoder ensure hidden layer narrower visible layer prevents model learning identity function ways prevent trivial solution allow use complete representation one approach impose sparsity constraints activation hidden units ranzato another approach add noise inputs called denoising auto encoder vincent example corrupt inputs example setting zero model learn predict missing entries shown equivalent certain approximate form maximum likelihood training known score matching applied rbm vincent course stack models top learn deep stacked denoising auto encoder discriminatively ﬁne tuned like feedforward neural network desired 
[deep, learning, applications, deep, networks] figure small convolutional rbm two groups hidden units associated ﬁlter size two different views data ﬁrst window ﬁrst view computed using ﬁlter second view using ﬁlter similarly views data second window computed using respectively hamming distance say results retrieving documents key point total time independent size corpus course techniques fast document retrieval inverted indices rely fact individual words quite informative simply intersect documents contain word however performing image retrieval clear want work pixel level recently krizhevsky hinton showed deep autoencoder could learn good semantic hashing function outperformed previous techniques torralba weiss million tiny images dataset hard apply inverted indexing techniques real valued data although one could imagine vector quantizing image patches 
[deep, learning, applications, deep, networks, handwritten, digit, classiﬁcation, using, dbns] figure shows dbn hinton consisting hidden layers visible layer corresponds binary images handwritten digits mnist data set addition top rbm connected softmax layer units representing class label chapter deep learning legal judicial leading economic indicators european community monetary economic accounts earnings interbank markets government borrowings disasters accidents energy markets figure visualization bag words data reuters rcv corpus results using lsa results using deep auto encoder source figure hinton salakhutdinov used kind permission ruslan salakhutdinov ﬁrst hidden layers trained greedy unsupervised fashion mnist digits using epochs passes data stochastic gradient descent heuristic process took hours per layer hinton top layer trained using input activations lower hidden layer well class labels corresponding generative model test error network weights carefully ﬁne tuned training images using procedure process took week hinton model used classify performing deterministic bottom pass computing free energy top level rbm possible class label ﬁnal error test set misclassiﬁed examples shown figure best error rate method permutation invariant version mnist time permutation invariant mean method exploit fact input image generic methods work well permuted versions input see figure therefore applied kinds datasets method comes close svm degree polynomial kernel achieved error rate decoste schoelkopf way comparison nearest neighbor using examples achieves see mnistnndemo good although much simpler 
[deep, learning, applications, deep, networks, data, visualization, feature, discovery, using, deep, auto-encoders] deep autoencoders learn informative features raw data features often used input standard supervised learning methods illustrate consider ﬁtting deep auto encoder hidden bottleneck one get much improved performance task exploiting fact input image one way create distorted versions input adding small shifts translations see figure examples applying trick reduced svm error rate similar error rates achieved using convolutional neural networks section trained distorted images simard got however point dbns offer way learn prior knowledge without hand crafted applications deep networks figure precision recall curves document retrieval reuters rcv corpus source figure salakhutdinov used kind permission ruslan salakhutdinov text data results shown figure left show embedding produced lsa section right embedding produced auto encoder clear low dimensional representation created auto encoder captured lot meaning documents even though class labels used note various ways learning low dimensional continuous embeddings words proposed see turian details 
[deep, learning, applications, deep, networks, information, retrieval, using, deep, auto-encoders, semantic, hashing] view sucess rbms information retrieval discussed section natural wonder deep models even better fact shown figure interestingly use binary low dimensional representation middle layer deep auto encoder rather continuous representation used enables fast retrieval related documents example use bit code precompute binary representation documents create hash table mapping codewords documents approach known semantic hashing since binary representation semantically similar documents close hamming distance test documents reuters rcv results documents per entry table test time compute codeword query simply retrieve relevant documents constant time looking contents relevant address memory ﬁnd related documents compute codewords within details salakhutdinov hinton used reuters rcv data set consists newswire articles manually classiﬁed topics represent document counting many times top frequent words occurs trained deep auto encoder layers half data visible units use replicated softmax distribution hidden units middle layer gaussian distribution remaining units usual bernoulli logistic distribution ﬁne tuning auto encoder cross entropy loss function equivalent maximum likelihood multinoulli distribution used see hinton salakhutdinov details chapter deep learning 
[deep, learning, applications, deep, networks, learning, audio, features, using, convolutional, dbns] apply dbns time series unbounded length necessary use form parameter tying one way use convolutional dbns lee desjardins bengio use convolutional rbms basic unit models generative version convolutional neural nets discussed section basic idea illustrated figure hidden activation vector group computed convolving input vector group ﬁlter weight vector matrix words node within hidden group weighted combination subset inputs compute activaton hidden nodes sliding weight vector input allows model translation invariance since use weights matter input vector pattern occurs group ﬁlter corresponding pattern detector note number bit vectors hamming distance away often said goal deep learnng discover invariant features representation object change even nuisance variables lighting change however sometimes called nuisance variables may variables interest example task determine photograph taken morning evening lighting one salient features object identity may less relevant always one task signal another task noise unwise throw away apparently irrelevant information discussion formally binary signals deﬁne full conditionals convolutional rbm follows lee sigm sigm weight vector group bias terms represents convolution vectors common add max pooling layer well convolutional layer computes local maximum ﬁltered response allows small amount translation invariance also reduces size higher levels speeds computation consider ably deﬁning neural network simple deﬁning way allows information ﬂow backwards well forwards bit involved basic idea similar noisy cpd section deﬁne probabilistic relationship max node parts maxing see lee details note however top generative process difficult since max pooling operation throws away much information lee applies convolutional dbns depth auditory data input consists speech signals method recovers representation similar phonemes applied music classiﬁcation speaker identiﬁcation method outperforms tech niques using standard features mfcc features fed discriminative classiﬁer seide deep neural net used place gmm inside conventional hmm use dnns signiﬁcantly improved performance conversational speech recogni tion interview tech lead project said historically individual technologies speech recognition led improvements magnitude 
[deep, learning, applications, deep, networks, learning, image, features, using, convolutional, dbns] extend convolutional dbn straightforward way lee illustrated figure results layer system trained four classes visual objects cars motorbikes faces airplanes caltech dataset shown figure show results layers layer learns gabor like ﬁlters similar learned sparse coding shown figure see layer learned generic visual parts shared amongst object classes layer seems learned ﬁlters look like grandmother cells speciﬁc individual object classes cases individual objects 
[deep, learning, discussion] far discussing models inspired low level processing brain models produced useful features simple classiﬁcation tasks pure bottom early source http research microsoft com news features speechrecognition aspx chapter deep learning figure convolutional rbm max pooling layers input signal stack images color planes input layer passed different set ﬁlters hidden unit obtained convolving appropriate ﬁlter summing input planes ﬁnal layer obtained computing local maximum within small window source figure chen used kind permission chen faces cars airplanes motorbikes figure visualization ﬁlters learned convolutional dbn layers two three source figure lee used kind permission honglak lee approach scale challenging problems scene interpretation natural language understanding put problem perspective consider dbn handwritten digit classiﬁcation figure free parameters although lot tiny compared number neurons brain hinton says many parameters cubic millimetres mouse cortex several hundred networks complexity could within single voxel high resolution fmri scan suggests much bigger networks may required compete human shape recognition abilities hinton scale challenging problems various groups using gpus see raina parallel computing perhaps efficient approach work higher level abstraction inference done space objects parts rather discussion space bits pixels want bridge signal symbol divide symbol mean something atomic combined symbols compositional way question convert low level signals structured semantic represen tation known symbol grounding problem harnard traditionally symbols associated words natural language seems unlikely jump directly low level signals high level semantic concepts instead need intermediate level symbolic atomic parts simple way create parts real valued signals images apply vector quantization generates set visual words modelled using techniques chapter modeling bags words models however still quite shallow possible deﬁne learn deep models use discrete latent parts mention recent approaches give ﬂavor possibilites salakhutdinov combine rbms hierarchical latent dirichlet allocation methods trained unsupervised way zhu use latent graphs trained manner similar latent structural svm similar approach based grammars described girshick interesting techniques apply data driven machine learning methods rich structured symbolic style models seems like promising future direction machine learning 
