[introduction] inventors long dreamed creating machines think desire dates back least time ancient greece mythical ﬁgures pygmalion daedalus hephaestus may interpreted legendary inventors galatea talos pandora may regarded artiﬁcial life ovid martin sparkes tandy programmable computers ﬁrst conceived people wondered whether might become intelligent hundred years one built lovelace today artiﬁcial intelligence thriving ﬁeld many practical applications active research topics look intelligent software automate routine labor understand speech images make diagnoses medicine support basic scientiﬁc research early days artiﬁcial intelligence ﬁeld rapidly tackled solved problems intellectually diﬃcult human beings relatively straight forward computers problems described list formal math ematical rules true challenge artiﬁcial intelligence proved solving tasks easy people perform hard people describe formally problems solve intuitively feel automatic like recognizing spoken words faces images book solution intuitive problems solution allow computers learn experience understand world terms hierarchy concepts concept deﬁned terms relation simpler concepts gathering knowledge experience approach avoids need human operators formally specify knowledge computer needs hierarchy concepts allows computer learn complicated concepts building simpler ones draw graph showing chapter introduction concepts built top graph deep many layers reason call approach deep learning many early successes took place relatively sterile formal environments require computers much knowledge world example ibm deep blue chess playing system defeated world champion garry kasparov chess course simple hsu world containing sixty four locations thirty two pieces move rigidly circumscribed ways devising successful chess strategy tremendous accomplishment challenge due diﬃculty describing set chess pieces allowable moves computer chess completely described brief list completely formal rules easily provided ahead time programmer ironically abstract formal tasks among diﬃcult mental undertakings human among easiest computer computers long able defeat even best human chess player recently matching abilities average human beings recognize objects speech person everyday life requires immense amount knowledge world much knowledge subjective intuitive therefore diﬃcult articulate formal way computers need capture knowledge order behave intelligent way one key challenges artiﬁcial intelligence get informal knowledge computer several artiﬁcial intelligence projects sought hard code knowledge world formal languages computer reason statements formal languages automatically using logical inference rules known knowledge base approach artiﬁcial intelligence none projects led major success one famous projects cyc lenat guha cyc inference engine database statements language called cycl statements entered staﬀ human supervisors unwieldy process people struggle devise formal rules enough complexity accurately describe world example cyc failed understand story person named fred shaving morning inference linde engine detected inconsistency story knew people electrical parts fred holding electric razor believed entity fredwhileshaving contained electrical parts therefore asked whether fred still person shaving diﬃculties faced systems relying hard coded knowledge suggest systems need ability acquire knowledge extracting patterns raw data capability known machine learning introduction chapter introduction machine learning allowed computers tackle problems involving knowledge real world make decisions appear subjective simple machine learning algorithm called logistic regression determine whether recommend cesarean delivery mor yosef simple machine learning algorithm called separate legitimate mail spam mail naive bayes performance simple machine learning algorithms depends heavily representation data given example logistic regression used recommend cesarean delivery system examine patient directly instead doctor tells system several pieces relevant information presence absence uterine scar piece information included representation patient known feature logistic regression learns features patient correlates various outcomes however cannot inﬂuence way features deﬁned way logistic regression given mri scan patient rather doctor formalized report would able make useful predictions individual pixels mri scan negligible correlation complications might occur delivery dependence representations general phenomenon appears throughout computer science even daily life computer science opera tions searching collection data proceed exponentially faster collection structured indexed intelligently people easily perform arithmetic arabic numerals ﬁnd arithmetic roman numerals much time consuming surprising choice representation enormous eﬀect performance machine learning algorithms simple visual example see fig many artiﬁcial intelligence tasks solved designing right set features extract task providing features simple machine learning algorithm example useful feature speaker identiﬁcation sound estimate size speaker vocal tract therefore gives strong clue whether speaker man woman child however many tasks diﬃcult know features extracted example suppose would like write program detect cars photographs know cars wheels might like use presence wheel feature unfortunately diﬃcult describe exactly wheel looks like terms pixel values wheel simple geometric shape image may complicated shadows falling wheel sun glaring metal parts wheel fender car object foreground obscuring part wheel chapter introduction figure example diﬀerent representations suppose want separate two categories data drawing line scatterplot plot left represent data using cartesian coordinates task impossible plot right represent data polar coordinates task becomes simple solve vertical line figure produced collaboration david warde farley one solution problem use machine learning discover mapping representation output also representation approach known representation learning learned representations often result much better performance obtained hand designed representations also allow systems rapidly adapt new tasks minimal human intervention representation learning algorithm discover good set features simple task minutes complex task hours months manually designing features complex task requires great deal human time eﬀort take decades entire community researchers quintessential example representation learning algorithm toencoder encoder autoencoder combination function converts input data diﬀerent representation decoder function converts new representation back original format autoencoders trained preserve much information possible input run encoder decoder also trained make new representation various nice properties diﬀerent kinds autoencoders aim achieve diﬀerent kinds properties designing features algorithms learning features goal usually separate explain observed data context factors variation use word factors simply refer separate sources inﬂuence factors usually combined multiplication factors often quantities chapter introduction directly observed instead may exist either unobserved objects unobserved forces physical world aﬀect observable quantities may also exist constructs human mind provide useful simplifying explanations inferred causes observed data thought concepts abstractions help make sense rich variability data analyzing speech recording factors variation include speaker age sex accent words speaking analyzing image car factors variation include position car color angle brightness sun major source diﬃculty many real world artiﬁcial intelligence applications many factors variation inﬂuence every single piece data able observe individual pixels image red car might close black night shape car silhouette depends viewing angle applications require factors variation discard disentangle ones care course diﬃcult extract high level abstract features raw data many factors variation speaker accent identiﬁed using sophisticated nearly human level understanding data nearly diﬃcult obtain representation solve original problem representation learning ﬁrst glance seem help deep learning solves central problem representation learning introduc ing representations expressed terms simpler representations deep learning allows computer build complex concepts simpler con cepts fig shows deep learning system represent concept image person combining simpler concepts corners contours turn deﬁned terms edges quintessential example deep learning model feedforward deep network multilayer perceptron mlp multilayer perceptron mathe matical function mapping set input values output values function formed composing many simpler functions think application diﬀerent mathematical function providing new representation input idea learning right representation data provides one perspec tive deep learning another perspective deep learning depth allows computer learn multi step computer program layer representation thought state computer memory executing another set instructions parallel networks greater depth execute instructions sequence sequential instructions oﬀer great power later instructions refer back results earlier instructions according chapter introduction visible layer input pixels hidden layer edges hidden layer corners contours hidden layer object parts car person animal output object identity figure illustration deep learning model diﬃcult computer understand meaning raw sensory input data image represented collection pixel values function mapping set pixels object identity complicated learning evaluating mapping seems insurmountable tackled directly deep learning resolves diﬃculty breaking desired complicated mapping series nested simple mappings described diﬀerent layer model input presented named contains variables visible layer able observe series extracts increasingly abstract features hidden layers image layers called hidden values given data instead model must determine concepts useful explaining relationships observed data images visualizations kind feature represented hidden unit given pixels ﬁrst layer easily identify edges comparing brightness neighboring pixels given ﬁrst hidden layer description edges second hidden layer easily search corners extended contours recognizable collections edges given second hidden layer description image terms corners contours third hidden layer detect entire parts speciﬁc objects ﬁnding speciﬁc collections contours corners finally description image terms object parts contains used recognize objects present image images reproduced permission zeiler fergus chapter introduction element set element set logistic regression logistic regression figure illustration computational graphs mapping input output node performs operation depth length longest path input output depends deﬁnition constitutes possible computational step computation depicted graphs output logistic regression model logistic sigmoid function use addition multiplication logistic sigmoids elements computer language model depth three view logistic regression element model depth one view deep learning information layer activations necessarily encodes factors variation explain input representation also stores state information helps execute program make sense input state information could analogous counter pointer traditional computer program nothing content input speciﬁcally helps model organize processing two main ways measuring depth model ﬁrst view based number sequential instructions must executed evaluate architecture think length longest path ﬂow chart describes compute model outputs given inputs two equivalent computer programs diﬀerent lengths depending language program written function may drawn ﬂowchart diﬀerent depths depending functions allow used individual steps ﬂowchart fig illustrates choice language give two diﬀerent measurements architecture another approach used deep probabilistic models regards depth model depth computational graph depth graph describing concepts related case depth ﬂowchart computations needed compute representation chapter introduction concept may much deeper graph concepts system understanding simpler concepts reﬁned given information complex concepts example system observing image face one eye shadow may initially see one eye detecting face present infer second eye probably present well case graph concepts includes two layers layer eyes layer faces graph computations includes layers reﬁne estimate concept given times always clear two views depth computational graph depth probabilistic modeling graph relevant diﬀerent people choose diﬀerent sets smallest elements construct graphs single correct value depth architecture single correct value length computer program consensus much depth model requires qualify deep however deep learning safely regarded study models either involve greater amount composition learned functions learned concepts traditional machine learning summarize deep learning subject book approach speciﬁcally type machine learning technique allows computer systems improve experience data according authors book machine learning viable approach building systems operate complicated real world environments deep learning particular kind machine learning achieves great power ﬂexibility learning represent world nested hierarchy concepts concept deﬁned relation simpler concepts abstract representations computed terms less abstract ones fig illustrates relationship diﬀerent disciplines fig gives high level schematic works 
[introduction, read, book?] book useful variety readers wrote two main target audiences mind one target audiences university students undergraduate graduate learning machine learning including beginning career deep learning artiﬁcial intelligence research target audience software engineers machine learning statistics background want rapidly acquire one begin using deep learning product platform deep learning already proven useful many software disciplines including computer vision speech audio processing chapter introduction machine learning representation learning deep learning example knowledge bases example logistic regression example shallow autoencoders example mlps figure venn diagram showing deep learning kind representation learning turn kind machine learning used many approaches section venn diagram includes example technology chapter introduction input hand designed program output input hand designed features mapping features output input features mapping features output input simple features mapping features output additional layers abstract features rule based systems classic machine learning representation learning deep learning figure flowcharts showing diﬀerent parts system relate within diﬀerent disciplines shaded boxes indicate components able learn data chapter introduction natural language processing robotics bioinformatics chemistry video games search engines online advertising ﬁnance book organized three parts order best accommodate variety readers part introduces basic mathematical tools machine learning concepts part describes established deep learning algorithms essentially solved technologies part describes speculative ideas iii widely believed important future research deep learning readers feel free skip parts relevant given interests background readers familiar linear algebra probability fundamental machine learning concepts skip part example readers want implement working system need read beyond part help choose chapters read fig provides ﬂowchart showing high level organization book assume readers come computer science background assume familiarity programming basic understanding computational performance issues complexity theory introductory level calculus terminology graph theory 
[introduction, historical, trends, deep, learning] easiest understand deep learning historical context rather providing detailed history deep learning identify key trends deep learning long rich history gone many names reﬂecting diﬀerent philosophical viewpoints waxed waned popularity deep learning become useful amount available training data increased deep learning models grown size time computer hardware software infrastructure deep learning improved deep learning solved increasingly complicated applications increasing accuracy time chapter introduction introduction part applied math machine learning basics linear algebra probability information theory numerical computation machine learning basics part deep networks modern practices deep feedforward networks regularization optimization cnns rnns practical methodology applications part iii deep learning research linear factor models autoencoders representation learning structured probabilistic models monte carlo methods partition function inference deep generative models figure high level organization book arrow one chapter another indicates former chapter prerequisite material understanding latter chapter introduction 
[introduction, historical, trends, deep, learning, many, names, changing, fortunes, neural, net-, works] expect many readers book heard deep learning exciting new technology surprised see mention history book emerging ﬁeld fact deep learning dates back deep learning appears new relatively unpopular several years preceding current popularity gone many diﬀerent names recently become called deep learning ﬁeld rebranded many times reﬂecting inﬂuence diﬀerent researchers diﬀerent perspectives comprehensive history deep learning beyond scope textbook however basic context useful understanding deep learning broadly speaking three waves development deep learning deep learn ing known cybernetics connectionism deep learning known current resurgence name deep learning beginning quantitatively illustrated fig earliest learning algorithms recognize today intended computational models biological learning models learning happens could happen brain result one names deep learning gone artiﬁcial neural networks anns corresponding perspective deep learning models engineered systems inspired biological brain whether human brain brain another animal kinds neural networks used machine learning sometimes used understand brain function hinton shallice generally designed realistic models biological function neural perspective deep learning motivated two main ideas one idea brain provides proof example intelligent behavior possible conceptually straightforward path building intelligence reverse engineer computational principles behind brain duplicate functionality another perspective would deeply interesting understand brain principles underlie human intelligence machine learning models shed light basic scientiﬁc questions useful apart ability solve engineering applications modern term deep learning goes beyond neuroscientiﬁc perspective current breed machine learning models appeals general principle learning multiple levels composition applied machine learning frameworks necessarily neurally inspired chapter introduction year cybernetics connectionism neural networks figure ﬁgure shows two three historical waves artiﬁcial neural nets research measured frequency phrases cybernetics connectionism neural networks according google books third wave recent appear ﬁrst wave started cybernetics development theories biological learning implementations mcculloch pitts hebb ﬁrst models perceptron rosenblatt allowing training single neuron second wave started connectionist approach period back propagation train neural network one two rumelhart hidden layers current third wave deep learning started around hinton bengio ranzato appearing book form two waves similarly appeared book form much later corresponding scientiﬁc activity occurred chapter introduction earliest predecessors modern deep learning simple linear models motivated neuroscientiﬁc perspective models designed take set input values associate output models would learn set weights compute output ﬁrst wave neural networks research known cybernetics illustrated fig mcculloch pitts neuron early model mcculloch pitts brain function linear model could recognize two diﬀerent categories inputs testing whether positive negative course model correspond desired deﬁnition categories weights needed set correctly weights could set human operator perceptron rosenblatt became ﬁrst model could learn weights deﬁning categories given examples inputs category adaptive linear element adaline dates time simply returned value predict real number widrow hoﬀ could also learn predict numbers data simple learning algorithms greatly aﬀected modern landscape machine learning training algorithm used adapt weights ada line special case algorithm called stochastic gradient descent slightly modiﬁed versions stochastic gradient descent algorithm remain dominant training algorithms deep learning models today models based used perceptron adaline called linear models models remain widely used machine learning models though many cases trained diﬀerent ways original models trained linear models many limitations famously cannot learn xor function critics observed ﬂaws linear models caused backlash biologically inspired learning general minsky papert ﬁrst major dip popularity neural networks today neuroscience regarded important source inspiration deep learning researchers longer predominant guide ﬁeld main reason diminished role neuroscience deep learning research today simply enough information brain use guide obtain deep understanding actual algorithms used brain would need able monitor activity least thousands interconnected neurons simultaneously able far understanding even simple chapter introduction well studied parts brain olshausen field neuroscience given reason hope single deep learning algorithm solve many diﬀerent tasks neuroscientists found ferrets learn see auditory processing region brain brains rewired send visual signals area von melchner suggests much mammalian brain might use single algorithm solve diﬀerent tasks brain solves hypothesis machine learning research fragmented diﬀerent communities researchers studying natural language processing vision motion planning speech recognition today application communities still separate common deep learning research groups study many even application areas simultaneously able draw rough guidelines neuroscience basic idea many computational units become intelligent via interactions inspired brain neocognitron fukushima introduced powerful model architecture processing images inspired structure mammalian visual system later became basis modern convolutional network see sec lecun neural networks today based model neuron called rectiﬁed linear unit original cognitron fukushima introduced complicated version highly inspired knowledge brain function simpliﬁed modern version developed incorporating ideas many viewpoints nair hinton glorot citing neuroscience inﬂuence jarrett citing engineering oriented inﬂuences neuroscience important source inspiration need taken rigid guide know actual neurons compute diﬀerent functions modern rectiﬁed linear units greater neural realism yet led improvement machine learning performance also neuroscience successfully inspired several neural network architectures yet know enough biological learning neuroscience oﬀer much guidance learning algorithms use train architectures media accounts often emphasize similarity deep learning brain true deep learning researchers likely cite brain inﬂuence researchers working machine learning ﬁelds kernel machines bayesian statistics one view deep learning attempt simulate brain modern deep learning draws inspiration many ﬁelds especially applied math fundamentals like linear algebra probability information theory numerical optimization deep learning researchers cite neuroscience important source inspiration others concerned chapter introduction neuroscience worth noting eﬀort understand brain works algorithmic level alive well endeavor primarily known computational neuroscience separate ﬁeld study deep learning common researchers move back forth ﬁelds ﬁeld deep learning primarily concerned build computer systems able successfully solve tasks requiring intelligence ﬁeld computational neuroscience primarily concerned building accurate models brain actually works second wave neural network research emerged great part via movement called connectionism parallel distributed processing rumelhart mcclelland connectionism arose context cognitive science cognitive science interdisciplinary approach understand ing mind combining multiple diﬀerent levels analysis early cognitive scientists studied models symbolic reasoning despite popularity symbolic models diﬃcult explain terms brain could actually implement using neurons connectionists began study models cognition could actually grounded neural implementations touretzky minton reviving many ideas dating back work psychologist donald hebb hebb central idea connectionism large number simple computational units achieve intelligent behavior networked together insight applies equally neurons biological nervous systems hidden units computational models several key concepts arose connectionism movement remain central today deep learning one concepts distributed representation hinton idea input system represented many features feature involved representation many possible inputs example suppose vision system recognize cars trucks birds objects red green blue one way representing inputs would separate neuron hidden unit activates nine possible combinations red truck red car red bird green truck requires nine diﬀerent neurons neuron must independently learn concept color object identity one way improve situation use distributed representation three neurons describing color three neurons describing object identity requires six neurons total instead nine neuron describing redness able learn redness chapter introduction images cars trucks birds images one speciﬁc category objects concept distributed representation central book described greater detail chapter another major accomplishment connectionist movement suc cessful use back propagation train deep neural networks internal repre sentations popularization back propagation algorithm rumelhart algorithm waxed waned popularity lecun writing currently dominant approach training deep models researchers made important advances modeling sequences neural networks identiﬁed hochreiter bengio fundamental mathematical diﬃculties modeling long sequences described sec hochreiter schmidhuber introduced long short term memory lstm network resolve diﬃculties today lstm widely used many sequence modeling tasks including many natural language processing tasks google second wave neural networks research lasted mid ven tures based neural networks technologies began make unrealisti cally ambitious claims seeking investments research fulﬁll unreasonable expectations investors disappointed simultaneously ﬁelds machine learning made advances kernel machines boser cortes vapnik schölkopf jor graphical models dan achieved good results many important tasks two factors led decline popularity neural networks lasted time neural networks continued obtain impressive performance tasks canadian institute lecun bengio advanced research cifar helped keep neural networks research alive via neural computation adaptive perception ncap research initiative program united machine learning research groups led geoﬀrey hinton university toronto yoshua bengio university montreal yann lecun new york university cifar ncap research initiative multi disciplinary nature also included neuroscientists experts human computer vision point time deep networks generally believed diﬃcult train know algorithms existed since work quite well apparent circa issue perhaps simply algorithms computationally costly allow much experimentation hardware available time third wave neural networks research began breakthrough chapter introduction geoﬀrey hinton showed kind neural network called deep belief network could eﬃciently trained using strategy called greedy layer wise pretraining hinton described detail sec cifar aﬃliated research groups quickly showed strategy could used train many kinds deep networks bengio ranzato systematically helped improve generalization test examples wave neural networks research popularized use term deep learning emphasize researchers able train deeper neural networks possible focus attention theoretical importance depth bengio lecun delalleau bengio pascanu montufar time deep neural networks outperformed competing systems based machine learning technologies well hand designed functionality third wave popularity neural networks continues time writing though focus deep learning research changed dramatically within time wave third wave began focus new unsupervised learning techniques ability deep models generalize well small datasets today interest much older supervised learning algorithms ability deep models leverage large labeled datasets 
[introduction, historical, trends, deep, learning, increasing, dataset, sizes] one may wonder deep learning recently become recognized crucial technology though ﬁrst experiments artiﬁcial neural networks conducted deep learning successfully used commercial applications since often regarded art technology something expert could use recently true skill required get good performance deep learning algorithm fortunately amount skill required reduces amount training data increases learning algorithms reaching human performance complex tasks today nearly identical learning algorithms struggled solve toy problems though models train algorithms undergone changes simplify training deep architectures important new development today provide algorithms resources need succeed fig shows size benchmark datasets increased remarkably time trend driven increasing digitization society activities take place computers recorded computers increasingly networked together becomes easier centralize records curate chapter introduction dataset appropriate machine learning applications age big data made machine learning much easier key burden statistical estimation generalizing well new data observing small amount data considerably lightened rough rule thumb supervised deep learning algorithm generally achieve acceptable performance around labeled examples per category match exceed human performance trained dataset containing least million labeled examples working successfully datasets smaller important research area focusing particular take advantage large quantities unlabeled examples unsupervised semi supervised learning 
[introduction, historical, trends, deep, learning, increasing, model, sizes] another key reason neural networks wildly successful today enjoying comparatively little success since computational resources run much larger models today one main insights connection ism animals become intelligent many neurons work together individual neuron small collection neurons particularly useful biological neurons especially densely connected seen fig machine learning models number connections per neuron within order magnitude even mammalian brains decades terms total number neurons neural networks astonishingly small quite recently shown fig since introduction hidden units artiﬁcial neural networks doubled size roughly every years growth driven faster computers larger memory availability larger datasets larger networks able achieve higher accuracy complex tasks trend looks set continue decades unless new technologies allow faster scaling artiﬁcial neural networks number neurons human brain least biological neurons may represent complicated functions current artiﬁcial neurons biological neural networks may even larger plot portrays retrospect particularly surprising neural networks fewer neurons leech unable solve sophisticated artiﬁcial intelligence prob lems even today networks consider quite large computational systems point view smaller nervous system even relatively primitive vertebrate animals like frogs increase model size time due availability faster cpus chapter introduction figure dataset sizes increased greatly time early statisticians studied datasets using hundreds thousands manually compiled measurements garson gosset anderson fisher pioneers biologically inspired machine learning often worked small synthetic datasets low resolution bitmaps letters designed incur low computational cost demonstrate neural networks able learn speciﬁc kinds functions widrow hoﬀ rumelhart machine learning became statistical nature began leverage larger datasets containing tens thousands examples mnist dataset shown fig scans handwritten numbers ﬁrst decade lecun sophisticated datasets size cifar dataset krizhevsky hinton continued produced toward end decade throughout ﬁrst half signiﬁcantly larger datasets containing hundreds thousands tens millions examples completely changed possible deep learning datasets included public street view house numbers dataset netzer various versions imagenet dataset deng russakovsky sports dataset karpathy top graph see datasets translated sentences ibm dataset constructed canadian hansard wmt english french brown dataset schwenk typically far ahead dataset sizes chapter introduction figure example inputs mnist dataset nist stands national institute standards technology agency originally collected data stands modiﬁed since data preprocessed easier use machine learning algorithms mnist dataset consists scans handwritten digits associated labels describing digit contained image simple classiﬁcation problem one simplest widely used tests deep learning research remains popular despite quite easy modern techniques solve geoﬀrey hinton described drosophila machine learning meaning allows machine learning researchers study algorithms controlled laboratory conditions much biologists often study fruit ﬂies chapter introduction advent general purpose gpus described sec faster network connectivity better software infrastructure distributed computing one important trends history deep learning trend generally expected continue well future 
[introduction, historical, trends, deep, learning, increasing, accuracy, complexity, real-world, impact] since deep learning consistently improved ability provide accurate recognition prediction moreover deep learning consistently applied success broader broader sets applications earliest deep models used recognize individual objects tightly cropped extremely small images since rumelhart gradual increase size images neural networks could process modern object recognition networks process rich high resolution photographs requirement photo cropped near object recognized similarly earliest networks could recognize krizhevsky two kinds objects cases absence presence single kind object modern networks typically recognize least diﬀerent categories objects largest contest object recognition imagenet large scale visual recognition challenge ilsvrc held year dramatic moment meteoric rise deep learning came convolutional network challenge ﬁrst time wide margin bringing state art top error rate krizhevsky meaning convolutional network produces ranked list possible categories image correct category appeared ﬁrst ﬁve entries list test examples since competitions consistently deep convolutional nets writing advances deep learning brought latest top error rate contest shown fig deep learning also dramatic impact speech recognition improving throughout error rates speech recognition stagnated starting introduction deep learning dahl deng seide hinton speech recognition resulted sudden drop error rates error rates cut half explore history detail sec deep networks also spectacular successes pedestrian detection image segmentation sermanet farabet couprie yielded superhuman performance traﬃc sign classiﬁcation ciresan chapter introduction figure initially number connections neurons artiﬁcial neural networks limited hardware capabilities today number connections neurons mostly design consideration artiﬁcial neural networks nearly many connections per neuron cat quite common neural networks many connections per neuron smaller mammals like mice even human brain exorbitant amount connections per neuron biological neural network sizes wikipedia adaptive linear element widrow hoﬀ neocognitron fukushima gpu accelerated convolutional network chellapilla deep boltzmann machine salakhutdinov hinton unsupervised convolutional network jarrett gpu accelerated multilayer perceptron ciresan distributed autoencoder multi gpu convolutional network krizhevsky cots hpc unsupervised convolutional network coates googlenet szegedy chapter introduction time scale accuracy deep networks increased complexity tasks solve goodfellow showed neural networks could learn output entire sequence characters transcribed image rather identifying single object previously widely believed kind learning required labeling individual elements sequence recurrent neural networks gülçehre bengio lstm sequence model mentioned used model relationships sequences sequences rather ﬁxed inputs sequence sequence learning seems cusp revolutionizing another application machine translation sutskever bahdanau trend increasing complexity pushed logical conclusion introduction neural turing machines graves learn read memory cells write arbitrary content memory cells neural networks learn simple programs examples desired behavior example learn sort lists numbers given examples scrambled sorted sequences self programming technology infancy future could principle applied nearly task another crowning achievement deep learning extension domain reinforcement learning context reinforcement learning autonomous agent must learn perform task trial error without guidance human operator deepmind demonstrated reinforcement learning system based deep learning capable learning play atari video games reaching human level performance many tasks deep learning mnih also signiﬁcantly improved performance reinforcement learning robotics finn many applications deep learning highly proﬁtable deep learning used many top technology companies including google microsoft facebook ibm baidu apple adobe netﬂix nvidia nec advances deep learning also depended heavily advances software infrastructure software libraries theano bergstra bastien pylearn goodfellow torch collobert distbelief caﬀe mxnet dean jia chen tensorflow supported important research projects abadi commercial products deep learning also made contributions back sciences modern convolutional networks object recognition provide model visual processing chapter introduction neuroscientists study deep learning also provides useful dicarlo tools processing massive amounts data making useful predictions scientiﬁc ﬁelds successfully used predict molecules interact order help pharmaceutical companies design new drugs dahl search subatomic particles automatically parse baldi microscope images used construct map human brain knowles barley expect deep learning appear scientiﬁc ﬁelds future summary deep learning approach machine learning drawn heavily knowledge human brain statistics applied math developed past several decades recent years seen tremendous growth popularity usefulness due large part powerful com puters larger datasets techniques train deeper networks years ahead full challenges opportunities improve deep learning even bring new frontiers chapter introduction year sponge roundworm leech ant bee frog octopus human increasing neural network size time figure since introduction hidden units artiﬁcial neural networks doubled size roughly every years biological neural network sizes wikipedia perceptron rosenblatt adaptive linear element widrow hoﬀ neocognitron fukushima early back propagation network rumelhart recurrent neural network speech recognition robinson fallside multilayer perceptron speech recognition bengio mean ﬁeld sigmoid belief network saul lenet lecun echo state network jaeger haas deep belief network hinton gpu accelerated convolutional network chellapilla deep boltzmann machine salakhutdinov hinton gpu accelerated deep belief network raina unsupervised convolutional network jarrett gpu accelerated multilayer perceptron ciresan omp network coates distributed autoencoder multi gpu convolutional network krizhevsky cots hpc unsupervised convolutional network coates googlenet szegedy chapter introduction year decreasing error rate time figure since deep networks reached scale necessary compete imagenet large scale visual recognition challenge consistently competition every year yielded lower lower error rates time data russakovsky 
[learning, basics] part book introduces basic mathematical concepts needed understand deep learning begin general ideas applied math allow deﬁne functions many variables ﬁnd highest lowest points functions quantify degrees belief next describe fundamental goals machine learning describe accomplish goals specifying model represents certain beliefs designing cost function measures well beliefs correspond reality using training algorithm minimize cost function elementary framework basis broad variety machine learning algorithms including approaches machine learning deep subsequent parts book develop deep learning algorithms within framework 
[linear, algebra] linear algebra branch mathematics widely used throughout science engineering however linear algebra form continuous rather discrete mathematics many computer scientists little experience good understanding linear algebra essential understanding working many machine learning algorithms especially deep learning algorithms therefore precede introduction deep learning focused presentation key linear algebra prerequisites already familiar linear algebra feel free skip chapter previous experience concepts need detailed reference sheet review key formulas recommend matrix cookbook petersen pedersen exposure linear algebra chapter teach enough read book highly recommend also consult another resource focused exclusively teaching linear algebra shilov chapter completely omit many important linear algebra topics essential understanding deep learning 
[linear, algebra, scalars,, vectors,, matrices, tensors] study linear algebra involves several types mathematical objects scalars scalar single number contrast objects studied linear algebra usually arrays multiple numbers write scalars italics usually give scalars lower case variable names introduce specify kind number chapter linear algebra example might say let slope line deﬁning real valued scalar let number units deﬁning natural number scalar vectors vector array numbers numbers arranged order identify individual number index ordering typically give vectors lower case names written bold typeface elements vector identiﬁed writing name italic typeface subscript ﬁrst element second element also need say kind numbers stored vector element vector elements vector lies set formed taking cartesian product times denoted need explicitly identify elements vector write column enclosed square brackets ufee ufef ufef ufef uff uff uffa uffa uffa uffb think vectors identifying points space element giving coordinate along diﬀerent axis sometimes need index set elements vector case deﬁne set containing indices write set subscript example access deﬁne set write use sign index complement set example vector containing elements except vector containing elements except matrices matrix array numbers element identiﬁed two indices instead one usually give matrices upper case variable names bold typeface real valued matrix height width say usually identify elements matrix using name italic bold font indices listed separating commas example upper left entry bottom right entry identify numbers vertical coordinate writing horizontal coordinate example denotes horizontal cross section vertical coordinate known row likewise chapter linear algebra ufee uff uff uffb figure transpose matrix thought mirror image across main diagonal column need explicitly identify elements matrix write array enclosed square brackets sometimes may need index matrix valued expressions single letter case use subscripts expression convert anything lower case example gives element matrix computed applying function tensors cases need array two axes general case array numbers arranged regular grid variable number axes known denote tensor named tensor typeface identify element coordinates writing one important operation matrices transpose transpose matrix mirror image matrix across diagonal line called main diagonal running right starting upper left corner see fig graphical depiction operation denote transpose matrix deﬁned vectors thought matrices contain one column transpose vector therefore matrix one row sometimes chapter linear algebra deﬁne vector writing elements text inline row matrix using transpose operator turn standard column vector scalar thought matrix single entry see scalar transpose add matrices long shape adding corresponding elements also add scalar matrix multiply matrix scalar performing operation element matrix context deep learning also use less conventional notation allow addition matrix vector yielding another matrix words vector added row matrix shorthand eliminates need deﬁne matrix copied row addition implicit copying many locations called broadcasting 
[linear, algebra, multiplying, matrices, vectors] one important operations involving matrices multiplication two matrices matrix product matrices third matrix order product deﬁned must number columns rows shape shape shape write matrix product placing two matrices together product operation deﬁned note standard product two matrices matrix containing product individual elements operation exists called element wise product hadamard product denoted dot product two vectors dimensionality matrix product think matrix product computing dot product row column chapter linear algebra matrix product operations many useful properties make mathematical analysis matrices convenient example matrix multiplication distributive also associative matrix multiplication commutative condition always hold unlike scalar multiplication however dot product two vectors commutative transpose matrix product simple form allows demonstrate exploiting fact value product scalar therefore equal transpose since focus textbook linear algebra attempt develop comprehensive list useful properties matrix product reader aware many exist know enough linear algebra notation write system linear equations known matrix known vector vector unknown variables would like solve element one unknown variables row element provide another constraint rewrite even explicitly chapter linear algebra ufee uff uff uffb figure example identity matrix matrix vector product notation provides compact representation equations form 
[linear, algebra, identity, inverse, matrices] linear algebra oﬀers powerful tool called allows matrix inversion analytically solve many values describe matrix inversion ﬁrst need deﬁne concept identity matrix identity matrix matrix change vector multiply vector matrix denote identity matrix preserves dimensional vectors formally structure identity matrix simple entries along main diagonal entries zero see fig example matrix inverse denoted deﬁned matrix solve following steps chapter linear algebra course depends possible ﬁnd discuss conditions existence following section exists several diﬀerent algorithms exist ﬁnding closed form theory inverse matrix used solve equation many times diﬀerent values however primarily useful theoretical tool actually used practice software applications represented limited precision digital computer algorithms make use value usually obtain accurate estimates 
[linear, algebra, linear, dependence, span] order exist must exactly one solution every value however also possible system equations solutions inﬁnitely many solutions values possible one less inﬁnitely many solutions particular solutions also solution real analyze many solutions equation think columns specifying diﬀerent directions travel point origin speciﬁed vector zeros determine many ways reaching view element speciﬁes far travel directions specifying far move direction column general kind operation called linear combination formally linear combination set vectors given multiplying vector corresponding scalar coeﬃcient adding results span set vectors set points obtainable linear combination original vectors chapter linear algebra determining whether solution thus amounts testing whether span columns particular span known column space range order system solution values therefore require column space point excluded column space point potential value solution requirement column space implies immediately must least columns otherwise dimensionality column space would less example consider matrix target modifying value best allows trace plane within equation solution lies plane necessary condition every point solution suﬃcient condition possible columns redundant consider matrix columns identical column space matrix containing one copy replicated column words column space still line fails encompass even though two columns formally kind redundancy known linear dependence set vectors linearly independent vector set linear combination vectors add vector set linear combination vectors set new vector add points set span means column space matrix encompass matrix must contain least one set linearly independent columns condition necessary suﬃcient solution every value note requirement set exactly linear independent columns least set dimensional vectors mutually linearly independent columns matrix columns may one set order matrix inverse additionally need ensure one solution value need ensure matrix columns otherwise one way parametrizing solution together means matrix must square require columns must linearly independent square matrix linearly dependent columns known singular square square singular still possible solve chapter linear algebra equation however use method matrix inversion ﬁnd solution far discussed matrix inverses multiplied left also possible deﬁne inverse multiplied right square matrices left inverse right inverse equal 
[linear, algebra, norms] sometimes need measure size vector machine learning usually measure size vectors using function called formally norm norm given norms including norm functions mapping vectors non negative values intuitive level norm vector measures distance origin point rigorously norm function satisﬁes following properties triangle inequality norm known euclidean norm simply euclidean distance origin point identiﬁed norm used frequently machine learning often denoted simply subscript omitted also common measure size vector using squared norm calculated simply squared norm convenient work mathematically computationally norm example derivatives squared norm respect element depend corresponding element derivatives norm depend entire vector many contexts squared norm may undesirable chapter linear algebra increases slowly near origin several machine learning applications important discriminate elements exactly zero elements small nonzero cases turn function grows rate locations retains mathematical simplicity norm norm may simpliﬁed norm commonly used machine learning diﬀerence zero nonzero elements important every time element moves away norm increases sometimes measure size vector counting number nonzero elements authors refer function norm incorrect terminology number non zero entries vector norm scaling vector change number nonzero entries norm often used substitute number nonzero entries one norm commonly arises machine learning norm also known norm simpliﬁes absolute value max norm element largest magnitude vector max sometimes may also wish measure size matrix context deep learning common way otherwise obscure frobenius norm analogous norm vector dot product two vectors rewritten terms norms speciﬁcally cos angle 
[linear, algebra, special, kinds, matrices, vectors] special kinds matrices vectors particularly useful chapter linear algebra diagonal matrices consist mostly zeros non zero entries along main diagonal formally matrix diagonal already seen one example diagonal matrix identity matrix diagonal entries write diag denote square diagonal matrix whose diagonal entries given entries vector diagonal matrices interest part multiplying diagonal matrix computationally eﬃcient compute diag need scale element words diag inverting square diagonal matrix also eﬃcient inverse exists every diagonal entry nonzero case diag diag many cases may derive general machine learning algorithm terms arbitrary matrices obtain less expensive less descriptive algorithm restricting matrices diagonal diagonal matrices need square possible construct rectangular diagonal matrix non square diagonal matrices inverses still possible multiply cheaply non square diagonal matrix product involve scaling element either concatenating zeros result taller wide discarding last elements vector wider tall matrix matrix equal transpose symmetric symmetric matrices often arise entries generated function two arguments depend order arguments example matrix distance measurements giving distance point point distance functions symmetric unit vector unit norm vector vector vector orthogonal vectors nonzero norm means degree angle vectors may mutually orthogonal nonzero norm vectors orthogonal also unit norm call orthonormal orthogonal matrix square matrix whose rows mutually orthonormal whose columns mutually orthonormal chapter linear algebra implies orthogonal matrices interest inverse cheap compute pay careful attention deﬁnition orthogonal matrices counterintuitively rows merely orthogonal fully orthonormal special term matrix whose rows columns orthogonal orthonormal 
[linear, algebra, eigendecomposition] many mathematical objects understood better breaking constituent parts ﬁnding properties universal caused way choose represent example integers decomposed prime factors way represent number change depending whether write base ten binary always true representation conclude useful properties divisible integer multiple divisible much discover something true nature integer decomposing prime factors also decompose matrices ways show information functional properties obvious representation matrix array elements one widely used kinds matrix decomposition called eigen decomposition decompose matrix set eigenvectors eigenvalues eigenvector square matrix non zero vector multipli cation alters scale scalar known corresponding eigenvector one eigenvalue also ﬁnd left eigenvector usually concerned right eigenvectors eigenvector rescaled vector moreover still eigenvalue reason usually look unit eigenvectors suppose matrix linearly independent eigenvectors corresponding eigenvalues may concatenate chapter linear algebra figure example eﬀect eigenvectors eigenvalues matrix two orthonormal eigenvectors eigenvalue eigenvalue left plot set unit vectors unit circle right plot set points observing way distorts unit circle see scales space direction eigenvectors form matrix one eigenvector per column likewise concatenate eigenvalues form vector eigendecomposition given diag seen constructing matrices speciﬁc eigenvalues eigenvec tors allows stretch space desired directions however often want decompose matrices eigenvalues eigenvectors help analyze certain properties matrix much decomposing integer prime factors help understand behavior integer every matrix decomposed eigenvalues eigenvectors chapter linear algebra cases decomposition exists may involve complex rather real numbers fortunately book usually need decompose speciﬁc class matrices simple decomposition speciﬁcally every real symmetric matrix decomposed expression using real valued eigenvectors eigenvalues orthogonal matrix composed eigenvectors diagonal matrix eigenvalue associated eigenvector column denoted orthogonal matrix think scaling space direction see fig example real symmetric matrix guaranteed eigendecomposi tion eigendecomposition may unique two eigenvectors share eigenvalue set orthogonal vectors lying span also eigenvectors eigenvalue could equivalently choose using eigenvectors instead convention usually sort entries descending order convention eigendecomposition unique eigenvalues unique eigendecomposition matrix tells many useful facts matrix matrix singular eigenvalues zero eigendecomposition real symmetric matrix also used optimize quadratic expressions form subject whenever equal eigenvector takes value corresponding eigenvalue maximum value within constraint region maximum eigenvalue minimum value within constraint region minimum eigenvalue matrix whose eigenvalues positive called positive deﬁnite matrix whose eigenvalues positive zero valued called positive semideﬁnite likewise eigenvalues negative matrix negative deﬁnite eigenvalues negative zero valued negative semideﬁnite positive semideﬁnite matrices interesting guarantee positive deﬁnite matrices additionally guarantee  
[linear, algebra, singular, value, decomposition] sec saw decompose matrix eigenvectors eigenvalues singular value decomposition svd provides another way factorize matrix singular vectors singular values svd allows discover kind information eigendecomposition however svd chapter linear algebra generally applicable every real matrix singular value decomposition true eigenvalue decomposition example matrix square eigendecomposition deﬁned must use singular value decomposition instead recall eigendecomposition involves analyzing matrix discover matrix eigenvectors vector eigenvalues rewrite diag singular value decomposition similar except time write product three matrices suppose matrix deﬁned matrix matrix matrix matrices deﬁned special structure matrices deﬁned orthogonal matrices matrix deﬁned diagonal matrix note necessarily square elements along diagonal known singular values matrix columns known left singular vectors columns known right singular vectors actually interpret singular value decomposition terms eigendecomposition functions left singular vectors eigenvectors right singular vectors eigenvectors non zero singular values square roots eigenvalues true perhaps useful feature svd use partially generalize matrix inversion non square matrices see next section 
[linear, algebra, moore-penrose, pseudoinverse] matrix inversion deﬁned matrices square suppose want make left inverse matrix solve linear equation chapter linear algebra left multiplying side obtain depending structure problem may possible design unique mapping taller wide possible equation solution wider tall could multiple possible solutions moore penrose pseudoinverse allows make headway cases pseudoinverse deﬁned matrix lim practical algorithms computing pseudoinverse based deﬁni tion rather formula singular value decomposition pseudoinverse diagonal matrix obtained taking reciprocal non zero elements taking transpose resulting matrix columns rows solving linear equation using pseudoinverse provides one many possible solutions speciﬁcally provides solution minimal euclidean norm among possible solutions rows columns possible solution case using pseudoinverse gives close possible terms euclidean norm 
[linear, algebra, trace, operator] trace operator gives sum diagonal entries matrix trace operator useful variety reasons operations diﬃcult specify without resorting summation notation speciﬁed using chapter linear algebra matrix products trace operator example trace operator provides alternative way writing frobenius norm matrix writing expression terms trace operator opens opportunities manipulate expression using many useful identities example trace operator invariant transpose operator trace square matrix composed many factors also invariant moving last factor ﬁrst position shapes corresponding matrices allow resulting product deﬁned abc cab bca generally invariance cyclic permutation holds even resulting product diﬀerent shape example even though another useful fact keep mind scalar trace 
[linear, algebra, determinant] determinant square matrix denoted det function mapping matrices real scalars determinant equal product eigenvalues matrix absolute value determinant thought measure much multiplication matrix expands contracts space determinant space contracted completely along least one dimension causing lose volume determinant transformation volume preserving chapter linear algebra 
[linear, algebra, example, principal, components, analysis] one simple machine learning algorithm principal components analysis pca derived using knowledge basic linear algebra suppose collection points suppose would like apply lossy compression points lossy compression means storing points way requires less memory may lose precision would like lose little precision possible one way encode points represent lower dimensional version point ﬁnd corresponding code vector smaller take less memory store code points original data want ﬁnd encoding function produces code input decoding function produces reconstructed input given code pca deﬁned choice decoding function speciﬁcally make decoder simple choose use matrix multiplication map code back let matrix deﬁning decoding computing optimal code decoder could diﬃcult problem keep encoding problem easy pca constrains columns orthogonal note still technically orthogonal matrix unless problem described far many solutions possible increase scale decrease proportionally points give problem unique solution constrain columns unit norm order turn basic idea algorithm implement ﬁrst thing need ﬁgure generate optimal code point input point one way minimize distance input point reconstruction measure distance using norm principal components algorithm use norm arg min switch squared norm instead norm minimized value norm non negative squaring operation monotonically increasing non negative chapter linear algebra arguments arg min function minimized simpliﬁes deﬁnition norm distributive property scalar equal transpose change function minimized omit ﬁrst term since term depend arg min make progress must substitute deﬁnition arg min arg min orthogonality unit norm constraints arg min solve optimization problem using vector calculus see sec know makes algorithm eﬃcient optimally encode using matrix vector operation encode vector apply encoder function chapter linear algebra using matrix multiplication also deﬁne pca reconstruction operation next need choose encoding matrix revisit idea minimizing distance inputs reconstructions however since use matrix decode points longer consider points isolation instead must minimize frobenius norm matrix errors computed dimensions points arg min subject derive algorithm ﬁnding start considering case case single vector substituting simplifying problem reduces arg min subject formulation direct way performing substitution stylistically pleasing way write equation places scalar value right vector conventional write scalar coeﬃcients left vector operate therefore usually write formula arg min subject exploiting fact scalar transpose arg min subject reader aim become familiar cosmetic rearrangements point helpful rewrite problem terms single design matrix examples rather sum separate example vectors allow use compact notation let matrix deﬁned stacking vectors describing points rewrite problem arg min xdd subject  
[probability, information, theory] chapter describe probability theory information theory probability theory mathematical framework representing uncertain statements provides means quantifying uncertainty axioms deriving new uncertain statements artiﬁcial intelligence applications use probability theory two major ways first laws probability tell systems reason design algorithms compute approximate various expressions derived using probability theory second use probability statistics theoretically analyze behavior proposed systems probability theory fundamental tool many disciplines science engineering provide chapter ensure readers whose background primarily software engineering limited exposure probability theory understand material book probability theory allows make uncertain statements reason presence uncertainty information allows quantify amount uncertainty probability distribution already familiar probability theory information theory may wish skip chapter except sec describes graphs use describe structured probabilistic models machine learning absolutely prior experience subjects chapter suﬃcient successfully carry deep learning research projects suggest consult additional resource jaynes chapter probability information theory 
[probability, information, theory, probability?] many branches computer science deal mostly entities entirely deterministic certain programmer usually safely assume cpu execute machine instruction ﬂawlessly errors hardware occur rare enough software applications need designed account given many computer scientists software engineers work relatively clean certain environment surprising machine learning makes heavy use probability theory machine learning must always deal uncertain quantities sometimes may also need deal stochastic non deterministic quantities uncertainty stochasticity arise many sources researchers made compelling arguments quantifying uncertainty using probability since least many arguments presented summarized inspired pearl nearly activities require ability reason presence uncertainty fact beyond mathematical statements true deﬁnition diﬃcult think proposition absolutely true event absolutely guaranteed occur three possible sources uncertainty inherent stochasticity system modeled example interpretations quantum mechanics describe dynamics subatomic particles probabilistic also create theoretical scenarios postulate random dynamics hypothetical card game assume cards truly shuﬄed random order incomplete observability even deterministic systems appear stochastic cannot observe variables drive behavior system example monty hall problem game show contestant asked choose three doors wins prize held behind chosen door two doors lead goat third leads car outcome given contestant choice deterministic contestant point view outcome uncertain incomplete modeling use model must discard information observed discarded information results uncertainty model predictions example suppose build robot exactly observe location every object around chapter probability information theory robot discretizes space predicting future location objects discretization makes robot immediately become uncertain precise position objects object could anywhere within discrete cell observed occupy many cases practical use simple uncertain rule rather complex certain one even true rule deterministic modeling system ﬁdelity accommodate complex rule example simple rule birds cheap develop broadly useful rule form birds except young birds yet learned sick injured birds lost ability ﬂightless species birds including cassowary ostrich kiwi expensive develop maintain communicate eﬀort still brittle prone failure given need means representing reasoning uncertainty immediately obvious probability theory provide tools want artiﬁcial intelligence applications probability theory originally developed analyze frequencies events easy see probability theory used study events like drawing certain hand cards game poker kinds events often repeatable say outcome probability occurring means repeated experiment draw hand cards inﬁnitely many times proportion repetitions would result outcome kind reasoning seem immediately applicable propositions repeatable doctor analyzes patient says patient chance means something diﬀerent make inﬁnitely many replicas patient reason believe diﬀerent replicas patient would present symptoms yet varying underlying conditions case doctor diagnosing patient use probability represent degree belief indicating absolute certainty patient indicating absolute certainty patient former kind probability related directly rates events occur known frequentist probability latter related qualitative levels certainty known bayesian probability list several properties expect common sense reasoning uncertainty way satisfy properties treat bayesian probabilities behaving exactly frequentist probabilities example want compute probability player win poker game given certain set cards use exactly formulas compute probability patient disease given chapter probability information theory certain symptoms details small set common sense assumptions implies axioms must control kinds probability see ramsey probability seen extension logic deal uncertainty logic provides set formal rules determining propositions implied true false given assumption set propositions true false probability theory provides set formal rules determining likelihood proposition true given likelihood propositions 
[probability, information, theory, random, variables] random variable variable take diﬀerent values randomly typically denote random variable lower case letter plain typeface values take lower case script letters example possible values random variable take vector valued variables would write random variable one values random variable description states possible must coupled probability distribution speciﬁes likely states random variables may discrete continuous discrete random variable one ﬁnite countably inﬁnite number states note states necessarily integers also named states considered numerical value continuous random variable associated real value 
[probability, information, theory, probability, distributions] probability distribution description likely random variable set random variables take possible states way describe probability distributions depends whether variables discrete continuous 
[probability, information, theory, probability, distributions, discrete, variables, probability, mass, functions] probability distribution discrete variables may described using proba bility mass function pmf typically denote probability mass functions capital often associate random variable diﬀerent probability chapter probability information theory mass function reader must infer probability mass function use based identity random variable rather name function usually probability mass function maps state random variable probability random variable taking state probability denoted probability indicating certain probability indicating impossible sometimes disambiguate pmf use write name random variable explicitly sometimes deﬁne variable ﬁrst use notation specify distribution follows later probability mass functions act many variables time probability distribution many variables known joint probability distribution denotes probability simultaneously may also write brevity probability mass function random variable function must satisfy following properties domain must set possible states impossible event probability state less probable likewise event guaranteed happen probability state greater chance occurring refer property normalized without property could obtain probabilities greater one computing probability one many events occurring example consider single discrete random variable diﬀerent states place uniform distribution make states equally likely setting probability mass function see ﬁts requirements probability mass function value positive positive integer also see distribution properly normalized chapter probability information theory 
[probability, information, theory, probability, distributions, continuous, variables, probability, density, functions] working continuous random variables describe probability dis tributions using probability density function pdf rather probability mass function probability density function function must satisfy following properties domain must set possible states note require probability density function give probability speciﬁc state directly instead probability landing inside inﬁnitesimal region volume given integrate density function ﬁnd actual probability mass set points speciﬁcally probability lies set given integral set univariate example probability lies interval given example probability density function corresponding speciﬁc probability density continuous random variable consider uniform distribu tion interval real numbers function endpoints interval notation means parametrized consider argument function parameters deﬁne function ensure probability mass outside interval say within see nonnegative everywhere additionally integrates often denote follows uniform distribution writing 
[probability, information, theory, marginal, probability] sometimes know probability distribution set variables want know probability distribution subset probability distribution subset known marginal probability distribution example suppose discrete random variables know ﬁnd sum rule chapter probability information theory name marginal probability comes process computing marginal probabilities paper values written grid diﬀerent values rows diﬀerent values columns natural sum across row grid write margin paper right row continuous variables need use integration instead summation  
[probability, information, theory, conditional, probability] many cases interested probability event given event happened called conditional probability denote conditional probability given conditional probability computed formula conditional probability deﬁned cannot compute conditional probability conditioned event never happens important confuse conditional probability computing would happen action undertaken conditional probability person germany given speak german quite high randomly selected person taught speak german country origin change computing consequences action called making intervention query intervention queries domain causal modeling explore book 
[probability, information, theory, chain, rule, conditional, probabilities] joint probability distribution many random variables may decomposed conditional distributions one variable observation known chain rule product rule probability follows immediately deﬁnition conditional probability chapter probability information theory example applying deﬁnition twice get 
[probability, information, theory, independence, conditional, independence] two random variables independent probability distribution expressed product two factors one involving one involving two random variables conditionally independent given random variable conditional probability distribution factorizes way every value denote independence conditional independence compact notation means independent means conditionally independent given 
[probability, information, theory, expectation,, variance, covariance] expectation expected value function respect probability distribution average mean value takes drawn discrete variables computed summation continuous variables computed integral  chapter probability information theory identity distribution clear context may simply write name random variable expectation clear random variable expectation may omit subscript entirely default assume averages values random variables inside brackets likewise ambiguity may omit square brackets expectations linear example dependent variance gives measure much values function random variable vary sample diﬀerent values probability distribution var variance low values cluster near expected value square root variance known standard deviation covariance gives sense much two values linearly related well scale variables cov high absolute values covariance mean values change much far respective means time sign covariance positive variables tend take relatively high values simultaneously sign covariance negative one variable tends take relatively high value times takes relatively low value vice versa measures correlation normalize contribution variable order measure much variables related rather also aﬀected scale separate variables notions covariance dependence related fact distinct concepts related two variables independent zero covariance two variables non zero covariance dependent ever independence distinct property covariance two variables zero covariance must linear dependence independence stronger requirement zero covariance independence also excludes nonlinear relationships possible two variables dependent zero covariance example suppose ﬁrst sample real number uniform distribution interval next sample random variable chapter probability information theory probability choose value otherwise choose value generate random variable assigning clearly independent completely determines magnitude however cov covariance matrix random vector matrix cov cov diagonal elements covariance give variance cov var 
[probability, information, theory, common, probability, distributions] several simple probability distributions useful many contexts machine learning 
[probability, information, theory, common, probability, distributions, bernoulli, distribution] distribution distribution single binary random variable bernoulli controlled single parameter gives probability random variable equal following properties var 
[probability, information, theory, common, probability, distributions, multinoulli, distribution] multinoulli categorical distribution distribution single discrete variable diﬀerent states ﬁnite multinoulli distribution multinoulli term recently coined gustavo lacerdo popularized murphy multinoulli distribution special case distribution multinomial multinomial distribution distribution vectors representing many times categories visited samples drawn multinoulli distribution many texts use term multinomial refer multinoulli distributions without clarifying refer case chapter probability information theory parametrized vector gives probability state ﬁnal state probability givennote must constrainmultinoulli distributions often used refer distributions categories objects usually assume state numerical value etc reason usually need compute expectation variance multinoulli distributed random variables bernoulli multinoulli distributions suﬃcient describe distri bution domain model discrete variables feasible simply enumerate states dealing continuous variables uncountably many states distribution described small number parameters must impose strict limits distribution 
[probability, information, theory, common, probability, distributions, gaussian, distribution] commonly used distribution real numbers normal distribution also known gaussian distribution exp see fig plot density function two parameters control normal distribution parameter gives coordinate central peak also mean distribution standard deviation distribution given variance evaluate pdf need square invert need frequently evaluate pdf diﬀerent parameter values eﬃcient way parametrizing distribution use parameter control precision inverse variance distribution exp normal distributions sensible choice many applications absence prior knowledge form distribution real numbers take normal distribution good default choice two major reasons first many distributions wish model truly close normal distributions central limit theorem shows sum many independent random variables approximately normally distributed means chapter probability information theory figure normal distribution normal distribution exhibits classic bell curve shape coordinate central peak given width peak controlled example depict standard normal distribution practice many complicated systems modeled successfully normally distributed noise even system decomposed parts structured behavior second possible probability distributions variance normal distribution encodes maximum amount uncertainty real numbers thus think normal distribution one inserts least amount prior knowledge model fully developing justifying idea requires mathematical tools postponed sec normal distribution generalizes case known multivariate normal distribution may parametrized positive deﬁnite symmetric matrix det expparameter still gives mean distribution though vector valued parameter gives covariance matrix distribution univariate case wish evaluate pdf several times chapter probability information theory many diﬀerent values parameters covariance computationally eﬃcient way parametrize distribution since need invert evaluate pdf instead use precision matrix det expoften covariance matrix diagonal matrix even simpler version isotropic gaussian distribution whose covariance matrix scalar times identity matrix 
[probability, information, theory, common, probability, distributions, exponential, laplace, distributions] context deep learning often want probability distribution sharp point accomplish use exponential distribution exp exponential distribution uses indicator function assign probability zero negative values closely related probability distribution allows place sharp peak probability mass arbitrary point laplace distribution laplace exp 
[probability, information, theory, common, probability, distributions, dirac, distribution, empirical, distribution] cases wish specify mass probability distribution clusters around single point accomplished deﬁning pdf using dirac delta function dirac delta function deﬁned zero valued everywhere except yet integrates dirac delta function ordinary function associates value real valued output instead diﬀerent kind mathematical object called generalized function deﬁned terms properties integrated think dirac delta function limit point series functions put less less mass points chapter probability information theory deﬁning shifted obtain inﬁnitely narrow inﬁnitely high peak probability mass common use dirac delta distribution component empirical distribution puts probability mass points forming given data set collection samples dirac delta distribution necessary deﬁne empirical distribution continuous variables discrete variables situation simpler empirical distribution conceptualized multinoulli distribution probability associated possible input value simply equal empirical frequency value training set view empirical distribution formed dataset training examples specifying distribution sample train model dataset another important perspective empirical distribution probability density maximizes likelihood training data see sec 
[probability, information, theory, common, probability, distributions, mixtures, distributions] also common deﬁne probability distributions combining simpler probability distributions one common way combining distributions construct mixture distribution mixture distribution made several component distributions trial choice component distribution generates sample determined sampling component identity multinoulli distribution multinoulli distribution component identities already seen one example mixture distribution empirical distribution real valued variables mixture distribution one dirac component training example mixture model one simple strategy combining probability distributions create richer distribution chapter explore art building complex probability distributions simple ones detail chapter probability information theory mixture model allows brieﬂy glimpse concept paramount importance later latent variable random latent variable variable cannot observe directly component identity variable mixture model provides example latent variables may related joint distribution case distribution latent variable distribution relating latent variables visible variables determines shape distribution even though possible describe without reference latent variable latent variables discussed sec powerful common type mixture model gaussian mixture model components gaussians component separately parametrized mean covariance mixtures constraints example covariances could shared across components via constraint single gaussian distribution mixture gaussians might constrain covariance matrix component diagonal isotropic addition means covariances parameters gaussian mixture specify prior probability given component word prior indicates expresses model beliefs observed comparison posterior probability computed observation gaussian mixture model universal approximator densities sense smooth density approximated speciﬁc non zero amount error gaussian mixture model enough components fig shows samples gaussian mixture model 
[probability, information, theory, useful, properties, common, functions] certain functions arise often working probability distributions especially probability distributions used deep learning models one functions logistic sigmoid exp logistic sigmoid commonly used produce parameter bernoulli distribution range lies within valid range values parameter see fig graph sigmoid function sigmoid chapter probability information theory figure samples gaussian mixture model example three components left right ﬁrst component isotropic covariance matrix meaning amount variance direction second diagonal covariance matrix meaning control variance separately along axis aligned direction example variance along axis along axis third component full rank covariance matrix allowing control variance separately along arbitrary basis directions function saturates argument positive negative meaning function becomes ﬂat insensitive small changes input another commonly encountered function function softplus dugas log exp softplus function useful producing parameter normal distribution range also arises commonly manipulating expressions involving sigmoids name softplus function comes fact smoothed softened version max see fig graph softplus function following properties useful enough may wish memorize exp exp exp chapter probability information theory figure logistic sigmoid function figure softplus function chapter probability information theory log log log exp  function called logit statistics term rarely used machine learning provides extra justiﬁcation name softplus softplus function intended smoothed version positive part function max positive part function counterpart negative part function max obtain smooth function analogous negative part one use recovered positive part negative part via identity also possible recover using relationship shown 
[probability, information, theory, bayes’, rule] often ﬁnd situation know need know fortunately also know compute desired quantity using bayes rule note appears formula usually feasible compute need begin knowledge bayes rule straightforward derive deﬁnition conditional probability useful know name formula since many texts refer name named reverend thomas bayes ﬁrst discovered special case formula general version presented independently discovered pierre simon laplace chapter probability information theory 
[probability, information, theory, technical, details, continuous, variables] proper formal understanding continuous random variables probability density functions requires developing probability theory terms branch mathematics known measure theory measure theory beyond scope textbook brieﬂy sketch issues measure theory employed resolve sec saw probability continuous vector valued lying set given integral set choices set produce paradoxes example possible construct two sets sets generally constructed making heavy use inﬁnite precision real numbers example making fractal shaped sets sets deﬁned transforming set rational numbers one key contributions measure theory provide characterization set sets compute probability without encountering paradoxes book integrate sets relatively simple descriptions aspect measure theory never becomes relevant concern purposes measure theory useful describing theorems apply points apply corner cases measure theory provides rigorous way describing set points negligibly small set said measure zero formally deﬁne concept textbook however useful understand intuition set measure zero occupies volume space measuring example within line measure zero ﬁlled polygon positive measure likewise individual point measure zero union countably many sets measure zero also measure zero set rational numbers measure zero instance another useful term measure theory almost everywhere property holds almost everywhere holds throughout space except set measure zero exceptions occupy negligible amount space safely ignored many applications important results probability theory hold discrete values hold almost everywhere continuous values another technical detail continuous variables relates handling continuous random variables deterministic functions one another suppose two random variables invertible con banach tarski theorem provides fun example sets chapter probability information theory tinuous diﬀerentiable transformation one might expect actually case simple example suppose scalar random variables suppose use rule everywhere except interval interval means  violates deﬁnition probability distribution common mistake wrong fails account distortion space introduced function recall probability lying inﬁnitesimally small region volume given since expand contract space inﬁnitesimal volume surrounding space may diﬀerent volume space see correct problem return scalar case need preserve property solving obtain equivalently higher dimensions derivative generalizes determinant jacobian matrix matrix thus real valued vectors    det     
[probability, information, theory, information, theory] information theory branch applied mathematics revolves around quantifying much information present signal originally invented study sending messages discrete alphabets noisy channel communication via radio transmission context information theory tells design optimal codes calculate expected length messages sampled chapter probability information theory speciﬁc probability distributions using various encoding schemes context machine learning also apply information theory continuous variables message length interpretations apply ﬁeld fundamental many areas electrical engineering computer science textbook mostly use key ideas information theory characterize probability distributions quantify similarity probability distributions detail information theory see cover thomas mackay basic intuition behind information theory learning unlikely event occurred informative learning likely event occurred message saying sun rose morning uninformative unnecessary send message saying solar eclipse morning informative would like quantify information way formalizes intuition speciﬁcally likely events low information content extreme case events guaranteed happen information content whatsoever less likely events higher information content independent events additive information example ﬁnding tossed coin come heads twice convey twice much information ﬁnding tossed coin come heads order satisfy three properties deﬁne self information event log book always use log mean natural logarithm base deﬁnition therefore written units one nat amount nats information gained observing event probability texts use base logarithms units called information measured bits bits shannons rescaling information measured nats continuous use deﬁnition information analogy properties discrete case lost example event unit density still zero information despite event guaranteed occur chapter probability information theory figure plot shows distributions closer deterministic low shannon entropy distributions close uniform high shannon entropy horizontal axis plot probability binary random variable equal entropy given log log near distribution nearly deterministic random variable nearly always near distribution nearly deterministic random variable nearly always entropy maximal distribution uniform two outcomes self information deals single outcome quantify amount uncertainty entire probability distribution using shannon entropy log also denoted words shannon entropy distribution expected amount information event drawn distribution gives lower bound number bits logarithm base otherwise units diﬀerent needed average encode symbols drawn distribution distributions nearly deterministic outcome nearly certain low entropy distributions closer uniform high entropy see fig demonstration continuous shannon entropy known diﬀerential entropy two separate probability distributions random variable measure diﬀerent two distributions using kullback leibler divergence  log log log chapter probability information theory case discrete variables extra amount information measured bits use base logarithm machine learning usually use nats natural logarithm needed send message containing symbols drawn probability distribution use code designed minimize length messages drawn probability distribution divergence many useful properties notably non negative divergence distribution case discrete variables equal almost everywhere case continuous variables divergence non negative measures diﬀerence two distributions often conceptualized measuring sort distance distributions however true distance measure symmetric   asymmetry means important consequences choice whether use   see fig detail quantity closely related divergence cross entropy  similar divergence lacking term left log minimizing cross entropy respect equivalent minimizing divergence participate omitted term computing many quantities common encounter expres sions form log convention context information theory treat expressions lim log 
[probability, information, theory, structured, probabilistic, models] machine learning algorithms often involve probability distributions large number random variables often probability distributions involve direct interactions relatively variables using single function describe entire joint probability distribution ineﬃcient computationally statistically instead using single function represent probability distribution split probability distribution many factors multiply together example suppose three random variables suppose inﬂuences value inﬂuences value independent given represent probability distribution three chapter probability information theory argmin  argmin  figure divergence asymmetric suppose distribution wish approximate another distribution choice minimizing either   illustrate eﬀect choice using mixture two gaussians single gaussian choice direction divergence use problem dependent applications require approximation usually places high probability anywhere true distribution places high probability applications require approximation rarely places high probability anywhere true distribution places low probability choice direction divergence reﬂects considerations takes priority application left eﬀect minimizing  case select high probability high probability multiple modes chooses blur modes together order put high probability mass right eﬀect minimizing  case select low probability low probability multiple modes suﬃciently widely separated ﬁgure divergence minimized choosing single mode order avoid putting probability mass low probability areas modes illustrate outcome chosen emphasize left mode could also achieved equal value divergence choosing right mode modes separated suﬃciently strong low probability region direction divergence still choose blur modes chapter probability information theory variables product probability distributions two variables factorizations greatly reduce number parameters needed describe distribution factor uses number parameters exponential number variables factor means greatly reduce cost representing distribution able ﬁnd factorization distributions fewer variables describe kinds factorizations using graphs use word graph sense graph theory set vertices may connected edges represent factorization probability distribution graph call structured probabilistic model graphical model two main kinds structured probabilistic models directed undirected kinds graphical models use graph node graph corresponds random variable edge connecting two random variables means probability distribution able represent direct interactions two random variables directed models use graphs directed edges represent factoriza tions conditional probability distributions example speciﬁcally directed model contains one factor every random variable distribution factor consists conditional distribution given parents denoted see fig example directed graph factorization probability distributions represents undirected models use graphs undirected edges represent fac torizations set functions unlike directed case functions usually probability distributions kind set nodes connected called clique clique undirected model associated factor factors functions probability distributions output factor must non negative constraint factor must sum integrate like probability distribution probability conﬁguration random variables proportional product factors assignments result larger factor values chapter probability information theory figure directed graphical model random variables graph corresponds probability distributions factored graph allows quickly see properties distribution example interact directly interact indirectly via likely course guarantee product sum therefore divide normalizing constant deﬁned sum integral states product functions order obtain normalized probability distribution see fig example undirected graph factorization probability distributions represents keep mind graphical representations factorizations language describing probability distributions mutually exclusive families probability distributions directed undirected property probability distribution property particular description probability distribution probability distribution may described ways throughout part part book use structured probabilistic models merely language describe direct probabilistic relationships diﬀerent machine learning algorithms choose represent understanding structured probabilistic models needed discussion research topics part explore structured probabilistic models much greater iii detail chapter probability information theory figure undirected graphical model random variables graph corresponds probability distributions factored graph allows quickly see properties distribution example interact directly interact indirectly via chapter reviewed basic concepts probability theory relevant deep learning one set fundamental mathematical tools remains numerical methods 
[numerical, computation] machine learning algorithms usually require high amount numerical compu tation typically refers algorithms solve mathematical problems methods update estimates solution via iterative process rather analytically deriving formula providing symbolic expression correct lution common operations include optimization ﬁnding value argument minimizes maximizes function solving systems linear equations even evaluating mathematical function digital computer diﬃcult function involves real numbers cannot represented precisely using ﬁnite amount memory 
[numerical, computation, overﬂow, underﬂow] fundamental diﬃculty performing continuous math digital computer need represent inﬁnitely many real numbers ﬁnite number bit patterns means almost real numbers incur approximation error represent number computer many cases rounding error rounding error problematic especially compounds across many operations cause algorithms work theory fail practice designed minimize accumulation rounding error one form rounding error particularly devastating underﬂow ﬂow occurs numbers near zero rounded zero many functions behave qualitatively diﬀerently argument zero rather small positive number example usually want avoid division zero software chapter numerical computation environments raise exceptions occurs others return result placeholder number value taking logarithm zero usually treated becomes number used many arithmetic operations another highly damaging form numerical error overﬂow occurs overﬂow numbers large magnitude approximated arithmetic usually change inﬁnite values number values one example function must stabilized underﬂow overﬂow softmax function softmax function often used predict probabilities associated multinoulli distribution softmax function deﬁned softmax exp exp consider happens equal constant analytically see outputs equal numerically may occur large magnitude negative exp underﬂow means denominator softmax become ﬁnal result undeﬁned large positive exp overﬂow resulting expression whole undeﬁned diﬃculties resolved instead evaluating softmax max simple algebra shows value softmax function changed analytically adding subtracting scalar input vector subtracting max results largest argument exp rules possibility overﬂow likewise least one term denominator value rules possibility underﬂow denominator leading division zero still one small problem underﬂow numerator still cause expression whole evaluate zero means implement log softmax ﬁrst running softmax subroutine passing result log function could erroneously obtain instead must implement separate function calculates log softmax numerically stable way log softmax function stabilized using trick used stabilize function softmax part explicitly detail numerical considerations involved implementing various algorithms described book developers low level libraries keep numerical issues mind implementing deep learning algorithms readers book simply rely low level libraries provide stable implementations cases possible implement new algorithm new implementation automatically chapter numerical computation stabilized theano example bergstra bastien software package automatically detects stabilizes many common numerically unstable expressions arise context deep learning 
[numerical, computation, poor, conditioning] conditioning refers rapidly function changes respect small changes inputs functions change rapidly inputs perturbed slightly problematic scientiﬁc computation rounding errors inputs result large changes output consider function eigenvalue decomposition condition number max ratio magnitude largest smallest eigenvalue number large matrix inversion particularly sensitive error input sensitivity intrinsic property matrix result rounding error matrix inversion poorly conditioned matrices amplify pre existing errors multiply true matrix inverse practice error compounded numerical errors inversion process 
[numerical, computation, gradient-based, optimization] deep learning algorithms involve optimization sort optimization refers task either minimizing maximizing function altering usually phrase optimization problems terms minimizing maximization may accomplished via minimization algorithm minimizing function want minimize maximize called objective function minimizing may also call criterion cost function loss function error function book use terms interchangeably though machine learning publications assign special meaning terms often denote value minimizes maximizes function superscript example might say arg min chapter numerical computation figure illustration derivatives function used follow function downhill minimum technique called gradient descent assume reader already familiar calculus provide brief review calculus concepts relate optimization suppose function real numbers function denoted derivative derivative gives slope point words speciﬁes scale small change input order obtain corresponding change output  derivative therefore useful minimizing function tells change order make small improvement example know sign less small enough thus reduce moving small steps opposite sign derivative technique called gradient descent cauchy see fig example technique derivative provides information direction move points known critical points stationary points local minimum point lower neighboring points longer possible decrease making inﬁnitesimal steps local maximum point higher neighboring points chapter numerical computation figure examples three types critical points critical point point zero slope point either local minimum lower neighboring points local maximum higher neighboring points saddle point neighbors higher lower point possible increase making inﬁnitesimal steps critical points neither maxima minima known saddle points see fig examples type critical point point obtains absolute lowest value global minimum possible one global minimum multiple global minima function also possible local minima globally optimal context deep learning optimize functions may many local minima optimal many saddle points surrounded ﬂat regions makes optimization diﬃcult especially input function multidimensional therefore usually settle ﬁnding value low necessarily minimal formal sense see fig example often minimize functions multiple inputs concept minimization make sense must still one scalar output functions multiple inputs must make use concept partial derivatives partial derivative measures changes variable increases point gradient generalizes notion derivative case derivative respect vector gradient vector containing partial derivatives denoted element gradient partial derivative respect multiple dimensions chapter numerical computation figure optimization algorithms may fail ﬁnd global minimum multiple local minima plateaus present context deep learning generally accept solutions even though truly minimal long correspond signiﬁcantly low values cost function critical points points every element gradient equal zero directional derivative direction unit vector slope function direction words directional derivative derivative function respect evaluated using chain rule see minimize would like ﬁnd direction decreases fastest using directional derivative min min cos angle gradient substituting ignoring factors depend simpliﬁes min cos minimized points opposite direction gradient words gradient points directly uphill negative gradient points directly downhill decrease moving direction negative gradient known method steepest descent gradient descent steepest descent proposes new point chapter numerical computation learning rate positive scalar determining size step choose several diﬀerent ways popular approach set small constant sometimes solve step size makes directional derivative vanish another approach evaluate several values choose one results smallest objective function value last strategy called line search steepest descent converges every element gradient zero practice close zero cases may able avoid running iterative algorithm jump directly critical point solving equation although gradient descent limited optimization continuous spaces general concept making small moves approximately best small move towards better conﬁgurations generalized discrete spaces ascending objective function discrete parameters called hill climbing russel norvig 
[numerical, computation, gradient-based, optimization, beyond, gradient, jacobian, hessian, matrices] sometimes need ﬁnd partial derivatives function whose input output vectors matrix containing partial derivatives known jacobian matrix speciﬁcally function jacobian matrix deﬁned also sometimes interested derivative derivative known second derivative example function derivative respect derivative respect denoted single dimension denote second derivative tells ﬁrst derivative change vary input important tells whether gradient step cause much improvement would expect based gradient alone think second derivative measuring curvature suppose quadratic function many functions arise practice quadratic approximated well quadratic least locally function second derivative zero curvature perfectly ﬂat line value predicted using gradient gradient make step size along negative gradient cost function decrease second derivative negative function curves downward cost function actually decrease finally second derivative positive function curves upward cost function decrease less see fig chapter numerical computation negative curvature curvature positive curvature figure second derivative determines curvature function show quadratic functions various curvature dashed line indicates value cost function would expect based gradient information alone make gradient step downhill case negative curvature cost function actually decreases faster gradient predicts case curvature gradient predicts decrease correctly case positive curvature function decreases slower expected eventually begins increase large step sizes actually increase function inadvertently see diﬀerent forms curvature aﬀect relationship value cost function predicted gradient true value function multiple input dimensions many second derivatives derivatives collected together matrix called hessian matrix hessian matrix deﬁned equivalently hessian jacobian gradient anywhere second partial derivatives continuous diﬀerential operators commutative order swapped implies hessian matrix symmetric points functions encounter context deep learning symmetric hessian almost everywhere hessian matrix real symmetric decompose set real eigenvalues orthogonal basis chapter numerical computation eigenvectors second derivative speciﬁc direction represented unit vector given eigenvector second derivative direction given corresponding eigenvalue directions directional second derivative weighted average eigenvalues weights eigenvectors smaller angle receiving weight maximum eigenvalue determines maximum second derivative minimum eigenvalue determines minimum second derivative directional second derivative tells well expect gradient descent step perform make second order taylor series approximation function around current point gradient hessian use learning rate new point given  substituting approximation obtain   three terms original value function expected improvement due slope function correction must apply account curvature function last term large gradient descent step actually move uphill zero negative taylor series approximation predicts increasing forever decrease forever practice taylor series unlikely remain accurate large one must resort heuristic choices case positive solving optimal step size decreases taylor series approximation function yields worst case aligns eigenvector corresponding maximal eigenvalue max optimal step size given max extent function minimize approximated well quadratic function eigenvalues hessian thus determine scale learning rate second derivative used determine whether critical point local maximum local minimum saddle point recall critical point means increases move right decreases move left means chapter numerical computation small enough words move right slope begins point uphill right move left slope begins point uphill left thus conclude local minimum similarly conclude local maximum known second derivative test unfortunately test inconclusive case may saddle point part ﬂat region multiple dimensions need examine second derivatives function using eigendecomposition hessian matrix generalize second derivative test multiple dimensions critical point examine eigenvalues hessian determine whether critical point local maximum local minimum saddle point hessian positive deﬁnite eigenvalues positive point local minimum seen observing directional second derivative direction must positive making reference univariate second derivative test likewise hessian negative deﬁnite eigenvalues negative point local maximum multiple dimensions actually possible ﬁnd positive evidence saddle points cases least one eigenvalue positive least one eigenvalue negative know local maximum one cross section local minimum another cross section see fig example finally multidimensional second derivative test inconclusive like univariate version test inconclusive whenever non zero eigenvalues sign least one eigenvalue zero univariate second derivative test inconclusive cross section corresponding zero eigenvalue multiple dimensions wide variety diﬀerent second derivatives single point diﬀerent second derivative direction condition number hessian measures much second derivatives vary hessian poor condition number gradient descent performs poorly one direction derivative increases rapidly another direction increases slowly gradient descent unaware change derivative know needs explore preferentially direction derivative remains negative longer also makes diﬃcult choose good step size step size must small enough avoid overshooting minimum going uphill directions strong positive curvature usually means step size small make signiﬁcant progress directions less curvature see fig example issue resolved using information hessian matrix chapter numerical computation figure saddle point containing positive negative curvature function example along axis corresponding function curves upward axis eigenvector hessian positive eigenvalue along axis corresponding function curves downward direction eigenvector hessian negative eigenvalue name saddle point derives saddle like shape function quintessential example function saddle point one dimension necessary eigenvalue order get saddle point necessary positive negative eigenvalues think saddle point signs eigenvalues local maximum within one cross section local minimum within another cross section chapter numerical computation figure gradient descent fails exploit curvature information contained hessian matrix use gradient descent minimize quadratic function whose hessian matrix condition number means direction curvature ﬁve times curvature direction least curvature case curvature direction least curvature direction red lines indicate path followed gradient descent elongated quadratic function resembles long canyon gradient descent wastes time repeatedly descending canyon walls steepest feature step size somewhat large tendency overshoot bottom function thus needs descend opposite canyon wall next iteration large positive eigenvalue hessian corresponding eigenvector pointed direction indicates directional derivative rapidly increasing optimization algorithm based hessian could predict steepest direction actually promising search direction context chapter numerical computation guide search simplest method known newton method newton method based using second order taylor series expansion approximate near point solve critical point function obtain positive deﬁnite quadratic function newton method consists applying jump minimum function directly truly quadratic locally approximated positive deﬁnite quadratic newton method consists applying multiple times iteratively updating approximation jumping minimum approximation reach critical point much faster gradient descent would useful property near local minimum harmful property near saddle point discussed sec newton method appropriate nearby critical point minimum eigenvalues hessian positive whereas gradient descent attracted saddle points unless gradient points toward optimization algorithms gradient descent use gradient called ﬁrst order optimization algorithms optimization algorithms new ton method also use hessian matrix called second order optimization algorithms nocedal wright optimization algorithms employed contexts book applicable wide variety functions come almost guarantees family functions used deep learning quite complicated many ﬁelds dominant approach optimization design optimization algorithms limited family functions context deep learning sometimes gain guarantees restrict ing functions either lipschitz continuous lipschitz continuous derivatives lipschitz continuous function function whose rate change bounded lipschitz constant property useful allows quantify assumption small change input made algorithm gradient descent small change output lipschitz continuity also fairly weak constraint chapter numerical computation many optimization problems deep learning made lipschitz continuous relatively minor modiﬁcations perhaps successful ﬁeld specialized optimization convex optimiza tion convex optimization algorithms able provide many guarantees making stronger restrictions convex optimization algorithms applicable convex functions functions hessian positive semideﬁnite everywhere functions well behaved lack saddle points local minima necessarily global minima however problems deep learning diﬃcult express terms convex optimization convex optimization used subroutine deep learning algorithms ideas analysis convex optimization algorithms useful proving convergence deep learning algorithms however general importance convex optimization greatly diminished context deep learning information convex optimization see boyd vandenberghe rockafellar 
[numerical, computation, constrained, optimization] sometimes wish maximize minimize function possible values instead may wish ﬁnd maximal minimal value values set known constrained optimization points lie within set called feasible points constrained optimization terminology often wish ﬁnd solution small sense common approach situations impose norm constraint one simple approach constrained optimization simply modify gradient descent taking constraint account use small constant step size make gradient descent steps project result back use line search search step sizes yield new points feasible project point line back constraint region possible method made eﬃcient projecting gradient tangent space feasible region taking step beginning line search rosen sophisticated approach design diﬀerent unconstrained opti mization problem whose solution converted solution original constrained optimization problem example want minimize constrained exactly unit norm instead minimize chapter numerical computation cos sin respect return cos sin solution original problem approach requires creativity transformation optimization problems must designed speciﬁcally case encounter karush kuhn tucker kkt approach provides general solution constrained optimization kkt approach introduce new function called generalized lagrangian generalized lagrange function deﬁne lagrangian ﬁrst need describe terms equations inequalities want description terms functions functions equations involving called equality constraints inequalities involving called inequality constraints introduce new variables constraint called kkt multipliers generalized lagrangian deﬁned solve constrained minimization problem using unconstrained optimization generalized lagrangian observe long least one feasible point exists permitted value min max max optimal objective function value set optimal points min follows time constraints satisﬁed max max time constraint violated max max properties guarantee infeasible point ever optimal optimum within feasible points unchanged kkt approach generalizes method lagrange multipliers allows equality constraints inequality constraints chapter numerical computation perform constrained maximization construct generalized grange function leads optimization problem min max max may also convert problem maximization outer loop max min min sign term equality constraints matter may deﬁne addition subtraction wish optimization free choose sign inequality constraints particularly interesting say constraint active constraint active solution problem found using constraint would remain least local solution constraint removed possible inactive constraint excludes solutions example convex problem entire region globally optimal points wide ﬂat region equal cost could subset region eliminated constraints non convex problem could better local stationary points excluded constraint inactive convergence however point found convergence remains stationary point whether inactive constraints included inactive negative value solution min max max thus observe solution words know least one constraints must active solution gain intuition idea say either solution boundary imposed inequality must use kkt multiplier inﬂuence solution inequality inﬂuence solution represent zeroing kkt multiplier properties gradient generalized lagrangian zero constraints kkt multipliers satisﬁed called karush kuhn tucker kkt conditions karush kuhn tucker together properties describe optimal points constrained optimization problems information kkt approach see nocedal wright chapter numerical computation 
[numerical, computation, example, linear, least, squares] suppose want ﬁnd value minimizes specialized linear algebra algorithms solve problem eﬃciently however also explore solve using gradient based optimization simple example techniques work first need obtain gradient follow gradient downhill taking small steps see algorithm details algorithm algorithm minimize respect using gradient descent set step size tolerance small positive numbers end one also solve problem using newton method case true function quadratic quadratic approximation employed newton method exact algorithm converges global minimum single step suppose wish minimize function subject constraint introduce lagrangian solve problem min max smallest norm solution unconstrained least squares problem may found using moore penrose pseudoinverse point feasible solution constrained problem otherwise must ﬁnd chapter numerical computation solution constraint active diﬀerentiating lagrangian respect obtain equation tells solution take form magnitude must chosen result obeys constraint ﬁnd value performing gradient ascent observe norm exceeds derivative positive follow derivative uphill increase lagrangian respect increase coeﬃcient penalty increased solving linear equation yield solution smaller norm process solving linear equation adjusting continues correct norm derivative concludes mathematical preliminaries use develop machine learning algorithms ready build analyze full ﬂedged learning systems 
[machine, learning, basics] deep learning speciﬁc kind machine learning order understand deep learning well one must solid understanding basic principles machine learning chapter provides brief course important general principles applied throughout rest book novice readers want wider perspective encouraged consider machine learning textbooks comprehensive coverage fundamentals murphy bishop already familiar machine learning basics feel free skip ahead sec section covers per spectives traditional machine learning techniques strongly inﬂuenced development deep learning algorithms begin deﬁnition learning algorithm present example linear regression algorithm proceed describe challenge ﬁtting training data diﬀers challenge ﬁnding patterns generalize new data machine learning algorithms settings called hyperparameters must determined external learning algorithm discuss set using additional data machine learning essentially form applied statistics increased emphasis use computers statistically estimate complicated functions decreased emphasis proving conﬁdence intervals around functions therefore present two central approaches statistics frequentist estimators bayesian inference machine learning algorithms divided categories supervised learning unsupervised learning describe categories give examples simple learning algorithms category deep learning algorithms based optimization algorithm called stochastic gradient descent describe combine various algorithm components chapter machine learning basics optimization algorithm cost function model dataset build machine learning algorithm finally sec describe factors limited ability traditional machine learning generalize challenges motivated development deep learning algorithms overcome obstacles 
[machine, learning, basics, learning, algorithms] machine learning algorithm algorithm able learn data mean learning provides deﬁnition computer mitchell program said learn experience respect class tasks performance measure performance tasks measured improves experience one imagine wide variety experiences tasks performance measures make attempt book provide formal deﬁnition may used entities instead following sections provide intuitive descriptions examples diﬀerent kinds tasks performance measures experiences used construct machine learning algorithms 
[machine, learning, basics, learning, algorithms, task] machine learning allows tackle tasks diﬃcult solve ﬁxed programs written designed human beings scientiﬁc philosophical point view machine learning interesting developing understanding machine learning entails developing understanding principles underlie intelligence relatively formal deﬁnition word task process learning task learning means attaining ability perform task example want robot able walk walking task could program robot learn walk could attempt directly write program speciﬁes walk manually machine learning tasks usually described terms machine learning system process example collection example features quantitatively measured object event want machine learning system process typically represent example vector entry vector another feature example features image usually values pixels image chapter machine learning basics many kinds tasks solved machine learning common machine learning tasks include following classiﬁcation type task computer program asked specify categories input belongs solve task learning algorithm usually asked produce function model assigns input described vector category identiﬁed numeric code variants classiﬁcation task example outputs probability distribution classes example classiﬁcation task object recognition input image usually described set pixel brightness values output numeric code identifying object image example willow garage robot able act waiter recognize diﬀerent kinds drinks deliver people command good fellow modern object recognition best accomplished deep learning object krizhevsky ioﬀe szegedy recognition basic technology allows computers recognize faces taigman used automatically tag people photo collections allow computers interact naturally users classiﬁcation missing inputs classiﬁcation becomes challenging computer program guaranteed every measurement input vector always provided order solve classiﬁcation task learning algorithm deﬁne function mapping vector single input categorical output inputs may missing rather providing single classiﬁcation function learning algorithm must learn functions function corresponds classifying set diﬀerent subset inputs missing kind situation arises frequently medical diagnosis many kinds medical tests expensive invasive one way eﬃciently deﬁne large set functions learn probability distribution relevant variables solve classiﬁcation task marginalizing missing variables input variables obtain diﬀerent classiﬁcation functions needed possible set missing inputs need learn single function describing joint probability distribution see goodfellow example deep probabilistic model applied task way many tasks described section also generalized work missing inputs classiﬁcation missing inputs one example machine learning chapter machine learning basics regression type task computer program asked predict numerical value given input solve task learning algorithm asked output function type task similar classiﬁcation except format output diﬀerent example regression task prediction expected claim amount insured person make used set insurance premiums prediction future prices securities kinds predictions also used algorithmic trading transcription type task machine learning system asked observe relatively unstructured representation kind data transcribe discrete textual form example optical character recognition computer program shown photograph containing image text asked return text form sequence characters ascii unicode format google street view uses deep learning process address numbers way goodfellow another example speech recognition computer program provided audio waveform emits sequence characters word codes describing words spoken audio recording deep learning crucial component modern speech recognition systems used major companies including microsoft ibm google hinton machine translation machine translation task input already consists sequence symbols language computer program must convert sequence symbols another language commonly applied natural languages translate english french deep learning recently begun important impact kind task sutskever bahdanau structured output structured output tasks involve task output vector data structure containing multiple values important relationships diﬀerent elements broad category subsumes transcription translation tasks described also many tasks one example parsing mapping natural language sentence tree describes grammatical structure tagging nodes trees verbs nouns adverbs see collobert example deep learning applied parsing task another example pixel wise segmentation images computer program assigns every pixel image speciﬁc category example deep learning chapter machine learning basics used annotate locations roads aerial photographs mnih hinton output need form mirror structure input closely annotation style tasks example image captioning computer program observes image outputs natural language sentence describing image kiros mao vinyals donahue karpathy fang tasks called structured output tasks program must output several values tightly inter related example words produced image captioning program must form valid sentence anomaly detection type task computer program sifts set events objects ﬂags unusual atypical example anomaly detection task credit card fraud detection modeling purchasing habits credit card company detect misuse cards thief steals credit card credit card information thief purchases often come diﬀerent probability distribution purchase types credit card company prevent fraud placing hold account soon card used uncharacteristic purchase see survey chandola anomaly detection methods synthesis sampling type task machine learning algorithm asked generate new examples similar training data synthesis sampling via machine learning useful media applications expensive boring artist generate large volumes content hand example video games automatically generate textures large objects landscapes rather requiring artist manually label pixel cases luo want sampling synthesis procedure generate speciﬁc kind output given input example speech synthesis task provide written sentence ask program emit audio waveform containing spoken version sentence kind structured output task added qualiﬁcation single correct output input explicitly desire large amount variation output order output seem natural realistic imputation missing values type task machine learning algorithm given new example entries missing algorithm must provide prediction values missing entries chapter machine learning basics denoising type task machine learning algorithm given input corrupted example obtained unknown corruption process clean example learner must predict clean example corrupted version generally predict conditional probability distribution density estimation probability mass function estimation density estimation problem machine learning algorithm asked learn function model model interpreted probability density function continuous probability mass function discrete space examples drawn task well specify exactly means discuss performance measures algorithm needs learn structure data seen must know examples cluster tightly unlikely occur tasks described require learning algorithm least implicitly captured structure probability distribution density estimation allows explicitly capture distribution principle perform computations distribution order solve tasks well example performed density estimation obtain probability distribution use distribution solve missing value imputation task value missing values denoted given know distribution given practice density estimation always allow solve related tasks many cases required operations computationally intractable course many tasks types tasks possible types tasks list intended provide examples machine learning deﬁne rigid taxonomy tasks 
[machine, learning, basics, learning, algorithms, performance, measure] order evaluate abilities machine learning algorithm must design quantitative measure performance usually performance measure speciﬁc task carried system tasks classiﬁcation classiﬁcation missing inputs transcrip tion often measure accuracy model accuracy proportion examples model produces correct output also obtain chapter machine learning basics equivalent information measuring error rate proportion examples model produces incorrect output often refer error rate expected loss loss particular example correctly classiﬁed tasks density estimation make sense measure accuracy error rate kind loss instead must use diﬀerent performance metric gives model continuous valued score example common approach report average log probability model assigns examples usually interested well machine learning algorithm performs data seen since determines well work deployed real world therefore evaluate performance measures using data separate data used training machine test set learning system choice performance measure may seem straightforward objective often diﬃcult choose performance measure corresponds well desired behavior system cases diﬃcult decide measured example performing transcription task measure accuracy system transcribing entire sequences use ﬁne grained performance measure gives partial credit getting elements sequence correct performing regression task penalize system frequently makes medium sized mistakes rarely makes large mistakes kinds design choices depend application cases know quantity would ideally like measure measuring impractical example arises frequently context density estimation many best probabilistic models represent probability distributions implicitly computing actual probability value assigned speciﬁc point space many models intractable cases one must design alternative criterion still corresponds design objectives design good approximation desired criterion 
[machine, learning, basics, learning, algorithms, experience] machine learning algorithms broadly categorized unsupervised pervised kind experience allowed learning process learning algorithms book understood allowed experience entire dataset collection many examples dataset chapter machine learning basics deﬁned sec sometimes also call examples data points one oldest datasets studied statisticians machine learning searchers iris dataset collection measurements fisher diﬀerent parts iris plants individual plant corresponds one example features within example measurements parts plant sepal length sepal width petal length petal width dataset also records species plant belonged three diﬀerent species represented dataset unsupervised learning algorithms experience dataset containing many features learn useful properties structure dataset context deep learning usually want learn entire probability distribution generated dataset whether explicitly density estimation implicitly tasks like synthesis denoising unsupervised learning algorithms perform roles like clustering consists dividing dataset clusters similar examples supervised learning algorithms experience dataset containing features example also associated label target example iris dataset annotated species iris plant supervised learning algorithm study iris dataset learn classify iris plants three diﬀerent species based measurements roughly speaking unsupervised learning involves observing several examples random vector attempting implicitly explicitly learn proba bility distribution interesting properties distribution supervised learning involves observing several examples random vector associated value vector learning predict usually estimating term supervised learning originates view target provided instructor teacher shows machine learning system unsupervised learning instructor teacher algorithm must learn make sense data without guide unsupervised learning supervised learning formally deﬁned terms lines often blurred many machine learning technologies used perform tasks example chain rule probability states vector joint distribution decomposed decomposition means solve ostensibly unsupervised problem modeling splitting supervised learning problems alternatively chapter machine learning basics solve supervised learning problem learning using traditional unsupervised learning technologies learn joint distribution inferring though unsupervised learning supervised learning completely formal distinct concepts help roughly categorize things machine learning algorithms traditionally people refer regression classiﬁcation structured output problems supervised learning density estimation support tasks usually considered unsupervised learning variants learning paradigm possible example semi supervised learning examples include supervision target others multi instance learning entire collection examples labeled containing containing example class individual members collection labeled recent example multi instance learning deep models see kotzias machine learning algorithms experience ﬁxed dataset example reinforcement learning algorithms interact environment feedback loop learning system experiences algorithms beyond scope book please see sutton barto bertsekas tsitsiklis mnih information reinforcement learning deep learning approach reinforcement learning machine learning algorithms simply experience dataset dataset described many ways cases dataset collection examples turn collections features one common way describing dataset design design matrix matrix matrix containing diﬀerent example row column matrix corresponds diﬀerent feature instance iris dataset contains examples four features example means represent dataset design matrix sepal length plant sepal width plant etc describe learning algorithms book terms operate design matrix datasets course describe dataset design matrix must possible describe example vector vectors must size always possible example collection photographs diﬀerent widths heights diﬀerent photographs contain diﬀerent numbers pixels photographs may described length vector sec chapter describe handle diﬀerent types chapter machine learning basics heterogeneous data cases like rather describing dataset matrix rows describe set containing elements notation imply two example vectors size case supervised learning example contains label target well collection features example want use learning algorithm perform object recognition photographs need specify object appears photos might numeric code signifying person signifying car signifying cat etc often working dataset containing design matrix feature observations also provide vector labels providing label example course sometimes label may single number example want train speech recognition system transcribe entire sentences label example sentence sequence words formal deﬁnition supervised unsupervised learning rigid taxonomy datasets experiences structures described cover cases always possible design new ones new applications 
[machine, learning, basics, learning, algorithms, example, linear, regression] deﬁnition machine learning algorithm algorithm capable improving computer program performance task via experience somewhat abstract make concrete present example simple machine learning algorithm linear regression return example repeatedly introduce machine learning concepts help understand behavior name implies linear regression solves regression problem words goal build system take vector input predict value scalar output case linear regression output linear function input let value model predicts take deﬁne output vector parameters parameters values control behavior system case coeﬃcient multiply feature summing contributions features think set determine weights feature aﬀects prediction feature receives positive weight chapter machine learning basics increasing value feature increases value prediction feature receives negative weight increasing value feature decreases value prediction feature weight large magnitude large eﬀect prediction feature weight zero eﬀect prediction thus deﬁnition task predict outputting next need deﬁnition performance measure suppose design matrix example inputs use training evaluating well model performs also vector regression targets providing correct value examples dataset used evaluation call test set refer design matrix inputs test vector regression targets test one way measuring performance model compute mean squared error model test set test gives predictions model test set mean squared error given mse test test test intuitively one see error measure decreases test test also see mse test test test error increases whenever euclidean distance predictions targets increases make machine learning algorithm need design algorithm improve weights way reduces mse test algorithm allowed gain experience observing training set train train one intuitive way justify later sec minimize mean squared error training set mse train minimize mse train simply solve gradient mse train train train train train chapter machine learning basics linear regression example optimization figure linear regression problem training set consisting ten data points containing one feature one feature weight vector contains single parameter learn left observe linear regression learns set line comes close possible passing training points plotted point indicates value right found normal equations see minimizes mean squared error training set train train train train train train train train train train train train train train train train train train system equations whose solution given known normal equations evaluating constitutes simple learning algorithm example linear regression learning algorithm action see fig worth noting term linear regression often used refer slightly sophisticated model one additional parameter intercept term model mapping parameters predictions still linear function mapping features predictions aﬃne function extension aﬃne functions means plot model predictions still looks like line need pass origin instead adding bias parameter one continue use model weights augment chapter machine learning basics extra entry always set weight corresponding extra entry plays role bias parameter frequently use term linear referring aﬃne functions throughout book intercept term often called parameter aﬃne transfor bias mation terminology derives point view output transformation biased toward absence input term diﬀerent idea statistical bias statistical estimation algorithm expected estimate quantity equal true quantity linear regression course extremely simple limited learning algorithm provides example learning algorithm work subsequent sections describe basic principles underlying learning algorithm design demonstrate principles used build complicated learning algorithms 
[machine, learning, basics, capacity, overﬁtting, underﬁtting] central challenge machine learning must perform well new previously unseen inputs model trained ability perform well previously unobserved inputs called generalization typically training machine learning model access training set compute error measure training set called training error reduce training error far described simply optimization problem separates machine learning optimization want generalization error test error also called low well generalization error deﬁned expected value error new input expectation taken across diﬀerent possible inputs drawn distribution inputs expect system encounter practice typically estimate generalization error machine learning model measuring performance examples collected separately test set training set linear regression example trained model minimizing training error train train train actually care test error test test test aﬀect performance test set get observe training set ﬁeld statistical learning theory provides answers chapter machine learning basics training test set collected arbitrarily indeed little allowed make assumptions training test set collected make progress train test data generated probability distribution datasets called data generating process typically make set assumptions known collectively assumptions examples assumptions dataset independent train set test set identically distributed drawn probability distribution assumption allows describe data generating process probability distribution single example distribution used generate every train example every test example call shared underlying distribution data generating distribution denoted data probabilistic framework assumptions allow mathematically study relationship training error test error one immediate connection observe training test error expected training error randomly selected model equal expected test error model suppose probability distribution sample repeatedly generate train set test set ﬁxed value expected training set error exactly expected test set error expectations formed using dataset sampling process diﬀerence two conditions name assign dataset sample course use machine learning algorithm parameters ahead time sample datasets sample training set use choose parameters reduce training set error sample test set process expected test error greater equal expected value training error factors determining well machine learning algorithm perform ability make training error small make gap training test error small two factors correspond two central challenges machine learning underﬁtting overﬁtting underﬁtting occurs model able obtain suﬃciently low error value training set overﬁtting occurs gap training error test error large control whether model likely overﬁt underﬁt altering capacity informally model capacity ability wide variety chapter machine learning basics functions models low capacity may struggle training set models high capacity overﬁt memorizing properties training set serve well test set one way control capacity learning algorithm choosing hypothesis space set functions learning algorithm allowed select solution example linear regression algorithm set linear functions input hypothesis space generalize linear regression include polynomials rather linear functions hypothesis space increases model capacity polynomial degree one gives linear regression model already familiar prediction introducing another feature provided linear regression model learn model quadratic function though model implements quadratic function input output still linear function parameters still use normal equations train model closed form continue add powers additional features example obtain polynomial degree machine learning algorithms generally perform best capacity appropriate regard true complexity task need perform amount training data provided models insuﬃcient capacity unable solve complex tasks models high capacity solve complex tasks capacity higher needed solve present task may overﬁt fig shows principle action compare linear quadratic degree predictor attempting problem true underlying function quadratic linear function unable capture curvature true derlying problem underﬁts degree predictor capable representing correct function also capable representing inﬁnitely many functions pass exactly training points chapter machine learning basics parameters training examples little chance choosing solution generalizes well many wildly diﬀerent solutions exist example quadratic model perfectly matched true structure task generalizes well new data figure three models example training set training data generated synthetically randomly sampling values choosing deterministically evaluating quadratic function left linear function data suﬀers underﬁtting cannot capture curvature present data center quadratic function data generalizes well unseen points suﬀer signiﬁcant amount overﬁtting underﬁtting polynomial degree right data suﬀers overﬁtting used moore penrose pseudoinverse solve underdetermined normal equations solution passes training points exactly lucky enough extract correct structure deep valley two training points appear true underlying function also increases sharply left side data true function decreases area far described changing model capacity changing number input features simultaneously adding new parameters associated features fact many ways changing model capacity capacity determined choice model model speciﬁes family functions learning algorithm choose varying parameters order reduce training objective called representational capacity model many cases ﬁnding best function within family diﬃcult optimization problem practice learning algorithm actually ﬁnd best function merely one signiﬁcantly reduces training error additional limitations imperfection chapter machine learning basics optimization algorithm mean learning algorithm eﬀective capacity may less representational capacity model family modern ideas improving generalization machine learning models reﬁnements thought dating back philosophers least early ptolemy many early scholars invoke principle parsimony widely known occam razor principle states among competing hypotheses explain known observations equally well one choose simplest one idea formalized made precise century founders statistical learning theory vapnik chervonenkis vapnik blumer vapnik statistical learning theory provides various means quantifying model capacity among well known vapnik chervonenkis dimension dimension dimension measures capacity binary classiﬁer dimension deﬁned largest possible value exists training set diﬀerent points classiﬁer label arbitrarily quantifying capacity model allows statistical learning theory make quantitative predictions important results statistical learning theory show discrepancy training error generalization error bounded quantity grows model capacity grows shrinks number training examples increases vapnik chervonenkis vapnik blumer vapnik bounds provide intellectual justiﬁcation machine learning algorithms work rarely used practice working deep learning algorithms part bounds often quite loose part quite diﬃcult determine capacity deep learning algorithms problem determining capacity deep learning model especially diﬃcult eﬀective capacity limited capabilities optimization algorithm little theoretical understanding general non convex optimization problems involved deep learning must remember simpler functions likely generalize small gap training test error must still choose suﬃciently complex hypothesis achieve low training error typically training error decreases asymptotes minimum possible error value model capacity increases assuming error measure minimum value typically generalization error shaped curve function model capacity illustrated fig reach extreme case arbitrarily high capacity introduce concept non parametric models far seen parametric chapter machine learning basics optimal capacity capacity underﬁtting zone overﬁtting zone generalization gap training error generalization error figure typical relationship capacity error training test error behave diﬀerently left end graph training error generalization error high underﬁtting regime increase capacity training error decreases gap training generalization error increases eventually size gap outweighs decrease training error enter overﬁtting regime capacity large optimal capacity models linear regression parametric models learn function described parameter vector whose size ﬁnite ﬁxed data observed non parametric models limitation sometimes non parametric models theoretical abstractions algorithm searches possible probability distributions cannot implemented practice however also design practical non parametric models making complexity function training set size one example algorithm nearest neighbor regression unlike linear regression ﬁxed length vector weights nearest neighbor regression model simply stores training set asked classify test point model looks nearest entry training set returns associated regression target words arg min algorithm also generalized distance metrics norm learned distance metrics algorithm allowed goldberger break ties averaging values tied nearest algorithm able achieve minimum possible training error might greater zero two identical inputs associated diﬀerent outputs regression dataset finally also create non parametric learning algorithm wrapping parametric learning algorithm inside another algorithm increases number chapter machine learning basics parameters needed example could imagine outer loop learning changes degree polynomial learned linear regression top polynomial expansion input ideal model oracle simply knows true probability distribution generates data even model still incur error many problems may still noise distribution case supervised learning mapping may inherently stochastic may deterministic function involves variables besides included error incurred oracle making predictions true distribution called bayes error training generalization error vary size training set varies expected generalization error never increase number training examples increases non parametric models data yields better generalization best possible error achieved ﬁxed parametric model less optimal capacity asymptote error value exceeds bayes error see fig illustration note possible model optimal capacity yet still large gap training generalization error situation may able reduce gap gathering training examples 
[machine, learning, basics, capacity, overﬁtting, underﬁtting, free, lunch, theorem] learning theory claims machine learning algorithm generalize well ﬁnite training set examples seems contradict basic principles logic inductive reasoning inferring general rules limited set examples logically valid logically infer rule describing every member set one must information every member set part machine learning avoids problem oﬀering probabilistic rules rather entirely certain rules used purely logical reasoning machine learning promises ﬁnd rules probably correct members set concern unfortunately even resolve entire problem free lunch theorem machine learning wolpert states averaged possible data generating distributions every classiﬁcation algorithm error rate classifying previously unobserved points words sense machine learning algorithm universally better sophisticated algorithm conceive average performance possible tasks merely predicting every point belongs class chapter machine learning basics figure eﬀect training dataset size train test error well optimal model capacity constructed synthetic regression problem based adding moderate amount noise degree polynomial generated single test set generated several diﬀerent sizes training set size generated diﬀerent training sets order plot error bars showing conﬁdence intervals top mse train test set two diﬀerent models quadratic model model degree chosen minimize test error closed form quadratic model training error increases size training set increases larger datasets harder simultaneously test error decreases fewer incorrect hypotheses consistent training data quadratic model enough capacity solve task test error asymptotes high value test error optimal capacity asymptotes bayes error training error fall bayes error due ability training algorithm memorize speciﬁc instances training set training size increases inﬁnity training error ﬁxed capacity model quadratic model must rise least bayes error training set size increases optimal capacity bottom shown degree optimal polynomial regressor increases optimal capacity plateaus reaching suﬃcient complexity solve task chapter machine learning basics fortunately results hold average possible data generating distributions make assumptions kinds probability distributions encounter real world applications design learning algorithms perform well distributions means goal machine learning research seek universal learning algorithm absolute best learning algorithm instead goal understand kinds distributions relevant real world agent experiences kinds machine learning algorithms perform well data drawn kinds data generating distributions care 
[machine, learning, basics, capacity, overﬁtting, underﬁtting, regularization] free lunch theorem implies must design machine learning algorithms perform well speciﬁc task building set preferences learning algorithm preferences aligned learning problems ask algorithm solve performs better far method modifying learning algorithm discussed increase decrease model capacity adding removing functions hypothesis space solutions learning algorithm able choose gave speciﬁc example increasing decreasing degree polynomial regression problem view described far oversimpliﬁed behavior algorithm strongly aﬀected large make set functions allowed hypothesis space speciﬁc identity functions learning algorithm studied far linear regression hypothesis space consisting set linear functions input linear functions useful problems relationship inputs outputs truly close linear less useful problems behave nonlinear fashion example linear regression would perform well tried use predict sin thus control performance algorithms choosing kind functions allow draw solutions well controlling amount functions also give learning algorithm preference one solution hypothesis space another means functions eligible one preferred unpreferred solution chosen ﬁts training data signiﬁcantly better preferred solution example modify training criterion linear regression include weight decay perform linear regression weight decay minimize chapter machine learning basics sum comprising mean squared error training criterion expresses preference weights smaller squared norm speciﬁcally mse train value chosen ahead time controls strength preference smaller weights impose preference larger forces weights become smaller minimizing results choice weights make tradeoﬀ ﬁtting training data small gives solutions smaller slope put weight fewer features example control model tendency overﬁt underﬁt via weight decay train high degree polynomial regression model diﬀerent values see fig results figure high degree polynomial regression model example training set fig true function quadratic use models degree vary amount weight decay prevent high degree models overﬁtting left large force model learn function slope underﬁts represent constant function center medium value learning algorithm recovers curve right general shape even though model capable representing functions much complicated shape weight decay encouraged use simpler function described smaller coeﬃcients weight decay approaching zero using moore penrose right pseudoinverse solve underdetermined problem minimal regularization degree polynomial overﬁts signiﬁcantly saw fig generally regularize model learns function adding penalty called regularizer cost function case weight decay regularizer chapter see many chapter machine learning basics regularizers possible expressing preferences one function another general way controlling model capacity including excluding members hypothesis space think excluding function hypothesis space expressing inﬁnitely strong preference function weight decay example expressed preference linear functions deﬁned smaller weights explicitly via extra term criterion minimize many ways expressing preferences diﬀerent solutions implicitly explicitly together diﬀerent approaches known regularization regularization modiﬁcation make learning algorithm intended reduce generalization error training error regularization one central concerns ﬁeld machine learning rivaled importance optimization free lunch theorem made clear best machine learning algorithm particular best form regularization instead must choose form regularization well suited particular task want solve philosophy deep learning general book particular wide range tasks intellectual tasks people may solved eﬀectively using general purpose forms regularization 
[machine, learning, basics, hyperparameters, validation, sets] machine learning algorithms several settings use control behavior learning algorithm settings called hyperparameters values hyperparameters adapted learning algorithm though design nested learning procedure one learning algorithm learns best hyperparameters another learning algorithm polynomial regression example saw fig single hyper parameter degree polynomial acts capacity hyperparameter value used control strength weight decay another example hyperparameter sometimes setting chosen hyperparameter learning algo rithm learn diﬃcult optimize frequently learn hyperparameter appropriate learn hyper parameter training set applies hyperparameters control model capacity learned training set hyperparameters would always chapter machine learning basics choose maximum possible model capacity resulting overﬁtting refer fig example always training set better higher degree polynomial weight decay setting could lower degree polynomial positive weight decay setting solve problem need examples training validation set algorithm observe earlier discussed held test set composed examples coming distribution training set used estimate generalization error learner learning process completed important test examples used way make choices model including hyperparameters reason example test set used validation set therefore always construct validation set training data speciﬁcally split training data two disjoint subsets one subsets used learn parameters subset validation set used estimate generalization error training allowing hyperparameters updated accordingly subset data used learn parameters still typically called training set even though may confused larger pool data used entire training process subset data used guide selection hyperparameters called validation set typically one uses training data training validation since validation set used train hyperparameters validation set error underestimate generalization error though typically smaller amount training error hyperparameter optimization complete generalization error may estimated using test set practice test set used repeatedly evaluate performance diﬀerent algorithms many years especially consider attempts scientiﬁc community beating reported state art performance test set end optimistic evaluations test set well benchmarks thus become stale reﬂect true ﬁeld performance trained system thankfully community tends move new usually ambitious larger benchmark datasets 
[machine, learning, basics, hyperparameters, validation, sets, cross-validation] dividing dataset ﬁxed training set ﬁxed test set problematic results test set small small test set implies statistical uncertainty around estimated average test error making diﬃcult claim algorithm works better algorithm given task chapter machine learning basics dataset hundreds thousands examples serious issue dataset small alternative procedures allow one use examples estimation mean test error price increased computational cost procedures based idea repeating training testing computation diﬀerent randomly chosen subsets splits original dataset common fold cross validation procedure shown algorithm partition dataset formed splitting non overlapping subsets test error may estimated taking average test error across trials trial subset data used test set rest data used training set one problem exist unbiased estimators variance average error estimators bengio grandvalet approximations typically used 
[machine, learning, basics, estimators, bias, variance] ﬁeld statistics gives many tools used achieve machine learning goal solving task training set also generalize foundational concepts parameter estimation bias variance useful formally characterize notions generalization underﬁtting overﬁtting 
[machine, learning, basics, estimators, bias, variance, point, estimation] point estimation attempt provide single best prediction quantity interest general quantity interest single parameter vector parameters parametric model weights linear regression example sec also whole function order distinguish estimates parameters true value convention denote point estimate parameter let set independent identically distributed data points point estimator statistic function data deﬁnition require return value close true even range set allowable values deﬁnition point estimator general allows designer estimator great ﬂexibility almost function thus qualiﬁes estimator chapter machine learning basics algorithm fold cross validation algorithm used estimate generalization error learning algorithm given dataset small simple train test train valid split yield accurate estimation generalization error mean loss small test set may high variance dataset contains elements abstract examples example could stand input target pair case supervised learning input case unsupervised learning algorithm returns vector errors example whose mean estimated generalization error errors individual examples used compute conﬁdence interval around mean conﬁdence intervals well justiﬁed use cross validation still common practice use declare algorithm better algorithm conﬁdence interval error algorithm lies intersect conﬁdence interval algorithm deﬁne kfoldxv require given dataset elements require learning algorithm seen function takes dataset input outputs learned function require loss function seen function learned function example scalar require number folds split mutually exclusive subsets whose union end end return chapter machine learning basics good estimator function whose output close true underlying generated training data take frequentist perspective statistics assume true parameter value ﬁxed unknown point estimate function data since data drawn random process function data random therefore random variable point estimation also refer estimation relationship input target variables refer types point estimates function estimators function estimation mentioned sometimes interested performing function estimation function approximation trying predict variable given input vector assume function describes approximate relationship example may assume stands part predictable function estimation interested approximating model estimate function estimation really estimating parameter function estimator simply point estimator function space linear regression example discussed sec polynomial regression example discussed sec examples scenarios may interpreted either estimating parameter estimating function mapping review commonly studied properties point estimators discuss tell estimators 
[machine, learning, basics, estimators, bias, variance, bias] bias estimator deﬁned bias expectation data seen samples random variable true underlying value used deﬁne data generating distribution estimator said unbiased bias implies estimator said asymptotically unbiased lim bias implies lim example bernoulli distribution consider set samples independently identically distributed according bernoulli distri chapter machine learning basics bution mean common estimator parameter distribution mean training samples determine whether estimator biased substitute bias since bias say estimator unbiased example gaussian distribution estimator mean consider set samples independently identically distributed according gaussian distribution recall gaussian probability density function given exp common estimator gaussian mean parameter known sample mean chapter machine learning basics determine bias sample mean interested calculating expectation bias thus ﬁnd sample mean unbiased estimator gaussian mean parameter example estimators variance gaussian distribution example compare two diﬀerent estimators variance parameter gaussian distribution interested knowing either estimator biased ﬁrst estimator consider known sample variance sample mean deﬁned formally interested computing bias begin evaluating term returning conclude bias therefore sample variance biased estimator chapter machine learning basics unbiased sample variance estimator provides alternative approach name suggests estimator unbiased ﬁnd two estimators one biased unbiased estimators clearly desirable always best estimators see often use biased estimators possess important properties 
[machine, learning, basics, estimators, bias, variance, variance, standard, error] another property estimator might want consider much expect vary function data sample computed expectation estimator determine bias compute variance variance estimator simply variance var random variable training set alternately square root variance called standard error denoted variance standard error estimator provides measure would expect estimate compute data vary independently resample dataset underlying data generating process might like estimator exhibit low bias would also like relatively low variance compute statistic using ﬁnite number samples estimate true underlying parameter uncertain sense could obtained samples distribution statistics would chapter machine learning basics diﬀerent expected degree variation estimator source error want quantify standard error mean given uevar true variance samples standard error often estimated using estimate unfortunately neither square root sample variance square root unbiased estimator variance provide unbiased estimate standard deviation approaches tend underestimate true standard deviation still used practice square root unbiased estimator variance less underestimate large approximation quite reasonable standard error mean useful machine learning experiments often estimate generalization error computing sample mean error test set number examples test set determines accuracy estimate taking advantage central limit theorem tells mean approximately distributed normal distribution use standard error compute probability true expectation falls chosen interval example conﬁdence interval centered mean normal distribution mean variance machine learning experiments common say algorithm better algorithm upper bound conﬁdence interval error algorithm less lower bound conﬁdence interval error algorithm example bernoulli distribution consider set samples drawn independently identically bernoulli distribution recall time interested computing variance estimator var var chapter machine learning basics var variance estimator decreases function number examples dataset common property popular estimators return discuss consistency see sec 
[machine, learning, basics, estimators, bias, variance, trading, bias, variance, minimize, mean, squared, error] bias variance measure two diﬀerent sources error estimator bias measures expected deviation true value function parameter variance hand provides measure deviation expected estimator value particular sampling data likely cause happens given choice two estimators one bias one variance choose example imagine interested approximating function shown fig oﬀered choice model large bias one suﬀers large variance choose common way negotiate trade use cross validation empirically cross validation highly successful many real world tasks alter natively also compare mean squared error mse estimates mse bias var mse measures overall expected deviation squared error sense estimator true value parameter clear evaluating mse incorporates bias variance desirable estimators small mse estimators manage keep bias variance somewhat check relationship bias variance tightly linked machine learning concepts capacity underﬁtting overﬁtting case gen chapter machine learning basics capacity bias generalization error variance optimal capacity overﬁtting zone underﬁtting zone figure capacity increases axis bias dotted tends decrease variance dashed tends increase yielding another shaped curve generalization error bold curve vary capacity along one axis optimal capacity underﬁtting capacity optimum overﬁtting relationship similar relationship capacity underﬁtting overﬁtting discussed sec fig eralization error measured mse bias variance meaningful components generalization error increasing capacity tends increase variance decrease bias illustrated fig see shaped curve generalization error function capacity 
[machine, learning, basics, estimators, bias, variance, consistency] far discussed properties various estimators training set ﬁxed size usually also concerned behavior estimator amount training data grows particular usually wish number data points dataset increases point estimates converge true value corresponding parameters formally would like lim symbol means convergence probability condition described known consistency sometimes referred weak consistency strong consistency referring almost sure convergence almost sure chapter machine learning basics convergence sequence random variables value occurs lim consistency ensures bias induced estimator assured diminish number data examples grows however reverse true asymptotic unbiasedness imply consistency example consider estimating mean parameter normal distribution dataset consisting samples could use ﬁrst sample dataset unbiased estimator case estimator unbiased matter many data points seen course implies estimate asymptotically unbiased however consistent estimator case 
[machine, learning, basics, maximum, likelihood, estimation] previously seen deﬁnitions common estimators analyzed properties estimators come rather guessing function might make good estimator analyzing bias variance would like principle derive speciﬁc functions good estimators diﬀerent models common principle maximum likelihood principle consider set examples drawn independently true unknown data generating distribution data let model parametric family probability distributions space indexed words model maps conﬁguration real number estimating true probability data maximum likelihood estimator deﬁned arg max model arg max model product many probabilities inconvenient variety reasons example prone numerical underﬂow obtain convenient equivalent optimization problem observe taking logarithm likelihood change arg max conveniently transform product chapter machine learning basics sum arg max log model argmax change rescale cost function divide obtain version criterion expressed expectation respect empirical distribution data deﬁned training data arg max data log model one way interpret maximum likelihood estimation view minimizing dissimilarity empirical distribution data deﬁned training set model distribution degree dissimilarity two measured divergence divergence given data  model data log data log model term left function data generating process model means train model minimize divergence need minimize data log model course maximization minimizing divergence corresponds exactly minimizing cross entropy distributions many authors use term cross entropy identify speciﬁcally negative log likelihood bernoulli softmax distribution misnomer loss consisting negative log likelihood cross entropy empirical distribution deﬁned training set model example mean squared error cross entropy empirical distribution gaussian model thus see maximum likelihood attempt make model dis tribution match empirical distribution data ideally would like match true data generating distribution data direct access distribution optimal regardless whether maximizing likelihood minimizing divergence values objective functions diﬀerent software often phrase minimizing cost function maximum likelihood thus becomes minimization negative log likelihood nll equivalently minimization cross entropy perspective maximum likelihood minimum divergence becomes helpful case divergence known minimum value zero negative log likelihood actually become negative real valued chapter machine learning basics 
[machine, learning, basics, maximum, likelihood, estimation, conditional, log-likelihood, mean, squared, error] maximum likelihood estimator readily generalized case goal estimate conditional probability order predict given actually common situation forms basis supervised learning represents inputs observed targets conditional maximum likelihood estimator arg max examples assumed decomposed arg max log example linear regression maximum likelihood linear regression introduced earlier sec may justiﬁed maximum likelihood procedure previously motivated linear regression algorithm learns take input produce output value mapping chosen minimize mean squared error criterion introduced less arbitrarily revisit linear regression point view maximum likelihood estimation instead producing single prediction think model producing conditional distribution imagine inﬁnitely large training set might see several training examples input value diﬀerent values goal learning algorithm distribution diﬀerent values compatible derive linear regression algorithm obtained deﬁne function gives prediction mean gaussian example assume variance ﬁxed constant chosen user see choice functional form causes maximum likelihood estimation procedure yield learning algorithm developed since examples assumed conditional log likelihood given log log log chapter machine learning basics output linear regression input number training examples comparing log likelihood mean squared error mse train immediately see maximizing log likelihood respect yields estimate parameters minimizing mean squared error two criteria diﬀerent values location optimum justiﬁes use mse maximum likelihood estimation procedure see maximum likelihood estimator several desirable properties 
[machine, learning, basics, maximum, likelihood, estimation, properties, maximum, likelihood] main appeal maximum likelihood estimator shown best estimator asymptotically number examples terms rate convergence increases appropriate conditions maximum likelihood estimator property consistency see sec meaning number training examples approaches inﬁnity maximum likelihood estimate parameter converges true value parameter conditions true distribution data must lie within model family model otherwise estimator recover data true distribution data must correspond exactly one value wise maximum likelihood recover correct data able determine value used data generating processing inductive principles besides maximum likelihood estimator many share property consistent estimators however consis tent estimators diﬀer meaning one consistent statistic eﬃciency estimator may obtain lower generalization error ﬁxed number samples equivalently may require fewer examples obtain ﬁxed level generalization error statistical eﬃciency typically studied parametric case like linear regression goal estimate value parameter assuming possible identify true parameter value function way measure close true parameter expected mean squared error computing squared diﬀerence estimated true parameter chapter machine learning basics values expectation training samples data generating distribution parametric mean squared error decreases increases large cramér rao lower bound shows rao cramér consistent estimator lower mean squared error maximum likelihood estimator reasons consistency eﬃciency maximum likelihood often considered preferred estimator use machine learning number examples small enough yield overﬁtting behavior regularization strategies weight decay may used obtain biased version maximum likelihood less variance training data limited 
[machine, learning, basics, bayesian, statistics] far discussed frequentist statistics approaches based estimating single value making predictions thereafter based one estimate another approach consider possible values making prediction latter domain bayesian statistics discussed sec frequentist perspective true parameter value ﬁxed unknown point estimate random variable account function dataset seen random bayesian perspective statistics quite diﬀerent bayesian uses probability reﬂect degrees certainty states knowledge dataset directly observed random hand true parameter unknown uncertain thus represented random variable observing data represent knowledge using prior probability distribution sometimes referred simply prior gen erally machine learning practitioner selects prior distribution quite broad high entropy reﬂect high degree uncertainty value observing data example one might assume priori lies ﬁnite range volume uniform distribution many priors instead reﬂect preference simpler solutions smaller magnitude coeﬃcients function closer constant consider set data samples recover eﬀect data belief combining data likelihood prior via bayes rule chapter machine learning basics scenarios bayesian estimation typically used prior begins relatively uniform gaussian distribution high entropy observation data usually causes posterior lose entropy concentrate around highly likely values parameters relative maximum likelihood estimation bayesian estimation oﬀers two important diﬀerences first unlike maximum likelihood approach makes predictions using point estimate bayesian approach make predictions using full distribution example observing examples predicted distribution next data sample given value positive probability density contributes prediction next example contribution weighted posterior density observed still quite uncertain value uncertainty incorporated directly predictions might make sec discussed frequentist approach addresses uncertainty given point estimate evaluating variance variance estimator assessment estimate might change alternative samplings observed data bayesian answer question deal uncertainty estimator simply integrate tends protect well overﬁtting integral course application laws probability making bayesian approach simple justify frequentist machinery constructing estimator based rather hoc decision summarize knowledge contained dataset single point estimate second important diﬀerence bayesian approach estimation maximum likelihood approach due contribution bayesian prior distribution prior inﬂuence shifting probability mass density towards regions parameter space preferred practice priori prior often expresses preference models simpler smooth critics bayesian approach identify prior source subjective human judgment impacting predictions bayesian methods typically generalize much better limited training data available typically suﬀer high computational cost number training examples large chapter machine learning basics example bayesian linear regression consider bayesian esti mation approach learning linear regression parameters linear regression learn linear mapping input vector predict value scalar prediction parametrized vector given set training samples train train express prediction entire training set train train expressed gaussian conditional distribution train train train train train exp train train train train follow standard mse formulation assuming gaussian variance one follows reduce notational burden refer train train simply determine posterior distribution model parameter vector ﬁrst need specify prior distribution prior reﬂect naive belief value parameters sometimes diﬃcult unnatural express prior beliefs terms parameters model practice typically assume fairly broad distribution expressing high degree uncertainty real valued parameters common use gaussian prior distribution exp prior distribution mean vector covariance matrix respectively prior thus speciﬁed proceed determining posterior distribution model parameters unless reason assume particular covariance structure typically assume diagonal covariance matrix diag chapter machine learning basics exp exp exp deﬁne using new variables ﬁnd posterior may rewritten gaussian distribution exp exp terms include parameter vector omitted implied fact distribution must normalized integrate shows normalize multivariate gaussian distribution examining posterior distribution allows gain intuition eﬀect bayesian inference situations set set gives estimate frequentist linear regression weight decay penalty one diﬀerence bayesian estimate undeﬁned set zero allowed begin bayesian learning process inﬁnitely wide prior important diﬀerence bayesian estimate provides covariance matrix showing likely diﬀerent values rather providing estimate 
[machine, learning, basics, bayesian, statistics, posteriori] principled approach make predictions using full bayesian posterior distribution parameter still often desirable single point estimate one common reason desiring point estimate operations involving bayesian posterior interesting models intractable point estimate oﬀers tractable approximation rather simply returning maximum likelihood estimate still gain beneﬁt bayesian approach allowing prior inﬂuence choice point estimate one rational way choose maximum posteriori map point estimate map estimate chooses point maximal chapter machine learning basics posterior probability maximal probability density common case continuous map arg max arg max log log recognize right hand side log standard log likelihood term corresponding prior distribution log example consider linear regression model gaussian prior weights prior given log prior term proportional familiar weight decay penalty plus term depend aﬀect learning process map bayesian inference gaussian prior weights thus corresponds weight decay full bayesian inference map bayesian inference advantage leveraging information brought prior cannot found training data additional information helps reduce variance map point estimate comparison estimate however price increased bias many regularized estimation strategies maximum likelihood learning regularized weight decay interpreted making map approxima tion bayesian inference view applies regularization consists adding extra term objective function corresponds log regularization penalties correspond map bayesian inference example regularizer terms may logarithm probability distribution regularization terms depend data course prior probability distribution allowed map bayesian inference provides straightforward way design complicated yet interpretable regularization terms example complicated penalty term derived using mixture gaussians rather single gaussian distribution prior nowlan hinton 
[machine, learning, basics, supervised, learning, algorithms] recall sec supervised learning algorithms roughly speaking learning algorithms learn associate input output given training set examples inputs outputs many cases outputs may diﬃcult collect automatically must provided human supervisor term still applies even training set targets collected automatically chapter machine learning basics 
[machine, learning, basics, supervised, learning, algorithms, probabilistic, supervised, learning] supervised learning algorithms book based estimating probability distribution simply using maximum likelihood estimation ﬁnd best parameter vector parametric family distributions already seen linear regression corresponds family generalize linear regression classiﬁcation scenario deﬁning diﬀerent family probability distributions two classes class class need specify probability one classes probability class determines probability class two values must add normal distribution real valued numbers used linear regression parametrized terms mean value supply mean valid distribution binary variable slightly complicated mean must always one way solve problem use logistic sigmoid function squash output linear function interval interpret value probability approach known logistic regression somewhat strange name since use model classiﬁcation rather regression case linear regression able ﬁnd optimal weights solving normal equations logistic regression somewhat diﬃcult closed form solution optimal weights instead must search maximizing log likelihood minimizing negative log likelihood nll using gradient descent strategy applied essentially supervised learning problem writing parametric family conditional probability distributions right kind input output variables 
[machine, learning, basics, supervised, learning, algorithms, support, vector, machines] one inﬂuential approaches supervised learning support vector machine boser cortes vapnik model similar logistic regression driven linear function unlike logistic chapter machine learning basics regression support vector machine provide probabilities outputs class identity svm predicts positive class present positive likewise predicts negative class present negative one key innovation associated support vector machines kernel trick kernel trick consists observing many machine learning algorithms written exclusively terms dot products examples example shown linear function used support vector machine written training example vector coeﬃcients rewriting learning algorithm way allows replace output given feature function dot product function called kernel operator represents inner product analogous feature spaces may use literally vector inner product inﬁnite dimensional spaces need use kinds inner products example inner products based integration rather summation complete development kinds inner products beyond scope book replacing dot products kernel evaluations make predictions using function function nonlinear respect relationship linear also relationship linear kernel based function exactly equivalent preprocessing data applying inputs learning linear model new transformed space kernel trick powerful two reasons first allows learn models nonlinear function using convex optimization techniques guaranteed converge eﬃciently possible consider ﬁxed optimize optimization algorithm view decision function linear diﬀerent space second kernel function often admits implementation signiﬁcantly computational eﬃcient naively constructing two vectors explicitly taking dot product cases even inﬁnite dimensional would result inﬁnite computational cost naive explicit approach many cases nonlinear tractable function even intractable chapter machine learning basics example inﬁnite dimensional feature space tractable kernel construct feature mapping non negative integers suppose mapping returns vector containing ones followed inﬁnitely many zeros write kernel function min exactly equivalent corresponding inﬁnite dimensional dot product commonly used kernel gaussian kernel standard normal density kernel also known radial basis function rbf kernel value decreases along lines space radiating outward gaussian kernel corresponds dot product inﬁnite dimensional space derivation space less straightforward example kernel integers min think gaussian kernel performing kind template matching training example associated training label becomes template class test point near according euclidean distance gaussian kernel large response indicating similar template model puts large weight associated training label overall prediction combine many training labels weighted similarity corresponding training examples support vector machines algorithm enhanced using kernel trick many linear models enhanced way category algorithms employ kernel trick known kernel machines kernel methods williams rasmussen schölkopf major drawback kernel machines cost evaluating decision function linear number training examples example contributes term decision function support vector machines able mitigate learning vector contains mostly zeros classifying new example requires evaluating kernel function training examples non zero training examples known support vectors kernel machines also suﬀer high computational cost training dataset large revisit idea sec kernel machines generic kernels struggle generalize well explain sec modern incarnation deep learning designed overcome limitations kernel machines current deep learning renaissance began hinton demonstrated neural network could outperform rbf kernel svm mnist benchmark chapter machine learning basics 
[machine, learning, basics, supervised, learning, algorithms, simple, supervised, learning, algorithms] already brieﬂy encountered another non probabilistic supervised learning algorithm nearest neighbor regression generally nearest neighbors family techniques used classiﬁcation regression non parametric learning algorithm nearest neighbors restricted ﬁxed number parameters usually think nearest neighbors algorithm parameters rather implementing simple function training data fact even really training stage learning process instead test time want produce output new test input ﬁnd nearest neighbors training data return average corresponding values training set works essentially kind supervised learning deﬁne average values case classiﬁcation average one hot code vectors values interpret average one hot codes giving probability distribution classes non parametric learning algorithm nearest neighbor achieve high capacity example suppose multiclass classiﬁcation task measure performance loss setting nearest neighbor converges double bayes error number training examples approaches inﬁnity error excess bayes error results choosing single neighbor breaking ties equally distant neighbors randomly inﬁnite training data test points inﬁnitely many training set neighbors distance zero allow algorithm use neighbors vote rather randomly choosing one procedure converges bayes error rate high capacity nearest neighbors allows obtain high accuracy given large training set however high computational cost may generalize badly given small ﬁnite training set one weakness nearest neighbors cannot learn one feature discriminative another example imagine regression task drawn isotropic gaussian distribution single variable relevant output suppose feature simply encodes output directly cases nearest neighbor regression able detect simple pattern nearest neighbor points determined large number features lone feature thus output small training sets essentially random chapter machine learning basics figure diagrams describing decision tree works top node tree chooses send input example child node left child node right internal nodes drawn circles leaf nodes squares node displayed binary string identiﬁer corresponding position tree obtained appending bit parent identiﬁer choose left top choose right bottom bottom tree divides space regions plane shows decision tree might divide nodes tree plotted plane internal node drawn along dividing line uses categorize examples leaf nodes drawn center region examples receive result piecewise constant function one piece per leaf leaf requires least one training example deﬁne possible decision tree learn function local maxima number training examples chapter machine learning basics another type learning algorithm also breaks input space regions separate parameters region decision tree breiman many variants shown fig node decision tree associated region input space internal nodes break region one sub region child node typically using axis aligned cut space thus sub divided non overlapping regions one one correspondence leaf nodes input regions leaf node usually maps every point input region output decision trees usually trained specialized algorithms beyond scope book learning algorithm considered non parametric allowed learn tree arbitrary size though decision trees usually regularized size constraints turn parametric models practice decision trees typically used axis aligned splits constant outputs within node struggle solve problems easy even logistic regression example two class problem positive class occurs wherever decision boundary axis aligned decision tree thus need approximate decision boundary many nodes implementing step function constantly walks back forth across true decision function axis aligned steps seen nearest neighbor predictors decision trees many limitations nonetheless useful learning algorithms computational resources constrained also build intuition sophisticated learning algorithms thinking similarities diﬀerences sophisticated algorithms decision tree baselines see machine murphy bishop hastie learning textbooks material traditional supervised learning algorithms 
[machine, learning, basics, unsupervised, learning, algorithms] another example simple representation learning algorithm means clustering means clustering algorithm divides training set diﬀerent clusters examples near thus think algorithm providing dimensional one hot code vector representing input belongs cluster entries representation zero one hot code provided means clustering example sparse representation majority entries zero every input later develop algorithms learn ﬂexible sparse representations one entry non zero input one hot codes extreme example sparse representations lose many beneﬁts distributed representation one hot code still confers statistical advantages naturally conveys idea examples cluster similar confers computational advantage entire representation may captured single integer means algorithm works initializing diﬀerent centroids diﬀerent values alternating two diﬀerent steps convergence one step training example assigned cluster index nearest centroid step centroid updated mean training examples assigned cluster chapter machine learning basics one diﬃculty pertaining clustering clustering problem inherently ill posed sense single criterion measures well clustering data corresponds real world measure properties clustering average euclidean distance cluster centroid members cluster allows tell well able reconstruct training data cluster assignments know well cluster assignments correspond properties real world moreover may many diﬀerent clusterings correspond well property real world may hope ﬁnd clustering relates one feature obtain diﬀerent equally valid clustering relevant task example suppose run two clustering algorithms dataset consisting images red trucks images red cars images gray trucks images gray cars ask clustering algorithm ﬁnd two clusters one algorithm may ﬁnd cluster cars cluster trucks another may ﬁnd cluster red vehicles cluster gray vehicles suppose also run third clustering algorithm allowed determine number clusters may assign examples four clusters red cars red trucks gray cars gray trucks new clustering least captures information attributes lost information similarity red cars diﬀerent cluster gray cars diﬀerent cluster gray trucks output clustering algorithm tell red cars similar gray cars gray trucks diﬀerent things know issues illustrate reasons may prefer distributed representation one hot representation distributed representation could two attributes vehicle one representing color one representing whether car truck still entirely clear optimal distributed representation learning algorithm know whether two attributes interested color car versus truck rather manufacturer age many attributes reduces burden algorithm guess single attribute care allows measure similarity objects ﬁne grained way comparing many attributes instead testing whether one attribute matches 
[machine, learning, basics, unsupervised, learning, algorithms, principal, components, analysis] sec saw principal components analysis algorithm provides means compressing data also view pca unsupervised learning algorithm learns representation data representation based two criteria simple representation described pca learns chapter machine learning basics figure pca learns linear projection aligns direction greatest variance axes new space left original data consists samples space variance might occur along directions axis aligned right transformed data varies along axis direction second variance along representation lower dimensionality original input also learns representation whose elements linear correlation ﬁrst step toward criterion learning representations whose elements statistically independent achieve full independence representation learning algorithm must also remove nonlinear relationships variables pca learns orthogonal linear transformation data projects input representation shown fig sec saw could learn one dimensional representation best reconstructs original data sense mean squared error representation actually corresponds ﬁrst principal component data thus use pca simple eﬀective dimensionality reduction method preserves much information data possible measured least squares reconstruction error following study pca representation decorrelates original data representation let consider dimensional design matrix assume data mean zero case data easily centered subtracting mean examples preprocessing step unbiased sample covariance matrix associated given var chapter machine learning basics pca ﬁnds representation linear transformation var diagonal sec saw principal components design matrix given eigenvectors view section exploit alternative derivation principal components principal components may also obtained via singular value decomposition speciﬁcally right singular vectors see let right singular vectors decomposition recover original eigenvector equation eigenvector basis svd helpful show pca results diagonal var using svd express variance var use fact matrix singular value deﬁnition deﬁned orthonormal shows take ensure covariance diagonal required var time use fact deﬁnition svd chapter machine learning basics analysis shows project data via linear transformation resulting representation diagonal covariance matrix given immediately implies individual elements mutually uncorrelated ability pca transform data representation elements mutually uncorrelated important property pca simple example representation attempt disentangle unknown factors variation underlying data case pca disentangling takes form ﬁnding rotation input space described aligns principal axes variance basis new representation space associated correlation important category dependency elements data also interested learning representations disentangle complicated forms feature dependencies need done simple linear transformation 
[machine, learning, basics, stochastic, gradient, descent] nearly deep learning powered one important algorithm stochastic gradient descent sgd stochastic gradient descent extension gradient chapter machine learning basics descent algorithm introduced sec recurring problem machine learning large training sets necessary good generalization large training sets also computationally expensive cost function used machine learning algorithm often decomposes sum training examples per example loss function example negative conditional log likelihood training data written data per example loss log additive cost functions gradient descent requires computing computational cost operation training set size grows billions examples time take single gradient step becomes prohibitively long insight stochastic gradient descent gradient expectation expectation may approximately estimated using small set samples speciﬁcally step algorithm sample minibatch examples drawn uniformly training set minibatch size typically chosen relatively small number examples ranging hundred crucially usually held ﬁxed training set size grows may training set billions examples using updates computed hundred examples estimate gradient formed using examples minibatch stochastic gradient descent algorithm follows estimated gradient downhill learning rate chapter machine learning basics gradient descent general often regarded slow unreliable past application gradient descent non convex optimization problems regarded foolhardy unprincipled today know machine learning models described part work well trained gradient descent optimization algorithm may guaranteed arrive even local minimum reasonable amount time often ﬁnds low value cost function quickly enough useful stochastic gradient descent many important uses outside context deep learning main way train large linear models large datasets ﬁxed model size cost per sgd update depend training set size practice often use larger model training set size increases forced number updates required reach convergence usually increases training set size however approaches inﬁnity model eventually converge best possible test error sgd sampled every example training set increasing extend amount training time needed reach model best possible test error point view one argue asymptotic cost training model sgd function prior advent deep learning main way learn nonlinear models use kernel trick combination linear model many kernel learning algorithms require constructing matrix constructing matrix computational cost clearly undesirable datasets billions examples academia starting deep learning initially interesting able generalize new examples better competing algorithms trained medium sized datasets tens thousands examples soon deep learning garnered additional interest industry provided scalable way training nonlinear models large datasets stochastic gradient descent many enhancements described chapter 
[machine, learning, basics, building, machine, learning, algorithm] nearly deep learning algorithms described particular instances fairly simple recipe combine speciﬁcation dataset cost function optimization procedure model example linear regression algorithm combines dataset consisting chapter machine learning basics cost function data log model model speciﬁcation model cases optimization algorithm deﬁned solving gradient cost zero using normal equations realizing replace components mostly independently others obtain wide variety algorithms cost function typically includes least one term causes learning process perform statistical estimation common cost function negative log likelihood minimizing cost function causes maximum likelihood estimation cost function may also include additional terms regularization terms example add weight decay linear regression cost function obtain data log model still allows closed form optimization change model nonlinear cost functions longer optimized closed form requires choose iterative numerical optimization procedure gradient descent recipe constructing learning algorithm combining models costs optimization algorithms supports supervised unsupervised learning linear regression example shows support supervised learning unsupervised learning supported deﬁning dataset contains providing appropriate unsupervised cost model example obtain ﬁrst pca vector specifying loss function data model deﬁned norm one reconstruction function cases cost function may function cannot actually evaluate computational reasons cases still approximately minimize using iterative numerical optimization long way approximating gradients machine learning algorithms make use recipe though may immediately obvious machine learning algorithm seems especially unique chapter machine learning basics hand designed usually understood using special case optimizer models decision trees means require special case optimizers cost functions ﬂat regions make inappropriate minimization gradient based optimizers recognizing machine learning algorithms described using recipe helps see diﬀerent algorithms part taxonomy methods related tasks work similar reasons rather long list algorithms separate justiﬁcations 
[machine, learning, basics, challenges, motivating, deep, learning] simple machine learning algorithms described chapter work well wide variety important problems however succeeded solving central problems recognizing speech recognizing objects development deep learning motivated part failure traditional algorithms generalize well tasks section challenge generalizing new examples becomes exponentially diﬃcult working high dimensional data mechanisms used achieve generalization traditional machine learning insuﬃcient learn complicated functions high dimensional spaces spaces also often impose high computational costs deep learning designed overcome obstacles 
[machine, learning, basics, challenges, motivating, deep, learning, curse, dimensionality] many machine learning problems become exceedingly diﬃcult number dimensions data high phenomenon known curse dimensionality particular concern number possible distinct conﬁgurations set variables increases exponentially number variables increases chapter machine learning basics figure number relevant dimensions data increases left right number conﬁgurations interest may grow exponentially left one dimensional example one variable care distinguish regions interest enough examples falling within regions region corresponds cell illustration learning algorithms easily generalize correctly straightforward way generalize estimate value target function within region possibly interpolate neighboring regions center dimensions center diﬃcult distinguish diﬀerent values variable need keep track regions need least many examples cover regions dimensions grows right regions least many examples dimensions values distinguished along axis seem need regions examples instance curse dimensionality figure graciously provided nicolas chapados curse dimensionality arises many places computer science especially machine learning one challenge posed curse dimensionality statistical challenge illustrated fig statistical challenge arises number possible conﬁgurations much larger number training examples understand issue let consider input space organized grid like ﬁgure low dimensions describe space low number grid cells mostly occupied data generalizing new data point usually tell simply inspecting training examples lie cell new input example estimating probability density point return number training examples unit volume cell divided total number training examples wish classify example return common class training examples cell regression average target values observed examples cell cells seen example high dimensional spaces number conﬁgurations going huge much larger number examples conﬁgurations training example associated chapter machine learning basics could possibly say something meaningful new conﬁgurations many traditional machine learning algorithms simply assume output new point approximately output nearest training point 
[machine, learning, basics, challenges, motivating, deep, learning, local, constancy, smoothness, regularization] order generalize well machine learning algorithms need guided prior beliefs kind function learn previously seen priors incorporated explicit beliefs form probability distributions parameters model informally may also discuss prior beliefs directly inﬂuencing indirectly acting parameters function via eﬀect function additionally informally discuss prior beliefs expressed implicitly choosing algorithms biased toward choosing class functions another even though biases may expressed even possible express terms probability distribution representing degree belief various functions among widely used implicit priors smoothness prior local constancy prior prior states function learn change much within small region many simpler algorithms rely exclusively prior generalize well result fail scale statistical challenges involved solving level tasks throughout book describe deep learning introduces additional explicit implicit priors order reduce generalization error sophisticated tasks explain smoothness prior alone insuﬃcient tasks many diﬀerent ways implicitly explicitly express prior belief learned function smooth locally constant diﬀerent methods designed encourage learning process learn function satisﬁes condition conﬁgurations small change words know good answer input example labeled training example answer probably good neighborhood several good answers neighborhood would combine form averaging interpolation produce answer agrees many much possible extreme example local constancy approach nearest neighbors chapter machine learning basics family learning algorithms predictors literally constant region containing points set nearest neighbors training set number distinguishable regions cannot number training examples nearest neighbors algorithm copies output nearby training examples kernel machines interpolate training set outputs associated nearby training examples important class kernels family local kernels large decreases grow farther apart local kernel thought similarity function performs template matching measuring closely test example resembles training example much modern motivation deep learning derived studying limitations local template matching deep models able succeed cases local template matching fails bengio decision trees also suﬀer limitations exclusively smoothness based learning break input space many regions leaves use separate parameter sometimes many parameters extensions decision trees region target function requires tree least leaves represented accurately least training examples required tree multiple needed achieve level statistical conﬁdence predicted output general distinguish regions input space methods require examples typically parameters parameters associated regions case nearest neighbor scenario training example used deﬁne one region illustrated fig way represent complex function many regions distinguished number training examples clearly assuming smoothness underlying function allow learner example imagine target function kind checkerboard checkerboard contains many variations simple structure imagine happens number training examples substantially smaller number black white squares checkerboard based local generalization smoothness local constancy prior would guaranteed correctly guess color new point lies within checkerboard square training example guarantee learner could correctly extend checkerboard pattern points lying squares contain training examples prior alone information chapter machine learning basics figure illustration nearest neighbor algorithm breaks input space regions example represented circle within region deﬁnes region boundary represented lines value associated example deﬁnes output points within corresponding region regions deﬁned nearest neighbor matching form geometric pattern called voronoi diagram number contiguous regions cannot grow faster number training examples ﬁgure illustrates behavior nearest neighbor algorithm speciﬁcally machine learning algorithms rely exclusively local smoothness prior generalization exhibit similar behaviors training example informs learner generalize neighborhood immediately surrounding example chapter machine learning basics example tells color square way get colors entire checkerboard right cover cells least one example smoothness assumption associated non parametric learning algo rithms work extremely well long enough examples learning algorithm observe high points peaks low points valleys true underlying function learned generally true function learned smooth enough varies enough dimensions high dimensions even smooth function change smoothly diﬀerent way along dimension function additionally behaves diﬀerently diﬀerent regions become extremely complicated describe set training examples function complicated want distinguish huge number regions compared number examples hope generalize well answer questions yes key insight large number regions deﬁned examples long introduce dependencies regions via additional assumptions underlying data generating distribution way actually generalize non locally many bengio monperrus bengio diﬀerent deep learning algorithms provide implicit explicit assumptions reasonable broad range tasks order capture advantages approaches machine learning often make stronger task speciﬁc sumptions example could easily solve checkerboard task providing assumption target function periodic usually include strong task speciﬁc assumptions neural networks generalize much wider variety structures tasks structure much complex limited simple manually speciﬁed properties periodicity want learning algorithms embody general purpose assumptions core idea deep learning assume data generated composition factors features potentially multiple levels hierarchy many similarly generic assumptions improve deep learning algorithms apparently mild assumptions allow exponential gain relationship number examples number regions distinguished exponential gains described precisely sec sec sec exponential advantages conferred use deep distributed representations counter exponential challenges posed curse dimensionality chapter machine learning basics 
[machine, learning, basics, challenges, motivating, deep, learning, manifold, learning] important concept underlying many ideas machine learning manifold connected region mathematically set points associated manifold neighborhood around point given point manifold locally appears euclidean space everyday life experience surface world plane fact spherical manifold space deﬁnition neighborhood surrounding point implies existence transformations applied move manifold one position neighboring one example world surface manifold one walk north south east west although formal mathematical meaning term manifold machine learning tends used loosely designate connected set points approximated well considering small number degrees freedom dimensions embedded higher dimensional space dimension corresponds local direction variation see fig example training data lying near one dimensional manifold embedded two dimensional space context machine learning allow dimensionality manifold vary one point another often happens manifold intersects example ﬁgure eight manifold single dimension places two dimensions intersection center figure data sampled distribution two dimensional space actually concentrated near one dimensional manifold like twisted string solid line indicates underlying manifold learner infer chapter machine learning basics many machine learning problems seem hopeless expect machine learning algorithm learn functions interesting variations across manifold learning algorithms surmount obstacle assuming consists invalid inputs interesting inputs occur along collection manifolds containing small subset points interesting variations output learned function occurring along directions lie manifold interesting variations happening move one manifold another manifold learning introduced case continuous valued data unsupervised learning setting although probability concentration idea generalized discrete data supervised learning setting key assumption remains probability mass highly concentrated assumption data lies along low dimensional manifold may always correct useful argue context tasks involve processing images sounds text manifold assumption least approximately correct evidence favor assumption consists two categories observations ﬁrst observation favor manifold hypothesis probability distribution images text strings sounds occur real life highly concentrated uniform noise essentially never resembles structured inputs domains fig shows instead uniformly sampled points look like patterns static appear analog television sets signal available similarly generate document picking letters uniformly random probability get meaningful english language text almost zero long sequences letters correspond natural language sequence distribution natural language sequences occupies small volume total space sequences letters chapter machine learning basics figure sampling images uniformly random randomly picking pixel according uniform distribution gives rise noisy images although non zero probability generate image face object frequently encountered applications never actually observe happening practice suggests images encountered applications occupy negligible proportion volume image space course concentrated probability distributions suﬃcient show data lies reasonably small number manifolds must also establish examples encounter connected chapter machine learning basics examples example surrounded highly similar examples may reached applying transformations traverse manifold second argument favor manifold hypothesis also imagine neighborhoods transformations least informally case images certainly think many possible transformations allow trace manifold image space gradually dim brighten lights gradually move rotate objects image gradually alter colors surfaces objects etc remains likely multiple manifolds involved applications example manifold images human faces may connected manifold images cat faces thought experiments supporting manifold hypotheses convey tuitive reasons supporting rigorous experiments cayton narayanan mitter schölkopf roweis saul tenenbaum brand belkin niyogi donoho grimes weinberger saul clearly support hypothesis large class datasets interest data lies low dimensional manifold natural machine learning algorithms represent data terms coordinates manifold rather terms coordinates everyday life think roads manifolds embedded space give directions speciﬁc addresses terms address numbers along roads terms coordinates space extracting manifold coordinates challenging holds promise improve many machine learning algorithms general principle applied many contexts fig shows manifold structure dataset consisting faces end book developed methods necessary learn manifold structure fig see machine learning algorithm successfully accomplish goal concludes part provided basic concepts mathematics machine learning employed throughout remaining parts book prepared embark upon study deep learning chapter machine learning basics figure training examples qmul multiview face dataset gong subjects asked move way cover two dimensional manifold corresponding two angles rotation would like learning algorithms able discover disentangle manifold coordinates fig illustrates feat 
[deep, feedforward, networks] deep feedforward networks feedforward neural networks multi also often called layer perceptrons mlps quintessential deep learning models goal feedforward network approximate function example classiﬁer maps input category feedforward network deﬁnes mapping learns value parameters result best function approximation models called feedforward information ﬂows function evaluated intermediate computations used deﬁne ﬁnally output feedback connections outputs model fed back feedforward neural networks extended include feedback connections called recurrent neural networks presented chapter feedforward networks extreme importance machine learning practi tioners form basis many important commercial applications example convolutional networks used object recognition photos specialized kind feedforward network feedforward networks conceptual stepping stone path recurrent networks power many natural language applications feedforward neural networks called typically rep networks resented composing together many diﬀerent functions model associated directed acyclic graph describing functions composed together example might three functions connected chain form chain structures commonly used structures neural networks case called ﬁrst layer network called second layer overall length chapter deep feedforward networks chain gives model terminology depth name deep learning arises ﬁnal layer feedforward network called output layer neural network training drive match training data provides noisy approximate examples evaluated diﬀerent training points example accompanied label training examples specify directly output layer must point must produce value close behavior layers directly speciﬁed training data learning algorithm must decide use layers produce desired output training data say individual layer instead learning algorithm must decide use layers best implement approximation training data show desired output layers layers called hidden layers finally networks called neural loosely inspired neuroscience hidden layer network typically vector valued dimensionality hidden layers determines model width element vector may interpreted playing role analogous neuron rather thinking layer representing single vector vector function also think layer consisting many act parallel units representing vector scalar function unit resembles neuron sense receives input many units computes activation value idea using many layers vector valued representation drawn neuroscience choice functions used compute representations also loosely guided neuroscientiﬁc observations functions biological neurons compute however modern neural network research guided many mathematical engineering disciplines goal neural networks perfectly model brain best think feedforward networks function approximation machines designed achieve statistical generalization occasionally drawing insights know brain rather models brain function one way understand feedforward networks begin linear models consider overcome limitations linear models logistic regression linear regression appealing may eﬃciently reliably either closed form convex optimization linear models also obvious defect model capacity limited linear functions model cannot understand interaction two input variables extend linear models represent nonlinear functions apply linear model transformed input chapter deep feedforward networks nonlinear transformation equivalently apply kernel trick described sec obtain nonlinear learning algorithm based implicitly applying mapping think providing set features describing providing new representation question choose mapping one option use generic inﬁnite dimensional implicitly used kernel machines based rbf kernel high enough dimension always enough capacity training set generalization test set often remains poor generic feature mappings usually based principle local smoothness encode enough prior information solve advanced problems another option manually engineer advent deep learning dominant approach approach requires decades human eﬀort separate task practitioners specializing diﬀerent domains speech recognition computer vision little transfer domains strategy deep learning learn approach model parameters use learn broad class functions parameters map desired output example deep feedforward network deﬁning hidden layer approach one three gives convexity training problem beneﬁts outweigh harms approach parametrize representation use optimization algorithm ﬁnd corresponds good representation wish approach capture beneﬁt ﬁrst approach highly generic using broad family approach also capture beneﬁt second approach human practitioners encode knowledge help generalization designing families expect perform well advantage human designer needs ﬁnd right general function family rather ﬁnding precisely right function general principle improving models learning features extends beyond feedforward networks described chapter recurring theme deep learning applies kinds models described throughout book feedforward networks application principle learning deterministic chapter deep feedforward networks mappings lack feedback connections models presented later apply principles learning stochastic mappings learning functions feedback learning probability distributions single vector begin chapter simple example feedforward network next address design decisions needed deploy feedforward network first training feedforward network requires making many design decisions necessary linear model choosing optimizer cost function form output units review basics gradient based learning proceed confront design decisions unique feedforward networks feedforward networks introduced concept hidden layer requires choose activation functions used compute hidden layer values must also design architecture network including many layers network contain networks connected many units layer learning deep neural networks requires computing gradients complicated functions present back propagation algorithm modern generalizations used eﬃciently compute gradients finally close historical perspective 
[deep, feedforward, networks, example, learning, xor] make idea feedforward network concrete begin example fully functioning feedforward network simple task learning xor function xor function exclusive operation two binary values exactly one binary values equal xor function returns otherwise returns xor function provides target function want learn model provides function learning algorithm adapt parameters make similar possible simple example concerned statistical generalization want network perform correctly four points train network four points challenge training set treat problem regression problem use mean squared error loss function choose loss function simplify math example much possible see later appropriate chapter deep feedforward networks approaches modeling binary data evaluated whole training set mse loss function must choose form model suppose choose linear model consisting model deﬁned minimize closed form respect using normal equations solving normal equations obtain linear model simply outputs everywhere happen fig shows linear model able represent xor function one way solve problem use model learns diﬀerent feature space linear model able represent solution speciﬁcally introduce simple feedforward network one hidden layer containing two hidden units see fig illustration model feedforward network vector hidden units computed function values hidden units used input second layer second layer output layer network output layer still linear regression model applied rather network contains two functions chained together complete model function compute linear models served well far may tempting make linear well unfortunately linear feedforward network whole would remain linear function input ignoring intercept terms moment suppose could represent function clearly must use nonlinear function describe features neural networks using aﬃne transformation controlled learned parameters followed ﬁxed nonlinear function called activation function use strategy deﬁning provides weights linear transformation biases previously describe linear regression model used vector weights scalar bias parameter describe chapter deep feedforward networks original space learned space figure solving xor problem learning representation bold numbers printed plot indicate value learned function must output point left linear model applied directly original input cannot implement xor function model output must increase increases model output must decrease increases linear model must apply ﬁxed coeﬃcient linear model therefore cannot use value change coeﬃcient cannot solve problem right transformed space represented features extracted neural network linear model solve problem example solution two points must output collapsed single point feature space words nonlinear features mapped single point feature space linear model describe function increasing decreasing example motivation learning feature space make model capacity greater training set realistic applications learned representations also help model generalize chapter deep feedforward networks figure example feedforward network drawn two diﬀerent styles speciﬁcally feedforward network use solve xor example single hidden layer containing two units left style draw every unit node graph style explicit unambiguous networks larger example consume much space style draw node right graph entire vector representing layer activations style much compact sometimes annotate edges graph name parameters describe relationship two layers indicate matrix describes mapping vector describes mapping typically omit intercept parameters associated layer labeling kind drawing aﬃne transformation input vector output scalar describe aﬃne transformation vector vector entire vector bias parameters needed activation function typically chosen function applied element wise modern neural networks default recommendation use rectiﬁed linear unit relu jarrett nair hinton glorot deﬁned activation function depicted fig max specify complete network max specify solution xor problem let chapter deep feedforward networks rectiﬁed linear activation function figure rectiﬁed linear activation function activation function default activation function recommended use feedforward neural networks applying function output linear transformation yields nonlinear transformation however function remains close linear sense piecewise linear function two linear pieces rectiﬁed linear units nearly linear preserve many properties make linear models easy optimize gradient based methods also preserve many properties make linear models generalize well common principle throughout computer science build complicated systems minimal components much turing machine memory needs able store states build universal function approximator rectiﬁed linear functions chapter deep feedforward networks walk way model processes batch inputs let design matrix containing four points binary input space one example per row ufee ufef ufef uff uff uffa uffa uffb ﬁrst step neural network multiply input matrix ﬁrst layer weight matrix ufee ufef ufef uff uff uffa uffa uffb next add bias vector obtain ufee ufef ufef uff uff uffa uffa uffb space examples lie along line slope move along line output needs begin rise drop back linear model cannot implement function ﬁnish computing value example apply rectiﬁed linear transformation ufee ufef ufef uff uff uffa uffa uffb transformation changed relationship examples longer lie single line shown fig lie space linear model solve problem ﬁnish multiplying weight vector ufee ufef ufef uff uff uffa uffa uffb chapter deep feedforward networks neural network obtained correct answer every example batch example simply speciﬁed solution showed obtained zero error real situation might billions model parameters billions training examples one cannot simply guess solution instead gradient based optimization algorithm ﬁnd parameters produce little error solution described xor problem global minimum loss function gradient descent could converge point equivalent solutions xor problem gradient descent could also ﬁnd convergence point gradient descent depends initial values parameters practice gradient descent would usually ﬁnd clean easily understood integer valued solutions like one presented 
[deep, feedforward, networks, gradient-based, learning] designing training neural network much diﬀerent training machine learning model gradient descent sec described build machine learning algorithm specifying optimization procedure cost function model family largest diﬀerence linear models seen far neural networks nonlinearity neural network causes interesting loss functions become non convex means neural networks usually trained using iterative gradient based optimizers merely drive cost function low value rather linear equation solvers used train linear regression models convex optimization algorithms global conver gence guarantees used train logistic regression svms convex optimization converges starting initial parameters theory practice robust encounter numerical problems stochastic gradient descent applied non convex loss functions convergence guarantee sensitive values initial parameters feedforward neural networks important initialize weights small random values biases may initialized zero small positive values iterative gradient based opti mization algorithms used train feedforward networks almost deep models described detail chapter parameter initialization particular discussed sec moment suﬃces understand training algorithm almost always based using gradient descend cost function one way another speciﬁc algorithms improvements reﬁnements ideas gradient descent introduced sec chapter deep feedforward networks speciﬁcally often improvements stochastic gradient descent algorithm introduced sec course train models linear regression support vector machines gradient descent fact common training set extremely large point view training neural network much diﬀerent training model computing gradient slightly complicated neural network still done eﬃciently exactly sec describe obtain gradient using back propagation algorithm modern generalizations back propagation algorithm machine learning models apply gradient based learning must choose cost function must choose represent output model revisit design considerations special emphasis neural networks scenario 
[deep, feedforward, networks, gradient-based, learning, cost, functions] important aspect design deep neural network choice cost function fortunately cost functions neural networks less parametric models linear models cases parametric model deﬁnes distribution simply use principle maximum likelihood means use cross entropy training data model predictions cost function sometimes take simpler approach rather predicting complete probability distribution merely predict statistic conditioned specialized loss functions allow train predictor estimates total cost function used train neural network often combine one primary cost functions described regularization term already seen simple examples regularization applied linear models sec weight decay approach used linear models also directly applicable deep neural networks among popular regularization strategies advanced regularization strategies neural networks described chapter learning conditional distributions maximum likelihood modern neural networks trained using maximum likelihood means cost function simply negative log likelihood equivalently described chapter deep feedforward networks cross entropy training data model distribution cost function given data log model speciﬁc form cost function changes model model depending speciﬁc form log model expansion equation typically yields terms depend model parameters may discarded example saw sec model recover mean squared error cost data const scaling factor term depend discarded constant based variance gaussian distribution case chose parametrize previously saw equivalence maximum likelihood estimation output distribution minimization mean squared error holds linear model fact equivalence holds regardless used predict mean gaussian advantage approach deriving cost function maximum likelihood removes burden designing cost functions model specifying model automatically determines cost function log one recurring theme throughout neural network design gradient cost function must large predictable enough serve good guide learning algorithm functions saturate become ﬂat undermine objective make gradient become small many cases happens activation functions used produce output hidden units output units saturate negative log likelihood helps avoid problem many models many output units involve exp function saturate argument negative log function negative log likelihood cost function undoes exp output units discuss interaction cost function choice output unit sec one unusual property cross entropy cost used perform maximum likelihood estimation usually minimum value applied models commonly used practice discrete output variables models parametrized way cannot represent probability zero one come arbitrarily close logistic regression example model real valued output variables model chapter deep feedforward networks control density output distribution example learning variance parameter gaussian output distribution becomes possible assign extremely high density correct training set outputs resulting cross entropy approaching negative inﬁnity regularization techniques described chapter provide several diﬀerent ways modifying learning problem model cannot reap unlimited reward way learning conditional statistics instead learning full probability distribution often want learn one conditional statistic given example may predictor wish predict mean use suﬃciently powerful neural network think neural network able represent function wide class functions class limited features continuity boundedness rather speciﬁc parametric form point view view cost function rather function functional functional mapping functions real numbers thus think learning choosing function rather merely choosing set parameters design cost functional minimum occur speciﬁc function desire example design cost functional minimum lie function maps expected value given solving optimization problem respect function requires mathematical tool called calculus variations described sec necessary understand calculus variations understand content chapter moment necessary understand calculus variations may used derive following two results ﬁrst result derived using calculus variations solving optimiza tion problem arg min data yields data long function lies within class optimize words could train inﬁnitely many samples true data generating distribution minimizing mean squared error cost function gives function predicts mean value chapter deep feedforward networks diﬀerent cost functions give diﬀerent statistics second result derived using calculus variations arg min data yields function predicts median value long function may described family functions optimize cost function commonly called mean absolute error unfortunately mean squared error mean absolute error often lead poor results used gradient based optimization output units saturate produce small gradients combined cost functions one reason cross entropy cost function popular mean squared error mean absolute error even necessary estimate entire distribution 
[deep, feedforward, networks, gradient-based, learning, output, units] choice cost function tightly coupled choice output unit time simply use cross entropy data distribution model distribution choice represent output determines form cross entropy function kind neural network unit may used output also used hidden unit focus use units outputs model principle used internally well revisit units additional detail use hidden units sec throughout section suppose feedforward network provides set hidden features deﬁned role output layer provide additional transformation features complete task network must perform linear units gaussian output distributions one simple kind output unit output unit based aﬃne transformation nonlinearity often called linear units given features layer linear output units produces vector linear output layers often used produce mean conditional gaussian distribution chapter deep feedforward networks maximizing log likelihood equivalent minimizing mean squared error maximum likelihood framework makes straightforward learn covariance gaussian make covariance gaussian function input however covariance must constrained positive deﬁnite matrix inputs diﬃcult satisfy constraints linear output layer typically output units used parametrize covariance approaches modeling covariance described shortly sec linear units saturate pose little diﬃculty gradient based optimization algorithms may used wide variety optimization algorithms sigmoid units bernoulli output distributions many tasks require predicting value binary variable classiﬁcation problems two classes cast form maximum likelihood approach deﬁne bernoulli distribution conditioned bernoulli distribution deﬁned single number neural net needs predict number valid probability must lie interval satisfying constraint requires careful design eﬀort suppose use linear unit threshold value obtain valid probability max min would indeed deﬁne valid conditional distribution would able train eﬀectively gradient descent time strayed outside unit interval gradient output model respect parameters would gradient typically problematic learning algorithm longer guide improve corresponding parameters instead better use diﬀerent approach ensures always strong gradient whenever model wrong answer approach based using sigmoid output units combined maximum likelihood sigmoid output unit deﬁned chapter deep feedforward networks logistic sigmoid function described sec think sigmoid output unit two components first uses linear layer compute next uses sigmoid activation function convert probability omit dependence moment discuss deﬁne probability distribution using value sigmoid motivated constructing unnormalized probability distribution sum divide appropriate constant obtain valid probability distribution begin assumption unnormalized log probabilities linear exponentiate obtain unnormalized probabilities normalize see yields bernoulli distribution controlled sigmoidal transformation log exp exp exp probability distributions based exponentiation normalization common throughout statistical modeling literature variable deﬁning distribution binary variables called logit approach predicting probabilities log space natural use maximum likelihood learning cost function used maximum likelihood log log cost function undoes exp sigmoid without eﬀect saturation sigmoid could prevent gradient based learning making good progress loss function maximum likelihood learning bernoulli parametrized sigmoid log log derivation makes use properties sec rewriting loss terms softplus function see saturates negative saturation thus occurs model already right answer positive negative wrong sign argument softplus function chapter deep feedforward networks may simpliﬁed becomes large wrong sign softplus function asymptotes toward simply returning argument derivative respect asymptotes sign limit extremely incorrect softplus function shrink gradient property useful means gradient based learning act quickly correct mistaken use loss functions mean squared error loss saturate anytime saturates sigmoid activation function saturates becomes negative saturates becomes positive gradient shrink small useful learning whenever happens whether model correct answer incorrect answer reason maximum likelihood almost always preferred approach training sigmoid output units analytically logarithm sigmoid always deﬁned ﬁnite sigmoid returns values restricted open interval rather using entire closed interval valid probabilities software implementations avoid numerical problems best write negative log likelihood function rather function sigmoid function underﬂows zero taking logarithm yields negative inﬁnity softmax units multinoulli output distributions time wish represent probability distribution discrete variable possible values may use softmax function seen generalization sigmoid function used represent probability distribution binary variable softmax functions often used output classiﬁer represent probability distribution diﬀerent classes rarely softmax functions used inside model wish model choose one diﬀerent options internal variable case binary variables wished produce single number number needed lie wanted logarithm number well behaved gradient based optimization log likelihood chose instead predict number log exponentiating normalizing gave bernoulli distribution controlled sigmoid function chapter deep feedforward networks generalize case discrete variable values need produce vector require element also entire vector sums represents valid probability distribution approach worked bernoulli distribution generalizes multinoulli distribution first linear layer predicts unnormalized log probabilities log softmax function exponentiate normalize obtain desired formally softmax function given softmax exp exp logistic sigmoid use exp function works well training softmax output target value using maximum log likelihood case wish maximize log log softmax deﬁning softmax terms exp natural log log likelihood undo softmax exp log softmax log exp ﬁrst term shows input always direct con tribution cost function term cannot saturate know learning proceed even contribution second term becomes small maximizing log likelihood ﬁrst term encourages pushed second term encourages pushed gain intuition second term log exp observe term roughly approximated max approximation based idea exp insigniﬁcant noticeably less max intuition gain approximation negative log likelihood cost function always strongly penalizes active incorrect prediction correct answer already largest input softmax term log exp max terms roughly cancel example contribute little overall training cost dominated examples yet correctly classiﬁed far discussed single example overall unregularized maximum likelihood drive model learn parameters drive softmax predict chapter deep feedforward networks fraction counts outcome observed training set softmax maximum likelihood consistent estimator guaranteed happen long model family capable representing training distribution practice limited model capacity imperfect optimization mean model able approximate fractions many objective functions log likelihood work well softmax function speciﬁcally objective functions use log undo exp softmax fail learn argument exp becomes negative causing gradient vanish particular squared error poor loss function softmax units fail train model change output even model makes highly conﬁdent incorrect predictions bridle understand loss functions fail need examine softmax function like sigmoid softmax activation saturate sigmoid function single output saturates input extremely negative extremely positive case softmax multiple output values output values saturate diﬀerences input values become extreme softmax saturates many cost functions based softmax also saturate unless able invert saturating activating function see softmax function responds diﬀerence inputs observe softmax output invariant adding scalar inputs softmax softmax using property derive numerically stable variant softmax softmax softmax max reformulated version allows evaluate softmax small numerical errors even contains extremely large extremely negative numbers amining numerically stable variant see softmax function driven amount arguments deviate max output softmax saturates corresponding input maximal max much greater inputs output softmax also saturate maximal maximum much greater generalization way sigmoid units saturate chapter deep feedforward networks cause similar diﬃculties learning loss function designed compensate argument softmax function produced two diﬀerent ways common simply earlier layer neural network output every element described using linear layer straightforward approach actually overparametrizes distribution constraint outputs must sum means parameters necessary probability value may obtained subtracting ﬁrst probabilities thus impose requirement one element ﬁxed example require indeed exactly sigmoid unit deﬁning equivalent deﬁning softmax two dimensional argument argument approaches softmax describe set probability distributions diﬀerent learning dynamics practice rarely much diﬀerence using overparametrized version restricted version simpler implement overparametrized version neuroscientiﬁc point view interesting think softmax way create form competition units participate softmax outputs always sum increase value one unit necessarily corresponds decrease value others analogous lateral inhibition believed exist nearby neurons cortex extreme diﬀerence maximal others large magnitude becomes form one outputs nearly winner take others nearly name softmax somewhat confusing function closely related argmax function max function term soft derives fact softmax function continuous diﬀerentiable argmax function result represented one hot vector continuous diﬀerentiable softmax function thus provides softened version argmax corresponding soft version maximum function softmax would perhaps better call softmax function softargmax current name entrenched convention output types linear sigmoid softmax output units described common neural networks generalize almost kind output layer wish principle maximum likelihood provides guide design chapter deep feedforward networks good cost function nearly kind output layer general deﬁne conditional distribution principle maximum likelihood suggests use cost function log general think neural network representing function outputs function direct predictions value instead provides parameters distribution loss function interpreted log example may wish learn variance conditional gaussian given simple case variance constant closed form expression maximum likelihood estimator variance simply empirical mean squared diﬀerence observations expected value computationally expensive approach require writing special case code simply include variance one properties distribution controlled negative log likelihood log provide cost function appropriate terms necessary make optimization procedure incrementally learn variance simple case standard deviation depend input make new parameter network copied directly new parameter might could parameter representing could parameter representing depending choose parametrize distribution may wish model predict diﬀerent amount variance diﬀerent values called heteroscedastic model heteroscedastic case simply make speciﬁcation variance one values output typical way formulate gaussian distribution using precision rather variance described multivariate case common use diagonal precision matrix diag formulation works well gradient descent formula log likelihood gaussian distribution parametrized involves mul tiplication addition log gradient multiplication addition logarithm operations well behaved comparison parametrized output terms variance would need use division division function becomes arbitrarily steep near zero large gradients help learning arbitrarily large gradients usually result instability parametrized output terms standard deviation log likelihood would still involve division would also involve squaring gradient squaring operation vanish near zero making diﬃcult learn parameters squared chapter deep feedforward networks regardless whether use standard deviation variance precision must ensure covariance matrix gaussian positive deﬁnite eigenvalues precision matrix reciprocals eigenvalues covariance matrix equivalent ensuring precision matrix positive deﬁnite use diagonal matrix scalar times diagonal matrix condition need enforce output model positivity suppose raw activation model used determine diagonal precision use softplus function obtain positive precision vector strategy applies equally using variance standard deviation rather precision using scalar times identity rather diagonal matrix rare learn covariance precision matrix richer structure diagonal covariance full conditional parametrization must chosen guarantees positive deﬁniteness predicted covariance matrix achieved writing unconstrained square matrix one practical issue matrix full rank computing likelihood expensive matrix requiring computation determinant inverse equivalently commonly done eigendecomposition often want perform multimodal regression predict real values come conditional distribution several diﬀerent peaks space value case gaussian mixture natural representation output jacobs bishop neural networks gaussian mixtures output often called mixture density networks gaussian mixture output components deﬁned conditional probability distribution neural network must three outputs vector deﬁning matrix providing tensor providing outputs must satisfy diﬀerent constraints mixture components form multinoulli distribution diﬀerent components associated latent variable consider latent observe data given input target possible know certainty gaussian component responsible imagine generated picking one make unobserved choice random variable chapter deep feedforward networks typically obtained softmax dimensional vector guarantee outputs positive sum means indicate center mean associated gaussian component unconstrained typically nonlinearity output units vector network must output matrix containing dimensional vectors learning means maximum likelihood slightly complicated learning means distribution one output mode want update mean component actually produced observation practice know component produced observation expression negative log likelihood naturally weights example contribution loss component probability component produced example covariances specify covariance matrix component learning single gaussian component typically use diagonal matrix avoid needing compute determinants learning means mixture maximum likelihood complicated needing assign partial responsibility point mixture component gradient descent automatically follow correct process given correct speciﬁcation negative log likelihood mixture model reported gradient based optimization conditional gaussian mixtures output neural networks unreliable part one gets divisions variance numerically unstable variance gets small particular example yielding large gradients one solution clip gradients see sec another scale gradients heuristically murray larochelle gaussian mixture outputs particularly eﬀective generative models speech schuster movements physical objects graves mixture density strategy gives way network represent multiple output modes control variance output crucial obtaining high degree quality real valued domains example mixture density network shown fig general may wish continue model larger vectors containing variables impose richer richer structures output variables example may wish neural network output sequence characters forms sentence cases may continue use principle maximum likelihood applied model model use chapter deep feedforward networks figure samples drawn neural network mixture density output layer input sampled uniform distribution output sampled model neural network able learn nonlinear mappings input parameters output distribution parameters include probabilities governing three mixture components generate output well parameters mixture component mixture component gaussian predicted mean variance aspects output distribution able vary respect input nonlinear ways describe becomes complex enough beyond scope chapter chapter describes use recurrent neural networks deﬁne models sequences part describes advanced techniques modeling arbitrary iii probability distributions 
[deep, feedforward, networks, hidden, units] far focused discussion design choices neural networks common parametric machine learning models trained gradient based optimization turn issue unique feedforward neural networks choose type hidden unit use hidden layers model design hidden units extremely active area research yet many deﬁnitive guiding theoretical principles rectiﬁed linear units excellent default choice hidden unit many types hidden units available diﬃcult determine use kind though rectiﬁed linear units usually acceptable choice chapter deep feedforward networks describe basic intuitions motivating type hidden units intuitions used suggest try units usually impossible predict advance work best design process consists trial error intuiting kind hidden unit may work well training network kind hidden unit evaluating performance validation set hidden units included list actually diﬀerentiable input points example rectiﬁed linear function max diﬀerentiable may seem like invalidates use gradient based learning algorithm practice gradient descent still performs well enough models used machine learning tasks part neural network training algorithms usually arrive local minimum cost function instead merely reduce value signiﬁcantly shown fig ideas described chapter expect training actually reach point gradient acceptable minima cost function correspond points undeﬁned gradient hidden units diﬀerentiable usually non diﬀerentiable small number points general function left derivative deﬁned slope function immediately left right derivative deﬁned slope function immediately right function diﬀerentiable left derivative right derivative deﬁned equal functions used context neural networks usually deﬁned left derivatives deﬁned right derivatives case max left derivative right derivative software implementations neural network training usually return one one sided derivatives rather reporting derivative undeﬁned raising error may heuristically justiﬁed observing gradient based optimization digital computer subject numerical error anyway function asked evaluate unlikely underlying value truly instead likely small value rounded contexts theoretically pleasing justiﬁcations available usually apply neural network training important point practice one safely disregard non diﬀerentiability hidden unit activation functions described unless indicated otherwise hidden units described accepting vector inputs computing aﬃne transformation applying element wise nonlinear function hidden units distinguished choice form activation function chapter deep feedforward networks 
[deep, feedforward, networks, hidden, units, rectiﬁed, linear, units, generalizations] rectiﬁed linear units use activation function max rectiﬁed linear units easy optimize similar linear units diﬀerence linear unit rectiﬁed linear unit rectiﬁed linear unit outputs zero across half domain makes derivatives rectiﬁed linear unit remain large whenever unit active gradients large also consistent second derivative rectifying operation almost everywhere derivative rectifying operation everywhere unit active means gradient direction far useful learning would activation functions introduce second order eﬀects rectiﬁed linear units typically used top aﬃne transformation initializing parameters aﬃne transformation good practice set elements small positive value makes likely rectiﬁed linear units initially active inputs training set allow derivatives pass several generalizations rectiﬁed linear units exist general izations perform comparably rectiﬁed linear units occasionally perform better one drawback rectiﬁed linear units cannot learn via gradient based methods examples activation zero variety generalizations rectiﬁed linear units guarantee receive gradient every three generalizations rectiﬁed linear units based using non zero slope max min absolute value rectiﬁcation ﬁxes obtain used object recognition images makes sense seek features jarrett invariant polarity reversal input illumination generalizations rectiﬁed linear units broadly applicable leaky relu maas ﬁxes small value like parametric relu prelu treats learnable parameter maxout units generalize rectiﬁed linear units goodfellow instead applying element wise function maxout units divide groups values maxout unit outputs maximum element one chapter deep feedforward networks groups max indices inputs group provides way learning piecewise linear function responds multiple directions input space maxout unit learn piecewise linear convex function pieces maxout units thus seen learning activation function rather relationship units large enough maxout unit learn approximate convex function arbitrary ﬁdelity particular maxout layer two pieces learn implement function input traditional layer using rectiﬁed linear activation function absolute value rectiﬁcation function leaky parametric relu learn implement totally diﬀerent function altogether maxout layer course parametrized diﬀerently layer types learning dynamics diﬀerent even cases maxout learns implement function one layer types maxout unit parametrized weight vectors instead one maxout units typically need regularization rectiﬁed linear units work well without regularization training set large number pieces per unit kept low cai maxout units beneﬁts cases one gain sta tistical computational advantages requiring fewer parameters speciﬁcally features captured diﬀerent linear ﬁlters summarized without losing information taking max group features next layer get times fewer weights unit driven multiple ﬁlters maxout units redun dancy helps resist phenomenon called catastrophic forgetting neural networks forget perform tasks trained past goodfellow rectiﬁed linear units generalizations based principle models easier optimize behavior closer linear general principle using linear behavior obtain easier optimization also applies contexts besides deep linear networks recurrent networks learn sequences produce sequence states outputs training one needs propagate information several time steps much easier linear computations directional derivatives magnitude near involved one best performing recurrent network chapter deep feedforward networks architectures lstm propagates information time via summation particular straightforward kind linear activation discussed sec 
[deep, feedforward, networks, hidden, units, logistic, sigmoid, hyperbolic, tangent] prior introduction rectiﬁed linear units neural networks used logistic sigmoid activation function hyperbolic tangent activation function tanh activation functions closely related tanh already seen sigmoid units output units used predict probability binary variable unlike piecewise linear units sigmoidal units saturate across domain saturate high value positive saturate low value negative strongly sensitive input near widespread saturation sigmoidal units make gradient based learning diﬃcult reason use hidden units feedforward networks discouraged use output units compatible use gradient based learning appropriate cost function undo saturation sigmoid output layer sigmoidal activation function must used hyperbolic tangent activation function typically performs better logistic sigmoid resembles identity function closely sense tanh tanh similar identity near training deep neural network tanh tanh resembles training linear model long activations network kept small makes training tanh network easier sigmoidal activation functions common settings feed forward networks recurrent networks many probabilistic models autoencoders additional requirements rule use piecewise linear activation functions make sigmoidal units appealing despite drawbacks saturation chapter deep feedforward networks 
[deep, feedforward, networks, hidden, units, hidden, units] many types hidden units possible used less frequently general wide variety diﬀerentiable functions perform perfectly well many unpublished activation functions perform well popular ones provide concrete example authors tested feedforward network using cos mnist dataset obtained error rate less competitive results obtained using conventional activation functions research development new techniques common test many diﬀerent activation functions ﬁnd several variations standard practice perform comparably means usually new hidden unit types published clearly demonstrated provide signiﬁcant improvement new hidden unit types perform roughly comparably known types common uninteresting would impractical list hidden unit types appeared literature highlight especially useful distinctive ones one possibility activation one also think using identity function activation function already seen linear unit useful output neural network may also used hidden unit every layer neural network consists linear transformations network whole linear however acceptable layers neural network purely linear consider neural network layer inputs outputs may replace two layers one layer using weight matrix using weight matrix ﬁrst layer activation function essentially factored weight matrix original layer based factored approach compute produces outputs together contain parameters contains parameters small considerable saving parameters comes cost constraining linear transformation low rank low rank relationships often suﬃcient linear hidden units thus oﬀer eﬀective way reducing number parameters network softmax units another kind unit usually used output described sec may sometimes used hidden unit softmax units naturally represent probability distribution discrete variable possible values may used kind switch kinds hidden units usually used advanced architectures explicitly learn manipulate memory described sec chapter deep feedforward networks reasonably common hidden unit types include radial basis function rbf unit exp function becomes active approaches template saturates diﬃcult optimize softplus log smooth version rectiﬁer introduced function approximation dugas nair hinton conditional distributions undirected probabilistic models compared softplus rectiﬁer found glorot better results latter use softplus generally discouraged softplus demonstrates performance hidden unit types counterintuitive one might expect advantage rectiﬁer due diﬀerentiable everywhere due saturating less completely empirically hard tanh shaped similarly tanh rectiﬁer unlike latter bounded max min introduced collobert hidden unit design remains active area research many useful hidden unit types remain discovered 
[deep, feedforward, networks, architecture, design] another key design consideration neural networks determining architecture word architecture refers overall structure network many units units connected neural networks organized groups units called layers neural network architectures arrange layers chain structure layer function layer preceded structure ﬁrst layer given second layer given chapter deep feedforward networks chain based architectures main architectural considerations choose depth network width layer see network even one hidden layer suﬃcient training set deeper networks often able use far fewer units per layer far fewer parameters often generalize test set also often harder optimize ideal network architecture task must found via experimentation guided monitoring validation set error 
[deep, feedforward, networks, architecture, design, universal, approximation, properties, depth] linear model mapping features outputs via matrix multiplication deﬁnition represent linear functions advantage easy train many loss functions result convex optimization problems applied linear models unfortunately often want learn nonlinear functions ﬁrst glance might presume learning nonlinear function requires designing specialized model family kind nonlinearity want learn fortunately feedforward networks hidden layers provide universal approxi mation framework speciﬁcally universal approximation theorem hornik cybenko states feedforward network linear output layer least one hidden layer squashing activation function logistic sigmoid activation function approximate borel measurable function one ﬁnite dimensional space another desired non zero amount error provided network given enough hidden units derivatives feedforward network also approximate derivatives function arbitrarily well concept borel measurability hornik beyond scope book purposes suﬃces say continuous function closed bounded subset borel measurable therefore may approximated neural network neural network may also approximate function mapping ﬁnite dimensional discrete space another original theorems ﬁrst stated terms units activation functions saturate negative positive arguments universal approximation theorems also proven wider class activation functions includes commonly used rectiﬁed linear unit leshno universal approximation theorem means regardless function trying learn know large mlp able represent function however guaranteed training algorithm able learn function even mlp able represent function learning fail two diﬀerent reasons first optimization algorithm used training chapter deep feedforward networks may able ﬁnd value parameters corresponds desired function second training algorithm might choose wrong function due overﬁtting recall sec free lunch theorem shows universally superior machine learning algorithm feedforward networks provide universal system representing functions sense given function exists feedforward network approximates function universal procedure examining training set speciﬁc examples choosing function generalize points training set universal approximation theorem says exists network large enough achieve degree accuracy desire theorem say large network provides bounds barron size single layer network needed approximate broad class functions unfortunately worse case exponential number hidden units possibly one hidden unit corresponding input conﬁguration needs distinguished may required easiest see binary case number possible binary functions vectors selecting one function requires bits general require degrees freedom summary feedforward network single layer suﬃcient represent function layer may infeasibly large may fail learn generalize correctly many circumstances using deeper models reduce number units required represent desired function reduce amount generalization error exist families functions approximated eﬃciently architecture depth greater value require much larger model depth restricted less equal many cases number hidden units required shallow model exponential results ﬁrst proven models resemble continuous diﬀerentiable neural networks used machine learning since extended models ﬁrst results circuits logic gates later håstad work extended results linear threshold units non negative weights networks håstad goldmann hajnal continuous valued activations many modern maass maass neural networks use rectiﬁed linear units demonstrated leshno shallow networks broad family non polynomial activation functions including rectiﬁed linear units universal approximation properties results address questions depth eﬃciency specify suﬃciently wide rectiﬁer network could represent function pascanu chapter deep feedforward networks showed functions representable montufar deep rectiﬁer net require exponential number hidden units shallow one hidden layer network precisely showed piecewise linear networks obtained rectiﬁer nonlinearities maxout units represent functions number regions exponential depth network fig illustrates network absolute value rectiﬁcation creates mirror images function computed top hidden unit respect input hidden unit hidden unit speciﬁes fold input space order create mirror responses sides absolute value nonlinearity composing folding operations obtain exponentially large number piecewise linear regions capture kinds regular repeating patterns figure intuitive geometric explanation exponential advantage deeper rectiﬁer networks formally shown pascanu montufar left absolute value rectiﬁcation unit output every pair mirror points input mirror axis symmetry given hyperplane deﬁned weights bias unit function computed top unit green decision surface mirror image simpler pattern across axis symmetry center function obtained folding space around axis symmetry right another repeating pattern folded top ﬁrst another downstream unit obtain another symmetry repeated four times two hidden layers precisely main theorem states montufar number linear regions carved deep rectiﬁer network inputs depth units per hidden layer exponential depth case maxout networks ﬁlters per unit number linear regions chapter deep feedforward networks course guarantee kinds functions want learn applications machine learning particular share property may also want choose deep model statistical reasons time choose speciﬁc machine learning algorithm implicitly stating set prior beliefs kind function algorithm learn choosing deep model encodes general belief function want learn involve composition several simpler functions interpreted representation learning point view saying believe learning problem consists discovering set underlying factors variation turn described terms simpler underlying factors variation alternately interpret use deep architecture expressing belief function want learn computer program consisting multiple steps step makes use previous step output intermediate outputs necessarily factors variation instead analogous counters pointers network uses organize internal processing empirically greater depth seem result better generalization wide variety tasks bengio erhan bengio mesnil ciresan krizhevsky sermanet farabet couprie kahou goodfellow szegedy see fig fig examples empirical results suggests using deep architectures indeed express useful prior space functions model learns 
[deep, feedforward, networks, architecture, design, architectural, considerations] far described neural networks simple chains layers main considerations depth network width layer practice neural networks show considerably diversity many neural network architectures developed speciﬁc tasks specialized architectures computer vision called convolutional networks described chapter feedforward networks may also generalized recurrent neural networks sequence processing described chapter architectural considerations general layers need connected chain even though common practice many architectures build main chain add extra architectural features skip connections going layer layer higher skip connections make easier gradient ﬂow output layers layers nearer input chapter deep feedforward networks figure empirical results showing deeper networks generalize better used transcribe multi digit numbers photographs addresses data goodfellow test set accuracy consistently increases increasing depth see fig control experiment demonstrating increases model size yield eﬀect chapter deep feedforward networks number parameters 
[deep, feedforward, networks, architecture, design, eﬀect, number, parameters] convolutional fully connected convolutional figure deeper models tend perform better merely model larger experiment goodfellow shows increasing number parameters layers convolutional networks without increasing depth nearly eﬀective increasing test set performance legend indicates depth network used make curve whether curve represents variation size convolutional fully connected layers observe shallow models context overﬁt around million parameters deep ones beneﬁt million suggests using deep model expresses useful preference space functions model learn speciﬁcally expresses belief function consist many simpler functions composed together could result either learning representation composed turn simpler representations corners deﬁned terms edges learning program sequentially dependent steps ﬁrst locate set objects segment recognize chapter deep feedforward networks another key consideration architecture design exactly connect pair layers default neural network layer described linear transformation via matrix every input unit connected every output unit many specialized networks chapters ahead fewer connections unit input layer connected small subset units output layer strategies reducing number connections reduce number parameters amount computation required evaluate network often highly problem dependent example convolutional networks described chapter use specialized patterns sparse connections eﬀective computer vision problems chapter diﬃcult give much speciﬁc advice concerning architecture generic neural network subsequent chapters develop particular architectural strategies found work well diﬀerent application domains 
[deep, feedforward, networks, back-propagation, diﬀerentiation, algo-, rithms] use feedforward neural network accept input produce output information ﬂows forward network inputs provide initial information propagates hidden units layer ﬁnally produces called forward propagation training forward propagation continue onward produces scalar cost back propagation backprop algorithm often simply called rumelhart allows information cost ﬂow backwards network order compute gradient computing analytical expression gradient straightforward numerically evaluating expression computationally expensive back propagation algorithm using simple inexpensive procedure term back propagation often misunderstood meaning whole learning algorithm multi layer neural networks actually back propagation refers method computing gradient another algorithm stochastic gradient descent used perform learning using gradient furthermore back propagation often misunderstood speciﬁc multi layer neural networks principle compute derivatives function functions correct response report derivative function undeﬁned speciﬁcally describe compute gradient arbitrary function set variables whose derivatives desired additional set variables inputs function chapter deep feedforward networks whose derivatives required learning algorithms gradient often require gradient cost function respect parameters many machine learning tasks involve computing derivatives either part learning process analyze learned model back propagation algorithm applied tasks well restricted computing gradient cost function respect parameters idea computing derivatives propagating information network general used compute values jacobian function multiple outputs restrict description commonly used case single output 
[deep, feedforward, networks, back-propagation, diﬀerentiation, algo-, rithms, computational, graphs] far discussed neural networks relatively informal graph language describe back propagation algorithm precisely helpful precise computational graph language many ways formalizing computation graphs possible use node graph indicate variable variable may scalar vector matrix tensor even variable another type formalize graphs also need introduce idea operation operation simple function one variables graph language accompanied set allowable operations functions complicated operations set may described composing many operations together without loss generality deﬁne operation return single output variable lose generality output variable multiple entries vector software implementations back propagation usually support operations multiple outputs avoid case description introduces many extra details important conceptual understanding variable computed applying operation variable draw directed edge sometimes annotate output node name operation applied times omit label operation clear context examples computational graphs shown fig chapter deep feedforward networks dot matmul relu dot sqr sum figure examples computational graphs graph using operation compute graph logistic regression prediction intermediate expressions names algebraic expression need names graph simply name variable computational graph expression max computes design matrix rectiﬁed linear unit activations given design matrix containing minibatch inputs examples applied one operation variable possible apply one operation show computation graph applies one operation weights linear regression model weights used make prediction weight decay penalty chapter deep feedforward networks 
[deep, feedforward, networks, back-propagation, diﬀerentiation, algo-, rithms, chain, rule, calculus] chain rule calculus confused chain rule probability used compute derivatives functions formed composing functions whose derivatives known back propagation algorithm computes chain rule speciﬁc order operations highly eﬃcient let real number let functions mapping real number real number suppose chain rule states generalize beyond scalar case suppose maps maps vector notation may equivalently written jacobian matrix see gradient variable obtained multiplying jacobian matrix gradient back propagation algorithm consists performing jacobian gradient product operation graph usually apply back propagation algorithm merely vectors rather tensors arbitrary dimensionality conceptually exactly back propagation vectors diﬀerence numbers arranged grid form tensor could imagine ﬂattening tensor vector run back propagation computing vector valued gradient reshaping gradient back tensor rearranged view back propagation still multiplying jacobians gradients denote gradient value respect tensor write vector indices multiple coordinates example tensor indexed three coordinates abstract away using single variable represent complete tuple indices possible index tuples gives exactly chapter deep feedforward networks possible integer indices vector gives using notation write chain rule applies tensors 
[deep, feedforward, networks, back-propagation, diﬀerentiation, algo-, rithms, recursively, applying, chain, rule, obtain, backprop] using chain rule straightforward write algebraic expression gradient scalar respect node computational graph produced scalar however actually evaluating expression computer introduces extra considerations speciﬁcally many subexpressions may repeated several times within overall expression gradient procedure computes gradient need choose whether store subexpressions recompute several times example repeated subexpressions arise given fig cases computing subexpression twice would simply wasteful complicated graphs exponentially many wasted computations making naive implementation chain rule infeasible cases computing subexpression twice could valid way reduce memory consumption cost higher runtime ﬁrst begin version back propagation algorithm speciﬁes actual gradient computation directly algorithm along algorithm associated forward computation order actually done according recursive application chain rule one could either directly perform computations view description algorithm symbolic speciﬁcation computational graph computing back propagation ever formulation make explicit manipulation construction symbolic graph performs gradient computation formulation presented sec algorithm also generalize nodes contain arbitrary tensors first consider computational graph describing compute single scalar say loss training example scalar quantity whose gradient want obtain respect input nodes words wish compute application back propagation computing gradients gradient descent parameters cost associated example minibatch correspond parameters model chapter deep feedforward networks assume nodes graph ordered way compute output one starting going deﬁned algorithm node associated operation computed evaluating function set nodes parents algorithm procedure performs computations mapping inputs output deﬁnes computational graph node computes numerical value applying function set arguments comprises values previous nodes input computational graph vector set ﬁrst nodes output computational graph read last output node end end return algorithm speciﬁes forward propagation computation could put graph order perform back propagation construct computational graph depends adds extra set nodes form subgraph one node per node computation proceeds exactly reverse order computation node computes derivative associated forward graph node done using chain rule respect scalar output speciﬁed algorithm subgraph contains exactly one edge edge node node edge associated computation addition dot product performed node gradient already computed respect nodes children chapter deep feedforward networks vector containing partial derivatives children nodes summarize amount computation required performing back propagation scales linearly number edges computation edge corresponds computing partial derivative one node respect one parents well performing one multiplication one addition generalize analysis tensor valued nodes way group multiple scalar values node enable eﬃcient implementations algorithm simpliﬁed version back propagation algorithm computing derivatives respect variables graph example intended understanding showing simpliﬁed case variables scalars wish compute derivatives respect simpliﬁed version computes derivatives nodes graph computational cost algorithm proportional number edges graph assuming partial derivative associated edge requires constant time order number computations forward propagation function parents thus linking nodes forward graph added back propagation graph run forward propagation algorithm example obtain activa tions network initialize grad_table data structure store derivatives computed entry grad table store computed value grad table next line computes using stored values grad table grad table end return grad table back propagation algorithm designed reduce number common subexpressions without regard memory speciﬁcally performs order one jacobian product per node graph seen fact algorithm backprop visits edge node node graph exactly order obtain associated partial derivative back propagation thus avoids exponential explosion repeated subexpressions chapter deep feedforward networks however algorithms may able avoid subexpressions performing simpliﬁcations computational graph may able conserve memory recomputing rather storing subexpressions revisit ideas describing back propagation algorithm 
[deep, feedforward, networks, back-propagation, diﬀerentiation, algo-, rithms, back-propagation, computation, fully-connected, mlp] clarify deﬁnition back propagation computation let consider speciﬁc graph associated fully connected multi layer mlp algorithm ﬁrst shows forward propagation maps parameters supervised loss associated single input target training example output neural network provided input algorithm shows corresponding computation done applying back propagation algorithm graph algorithm algorithm demonstrations chosen simple straightforward understand however specialized one speciﬁc problem modern software implementations based generalized form back propagation described sec accommodate computa tional graph explicitly manipulating data structure representing symbolic computation 
[deep, feedforward, networks, back-propagation, diﬀerentiation, algo-, rithms, symbol-to-symbol, derivatives] algebraic expressions computational graphs operate symbols variables speciﬁc values algebraic graph based representations called symbolic representations actually use train neural network must assign speciﬁc values symbols replace symbolic input network speciﬁc value numeric approaches back propagation take computational graph set numerical values inputs graph return set numerical values describing gradient input values call approach symbol number diﬀerentiation approach used libraries torch caﬀe collobert jia another approach take computational graph add additional nodes graph provide symbolic description desired derivatives chapter deep feedforward networks figure computational graph results repeated subexpressions computing gradient let input graph use function operation apply every step chain compute apply obtain suggests implementation compute value store variable approach taken back propagation algorithm alternative approach suggested subexpression appears alternative approach recomputed time needed memory required store value expressions low back propagation approach clearly preferable reduced runtime however also valid implementation chain rule useful memory limited chapter deep feedforward networks algorithm forward propagation typical deep neural network computation cost function loss depends output target see sec examples loss functions obtain total cost loss may added regularizer contains parameters weights biases algorithm shows compute gradients respect parameters simplicity demonstration uses single input example practical applications use minibatch see sec realistic demonstration require network depth require weight matrices model require bias parameters model require input process require target output end approach taken theano bergstra bastien tensorflow example approach works abadi illustrated fig primary advantage approach derivatives described language original expression derivatives another computational graph possible run back propagation diﬀerentiating derivatives order obtain higher derivatives computation higher order derivatives described sec use latter approach describe back propagation algorithm terms constructing computational graph derivatives subset graph may evaluated using speciﬁc numerical values later time allows avoid specifying exactly operation computed instead generic graph evaluation engine evaluate every node soon parents values available description symbol symbol based approach subsumes symbol number approach symbol number approach understood performing exactly computations done graph built symbol symbol approach key diﬀerence symbol number chapter deep feedforward networks algorithm backward computation deep neural network algo rithm uses addition input target computation yields gradients activations layer starting output layer going backwards ﬁrst hidden layer gradients interpreted indication layer output change reduce error one obtain gradient parameters layer gradients weights biases immediately used part stochas tic gradient update performing update right gradients computed used gradient based optimization methods forward computation compute gradient output layer convert gradient layer output gradient pre nonlinearity activation element wise multiplication element wise compute gradients weights biases including regularization term needed propagate gradients next lower level hidden layer activations end chapter deep feedforward networks figure example symbol symbol approach computing derivatives approach back propagation algorithm need ever access actual speciﬁc numeric values instead adds nodes computational graph describing compute derivatives generic graph evaluation engine later compute derivatives speciﬁc numeric values left example begin graph representing run back propagation algorithm instructing right construct graph expression corresponding example explain back propagation algorithm works purpose illustrate desired result computational graph symbolic description derivative approach expose graph 
[deep, feedforward, networks, back-propagation, diﬀerentiation, algo-, rithms, general, back-propagation] back propagation algorithm simple compute gradient scalar respect one ancestors graph begin observing gradient respect given compute gradient respect parent graph multiplying current gradient jacobian operation produced continue multiplying jacobians traveling backwards graph way reach node may reached going backwards two paths simply sum gradients arriving diﬀerent paths node formally node graph corresponds variable achieve maximum generality describe variable tensor tensor chapter deep feedforward networks general number dimensions subsume scalars vectors matrices assume variable associated following subroutines get operation returns operation computes repre sented edges coming computational graph example may python class representing matrix multiplication operation get_operation function suppose variable created matrix multiplication get operation returns pointer instance corresponding class get consumers returns list variables children computational graph get inputs returns list variables parents computational graph operation also associated bprop operation bprop operation compute jacobian vector product described back propagation algorithm able achieve great generality operation responsible knowing back propagate edges graph participates example might use matrix multiplication operation create variable suppose gradient scalar respect given matrix multiplication operation responsible deﬁning two back propagation rules one input arguments call bprop method request gradient respect given gradient output bprop method matrix multiplication operation must state gradient respect given likewise call bprop method request gradient respect matrix operation responsible implementing bprop method specifying desired gradient given back propagation algorithm need know diﬀerentiation rules needs call operation bprop rules right arguments formally bprop inputs must return inputs implementation chain rule expressed inputs list inputs supplied operation mathematical function operation implements input whose gradient wish compute gradient output operation chapter deep feedforward networks bprop method always pretend inputs distinct even example mul operator passed two copies compute bprop method still return derivative respect inputs back propagation algorithm later add arguments together obtain correct total derivative software implementations back propagation usually provide opera tions bprop methods users deep learning software libraries able back propagate graphs built using common operations like matrix multiplication exponents logarithms software engineers build new implementation back propagation advanced users need add operation existing library must usually derive bprop method new operations manually back propagation algorithm formally described algorithm algorithm outermost skeleton back propagation algorithm portion simple setup cleanup work important work happens subroutine algorithm build_grad require target set variables whose gradients must computed require computational graph require variable diﬀerentiated let pruned contain nodes ancestors descendents nodes initialize data structure associating tensors gradients grad_table grad table build grad grad table end return restricted grad_table sec motivated back propagation strategy avoiding comput ing subexpression chain rule multiple times naive algorithm could exponential runtime due repeated subexpressions speciﬁed back propagation algorithm understand com putational cost assume operation evaluation roughly cost may analyze computational cost terms number operations executed keep mind refer operation fundamental unit computational graph might actually consist chapter deep feedforward networks algorithm inner loop subroutine build grad grad table back propagation algorithm called back propagation algorithm deﬁned algorithm require variable whose gradient added grad_table require graph modify require restriction nodes participate gradient require grad_table data structure mapping nodes gradients grad_table return grad table end get consumers get operation build grad grad table bprop get inputs end grad table insert operations creating return many arithmetic operations example might graph treats matrix multiplication single operation computing gradient graph nodes never execute operations store output operations counting operations computational graph individual operations executed underlying hardware important remember runtime operation may highly variable example multiplying two matrices contain millions entries might correspond single operation graph see computing gradient requires operations forward propagation stage worst execute nodes original graph depending values want compute may need execute entire graph back propagation algorithm adds one jacobian vector product expressed nodes per edge original graph computational graph directed acyclic graph edges kinds graphs commonly used practice situation even better neural network cost functions roughly chain structured causing back propagation cost far chapter deep feedforward networks better naive approach might need execute exponentially many nodes potentially exponential cost seen expanding rewriting recursive chain rule non recursively path since number paths node node grow exponentially length paths number terms sum number paths grow exponentially depth forward propagation graph large cost would incurred computation would redone many times avoid recomputation think back propagation table ﬁlling algorithm takes advantage storing intermediate results node graph corresponding slot table store gradient node ﬁlling table entries order back propagation avoids repeating many common subexpressions table ﬁlling strategy sometimes called dynamic programming 
[deep, feedforward, networks, back-propagation, diﬀerentiation, algo-, rithms, example, back-propagation, mlp, training] example walk back propagation algorithm used train multilayer perceptron develop simple multilayer perception single hidden layer train model use minibatch stochastic gradient descent back propagation algorithm used compute gradient cost single minibatch speciﬁcally use minibatch examples training set formatted design matrix vector associated class labels network computes layer hidden features max simplify presentation use biases model assume graph language includes relu operation compute max element wise predictions unnormalized log probabilities classes given assume graph language includes cross_entropy operation computes cross entropy targets probability distribution deﬁned unnormalized log probabilities resulting cross entropy deﬁnes cost mle minimizing cross entropy performs maximum likelihood estimation classiﬁer however make example realistic chapter deep feedforward networks matmul relu sqr sum matmul mle mle cross_entropy sqr sum figure computational graph used compute cost used train example single layer mlp using cross entropy loss weight decay also include regularization term total cost mle ufeb ufed uff uff consists cross entropy weight decay term coeﬃcient computational graph illustrated fig computational graph gradient example large enough would tedious draw read demonstrates one beneﬁts back propagation algorithm automatically generate gradients would straightforward tedious software engineer derive manually roughly trace behavior back propagation algorithm looking forward propagation graph fig train wish compute two diﬀerent paths leading backward weights one cross entropy cost one weight decay cost weight decay cost relatively simple always contribute gradient chapter deep feedforward networks path cross entropy cost slightly complicated let gradient unnormalized log probabilities provided cross_entropy operation back propagation algorithm needs explore two diﬀerent branches shorter branch adds gradient using back propagation rule second argument matrix multiplication operation branch corresponds longer chain descending along network first back propagation algorithm computes using back propagation rule ﬁrst argument matrix multiplication operation next relu operation uses back propagation rule zero components gradient corresponding entries less let result called last step back propagation algorithm use back propagation rule second argument operation add matmul gradient gradients computed responsibility gradient descent algorithm another optimization algorithm use gradients update parameters mlp computational cost dominated cost matrix multiplication forward propagation stage multiply weight matrix resulting multiply adds number weights backward propagation stage multiply transpose weight matrix computational cost main memory cost algorithm need store input nonlinearity hidden layer value stored time computed backward pass returned point memory cost thus number examples minibatch number hidden units 
[deep, feedforward, networks, back-propagation, diﬀerentiation, algo-, rithms, complications] description back propagation algorithm simpler imple mentations actually used practice noted restricted deﬁnition operation function returns single tensor software implementations need support operations return one tensor example wish compute maximum value tensor index value best compute single pass memory eﬃcient implement procedure single operation two outputs described control memory consumption back propagation back propagation often involves summation many tensors together chapter deep feedforward networks naive approach tensors would computed separately would added second step naive approach overly high memory bottleneck avoided maintaining single buﬀer adding value buﬀer computed real world implementations back propagation also need handle various data types bit ﬂoating point bit ﬂoating point integer values policy handling types takes special care design operations undeﬁned gradients important track cases determine whether gradient requested user undeﬁned various technicalities make real world diﬀerentiation complicated technicalities insurmountable chapter described key intellectual tools needed compute derivatives important aware many subtleties exist 
[deep, feedforward, networks, back-propagation, diﬀerentiation, algo-, rithms, diﬀerentiation, outside, deep, learning, community] deep learning community somewhat isolated broader computer science community largely developed cultural attitudes concerning perform diﬀerentiation generally ﬁeld automatic diﬀerentiation concerned compute derivatives algorithmically back propagation algorithm described one approach automatic diﬀerentiation special case broader class techniques called reverse mode accumulation approaches evaluate subexpressions chain rule diﬀerent orders general determining order evaluation results lowest computational cost diﬃcult problem finding optimal sequence operations compute gradient complete naumann sense may require simplifying algebraic expressions least expensive form example suppose variables representing probabilities variables representing unnormalized log probabilities suppose deﬁne exp exp build softmax function exponentiation summation division operations construct cross entropy loss log human mathematician observe derivative respect takes simple form back propagation algorithm capable simplifying gradient way instead explicitly propagate gradients chapter deep feedforward networks logarithm exponentiation operations original graph software libraries theano able bergstra bastien perform kinds algebraic substitution improve graph proposed pure back propagation algorithm forward graph single output node partial derivative computed constant amount computation back propagation guarantees number computations gradient computation order number computations forward computation seen algorithm local partial derivative needs computed along associated multiplication addition recursive chain rule formulation overall computation therefore edges however potentially reduced simplifying computational graph constructed back propagation complete task implementations theano tensorflow use heuristics based matching known simpliﬁcation patterns order iteratively attempt simplify graph deﬁned back propagation computation gradient scalar output back propagation extended compute jacobian either diﬀerent scalar nodes graph tensor valued node containing values naive implementation may need times computation scalar internal node original forward graph naive implementation computes gradients instead single gradient number outputs graph larger number inputs sometimes preferable use another form automatic diﬀerentiation called forward mode accumulation forward mode computation proposed obtaining real time computation gradients recurrent networks example williams zipser also avoids need store values gradients whole graph trading computational eﬃciency memory relationship forward mode backward mode analogous relationship left multiplying versus right multiplying sequence matrices abcd matrices thought jacobian matrices example column vector many rows corresponds graph single output many inputs starting multiplications end going backwards requires matrix vector products corresponds backward mode instead starting multiply left would involve series matrix matrix products makes whole computation much expensive however fewer rows columns cheaper run multiplications left right corresponding forward mode chapter deep feedforward networks many communities outside machine learning common implement diﬀerentiation software acts directly traditional programming language code python code automatically generates programs diﬀerent functions written languages deep learning community computational graphs usually represented explicit data structures created specialized libraries specialized approach drawback requiring library developer deﬁne bprop methods every operation limiting user library operations deﬁned however specialized approach also beneﬁt allowing customized back propagation rules developed operation allowing developer improve speed stability non obvious ways automatic procedure would presumably unable replicate back propagation therefore way optimal way computing gradient practical method continues serve deep learning community well future diﬀerentiation technology deep networks may improve deep learning practitioners become aware advances broader ﬁeld automatic diﬀerentiation 
[deep, feedforward, networks, back-propagation, diﬀerentiation, algo-, rithms, higher-order, derivatives] software frameworks support use higher order derivatives among deep learning software frameworks includes least theano tensorflow libraries use kind data structure describe expressions derivatives use describe original function diﬀerentiated means symbolic diﬀerentiation machinery applied derivatives context deep learning rare compute single second derivative scalar function instead usually interested properties hessian matrix function hessian matrix size typical deep learning applications number parameters model could easily number billions entire hessian matrix thus infeasible even represent instead explicitly computing hessian typical deep learning approach use krylov methods krylov methods set iterative techniques performing various operations like approximately inverting matrix ﬁnding approximations eigenvectors eigenvalues without using operation matrix vector products order use krylov methods hessian need able compute product hessian matrix arbitrary vector chapter deep feedforward networks straightforward technique compute christianson gradient computations expression may computed automati cally appropriate software library note outer gradient expression takes gradient function inner gradient expression vector produced computational graph important specify automatic diﬀerentiation software diﬀerentiate graph produced computing hessian usually advisable possible hessian vector products one simply computes one hot vector entries equal 
[deep, feedforward, networks, historical, notes] feedforward networks seen eﬃcient nonlinear function approximators based using gradient descent minimize error function approximation point view modern feedforward network culmination centuries progress general function approximation task chain rule underlies back propagation algorithm invented century calculus algebra leibniz hôpital long used solve optimization problems closed form gradient descent introduced technique iteratively approximating solution optimization problems century cauchy beginning function approximation techniques used motivate machine learning models perceptron however earliest models based linear models critics including marvin minsky pointed several ﬂaws linear model family inability learn xor function led backlash entire neural network approach learning nonlinear functions required development multilayer per ceptron means computing gradient model eﬃcient applications chain rule based dynamic programming began appear mostly control applications kelley bryson denham dreyfus bryson dreyfus also sensitivity analysis linnainmaa werbos proposed applying techniques training artiﬁcial neural networks idea ﬁnally developed practice independently rediscovered diﬀerent ways lecun parker chapter deep feedforward networks rumelhart book parallel distributed processing presented results ﬁrst successful experiments back propagation chapter contributed greatly popularization rumelhart back propagation initiated active period research multi layer neural networks however ideas put forward authors book particular rumelhart hinton much beyond back propagation include crucial ideas possible computational implementation several central aspects cognition learning came name connectionism importance given connections neurons locus learning memory particular ideas include notion distributed representation hinton following success back propagation neural network research gained pop ularity reached peak early afterwards machine learning techniques became popular modern deep learning renaissance began core ideas behind modern feedforward networks changed sub stantially since back propagation algorithm approaches gradient descent still use improvement neural network performance attributed two factors first larger datasets reduced degree statistical generalization challenge neural networks second neural networks become much larger due powerful computers better software infrastructure however small number algorithmic changes improved performance neural networks noticeably one algorithmic changes replacement mean squared error cross entropy family loss functions mean squared error popular gradually replaced cross entropy losses principle maximum likelihood ideas spread statistics community machine learning community use cross entropy losses greatly improved performance models sigmoid softmax outputs previously suﬀered saturation slow learning using mean squared error loss major algorithmic change greatly improved performance feedforward networks replacement sigmoid hidden units piecewise linear hidden units rectiﬁed linear units rectiﬁcation using max function introduced early neural network models dates back least far cognitron neocognitron fukushima early models use rectiﬁed linear units instead applied rectiﬁcation chapter deep feedforward networks nonlinear functions despite early popularity rectiﬁcation rectiﬁcation largely replaced sigmoids perhaps sigmoids perform better neural networks small early rectiﬁed linear units avoided due somewhat superstitious belief activation functions non diﬀerentiable points must avoided began change jarrett observed using rectifying nonlinearity single important factor improving performance recognition system among several diﬀerent factors neural network architecture design small datasets observed using rectifying non jarrett linearities even important learning weights hidden layers random weights suﬃcient propagate useful information rectiﬁed linear network allowing classiﬁer layer top learn map diﬀerent feature vectors class identities data available learning begins extract enough useful knowledge exceed performance randomly chosen parameters glorot showed learning far easier deep rectiﬁed linear networks deep networks curvature two sided saturation activation functions rectiﬁed linear units also historical interest show neuroscience continued inﬂuence development deep learning algorithms motivate rectiﬁed linear units glorot biological considerations half rectifying nonlinearity intended capture properties biological neurons inputs biological neurons completely inactive inputs biological neuron output proportional input time biological neurons operate regime inactive sparse activations modern resurgence deep learning began feedforward networks continued bad reputation widely believed feedforward networks would perform well unless assisted models probabilistic models today known right resources engineering practices feedforward networks perform well today gradient based learning feedforward networks used tool develop probabilistic models variational autoencoder generative adversarial networks described chapter rather viewed unreliable technology must supported techniques gradient based learning feedforward networks viewed since powerful technology may applied many machine learning tasks community used unsupervised learning support supervised learning ironically common use supervised learning support unsupervised learning chapter deep feedforward networks feedforward networks continue unfulﬁlled potential future expect applied many tasks advances optimization algorithms model design improve performance even chapter primarily described neural network family models subsequent chapters turn use models regularize train 
[regularization, deep, learning] central problem machine learning make algorithm perform well training data also new inputs many strategies used machine learning explicitly designed reduce test error possibly expense increased training error strategies known collectively regularization see great many forms regularization available deep learning practitioner fact developing eﬀective regularization strategies one major research eﬀorts ﬁeld chapter introduced basic concepts generalization underﬁtting overﬁt ting bias variance regularization already familiar notions please refer chapter continuing one chapter describe regularization detail focusing regular ization strategies deep models models may used building blocks form deep models sections chapter deal standard concepts machine learning already familiar concepts feel free skip relevant sections however chapter concerned extension basic concepts particular case neural networks sec deﬁned regularization modiﬁcation make learning algorithm intended reduce generalization error training error many regularization strategies put extra constraints machine learning model adding restrictions parameter values add extra terms objective function thought corresponding soft constraint parameter values chosen carefully extra constraints penalties lead improved performance chapter regularization deep learning test set sometimes constraints penalties designed encode speciﬁc kinds prior knowledge times constraints penalties designed express generic preference simpler model class order promote generalization sometimes penalties constraints necessary make underdetermined problem determined forms regularization known ensemble methods combine multiple hypotheses explain training data context deep learning regularization strategies based regularizing estimators regularization estimator works trading increased bias reduced variance eﬀective regularizer one makes proﬁtable trade reducing variance signiﬁcantly overly increasing bias discussed generalization overﬁtting chapter focused three situations model family trained either excluded true data generating process corresponding underﬁtting inducing bias matched true data generating process included generating process also many possible generating processes overﬁtting regime variance rather bias dominates estimation error goal regularization take model third regime second regime practice overly complex model family necessarily include target function true data generating process even close approximation either almost never access true data generating process never know sure model family estimated includes generating process however applications deep learning algorithms domains true data generating process almost certainly outside model family deep learning algorithms typically applied extremely complicated domains images audio sequences text true generation process essentially involves simulating entire universe extent always trying square peg data generating process round hole model family means controlling complexity model simple matter ﬁnding model right size right number parameters instead might ﬁnd indeed practical deep learning scenarios almost always ﬁnd best ﬁtting model sense minimizing generalization error large model regularized appropriately review several strategies create large deep regularized model chapter regularization deep learning 
[regularization, deep, learning, parameter, norm, penalties)[regularization, deep, learning, parameter, norm, penalties, parameter, regularization] already seen sec one simplest common kinds parameter norm penalty parameter norm penalty commonly known weight decay regularization strategy drives weights closer origin adding regularization term objective function academic communities regularization also known ridge regression tikhonov regularization gain insight behavior weight decay regularization studying gradient regularized objective function simplify presentation assume bias parameter model following total objective function corresponding parameter gradient take single gradient step update weights perform update written another way update  see addition weight decay term modiﬁed learning rule multiplicatively shrink weight vector constant factor step performing usual gradient update describes happens single step happens entire course training simplify analysis making quadratic approximation objective function neighborhood value weights obtains minimal unregularized training cost arg min objective function truly quadratic case ﬁtting linear regression model generally could regularize parameters near speciﬁc point space surprisingly still get regularization eﬀect better results obtained value closer true one zero default value makes sense know correct value positive negative since far common regularize model parameters towards zero focus special case exposition chapter regularization deep learning mean squared error approximation perfect approximation given hessian matrix respect evaluated ﬁrst order term quadratic approximation deﬁned minimum gradient vanishes likewise location minimum conclude positive semideﬁnite minimum occurs gradient equal study eﬀect weight decay modify adding weight decay gradient solve minimum regularized version use variable represent location minimum approaches regularized solution approaches happens grows real symmetric decompose diagonal matrix orthonormal basis eigenvectors applying decomposition obtain see eﬀect weight decay rescale along axes deﬁned eigenvectors speciﬁcally component aligned eigenvector rescaled factor may wish review kind scaling works ﬁrst explained fig along directions eigenvalues relatively large example eﬀect regularization relatively small however components shrunk nearly zero magnitude eﬀect illustrated fig chapter regularization deep learning figure illustration eﬀect weight decay regularization value optimal solid ellipses represent contours equal value unregularized objective dotted circles represent contours equal value regularizer point competing objectives reach equilibrium ﬁrst dimension eigenvalue hessian small objective function increase much moving horizontally away objective function express strong preference along direction regularizer strong eﬀect axis regularizer pulls close zero second dimension objective function sensitive movements away corresponding eigenvalue large indicating high curvature result weight decay aﬀects position relatively little directions along parameters contribute signiﬁcantly reducing objective function preserved relatively intact directions contribute reducing objective function small eigenvalue hessian tells movement direction signiﬁcantly increase gradient components weight vector corresponding unimportant directions decayed away use regularization throughout training far discussed weight decay terms eﬀect optimization abstract general quadratic cost function eﬀects relate machine learning particular ﬁnd studying linear regression model true cost function quadratic therefore amenable kind analysis used far applying analysis able obtain special case results solution phrased terms training data linear regression cost function chapter regularization deep learning sum squared errors add regularization objective function changes changes normal equations solution matrix proportional covariance matrix using regularization replaces matrix new matrix original one addition diagonal diagonal entries matrix correspond variance input feature see regularization causes learning algorithm perceive input higher variance makes shrink weights features whose covariance output target low compared added variance 
[regularization, deep, learning, parameter, norm, penalties, regularization] weight decay common form weight decay ways penalize size model parameters another option use regularization formally regularization model parameter deﬁned sum absolute values individual parameters discuss eﬀect regularization simple linear regression model bias parameter studied analysis regularization particular interested delineating diﬀerences forms regularization could regularize parameters towards value zero instead towards parameter value case regularization would introduce term chapter regularization deep learning regularization weight decay weight decay controls strength regularization scaling penalty using positive hyperparameter thus regularized objective function given corresponding gradient actually sub gradient sign simply sign applied element wise sign inspecting see immediately eﬀect regu larization quite diﬀerent regularization speciﬁcally see regularization contribution gradient longer scales linearly instead constant factor sign equal sign one consequence form gradient necessarily see clean algebraic solutions quadratic approximations regularization simple linear model quadratic cost function represent via taylor series alternately could imagine truncated taylor series approximating cost function sophisticated model gradient setting given hessian matrix respect evaluated penalty admit clean algebraic expressions case fully general hessian also make simplifying assumption hessian diagonal diag assumption holds data linear regression problem preprocessed remove correlation input features may accomplished using pca quadratic approximation regularized objective function decom poses sum parameters problem minimizing approximate cost function analytical solution dimension following form sign max chapter regularization deep learning consider situation two possible outcomes case optimal value regularized objective simply occurs contribution regularized objective overwhelmed direction regularization pushes value zero case case regularization move optimal value zero instead shifts direction distance equal similar process happens penalty making less negative comparison regularization regularization results solution sparse sparsity context refers fact parameters optimal value zero sparsity regularization qualitatively diﬀerent behavior arises regularization gave solution regularization revisit equation using assumption diagonal positive deﬁnite hessian introduced analysis regularization ﬁnd nonzero remains nonzero demonstrates regularization cause parameters become sparse regularization may large enough sparsity property induced regularization used extensively feature selection mechanism feature selection simpliﬁes machine learning problem choosing subset available features used particular well known lasso least absolute shrinkage tibshirani selection operator model integrates penalty linear model least squares cost function penalty causes subset weights become zero suggesting corresponding features may safely discarded sec saw many regularization strategies interpreted map bayesian inference particular regularization equivalent map bayesian inference gaussian prior weights regu larization penalty used regularize cost function equivalent log prior term maximized map bayesian inference prior isotropic laplace distribution log log laplace log log chapter regularization deep learning point view learning via maximization respect ignore terms depend log log 
[regularization, deep, learning, norm, penalties, constrained, optimization] consider cost function regularized parameter norm penalty recall sec minimize function subject constraints constructing generalized lagrange function consisting original objective function plus set penalties penalty product coeﬃcient called karush kuhn tucker kkt multiplier function representing whether constraint satisﬁed wanted constrain less constant could construct generalized lagrange function solution constrained problem given arg min max described sec solving problem requires modifying sec provides worked example linear regression constraint many diﬀerent procedures possible may use gradient descent others may use analytical solutions gradient zero procedures must increase whenever decrease whenever positive encourage shrink optimal value encourage shrink strongly make become less gain insight eﬀect constraint view problem function arg min arg min exactly regularized training problem minimizing thus think parameter norm penalty imposing constraint weights norm weights constrained lie ball norm weights constrained lie region chapter regularization deep learning limited norm usually know size constraint region impose using weight decay coeﬃcient value directly tell value principle one solve relationship depends form know exact size constraint region control roughly increasing decreasing order grow shrink constraint region larger result smaller constraint region smaller result larger constraint region sometimes may wish use explicit constraints rather penalties described sec modify algorithms stochastic gradient descent take step downhill project back nearest point satisﬁes useful idea value appropriate want spend time searching value corresponds another reason use explicit constraints reprojection rather enforcing constraints penalties penalties cause non convex optimization procedures get stuck local minima corresponding small training neural networks usually manifests neural networks train several dead units units contribute much behavior function learned network weights going small training penalty norm weights conﬁgurations locally optimal even possible signiﬁcantly reduce making weights larger explicit constraints implemented projection work much better cases encourage weights approach origin explicit constraints implemented projection eﬀect weights become large attempt leave constraint region finally explicit constraints reprojection useful impose stability optimization procedure using high learning rates possible encounter positive feedback loop large weights induce large gradients induce large update weights updates consistently increase size weights rapidly moves away origin numerical overﬂow occurs explicit constraints reprojection prevent feedback loop continuing increase magnitude weights without bound recommend using constraints combined hinton high learning rate allow rapid exploration parameter space maintaining stability particular hinton recommend strategy introduced srebro shraibman constraining norm column weight matrix chapter regularization deep learning neural net layer rather constraining frobenius norm entire weight matrix constraining norm column separately prevents one hidden unit large weights converted constraint penalty lagrange function would similar weight decay separate kkt multiplier weights hidden unit kkt multipliers would dynamically updated separately make hidden unit obey constraint practice column norm limitation always implemented explicit constraint reprojection 
[regularization, deep, learning, regularization, under-constrained, problems] cases regularization necessary machine learning problems prop erly deﬁned many linear models machine learning including linear regression pca depend inverting matrix possible whenever singular matrix singular whenever data generating distri bution truly variance direction variance observed direction fewer examples rows input features columns case many forms regularization correspond inverting instead regularized matrix guaranteed invertible linear problems closed form solutions relevant matrix invertible also possible problem closed form solution underdetermined example logistic regression applied problem classes linearly separable weight vector able achieve perfect classiﬁcation also achieve perfect classiﬁcation higher likelihood iterative optimization procedure like stochastic gradient descent continually increase magnitude theory never halt practice numerical implementation gradient descent eventually reach suﬃciently large weights cause numerical overﬂow point behavior depend programmer decided handle values real numbers forms regularization able guarantee convergence iterative methods applied underdetermined problems example weight decay cause gradient descent quit increasing magnitude weights slope likelihood equal weight decay coeﬃcient idea using regularization solve underdetermined problems extends beyond machine learning idea useful several basic linear algebra problems saw sec solve underdetermined linear equations using chapter regularization deep learning moore penrose pseudoinverse recall one deﬁnition pseudoinverse matrix lim recognize performing linear regression weight decay speciﬁcally limit regularization coeﬃcient shrinks zero thus interpret pseudoinverse stabilizing underdetermined problems using regularization 
[regularization, deep, learning, dataset, augmentation] best way make machine learning model generalize better train data course practice amount data limited one way get around problem create fake data add training set machine learning tasks reasonably straightforward create new fake data approach easiest classiﬁcation classiﬁer needs take compli cated high dimensional input summarize single category identity means main task facing classiﬁer invariant wide variety transformations generate new pairs easily transforming inputs training set approach readily applicable many tasks example diﬃcult generate new fake data density estimation task unless already solved density estimation problem dataset augmentation particularly eﬀective technique speciﬁc classiﬁcation problem object recognition images high dimensional include enormous variety factors variation many easily simulated operations like translating training images pixels direction often greatly improve generalization even model already designed partially translation invariant using convolution pooling techniques described chapter many operations rotating image scaling image also proven quite eﬀective one must careful apply transformations would change correct class example optical character recognition tasks require recognizing diﬀerence diﬀerence horizontal ﬂips rotations appropriate ways augmenting datasets tasks chapter regularization deep learning also transformations would like classiﬁers invariant easy perform example plane rotation implemented simple geometric operation input pixels dataset augmentation eﬀective speech recognition tasks well jaitly hinton injecting noise input neural network sietsma dow also seen form data augmentation many classiﬁcation even regression tasks task still possible solve even small random noise added input neural networks prove robust noise however tang eliasmith one way improve robustness neural networks simply train random noise applied inputs input noise injection part unsupervised learning algorithms denoising autoencoder vincent noise injection also works noise applied hidden units seen dataset augmentation multiple levels abstraction poole recently showed approach highly eﬀective provided magnitude noise carefully tuned dropout powerful regularization strategy described sec seen process constructing new inputs multiplying noise comparing machine learning benchmark results important take eﬀect dataset augmentation account often hand designed dataset augmentation schemes dramatically reduce generalization error machine learning technique compare performance one machine learning algorithm another necessary perform controlled experiments comparing machine learning algorithm machine learning algorithm necessary make sure algorithms evaluated using hand designed dataset augmentation schemes suppose algorithm performs poorly dataset augmentation algorithm performs well combined numerous synthetic transformations input case likely synthetic transformations caused improved performance rather use machine learning algorithm sometimes deciding whether experiment properly controlled requires subjective judgment example machine learning algorithms inject noise input performing form dataset augmentation usually operations generally applicable adding gaussian noise input considered part machine learning algorithm operations speciﬁc one application domain randomly cropping image considered separate pre processing steps chapter regularization deep learning 
[regularization, deep, learning, noise, robustness] sec motivated use noise applied inputs dataset aug mentation strategy models addition noise inﬁnitesimal variance input model equivalent imposing penalty norm weights general case important bishop remember noise injection much powerful simply shrinking parameters especially noise added hidden units noise applied hidden units important topic merit separate discussion dropout algorithm described sec main development approach another way noise used service regularizing models adding weights technique used primarily context recurrent neural networks jim graves interpreted stochastic implementation bayesian inference weights bayesian treatment learning would consider model weights uncertain representable via probability distribution reﬂects uncertainty adding noise weights practical stochastic way reﬂect uncertainty graves also interpreted equivalent assumptions traditional form regularization adding noise weights shown eﬀective regularization strategy context recurrent neural networks jim graves following present analysis eﬀect weight noise standard feedforward neural network introduced chapter study regression setting wish train function maps set features scalar using least squares cost function model predictions true values training set consists labeled examples assume input presentation also include random perturbation network weights let imagine standard layer mlp denote perturbed model despite injection noise still interested minimizing squared error output network objective function thus becomes chapter regularization deep learning small minimization added weight noise covariance equivalent minimization additional regularization term form regularization encourages parameters regions parameter space small perturbations weights relatively small inﬂuence output words pushes model regions model relatively insensitive small variations weights ﬁnding points merely minima minima surrounded ﬂat regions hochreiter schmidhuber simpliﬁed case linear regression instance regularization term collapses function parameters therefore contribute gradient respect model parameters 
[regularization, deep, learning, noise, robustness, injecting, noise, output, targets] datasets amount mistakes labels harmful maximize log mistake one way prevent explicitly model noise labels example assume small constant training set label correct probability otherwise possible labels might correct assumption easy incorporate cost function analytically rather explicitly drawing noise samples example label smoothing regularizes model based softmax output values replacing hard classiﬁcation targets targets respectively standard cross entropy loss may used soft targets maximum likelihood learning softmax classiﬁer hard targets may actually never converge softmax never predict probability exactly exactly continue learn larger larger weights making extreme predictions forever possible prevent scenario using regularization strategies like weight decay label smoothing advantage preventing pursuit hard probabilities without discouraging correct classiﬁcation strategy used since continues featured prominently modern neural networks szegedy 
[regularization, deep, learning, semi-supervised, learning] paradigm semi supervised learning unlabeled examples labeled examples used estimate predict chapter regularization deep learning context deep learning semi supervised learning usually refers learning representation goal learn representation examples class similar representations unsupervised learning provide useful cues group examples representation space examples cluster tightly input space mapped similar representations linear classiﬁer new space may achieve better generalization many cases belkin niyogi chapelle long standing variant approach application principal components analysis pre processing step applying classiﬁer projected data instead separate unsupervised supervised components model one construct models generative model either shares parameters discriminative model one trade supervised criterion log unsupervised generative one log log generative criterion expresses particular form prior belief solution supervised learning problem namely structure lasserre connected structure way captured shared parametrization controlling much generative criterion included total criterion one ﬁnd better trade purely generative purely discriminative training criterion lasserre larochelle bengio salakhutdinov hinton describe method learning kernel function kernel machine used regression usage unlabeled examples modeling improves quite signiﬁcantly see information semi supervised learning chapelle 
[regularization, deep, learning, multi-task, learning] multi task learning way improve generalization pooling caruana examples seen soft constraints imposed parameters arising several tasks way additional training examples put pressure parameters model towards values generalize well part model shared across tasks part model constrained towards good values assuming sharing justiﬁed often yielding better generalization chapter regularization deep learning fig illustrates common form multi task learning diﬀerent supervised tasks predicting given share input well intermediate level representation shared capturing common pool factors model generally divided two kinds parts associated parameters task speciﬁc parameters beneﬁt examples task achieve good generalization upper layers neural network fig generic parameters shared across tasks beneﬁt pooled data tasks lower layers neural network fig shared shared figure multi task learning cast several ways deep learning frameworks ﬁgure illustrates common situation tasks share common input involve diﬀerent target random variables lower layers deep network whether supervised feedforward includes generative component downward arrows shared across tasks task speciﬁc parameters associated respectively weights learned top yielding shared representation shared underlying assumption exists common pool factors explain variations input task associated subset factors example additionally assumed top level hidden units specialized task respectively predicting intermediate level representation shared shared across tasks unsupervised learning context makes sense top level factors associated none output tasks factors explain input variations relevant predicting chapter regularization deep learning figure learning curves showing negative log likelihood loss changes time indicated number training iterations dataset epochs example train maxout network mnist observe training objective decreases consistently time validation set average loss eventually begins increase forming asymmetric shaped curve improved generalization generalization error bounds baxter achieved shared parameters statistical strength greatly improved proportion increased number examples shared parameters compared scenario single task models course happen assumptions statistical relationship diﬀerent tasks valid meaning something shared across tasks point view deep learning underlying prior belief following among factors explain variations observed data associated diﬀerent tasks shared across two tasks 
[regularization, deep, learning, early, stopping] training large models suﬃcient representational capacity overﬁt task often observe training error decreases steadily time validation set error begins rise see fig example behavior behavior occurs reliably means obtain model better validation set error thus hopefully better test set error returning parameter setting point chapter regularization deep learning time lowest validation set error instead running optimization algorithm reach local minimum validation error run error validation set improved amount time every time error validation set improves store copy model parameters training algorithm terminates return parameters rather latest parameters procedure speciﬁed formally algorithm algorithm early stopping meta algorithm determining best amount time train meta algorithm general strategy works well variety training algorithms ways quantifying error validation set let number steps evaluations let patience number times observe worsening validation set error giving let initial parameters update running training algorithm steps validationseterror else end end best parameters best number training steps strategy known early stopping probably commonly used form regularization deep learning popularity due eﬀectiveness simplicity chapter regularization deep learning one way think early stopping eﬃcient hyperparameter selection algorithm view number training steps another hyperparameter see fig hyperparameter shaped validation set performance curve hyperparameters control model capacity shaped validation set performance curve illustrated fig case early stopping controlling eﬀective capacity model determining many steps take training set hyperparameters must chosen using expensive guess check process set hyperparameter start training run training several steps see eﬀect training time hyperparameter unique deﬁnition single run training tries many values hyperparameter signiﬁcant cost choosing hyperparameter automatically via early stopping running validation set evaluation periodically training ideally done parallel training process separate machine separate cpu separate gpu main training process resources available cost periodic evaluations may reduced using validation set small compared training set evaluating validation set error less frequently obtaining lower resolution estimate optimal training time additional cost early stopping need maintain copy best parameters cost generally negligible acceptable store parameters slower larger form memory example training gpu memory storing optimal parameters host memory disk drive since best parameters written infrequently never read training occasional slow writes little eﬀect total training time early stopping unobtrusive form regularization requires almost change underlying training procedure objective function set allowable parameter values means easy use early stopping without damaging learning dynamics contrast weight decay one must careful use much weight decay trap network bad local minimum corresponding solution pathologically small weights early stopping may used either alone conjunction regulariza tion strategies even using regularization strategies modify objective function encourage better generalization rare best generalization occur local minimum training objective early stopping requires validation set means training data fed model best exploit extra data one perform extra training initial training early stopping completed second extra chapter regularization deep learning training step training data included two basic strategies one use second training procedure one strategy algorithm initialize model retrain data second training pass train number steps early stopping procedure determined optimal ﬁrst pass subtleties associated procedure example good way knowing whether retrain number parameter updates number passes dataset second round training pass dataset require parameter updates training set bigger algorithm meta algorithm using early stopping determine long train retraining data let train train training set split train train subtrain valid subtrain valid respectively run early stopping algorithm starting random using subtrain subtrain training data valid valid validation data returns optimal number steps set random values train train train steps another strategy using data keep parameters obtained ﬁrst round training continue training using data stage longer guide stop terms number steps instead monitor average loss function validation set continue training falls value training set objective early stopping procedure halted strategy avoids high cost retraining model scratch well behaved example guarantee objective validation set ever reach target value strategy even guaranteed terminate procedure presented formally algorithm early stopping also useful reduces computational cost training procedure besides obvious reduction cost due limiting number training iterations also beneﬁt providing regularization without requiring addition penalty terms cost function computation gradients additional terms chapter regularization deep learning algorithm meta algorithm using early stopping determine objec tive value start overﬁt continue training value reached let train train training set split train train subtrain valid subtrain valid respectively run early stopping algorithm starting random using subtrain subtrain training data valid valid validation data updates subtrain subtrain valid valid train train train steps end early stopping acts regularizer far stated early stopping regularization strategy supported claim showing learning curves validation set error shaped curve actual mechanism early stopping regularizes model bishop argued early stopping eﬀect sjöberg ljung restricting optimization procedure relatively small volume parameter space neighborhood initial parameter value speciﬁcally imagine taking optimization steps corresponding training iterations learning rate view product  measure eﬀective capacity assuming gradient bounded restricting number iterations learning rate limits volume parameter space reachable sense  behaves reciprocal coeﬃcient used weight decay indeed show case simple linear model quadratic error function simple gradient descent early stopping equivalent regularization order compare classical regularization examine simple setting parameters linear weights model cost function quadratic approximation neighborhood empirically optimal value weights hessian matrix respect evaluated given assumption minimum know positive semideﬁnite chapter regularization deep learning figure illustration eﬀect early stopping left solid contour lines indicate contours negative log likelihood dashed line indicates trajectory taken sgd beginning origin rather stopping point minimizes cost early stopping results trajectory stopping earlier point illustration eﬀect right regularization comparison dashed circles indicate contours penalty causes minimum total cost lie nearer origin minimum unregularized cost local taylor series approximation gradient given going study trajectory followed parameter vector training simplicity let set initial parameter vector origin let suppose update parameters via gradient descent  let rewrite expression space eigenvectors exploiting eigendecomposition diagonal matrix orthonormal basis eigenvectors neural networks obtain symmetry breaking hidden units cannot initialize parameters discussed sec however argument holds initial value chapter regularization deep learning assuming chosen small enough guarantee  parameter trajectory training parameter updates follows expression regularization rearranged comparing see hyperparameters chosen regularization early stopping seen equivalent least quadratic approximation objective function going even taking logarithms using series expansion log conclude small   assumptions number training iterations plays role inversely proportional regularization parameter inverse plays role weight decay coeﬃcient parameter values corresponding directions signiﬁcant curvature objective function regularized less directions less curvature course context early stopping really means parameters correspond directions signiﬁcant curvature tend learn early relative parameters corresponding directions less curvature derivations section shown trajectory length ends point corresponds minimum regularized objective early stopping course mere restriction trajectory length instead early stopping typically involves monitoring validation set error order stop trajectory particularly good point space early stopping therefore advantage weight decay early stopping automatically determines correct amount regularization weight decay requires many training experiments diﬀerent values hyperparameter chapter regularization deep learning 
[regularization, deep, learning, parameter, tying, parameter, sharing] thus far chapter discussed adding constraints penalties parameters always done respect ﬁxed region point example regularization weight decay penalizes model parameters deviating ﬁxed value zero however sometimes may need ways express prior knowledge suitable values model parameters sometimes might know precisely values parameters take know knowledge domain model architecture dependencies model parameters common type dependency often want express certain parameters close one another consider following scenario two models performing classiﬁcation task set classes somewhat diﬀerent input distributions formally model parameters model parameters two models map input two diﬀerent related outputs let imagine tasks similar enough perhaps similar input output distributions believe model parameters close close leverage information regularization speciﬁcally use parameter norm penalty form  used penalty choices also possible kind approach proposed regularized lasserre parameters one model trained classiﬁer supervised paradigm close parameters another model trained unsupervised paradigm capture distribution observed input data architectures constructed many parameters classiﬁer model could paired corresponding parameters unsupervised model parameter norm penalty one way regularize parameters close one another popular way use constraints force sets parameters equal method regularization often referred parameter sharing interpret various models model components sharing unique set parameters signiﬁcant advantage parameter sharing regularizing parameters close via norm penalty subset parameters unique set need stored memory certain models convolutional neural network lead signiﬁcant reduction memory footprint model chapter regularization deep learning convolutional neural networks far popular extensive use parameter sharing occurs convolutional neural networks cnns applied computer vision natural images many statistical properties invariant translation example photo cat remains photo cat translated one pixel right cnns take property account sharing parameters across multiple image locations feature hidden unit weights computed diﬀerent locations input means ﬁnd cat cat detector whether cat appears column column image parameter sharing allowed cnns dramatically lower number unique model parameters signiﬁcantly increase network sizes without requiring corresponding increase training data remains one best examples eﬀectively incorporate domain knowledge network architecture cnns discussed detail chapter 
[regularization, deep, learning, sparse, representations] weight decay acts placing penalty directly model parameters another strategy place penalty activations units neural network encouraging activations sparse indirectly imposes complicated penalty model parameters already discussed sec penalization induces sparse parametrization meaning many parameters become zero close zero representational sparsity hand describes representation many elements representation zero close zero simpliﬁed view distinction illustrated context linear regression ufee ufef ufef ufef ufef uff uff uffa uffa uffa uffa uffb ufee ufef ufef ufef ufef uff uff uffa uffa uffa uffa uffb ufee ufef ufef ufef ufef ufef ufef uff uff uffa uffa uffa uffa uffa uffa uffb chapter regularization deep learning ufee ufef ufef ufef ufef uff uff uffa uffa uffa uffa uffb ufee ufef ufef ufef ufef uff uff uffa uffa uffa uffa uffb ufee ufef ufef ufef ufef ufef ufef uff uff uffa uffa uffa uffa uffa uffa uffb ﬁrst expression example sparsely parametrized linear regression model second linear regression sparse representa tion data function sense represents information present sparse vector representational regularization accomplished sorts mechanisms used parameter regularization norm penalty regularization representations performed adding loss function norm penalty representation penalty denoted denote regularized loss function weights relative contribution norm penalty term larger values corresponding regularization penalty parameters induces parameter sparsity penalty elements representation induces representational sparsity course penalty one choice penalty result sparse representation others include penalty derived student prior representation olshausen field bergstra divergence penalties especially larochelle bengio useful representations elements constrained lie unit interval lee goodfellow provide examples strategies based regularizing average activation across several examples near target value vector entry approaches obtain representational sparsity hard constraint activation values example orthogonal matching pursuit pati encodes input representation solves constrained optimization problem arg min number non zero entries problem solved eﬃciently constrained orthogonal method often called chapter regularization deep learning omp value speciﬁed indicate number non zero features allowed demonstrated omp eﬀective coates feature extractor deep architectures essentially model hidden units made sparse throughout book see many examples sparsity regularization used variety contexts 
[regularization, deep, learning, bagging, ensemble, methods] bagging bootstrap aggregating short technique reducing generalization error combining several models idea train several breiman diﬀerent models separately models vote output test examples example general strategy machine learning called model averaging techniques employing strategy known ensemble methods reason model averaging works diﬀerent models usually make errors test set consider example set regression models suppose model makes error example errors drawn zero mean multivariate normal distribution variances covariances error made average prediction ensemble models expected squared error ensemble predictor ufee uff uff uffb ufee uff ufeb ufed uff uff uff uffb case errors perfectly correlated mean squared error reduces model averaging help case errors perfectly uncorrelated expected squared error ensemble means expected squared error ensemble decreases linearly ensemble size words average ensemble perform least well members members make independent errors ensemble perform signiﬁcantly better members diﬀerent ensemble methods construct ensemble models diﬀerent ways example member ensemble could formed training completely diﬀerent kind model using diﬀerent algorithm objective function bagging chapter regularization deep learning first ensemble member second ensemble member original dataset first resampled dataset second resampled dataset figure cartoon depiction bagging works suppose train detector dataset depicted containing suppose make two diﬀerent resampled datasets bagging training procedure construct datasets sampling replacement ﬁrst dataset omits repeats dataset detector learns loop top digit corresponds second dataset repeat omit case detector learns loop bottom digit corresponds individual classiﬁcation rules brittle average output detector robust achieving maximal conﬁdence loops present method allows kind model training algorithm objective function reused several times speciﬁcally bagging involves constructing diﬀerent datasets dataset number examples original dataset dataset constructed sampling replacement original dataset means high probability dataset missing examples original dataset also contains several duplicate examples average around examples original dataset found resulting training set size original model trained dataset diﬀerences examples included dataset result diﬀerences trained models see fig example neural networks reach wide enough variety solution points often beneﬁt model averaging even models trained dataset diﬀerences random initialization random selection minibatches diﬀerences hyperparameters diﬀerent outcomes non deterministic imple mentations neural networks often enough cause diﬀerent members ensemble make partially independent errors chapter regularization deep learning model averaging extremely powerful reliable method reducing generalization error use usually discouraged benchmarking algorithms scientiﬁc papers machine learning algorithm beneﬁt substan tially model averaging price increased computation memory reason benchmark comparisons usually made using single model machine learning contests usually methods using model averag ing dozens models recent prominent example netﬂix grand prize koren techniques constructing ensembles designed make ensemble regularized individual models example technique called boosting freund schapire constructs ensemble higher capacity individual models boosting applied build ensembles neural networks schwenk bengio incrementally adding neural networks ensemble boosting also applied interpreting individual neural network ensemble incrementally adding hidden units bengio neural network 
[regularization, deep, learning, dropout] dropout srivastava provides computationally inexpensive powerful method regularizing broad family models ﬁrst approximation dropout thought method making bagging practical ensembles many large neural networks bagging involves training multiple models evaluating multiple models test example seems impractical model large neural network since training evaluating networks costly terms runtime memory common use ensembles ﬁve ten neural networks used six win ilsvrc szegedy rapidly becomes unwieldy dropout provides inexpensive approximation training evaluating bagged ensemble exponentially many neural networks speciﬁcally dropout trains ensemble consisting sub networks formed removing non output units underlying base network illustrated fig modern neural networks based series aﬃne transformations nonlinearities eﬀectively remove unit network multiplying output value zero procedure requires slight modiﬁcation models radial basis function networks take diﬀerence unit state reference value present dropout algorithm terms multiplication zero simplicity chapter regularization deep learning trivially modiﬁed work operations remove unit network recall learn bagging deﬁne diﬀerent models construct diﬀerent datasets sampling training set replacement train model dataset dropout aims approximate process exponentially large number neural networks speciﬁcally train dropout use minibatch based learning algorithm makes small steps stochastic gradient descent time load example minibatch randomly sample diﬀerent binary mask apply input hidden units network mask unit sampled independently others probability sampling mask value one causing unit included hyperparameter ﬁxed training begins function current value model parameters input example typically input unit included probability hidden unit included probability run forward propagation back propagation learning update usual fig illustrates run forward propagation dropout formally suppose mask vector speciﬁes units include deﬁnes cost model deﬁned parameters mask dropout training consists minimizing expectation contains exponentially many terms obtain unbiased estimate gradient sampling values dropout training quite bagging training case bagging models independent case dropout models share parameters model inheriting diﬀerent subset parameters parent neural network parameter sharing makes possible represent exponential number models tractable amount memory case bagging model trained convergence respective training set case dropout typically models explicitly trained usually model large enough would infeasible sample possible sub networks within lifetime universe instead tiny fraction possible sub networks trained single step parameter sharing causes remaining sub networks arrive good settings parameters diﬀerences beyond dropout follows bagging algorithm example training set encountered sub network indeed subset original training set sampled replacement make prediction bagged ensemble must accumulate votes members refer process inference context far chapter regularization deep learning base network ensemble sub networks figure dropout trains ensemble consisting sub networks constructed removing non output units underlying base network begin base network two visible units two hidden units sixteen possible subsets four units show sixteen subnetworks may formed dropping diﬀerent subsets units original network small example large proportion resulting networks input units path connecting input output problem becomes insigniﬁcant networks wider layers probability dropping possible paths inputs outputs becomes smaller chapter regularization deep learning figure example forward propagation feedforward network using dropout top example use feedforward network two input units one hidden layer two hidden units one output unit perform forward bottom propagation dropout randomly sample vector one entry input hidden unit network entries binary sampled independently probability entry hyperparameter usually hidden layers input unit network multiplied corresponding mask forward propagation continues rest network usual equivalent randomly selecting one sub networks fig running forward propagation chapter regularization deep learning description bagging dropout required model explicitly probabilistic assume model role output probability distribution case bagging model produces probability distribution prediction ensemble given arithmetic mean distributions case dropout sub model deﬁned mask vector deﬁnes prob ability distribution arithmetic mean masks given probability distribution used sample training time sum includes exponential number terms intractable evaluate except cases structure model permits form simpliﬁcation far deep neural nets known permit tractable simpliﬁcation instead approximate inference sampling averaging together output many masks even masks often suﬃcient obtain good performance however even better approach allows obtain good approximation predictions entire ensemble cost one forward propagation change using geometric mean rather arithmetic mean ensemble members predicted distributions warde farley present arguments empirical evidence geometric mean performs comparably arithmetic mean context geometric mean multiple probability distributions guaranteed probability distribution guarantee result probability distribution impose requirement none sub models assigns probability event renormalize resulting distribution unnormalized probability distribution deﬁned directly geometric mean given ensemble number units may dropped use uniform distribution simplify presentation non uniform distributions chapter regularization deep learning also possible make predictions must normalize ensemble ensemble ensemble ensemble key insight involved dropout approxi hinton mate ensemble evaluating one model model units weights going unit multiplied probability including unit motivation modiﬁcation capture right expected value output unit call approach weight scaling inference rule yet theoretical argument accuracy approximate inference rule deep nonlinear networks empirically performs well usually use inclusion probability weight scaling rule usually amounts dividing weights end training using model usual another way achieve result multiply states units training either way goal make sure expected total input unit test time roughly expected total input unit train time even though half units train time missing average many classes models nonlinear hidden units weight scaling inference rule exact simple example consider softmax regression classiﬁer input variables represented vector softmax index family sub models element wise multiplication input binary vector softmax ensemble predictor deﬁned normalizing geometric mean ensemble members predictions ensemble ensemble ensemble ensemble chapter regularization deep learning see weight scaling rule exact simplify ensemble ensemble softmax exp exp exp exp normalized safely ignore multiplication factors constant respect ensemble exp exp ufeb ufed uff uff exp substituting back obtain softmax classiﬁer weights weight scaling rule also exact settings including regression networks conditionally normal outputs deep networks hidden layers without nonlinearities however weight scaling rule approxi mation deep models nonlinearities though approximation theoretically characterized often works well empirically goodfellow found experimentally weight scaling approximation work better terms classiﬁcation accuracy monte carlo approximations ensemble predictor held true even monte carlo approximation allowed sample sub networks found gal ghahramani models obtain better classiﬁcation accuracy using twenty samples chapter regularization deep learning monte carlo approximation appears optimal choice inference approximation problem dependent srivastava showed dropout eﬀective standard computationally inexpensive regularizers weight decay ﬁlter norm constraints sparse activity regularization dropout may also combined forms regularization yield improvement one advantage dropout computationally cheap using dropout training requires computation per example per update generate random binary numbers multiply state depending implementation may also require memory store binary numbers back propagation stage running inference trained model cost per example dropout used though must pay cost dividing weights beginning run inference examples another signiﬁcant advantage dropout signiﬁcantly limit type model training procedure used works well nearly model uses distributed representation trained stochastic gradient descent includes feedforward neural networks probabilistic models restricted boltzmann machines srivastava recurrent neural networks bayer osendorfer pascanu many regularization strategies comparable power impose severe restrictions architecture model though cost per step applying dropout speciﬁc model negligible cost using dropout complete system signiﬁcant dropout regularization technique reduces eﬀective capacity model oﬀset eﬀect must increase size model typically optimal validation set error much lower using dropout comes cost much larger model many iterations training algorithm large datasets regularization confers little reduction generalization error cases computational cost using dropout larger models may outweigh beneﬁt regularization extremely labeled training examples available dropout less eﬀective bayesian neural networks outperform dropout neal alternative splicing dataset fewer examples xiong available srivastava additional unlabeled data available unsupervised feature learning gain advantage dropout wager showed applied linear regression dropout equivalent weight decay diﬀerent weight decay coeﬃcient chapter regularization deep learning input feature magnitude feature weight decay coeﬃcient determined variance similar results hold linear models deep models dropout equivalent weight decay stochasticity used training dropout necessary approach success means approximating sum sub models wang manning derived analytical approximations marginalization approximation known fast dropout resulted faster convergence time due reduced stochasticity computation gradient method also applied test time principled also computationally expensive approximation average sub networks weight scaling approximation fast dropout used nearly match performance standard dropout small neural network problems yet yielded signiﬁcant improvement applied large problem stochasticity necessary achieve regularizing eﬀect dropout also suﬃcient demonstrate warde farley designed control experiments using method called dropout boosting designed use exactly mask noise traditional dropout lack regularizing eﬀect dropout boosting trains entire ensemble jointly maximize log likelihood training set sense traditional dropout analogous bagging approach analogous boosting intended experiments dropout boosting show almost regularization eﬀect compared training entire network single model demonstrates interpretation dropout bagging value beyond interpretation dropout robustness noise regularization eﬀect bagged ensemble achieved stochastically sampled ensemble members trained perform well independently dropout inspired stochastic approaches training exponentially large ensembles models share weights dropconnect special case dropout product single scalar weight single hidden unit state considered unit dropped wan stochastic pooling form randomized pooling see sec building ensembles convolutional networks convolutional network attending diﬀerent spatial locations feature map far dropout remains widely used implicit ensemble method one key insights dropout training network stochastic behavior making predictions averaging multiple stochastic decisions implements form bagging parameter sharing earlier described chapter regularization deep learning dropout bagging ensemble models formed including excluding units however need model averaging strategy based inclusion exclusion principle kind random modiﬁcation admissible practice must choose modiﬁcation families neural networks able learn resist ideally also use model families allow fast approximate inference rule think form modiﬁcation parametrized vector training ensemble consisting possible values requirement ﬁnite number values example real valued srivastava showed multiplying weights outperform dropout based binary masks standard network automatically implements approximate inference ensemble without needing weight scaling far described dropout purely means performing eﬃcient approximate bagging however another view dropout goes dropout trains bagged ensemble models ensemble models share hidden units means hidden unit must able perform well regardless hidden units model hidden units must prepared swapped interchanged models hinton inspired idea biology sexual reproduction involves swapping genes two diﬀerent organisms creates evolutionary pressure genes become good become readily swapped diﬀerent organisms genes features robust changes environment able incorrectly adapt unusual features one organism model dropout thus regularizes hidden unit merely good feature feature good many contexts warde farley compared dropout training training large ensembles concluded dropout oﬀers additional improvements generalization error beyond obtained ensembles independent models important understand large portion power dropout arises fact masking noise applied hidden units seen form highly intelligent adaptive destruction information content input rather destruction raw values input example model learns hidden unit detects face ﬁnding nose dropping corresponds erasing information nose image model must learn another either redundantly encodes presence nose detects face another feature mouth traditional noise injection techniques add unstructured noise input able randomly erase information nose image face unless magnitude noise great nearly information chapter regularization deep learning image removed destroying extracted features rather original values allows destruction process make use knowledge input distribution model acquired far another important aspect dropout noise multiplicative noise additive ﬁxed scale rectiﬁed linear hidden unit added noise could simply learn become large order make added noise insigniﬁcant comparison multiplicative noise allow pathological solution noise robustness problem another deep learning algorithm batch normalization reparametrizes model way introduces additive multiplicative noise hidden units training time primary purpose batch normalization improve optimization noise regularizing eﬀect sometimes makes dropout unnecessary batch normalization described sec 
[regularization, deep, learning, adversarial, training] many cases neural networks begun reach human performance evaluated test set natural therefore wonder whether models obtained true human level understanding tasks order probe level understanding network underlying task search examples model misclassiﬁes found szegedy even neural networks perform human level accuracy nearly error rate examples intentionally constructed using optimization procedure search input near data point model output diﬀerent many cases similar human observer cannot tell diﬀerence original example adversarial example network make highly diﬀerent predictions see fig example adversarial examples many implications example computer security beyond scope chapter however interesting context regularization one reduce error rate original test set via adversarial training training adversarially perturbed examples training set szegedy goodfellow goodfellow showed one primary causes adversarial examples excessive linearity neural networks built primarily linear building blocks experiments overall function implement proves highly linear result linear functions easy chapter regularization deep learning sign sign panda nematode gibbon conﬁdence conﬁdence conﬁdence figure demonstration adversarial example generation applied googlenet imagenet adding imperceptibly small vector whose szegedy elements equal sign elements gradient cost function respect input change googlenet classiﬁcation image reproduced permission goodfellow optimize unfortunately value linear function change rapidly numerous inputs change input linear function weights change much large amount high dimensional adversarial training discourages highly sensitive locally linear behavior encouraging network locally constant neighborhood training data seen way explicitly introducing local constancy prior supervised neural nets adversarial training helps illustrate power using large function family combination aggressive regularization purely linear models like logistic regression able resist adversarial examples forced linear neural networks able represent functions range nearly linear nearly locally constant thus ﬂexibility capture linear trends training data still learning resist local perturbation adversarial examples also provide means accomplishing semi supervised learning point associated label dataset model assigns label model label may true label model high quality high probability providing true label seek adversarial example causes classiﬁer output label adversarial examples generated using true label label provided trained model called virtual adversarial examples miyato classiﬁer may trained assign label encourages classiﬁer learn function chapter regularization deep learning robust small changes anywhere along manifold unlabeled data lies assumption motivating approach diﬀerent classes usually lie disconnected manifolds small perturbation able jump one class manifold another class manifold 
[regularization, deep, learning, tangent, classiﬁer] many machine learning algorithms aim overcome curse dimensionality assuming data lies near low dimensional manifold described sec one early attempts take advantage manifold hypothesis tangent distance algorithm non parametric simard nearest neighbor algorithm metric used generic euclidean distance one derived knowledge manifolds near probability concentrates assumed trying classify examples examples manifold share category since classiﬁer invariant local factors variation correspond movement manifold would make sense use nearest neighbor distance points distance manifolds respectively belong although may computationally diﬃcult would require solving optimization problem ﬁnd nearest pair points cheap alternative makes sense locally approximate tangent plane measure distance two tangents tangent plane point achieved solving low dimensional linear system dimension manifolds course algorithm requires one specify tangent vectors related spirit tangent prop algorithm fig simard trains neural net classiﬁer extra penalty make output neural net locally invariant known factors variation factors variation correspond movement along manifold near examples class concentrate local invariance achieved requiring orthogonal known manifold tangent vectors equivalently directional derivative directions small adding regularization penalty chapter regularization deep learning regularizer course scaled appropriate hyperparameter neural networks would need sum many outputs rather lone output described simplicity tangent distance algorithm tangent vectors derived priori usually formal knowledge eﬀect transformations translation rotation scaling images tangent prop used supervised learning simard also context reinforcement learning thrun tangent propagation closely related dataset augmentation cases user algorithm encodes prior knowledge task specifying set transformations alter output network diﬀerence case dataset augmentation network explicitly trained correctly classify distinct inputs created applying inﬁnitesimal amount transformations tangent propagation require explicitly visiting new input point instead analytically regularizes model resist perturbation directions corresponding speciﬁed transformation analytical approach intellectually elegant two major drawbacks first regularizes model resist inﬁnitesimal perturbation explicit dataset augmentation confers resistance larger perturbations second inﬁnitesimal approach poses diﬃculties models based rectiﬁed linear units models shrink derivatives turning units shrinking weights able shrink derivatives saturating high value large weights sigmoid tanh units dataset augmentation works well rectiﬁed linear units diﬀerent subsets rectiﬁed units activate diﬀerent transformed versions original input tangent propagation also related double backprop drucker lecun adversarial training szegedy goodfellow double backprop regularizes jacobian small adversarial training ﬁnds inputs near original inputs trains model produce output original inputs tangent propagation dataset augmentation using manually speciﬁed transformations require model invariant certain speciﬁed directions change input double backprop adversarial training require model invariant directions change input long change small dataset augmentation non inﬁnitesimal version tangent propagation adversarial training non inﬁnitesimal version double backprop manifold tangent classiﬁer eliminates need rifai know tangent vectors priori see chapter autoencoders chapter regularization deep learning normal tangent figure illustration main idea tangent prop algorithm simard rifai manifold tangent classiﬁer regularize classiﬁer output function curve represents manifold diﬀerent class illustrated one dimensional manifold embedded two dimensional space one curve chosen single point drawn vector tangent class manifold parallel touching manifold vector normal class manifold orthogonal manifold multiple dimensions may many tangent directions many normal directions expect classiﬁcation function change rapidly moves direction normal manifold change moves along class manifold tangent propagation manifold tangent classiﬁer regularize change much moves along manifold tangent propagation requires user manually specify functions compute tangent directions specifying small translations images remain class manifold manifold tangent classiﬁer estimates manifold tangent directions training autoencoder training data use autoencoders estimate manifolds described chapter estimate manifold tangent vectors manifold tangent classiﬁer makes use technique avoid needing user speciﬁed tangent vectors illustrated fig estimated tangent vectors beyond classical invariants arise geometry images translation rotation scaling include factors must learned object speciﬁc moving body parts algorithm proposed manifold tangent classiﬁer therefore simple use autoencoder learn manifold structure unsupervised learning use tangents regularize neural net classiﬁer tangent prop chapter described general strategies used regularize neural networks regularization central theme machine learning chapter regularization deep learning revisited periodically remaining chapters another central theme machine learning optimization described next 
[optimization, training, deep, models] deep learning algorithms involve optimization many contexts example performing inference models pca involves solving optimization problem often use analytical optimization write proofs design algorithms many optimization problems involved deep learning diﬃcult neural network training quite common invest days months time hundreds machines order solve even single instance neural network training problem problem important expensive specialized set optimization techniques developed solving chapter presents optimization techniques neural network training unfamiliar basic principles gradient based optimization suggest reviewing chapter chapter includes brief overview numerical optimization general chapter focuses one particular case optimization ﬁnding param eters neural network signiﬁcantly reduce cost function typically includes performance measure evaluated entire training set well additional regularization terms begin description optimization used training algorithm machine learning task diﬀers pure optimization next present several concrete challenges make optimization neural networks diﬃcult deﬁne several practical algorithms including optimization algorithms strategies initializing parameters advanced algorithms adapt learning rates training leverage information contained chapter optimization training deep models second derivatives cost function finally conclude review several optimization strategies formed combining simple optimization algorithms higher level procedures 
[optimization, training, deep, models, learning, diﬀers, pure, optimization] optimization algorithms used training deep models diﬀer traditional optimization algorithms several ways machine learning usually acts indirectly machine learning scenarios care performance measure deﬁned respect test set may also intractable therefore optimize indirectly reduce diﬀerent cost function hope improve contrast pure optimization minimizing goal optimization algorithms training deep models also typically include specialization speciﬁc structure machine learning objective functions typically cost function written average training set data per example loss function predicted output input data empirical distribution supervised learning case target output throughout chapter develop unregularized supervised case arguments however trivial extend development example include arguments exclude arguments order develop various forms regularization unsupervised learning deﬁnes objective function respect training set would usually prefer minimize corresponding objective function expectation taken across data generating distribution data rather ﬁnite training set data 
[optimization, training, deep, models, learning, diﬀers, pure, optimization, empirical, risk, minimization] goal machine learning algorithm reduce expected generalization error given quantity known emphasize risk expectation taken true underlying distribution data knew true distribution data risk minimization would optimization task chapter optimization training deep models solvable optimization algorithm however know data training set samples machine learning problem simplest way convert machine learning problem back timization problem minimize expected loss training set means replacing true distribution empirical distribution deﬁned training set minimize empirical risk data number training examples training process based minimizing average training error known empirical risk minimization setting machine learning still similar straightforward optimization rather optimizing risk directly optimize empirical risk hope risk decreases signiﬁcantly well variety theoretical results establish conditions true risk expected decrease various amounts however empirical risk minimization prone overﬁtting models high capacity simply memorize training set many cases empirical risk minimization really feasible eﬀective modern optimization algorithms based gradient descent many useful loss functions loss useful derivatives derivative either zero undeﬁned everywhere two problems mean context deep learning rarely use empirical risk minimization instead must use slightly diﬀerent approach quantity actually optimize even diﬀerent quantity truly want optimize 
[optimization, training, deep, models, learning, diﬀers, pure, optimization, surrogate, loss, functions, early, stopping] sometimes loss function actually care say classiﬁcation error one optimized eﬃciently example exactly minimizing expected loss typically intractable exponential input dimension even linear classiﬁer marcotte savard situations one typically optimizes surrogate loss function instead acts proxy advantages example negative log likelihood correct class typically used surrogate loss negative log likelihood allows model estimate conditional probability classes given input model well pick classes yield least classiﬁcation error expectation chapter optimization training deep models cases surrogate loss function actually results able learn example test set loss often continues decrease long time training set loss reached zero training using log likelihood surrogate even expected loss zero one improve robustness classiﬁer pushing classes apart obtaining conﬁdent reliable classiﬁer thus extracting information training data would possible simply minimizing average loss training set important diﬀerence optimization general optimization use training algorithms training algorithms usually halt local minimum instead machine learning algorithm usually minimizes surrogate loss function halts convergence criterion based early stopping sec satisﬁed typically early stopping criterion based true underlying loss function loss measured validation set designed cause algorithm halt whenever overﬁtting begins occur training often halts surrogate loss function still large derivatives diﬀerent pure optimization setting optimization algorithm considered converged gradient becomes small 
[optimization, training, deep, models, learning, diﬀers, pure, optimization, batch, minibatch, algorithms] one aspect machine learning algorithms separates general optimization algorithms objective function usually decomposes sum training examples optimization algorithms machine learning typically compute update parameters based expected value cost function estimated using subset terms full cost function example maximum likelihood estimation problems viewed log space decompose sum example arg max log model maximizing sum equivalent maximizing expectation empirical distribution deﬁned training set data log model properties objective function used opti mization algorithms also expectations training set example chapter optimization training deep models commonly used property gradient data log model computing expectation exactly expensive requires evaluating model every example entire dataset practice compute expectations randomly sampling small number examples dataset taking average examples recall standard error mean estimated samples given true standard deviation value samples denominator shows less linear returns using examples estimate gradient compare two hypothetical estimates gradient one based examples another based examples latter requires times computation former reduces standard error mean factor optimization algorithms converge much faster terms total computation terms number updates allowed rapidly compute approximate estimates gradient rather slowly computing exact gradient another consideration motivating statistical estimation gradient small number samples redundancy training set worst case samples training set could identical copies sampling based estimate gradient could compute correct gradient single sample using times less computation naive approach practice unlikely truly encounter worst case situation may ﬁnd large numbers examples make similar contributions gradient optimization algorithms use entire training set called batch deterministic gradient methods process training examples simultaneously large batch terminology somewhat confusing word batch also often used describe minibatch used minibatch stochastic gradient descent typically term batch gradient descent implies use full training set use term batch describe group examples example common use term batch size describe size minibatch optimization algorithms use single example time sometimes called stochastic online sometimes methods term online usually reserved case examples drawn stream continually created examples rather ﬁxed size training set several passes made algorithms used deep learning fall somewhere using chapter optimization training deep models one less training examples traditionally called minibatch minibatch stochastic methods common simply call stochastic methods canonical example stochastic method stochastic gradient descent presented detail sec minibatch sizes generally driven following factors larger batches provide accurate estimate gradient less linear returns multicore architectures usually underutilized extremely small batches motivates using absolute minimum batch size reduction time process minibatch examples batch processed parallel typically case amount memory scales batch size many hardware setups limiting factor batch size kinds hardware achieve better runtime speciﬁc sizes arrays especially using gpus common power batch sizes oﬀer better runtime typical power batch sizes range sometimes attempted large models small batches oﬀer regularizing eﬀect wilson martinez perhaps due noise add learning process generalization error often best batch size training small batch size might require small learning rate maintain stability due high variance estimate gradient total runtime high due need make steps reduced learning rate takes steps observe entire training set diﬀerent kinds algorithms use diﬀerent kinds information mini batch diﬀerent ways algorithms sensitive sampling error others either use information diﬃcult estimate accurately samples use information ways amplify sampling errors methods compute updates based gradient usually relatively robust handle smaller batch sizes like second order methods use also hessian matrix compute updates typically require much larger batch sizes like large batch sizes required minimize ﬂuctuations estimates suppose estimated perfectly poor condition number multiplication chapter optimization training deep models inverse ampliﬁes pre existing errors case estimation errors small changes estimate thus cause large changes update even estimated perfectly course estimated approximately update contain even error would predict applying poorly conditioned operation estimate also crucial minibatches selected randomly computing unbiased estimate expected gradient set samples requires samples independent also wish two subsequent gradient estimates independent two subsequent minibatches examples also independent many datasets naturally arranged way successive examples highly correlated example might dataset medical data long list blood sample test results list might arranged ﬁrst ﬁve blood samples taken diﬀerent times ﬁrst patient three blood samples taken second patient blood samples third patient draw examples order list minibatches would extremely biased would represent primarily one patient many patients dataset cases order dataset holds signiﬁcance necessary shuﬄe examples selecting minibatches large datasets example datasets containing billions examples data center impractical sample examples truly uniformly random every time want construct minibatch fortunately practice usually suﬃcient shuﬄe order dataset store shuﬄed fashion impose ﬁxed set possible minibatches consecutive examples models trained thereafter use individual model forced reuse ordering every time passes training data however deviation true random selection seem signiﬁcant detrimental eﬀect failing ever shuﬄe examples way seriously reduce eﬀectiveness algorithm many optimization problems machine learning decompose examples well enough compute entire separate updates diﬀerent examples parallel words compute update minimizes one minibatch examples time compute update several minibatches asynchronous parallel distributed approaches discussed sec interesting motivation minibatch stochastic gradient descent follows gradient true generalization error long examples repeated implementations minibatch stochastic gradient chapter optimization training deep models descent shuﬄe dataset pass multiple times ﬁrst pass minibatch used compute unbiased estimate true generalization error second pass estimate becomes biased formed sampling values already used rather obtaining new fair samples data generating distribution fact stochastic gradient descent minimizes generalization error easiest see online learning case examples minibatches drawn stream data words instead receiving ﬁxed size training set learner similar living sees new example instant every example coming data generating distribution data scenario examples never repeated every experience fair sample data equivalence easiest derive discrete case generalization error written sum data exact gradient data already seen fact demonstrated log likelihood observe holds functions besides likelihood similar result derived continuous mild assumptions regarding data hence obtain unbiased estimator exact gradient generalization error sampling minibatch examples cor responding targets data generating distribution data computing gradient loss respect parameters minibatch updating direction performs sgd generalization error course interpretation applies examples reused nonetheless usually best make several passes training set unless training set extremely large multiple epochs used ﬁrst epoch follows unbiased gradient generalization error chapter optimization training deep models course additional epochs usually provide enough beneﬁt due decreased training error oﬀset harm cause increasing gap training error test error datasets growing rapidly size faster computing power becoming common machine learning applications use training example even make incomplete pass training set using extremely large training set overﬁtting issue underﬁtting computational eﬃciency become predominant concerns see also discussion eﬀect computational bottou bousquet bottlenecks generalization error number training examples grows 
[optimization, training, deep, models, challenges, neural, network, optimization] optimization general extremely diﬃcult task traditionally machine learning avoided diﬃculty general optimization carefully designing objective function constraints ensure optimization problem convex training neural networks must confront general non convex case even convex optimization without complications section summarize several prominent challenges involved optimization training deep models 
[optimization, training, deep, models, challenges, neural, network, optimization, ill-conditioning] challenges arise even optimizing convex functions prominent ill conditioning hessian matrix general problem numerical optimization convex otherwise described detail sec ill conditioning problem generally believed present neural network training problems ill conditioning manifest causing sgd get stuck sense even small steps increase cost function recall second order taylor series expansion cost function predicts gradient descent step add  cost ill conditioning gradient becomes problem exceeds  determine whether ill conditioning detrimental neural network training task one monitor squared gradient norm chapter optimization training deep models training time epochs training time epochs figure gradient descent often arrive critical point kind example gradient norm increases throughout training convolutional network used object detection left scatterplot showing norms individual gradient evaluations distributed time improve legibility one gradient norm plotted per epoch running average gradient norms plotted solid curve gradient norm clearly increases time rather decreasing would expect training process converged critical point despite increasing right gradient training process reasonably successful validation set classiﬁcation error decreases low level term many cases gradient norm shrink signiﬁcantly throughout learning term grows order magnitude result learning becomes slow despite presence strong gradient learning rate must shrunk compensate even stronger curvature fig shows example gradient increasing signiﬁcantly successful training neural network though ill conditioning present settings besides neural network training techniques used combat contexts less applicable neural networks example newton method excellent tool minimizing convex functions poorly conditioned hessian matrices subsequent sections argue newton method requires signiﬁcant modiﬁcation applied neural networks 
[optimization, training, deep, models, challenges, neural, network, optimization, local, minima] one prominent features convex optimization problem reduced problem ﬁnding local minimum local minimum chapter optimization training deep models guaranteed global minimum convex functions ﬂat region bottom rather single global minimum point point within ﬂat region acceptable solution optimizing convex function know reached good solution ﬁnd critical point kind non convex functions neural nets possible many local minima indeed nearly deep model essentially guaranteed extremely large number local minima however see necessarily major problem neural networks models multiple equivalently parametrized latent variables multiple local minima model identiﬁability problem model said identiﬁable suﬃciently large training set rule one setting model parameters models latent variables often identiﬁable obtain equivalent models exchanging latent variables example could take neural network modify layer swapping incoming weight vector unit incoming weight vector unit outgoing weight vectors layers units ways arranging hidden units kind non identiﬁability known weight space symmetry addition weight space symmetry many kinds neural networks additional causes non identiﬁability example rectiﬁed linear maxout network scale incoming weights biases unit also scale outgoing weights means cost function include terms weight decay depend directly weights rather models outputs every local minimum rectiﬁed linear maxout network lies dimensional hyperbola equivalent local minima model identiﬁability issues mean extremely large even uncountably inﬁnite amount local minima neural network cost function however local minima arising non identiﬁability equivalent cost function value result local minima problematic form non convexity local minima problematic high cost comparison global minimum one construct small neural networks even without hidden units local minima higher cost global minimum sontag sussman brady gori tesi local minima high cost common could pose serious problem gradient based optimization algorithms remains open question whether many local minima high cost chapter optimization training deep models networks practical interest whether optimization algorithms encounter many years practitioners believed local minima common problem plaguing neural network optimization today appear case problem remains active area research experts suspect suﬃciently large neural networks local minima low cost function value important ﬁnd true global minimum rather ﬁnd point parameter space low minimal cost saxe dauphin goodfellow choromanska many practitioners attribute nearly diﬃculty neural network optimiza tion local minima encourage practitioners carefully test speciﬁc problems test rule local minima problem plot norm gradient time norm gradient shrink insigniﬁcant size problem neither local minima kind critical point kind negative test rule local minima high dimensional spaces diﬃcult positively establish local minima problem many structures local minima also small gradients 
[optimization, training, deep, models, challenges, neural, network, optimization, plateaus, saddle, points, flat, regions] many high dimensional non convex functions local minima maxima fact rare compared another kind point zero gradient saddle point points around saddle point greater cost saddle point others lower cost saddle point hessian matrix positive negative eigenvalues points lying along eigenvectors associated positive eigenvalues greater cost saddle point points lying along negative eigenvalues lower value think saddle point local minimum along one cross section cost function local maximum along another cross section see fig illustration many classes random functions exhibit following behavior low dimensional spaces local minima common higher dimensional spaces local minima rare saddle points common function type expected ratio number saddle points local minima grows exponentially understand intuition behind behavior observe hessian matrix local minimum positive eigenvalues hessian matrix saddle point mixture positive negative eigenvalues imagine sign eigenvalue generated ﬂipping coin single dimension easy obtain local minimum tossing coin getting heads dimensional space exponentially unlikely coin tosses chapter optimization training deep models heads see review relevant theoretical work dauphin amazing property many random functions eigenvalues hessian become likely positive reach regions lower cost coin tossing analogy means likely coin come heads times critical point low cost means local minima much likely low cost high cost critical points high cost far likely saddle points critical points extremely high cost likely local maxima happens many classes random functions happen neural networks showed theoretically shallow autoencoders baldi hornik feedforward networks trained copy input output described chapter nonlinearities global minima saddle points local minima higher cost global minimum observed without proof results extend deeper networks without nonlinearities output networks linear function input useful study model nonlinear neural networks loss function non convex function parameters networks essentially multiple matrices composed together provided exact solutions saxe complete learning dynamics networks showed learning models captures many qualitative features observed training deep models nonlinear activation functions showed dauphin experimentally real neural networks also loss functions contain many high cost saddle points choromanska provided additional theoretical arguments showing another class high dimensional random functions related neural networks well implications proliferation saddle points training algo rithms ﬁrst order optimization algorithms use gradient information situation unclear gradient often become small near saddle point hand gradient descent empirically seems able escape saddle points many cases provided visualizations goodfellow several learning trajectories state art neural networks example given fig visualizations show ﬂattening cost function near prominent saddle point weights zero also show gradient descent trajectory rapidly escaping region goodfellow also argue continuous time gradient descent may shown analytically repelled rather attracted nearby saddle point situation may diﬀerent realistic uses gradient descent newton method clear saddle points constitute problem chapter optimization training deep models pro jec tio projection figure visualization cost function neural network image adapted permission goodfellow visualizations appear similar feedforward neural networks convolutional networks recurrent networks applied real object recognition natural language processing tasks surprisingly visualizations usually show many conspicuous obstacles prior success stochastic gradient descent training large models beginning roughly neural net cost function surfaces generally believed much non convex structure revealed projections primary obstacle revealed projection saddle point high cost near parameters initialized indicated blue path sgd training trajectory escapes saddle point readily training time spent traversing relatively ﬂat valley cost function may due high noise gradient poor conditioning hessian matrix region simply need circumnavigate tall mountain visible ﬁgure via indirect arcing path chapter optimization training deep models gradient descent designed move downhill explicitly designed seek critical point newton method however designed solve point gradient zero without appropriate modiﬁcation jump saddle point proliferation saddle points high dimensional spaces presumably explains second order methods succeeded replacing gradient descent neural network training introduced dauphin saddle free newton method second order optimization showed improves signiﬁcantly traditional version second order methods remain diﬃcult scale large neural networks saddle free approach holds promise could scaled kinds points zero gradient besides minima saddle points also maxima much like saddle points perspective optimization many algorithms attracted unmodiﬁed newton method maxima become exponentially rare high dimensional space like minima may also wide ﬂat regions constant value locations gradient also hessian zero degenerate locations pose major problems numerical optimization algorithms convex problem wide ﬂat region must consist entirely global minima general optimization problem region could correspond high value objective function 
[optimization, training, deep, models, challenges, neural, network, optimization, cliﬀs, exploding, gradients] neural networks many layers often extremely steep regions resembling cliﬀs illustrated fig result multiplication several large weights together face extremely steep cliﬀ structure gradient update step move parameters extremely far usually jumping cliﬀ structure altogether chapter optimization training deep models figure objective function highly nonlinear deep neural networks recurrent neural networks often contains sharp nonlinearities parameter space resulting multiplication several parameters nonlinearities give rise high derivatives places parameters get close cliﬀ region gradient descent update catapult parameters far possibly losing optimization work done figure adapted permission pascanu cliﬀ dangerous whether approach fortunately serious consequences avoided using gradient clipping heuristic described sec basic idea recall gradient specify optimal step size optimal direction within inﬁnitesimal region traditional gradient descent algorithm proposes make large step gradient clipping heuristic intervenes reduce step size small enough less likely outside region gradient indicates direction approximately steepest descent cliﬀ structures common cost functions recurrent neural networks models involve multiplication many factors one factor time step long temporal sequences thus incur extreme amount multiplication 
[optimization, training, deep, models, challenges, neural, network, optimization, long-term, dependencies] another diﬃculty neural network optimization algorithms must overcome arises computational graph becomes extremely deep feedforward networks many layers deep computational graphs recurrent networks described chapter construct deep computational graphs chapter optimization training deep models repeatedly applying operation time step long temporal sequence repeated application parameters gives rise especially pronounced diﬃculties example suppose computational graph contains path consists repeatedly multiplying matrix steps equivalent mul tiplying suppose eigendecomposition diag simple case straightforward see diag diag eigenvalues near absolute value either explode greater magnitude vanish less magnitude vanishing exploding gradient problem refers fact gradients graph also scaled according diag vanishing gradients make diﬃcult know direction parameters move improve cost function exploding gradients make learning unstable cliﬀ structures described earlier motivate gradient clipping example exploding gradient phenomenon repeated multiplication time step described similar power method algorithm used ﬁnd largest eigenvalue matrix corresponding eigenvector point view surprising eventually discard components orthogonal principal eigenvector recurrent networks use matrix time step feedforward networks even deep feedforward networks largely avoid vanishing exploding gradient problem sussillo defer discussion challenges training recurrent networks sec recurrent networks described detail 
[optimization, training, deep, models, challenges, neural, network, optimization, inexact, gradients] optimization algorithms primarily motivated case exact knowledge gradient hessian matrix practice usually noisy even biased estimate quantities nearly every deep learning algorithm relies sampling based estimates least insofar using minibatch training examples compute gradient cases objective function want minimize actually intractable objective function intractable typically gradient intractable well cases approximate gradient issues mostly arise chapter optimization training deep models advanced models part example contrastive divergence iii gives technique approximating gradient intractable log likelihood boltzmann machine various neural network optimization algorithms designed account imperfections gradient estimate one also avoid problem choosing surrogate loss function easier approximate true loss 
[optimization, training, deep, models, challenges, neural, network, optimization, poor, correspondence, local, global, structure] many problems discussed far correspond properties loss function single point diﬃcult make single step poorly conditioned current point lies cliﬀ saddle point hiding opportunity make progress downhill gradient possible overcome problems single point still perform poorly direction results improvement locally point toward distant regions much lower cost goodfellow argue much runtime training due length trajectory needed arrive solution fig shows learning trajectory spends time tracing wide arc around mountain shaped structure much research diﬃculties optimization focused whether training arrives global minimum local minimum saddle point practice neural networks arrive critical point kind fig shows neural networks often arrive region small gradient indeed critical points even necessarily exist example loss function log lack global minimum point instead asymptotically approach value model becomes conﬁdent classiﬁer discrete provided softmax negative log likelihood become arbitrarily close zero model able correctly classify every example training set impossible actually reach value zero likewise model real values negative log likelihood asymptotes negative inﬁnity able correctly predict value training set targets learning algorithm increase without bound see fig example failure local optimization ﬁnd good cost function value even absence local minima saddle points future research need develop understanding factors inﬂuence length learning trajectory better characterize outcome chapter optimization training deep models figure optimization based local downhill moves fail local surface point toward global solution provide example occur even saddle points local minima example cost function contains asymptotes toward low values minima main cause diﬃculty case initialized wrong side mountain able traverse higher dimensional space learning algorithms often circumnavigate mountains trajectory associated may long result excessive training time illustrated fig process many existing research directions aimed ﬁnding good initial points problems diﬃcult global structure rather developing algorithms use non local moves gradient descent essentially learning algorithms eﬀective training neural networks based making small local moves previous sections primarily focused correct direction local moves diﬃcult compute may able compute properties objective function gradient approximately bias variance estimate correct direction cases local descent may may deﬁne reasonably short path valid solution actually able follow local descent path objective function may issues poor conditioning discontinuous gradients causing region gradient provides good model objective function small cases local descent steps size may deﬁne reasonably short path solution able compute local descent direction steps size cases local descent may may deﬁne path solution path contains many steps following path incurs chapter optimization training deep models high computational cost sometimes local information provides guide function wide ﬂat region manage land exactly critical point usually latter scenario happens methods solve explicitly critical points newton method cases local descent deﬁne path solution cases local moves greedy lead along path moves downhill away solution fig along unnecessarily long trajectory solution fig currently understand problems relevant making neural network optimization diﬃcult active area research regardless problems signiﬁcant might avoided exists region space connected reasonably directly solution path local descent follow able initialize learning within well behaved region last view suggests research choosing good initial points traditional optimization algorithms use 
[optimization, training, deep, models, challenges, neural, network, optimization, theoretical, limits, optimization] several theoretical results show limits performance optimization algorithm might design neural networks blum rivest judd wolpert macready typically results little bearing use neural networks practice theoretical results apply case units neural network output discrete values however neural network units output smoothly increasing values make optimization via local search feasible theoretical results show exist problem classes intractable diﬃcult tell whether particular problem falls class results show ﬁnding solution network given size intractable practice ﬁnd solution easily using larger network many parameter settings correspond acceptable solution moreover context neural network training usually care ﬁnding exact minimum function reducing value suﬃciently obtain good generalization error theoretical analysis whether optimization algorithm accomplish goal extremely diﬃcult developing realistic bounds performance optimization algorithms therefore remains important goal machine learning research chapter optimization training deep models 
[optimization, training, deep, models, basic, algorithms] previously introduced gradient descent sec algorithm follows gradient entire training set downhill may accelerated considerably using stochastic gradient descent follow gradient randomly selected minibatches downhill discussed sec sec 
[optimization, training, deep, models, basic, algorithms, stochastic, gradient, descent] stochastic gradient descent sgd variants probably used optimization algorithms machine learning general deep learning particular discussed sec possible obtain unbiased estimate gradient taking average gradient minibatch examples drawn data generating distribution algorithm shows follow estimate gradient downhill algorithm stochastic gradient descent sgd update training iteration require learning rate require initial parameter stopping criterion met sample minibatch examples training set corresponding targets compute gradient estimate apply update end crucial parameter sgd algorithm learning rate previously described sgd using ﬁxed learning rate practice necessary gradually decrease learning rate time denote learning rate iteration sgd gradient estimator introduces source noise random sampling training examples vanish even arrive minimum comparison true gradient total cost function becomes small approach reach minimum using batch gradient descent batch gradient descent use ﬁxed learning rate suﬃcient condition guarantee convergence sgd chapter optimization training deep models practice common decay learning rate linearly iteration iteration common leave constant learning rate may chosen trial error usually best choose monitoring learning curves plot objective function function time art science guidance subject regarded skepticism using linear schedule parameters choose usually may set number iterations required make hundred passes training set usually set roughly value main question set large learning curve show violent oscillations cost function often increasing signiﬁcantly gentle oscillations ﬁne especially training stochastic cost function cost function arising use dropout learning rate low learning proceeds slowly initial learning rate low learning may become stuck high cost value typically optimal initial learning rate terms total training time ﬁnal cost value higher learning rate yields best performance ﬁrst iterations therefore usually best monitor ﬁrst several iterations use learning rate higher best performing learning rate time high causes severe instability important property sgd related minibatch online gradient based optimization computation time per update grow number training examples allows convergence even number training examples becomes large large enough dataset sgd may converge within ﬁxed tolerance ﬁnal test set error processed entire training set study convergence rate optimization algorithm common measure excess error min amount current cost function exceeds minimum possible cost sgd applied convex problem excess error iterations strongly convex case bounds cannot improved unless extra conditions assumed batch gradient descent enjoys better convergence rates stochastic gradient descent theory however cramér rao bound cramér rao states generalization error cannot decrease faster bottou chapter optimization training deep models bousquet argue therefore may worthwhile pursue optimization algorithm converges faster machine learning tasks faster convergence presumably corresponds overﬁtting moreover asymptotic analysis obscures many advantages stochastic gradient descent small number steps large datasets ability sgd make rapid initial progress evaluating gradient examples outweighs slow asymptotic convergence algorithms described remainder chapter achieve beneﬁts matter practice lost constant factors obscured asymptotic analysis one also trade beneﬁts batch stochastic gradient descent gradually increasing minibatch size course learning information sgd see bottou 
[optimization, training, deep, models, basic, algorithms, momentum] stochastic gradient descent remains popular optimization strategy learning sometimes slow method momentum polyak designed accelerate learning especially face high curvature small consistent gradients noisy gradients momentum algorithm accumulates exponentially decaying moving average past gradients continues move direction eﬀect momentum illustrated fig formally momentum algorithm introduces variable plays role velocity direction speed parameters move parameter space velocity set exponentially decaying average negative gradient name derives physical analogy momentum negative gradient force moving particle parameter space according newton laws motion momentum physics mass times velocity momentum learning algorithm assume unit mass velocity vector may also regarded momentum particle hyperparameter determines quickly contributions previous gradients exponentially decay update rule given velocity accumulates gradient elements larger relative previous gradients aﬀect current direction sgd algorithm momentum given algorithm chapter optimization training deep models figure momentum aims primarily solve two problems poor conditioning hessian matrix variance stochastic gradient illustrate momentum overcomes ﬁrst two problems contour lines depict quadratic loss function poorly conditioned hessian matrix red path cutting across contours indicates path followed momentum learning rule minimizes function step along way draw arrow indicating step gradient descent would take point see poorly conditioned quadratic objective looks like long narrow valley canyon steep sides momentum correctly traverses canyon lengthwise gradient steps waste time moving back forth across narrow axis canyon compare also fig shows behavior gradient descent without momentum chapter optimization training deep models previously size step simply norm gradient multiplied learning rate size step depends large aligned sequence gradients step size largest many successive gradients point exactly direction momentum algorithm always observes gradient accelerate direction reaching terminal velocity size step thus helpful think momentum hyperparameter terms example corresponds multiplying maximum speed relative gradient descent algorithm common values used practice include like learning rate may also adapted time typically begins small value later raised less important adapt time shrink time algorithm stochastic gradient descent sgd momentum require learning rate momentum parameter require initial parameter initial velocity stopping criterion met sample minibatch examples training set corresponding targets compute gradient estimate compute velocity update apply update end view momentum algorithm simulating particle subject continuous time newtonian dynamics physical analogy help build intuition momentum gradient descent algorithms behave position particle point time given particle experiences net force force causes particle accelerate rather viewing second order diﬀerential equation position introduce variable representing velocity particle time rewrite newtonian dynamics ﬁrst order diﬀerential equation chapter optimization training deep models momentum algorithm consists solving diﬀerential equations via numerical simulation simple numerical method solving diﬀerential equations euler method simply consists simulating dynamics deﬁned equation taking small ﬁnite steps direction gradient explains basic form momentum update speciﬁcally forces one force proportional negative gradient cost function force pushes particle downhill along cost function surface gradient descent algorithm would simply take single step based gradient newtonian scenario used momentum algorithm instead uses force alter velocity particle think particle like hockey puck sliding icy surface whenever descends steep part surface gathers speed continues sliding direction begins uphill one force necessary force gradient cost function particle might never come rest imagine hockey puck sliding one side valley straight side oscillating back forth forever assuming ice perfectly frictionless resolve problem add one force proportional physics terminology force corresponds viscous drag particle must push resistant medium syrup causes particle gradually lose energy time eventually converge local minimum use viscous drag particular part reason use mathematical convenience integer power velocity easy work however physical systems kinds drag based integer powers velocity example particle traveling air experiences turbulent drag force proportional square velocity particle moving along ground experiences dry friction force constant magnitude reject options turbulent drag proportional square velocity becomes weak velocity small powerful enough force particle come rest particle non zero initial velocity experiences force turbulent drag move away initial position forever distance starting point growing like log must therefore use lower power velocity use power zero representing dry friction force strong force due gradient cost function small non zero constant force due friction cause particle come rest reaching local minimum viscous drag avoids problems weak enough chapter optimization training deep models gradient continue cause motion minimum reached strong enough prevent motion gradient justify moving 
[optimization, training, deep, models, basic, algorithms, nesterov, momentum] sutskever introduced variant momentum algorithm inspired nesterov accelerated gradient method nesterov update rules case given parameters play similar role standard momentum method diﬀerence nesterov momentum standard momentum gradient evaluated nesterov momentum gradient evaluated current velocity applied thus one interpret nesterov momentum attempting add correction factor standard method momentum complete nesterov momentum algorithm presented algorithm convex batch gradient case nesterov momentum brings rate convergence excess error steps shown nesterov unfortunately stochastic gradient case nesterov momentum improve rate convergence algorithm stochastic gradient descent sgd nesterov momentum require learning rate momentum parameter require initial parameter initial velocity stopping criterion met sample minibatch examples training set corresponding labels apply interim update compute gradient interim point compute velocity update apply update end chapter optimization training deep models 
[optimization, training, deep, models, parameter, initialization, strategies] optimization algorithms iterative nature simply solve solution point optimization algorithms iterative nature applied right class optimization problems converge acceptable solutions acceptable amount time regardless initialization deep learning training algorithms usually either luxuries training algorithms deep learning models usually iterative nature thus require user specify initial point begin iterations moreover training deep models suﬃciently diﬃcult task algorithms strongly aﬀected choice initialization initial point determine whether algorithm converges initial points unstable algorithm encounters numerical diﬃculties fails altogether learning converge initial point determine quickly learning converges whether converges point high low cost also points comparable cost wildly varying generalization error initial point aﬀect generalization well modern initialization strategies simple heuristic designing improved initialization strategies diﬃcult task neural network optimization yet well understood initialization strategies based achieving nice properties network initialized however good understanding properties preserved circumstances learning begins proceed diﬃculty initial points may beneﬁcial viewpoint optimization detrimental viewpoint generalization understanding initial point aﬀects generalization especially primitive oﬀering little guidance select initial point perhaps property known complete certainty initial parameters need break symmetry diﬀerent units two hidden units activation function connected inputs units must diﬀerent initial parameters initial parameters deterministic learning algorithm applied deterministic cost model constantly update units way even model training algorithm capable using stochasticity compute diﬀerent updates diﬀerent units example one trains dropout usually best initialize unit compute diﬀerent function units may help make sure input patterns lost null space forward propagation gradient patterns lost null space back propagation goal unit compute diﬀerent function chapter optimization training deep models motivates random initialization parameters could explicitly search large set basis functions mutually diﬀerent often incurs noticeable computational cost example many outputs inputs could use gram schmidt orthogonalization initial weight matrix guaranteed unit computes diﬀerent function unit random initialization high entropy distribution high dimensional space computationally cheaper unlikely assign units compute function typically set biases unit heuristically chosen constants initialize weights randomly extra parameters example parameters encoding conditional variance prediction usually set heuristically chosen constants much like biases almost always initialize weights model values drawn randomly gaussian uniform distribution choice gaussian uniform distribution seem matter much exhaustively studied scale initial distribution however large eﬀect outcome optimization procedure ability network generalize larger initial weights yield stronger symmetry breaking eﬀect helping avoid redundant units also help avoid losing signal forward back propagation linear component layer larger values matrix result larger outputs matrix multiplication initial weights large may however result exploding values forward propagation back propagation recurrent networks large weights also result chaos extreme sensitivity small perturbations input behavior deterministic forward propagation procedure appears random extent exploding gradient problem mitigated gradient clipping thresholding values gradients performing gradient descent step large weights may also result extreme values cause activation function saturate causing complete loss gradient saturated units competing factors determine ideal initial scale weights perspectives regularization optimization give diﬀerent insights initialize network optimization perspective suggests weights large enough propagate information success fully regularization concerns encourage making smaller use optimization algorithm stochastic gradient descent makes small incremental changes weights tends halt areas nearer initial parameters whether due getting stuck region low gradient chapter optimization training deep models due triggering early stopping criterion based overﬁtting expresses prior ﬁnal parameters close initial parameters recall sec gradient descent early stopping equivalent weight decay models general case gradient descent early stopping weight decay provide loose analogy thinking eﬀect initialization think initializing parameters similar imposing gaussian prior mean point view makes sense choose near prior says likely units interact interact units interact likelihood term objective function expresses strong preference interact hand initialize large values prior speciﬁes units interact interact heuristics available choosing initial scale weights one heuristic initialize weights fully connected layer inputs outputs sampling weight glorot bengio suggest using normalized initialization latter heuristic designed compromise goal initializing layers activation variance goal initializing layers gradient variance formula derived using assumption network consists chain matrix multiplications nonlinearities real neural networks obviously violate assumption many strategies designed linear model perform reasonably well nonlinear counterparts saxe recommend initializing random orthogonal matrices carefully chosen scaling factor gain accounts nonlinearity applied layer derive speciﬁc values scaling factor diﬀerent types nonlinear activation functions initialization scheme also motivated model deep network sequence matrix multiplies without nonlinearities model initialization scheme guarantees total number training iterations required reach convergence independent depth increasing scaling factor pushes network toward regime activations increase norm propagate forward network gradients increase norm propagate backward showed sussillo setting gain factor correctly suﬃcient train networks deep layers without needing use orthogonal initializations key insight chapter optimization training deep models approach feedforward networks activations gradients grow shrink step forward back propagation following random walk behavior feedforward networks use diﬀerent weight matrix layer random walk tuned preserve norms feedforward networks mostly avoid vanishing exploding gradients problem arises weight matrix used step described sec unfortunately optimal criteria initial weights often lead optimal performance may three diﬀerent reasons first may using wrong criteria may actually beneﬁcial preserve norm signal throughout entire network second properties imposed initialization may persist learning begun proceed third criteria might succeed improving speed optimization inadvertently increase generalization error practice usually need treat scale weights hyperparameter whose optimal value lies somewhere roughly near exactly equal theoretical predictions one drawback scaling rules set initial weights standard deviation every individual weight becomes extremely small layers become large introduced alternative martens initialization scheme called sparse initialization unit initialized exactly non zero weights idea keep total amount input unit independent number inputs without making magnitude individual weight elements shrink sparse initialization helps achieve diversity among units initialization time however also imposes strong prior weights chosen large gaussian values takes long time gradient descent shrink incorrect large values initialization scheme cause problems units maxout units several ﬁlters must carefully coordinated computational resources allow usually good idea treat initial scale weights layer hyperparameter choose scales using hyperparameter search algorithm described sec random search choice whether use dense sparse initialization also made hyperparameter alternately one manually search best initial scales good rule thumb choosing initial scales look range standard deviation activations gradients single minibatch data weights small range activations across minibatch shrink activations propagate forward network repeatedly identifying ﬁrst layer unacceptably small activations increasing weights possible eventually obtain network reasonable chapter optimization training deep models initial activations throughout learning still slow point useful look range standard deviation gradients well activations procedure principle automated generally less computationally costly hyperparameter optimization based validation set error based feedback behavior initial model single batch data rather feedback trained model validation set long used heuristically protocol recently speciﬁed formally studied mishkin matas far focused initialization weights fortunately initialization parameters typically easier approach setting biases must coordinated approach settings weights setting biases zero compatible weight initialization schemes situations may set biases non zero values bias output unit often beneﬁcial initialize bias obtain right marginal statistics output assume initial weights small enough output unit determined bias justiﬁes setting bias inverse activation function applied marginal statistics output training set example output distribution classes distribution highly skewed distribution marginal probability class given element vector set bias vector solving equation softmax applies classiﬁers also models encounter part autoencoders boltzmann iii machines models layers whose output resemble input data helpful initialize biases layers match marginal distribution sometimes may want choose bias avoid causing much saturation initialization example may set bias relu hidden unit rather avoid saturating relu initialization approach compatible weight initialization schemes expect strong input biases though example recommended use random walk initialization sussillo sometimes unit controls whether units able participate function situations unit output another unit view gate determines whether situations want set bias chapter optimization training deep models time initialization otherwise chance learn example advocate setting bias jozefowicz forget gate lstm model described sec another common type parameter variance precision parameter example perform linear regression conditional variance estimate using model precision parameter usually initialize variance precision parameters safely another approach assume initial weights close enough zero biases may set ignoring eﬀect weights set biases produce correct marginal mean output set variance parameters marginal variance output training set besides simple constant random methods initializing model parame ters possible initialize model parameters using machine learning common strategy discussed part book initialize supervised model iii parameters learned unsupervised model trained inputs one also perform supervised training related task even performing supervised training unrelated task sometimes yield initialization oﬀers faster convergence random initialization initialization strategies may yield faster convergence better generalization encode information distribution initial parameters model others apparently perform well primarily set parameters right scale set diﬀerent units compute diﬀerent functions 
[optimization, training, deep, models, algorithms, adaptive, learning, rates] neural network researchers long realized learning rate reliably one hyperparameters diﬃcult set signiﬁcant impact model performance discussed sec sec cost often highly sensitive directions parameter space insensitive others momentum algorithm mitigate issues somewhat expense introducing another hyperparameter face natural ask another way believe directions sensitivity somewhat axis aligned make sense use separate learning rate parameter automatically adapt learning rates throughout course learning chapter optimization training deep models delta bar delta algorithm early heuristic approach jacobs adapting individual learning rates model parameters training approach based simple idea partial derivative loss respect given model parameter remains sign learning rate increase partial derivative respect parameter changes sign learning rate decrease course kind rule applied full batch optimization recently number incremental mini batch based methods introduced adapt learning rates model parameters section brieﬂy review algorithms 
[optimization, training, deep, models, algorithms, adaptive, learning, rates, adagrad] adagrad algorithm shown algorithm individually adapts learning rates model parameters scaling inversely proportional square root sum historical squared values duchi parameters largest partial derivative loss correspondingly rapid decrease learning rate parameters small partial derivatives relatively small decrease learning rate net eﬀect greater progress gently sloped directions parameter space context convex optimization adagrad algorithm enjoys desirable theoretical properties however empirically found training deep neural network models accumulation squared gradients beginning training result premature excessive decrease eﬀective learning rate adagrad performs well deep learning models 
[optimization, training, deep, models, algorithms, adaptive, learning, rates, rmsprop] rmsprop algorithm hinton modiﬁes adagrad perform better non convex setting changing gradient accumulation exponentially weighted moving average adagrad designed converge rapidly applied convex function applied non convex function train neural network learning trajectory may pass many diﬀerent structures eventually arrive region locally convex bowl adagrad shrinks learning rate according entire history squared gradient may made learning rate small arriving convex structure rmsprop uses exponentially decaying average discard history chapter optimization training deep models algorithm adagrad algorithm require global learning rate require initial parameter require small constant perhaps numerical stability initialize gradient accumulation variable stopping criterion met sample minibatch examples training set corresponding targets compute gradient accumulate squared gradient compute update division square root applied element wise apply update end extreme past converge rapidly ﬁnding convex bowl instance adagrad algorithm initialized within bowl rmsprop shown standard form algorithm combined nesterov momentum algorithm compared adagrad use moving average introduces new hyperparameter controls length scale moving average empirically rmsprop shown eﬀective practical timization algorithm deep neural networks currently one optimization methods employed routinely deep learning practitioners 
[optimization, training, deep, models, algorithms, adaptive, learning, rates, adam] adam yet another adaptive learning rate optimization kingma algorithm presented algorithm name adam derives phrase adaptive moments context earlier algorithms perhaps best seen variant combination rmsprop momentum important distinctions first adam momentum incorporated directly estimate ﬁrst order moment exponential weighting gradient straightforward way add momentum rmsprop apply momentum rescaled gradients use momentum combination rescaling clear theoretical motivation second adam includes bias corrections estimates ﬁrst order moments momentum term uncentered second order moments account initialization chapter optimization training deep models algorithm rmsprop algorithm require global learning rate decay rate require initial parameter require small constant usually used stabilize division small numbers initialize accumulation variables stopping criterion met sample minibatch examples training set corresponding targets compute gradient accumulate squared gradient compute parameter update applied element wise apply update end origin see algorithm rmsprop also incorporates estimate uncentered second order moment however lacks correction factor thus unlike adam rmsprop second order moment estimate may high bias early training adam generally regarded fairly robust choice hyperparameters though learning rate sometimes needs changed suggested default 
[optimization, training, deep, models, algorithms, adaptive, learning, rates, choosing, right, optimization, algorithm] section discussed series related algorithms seek address challenge optimizing deep models adapting learning rate model parameter point natural question algorithm one choose unfortunately currently consensus point schaul presented valuable comparison large number optimization algorithms across wide range learning tasks results suggest family algorithms adaptive learning rates represented rmsprop adadelta performed fairly robustly single best algorithm emerged currently popular optimization algorithms actively use include sgd sgd momentum rmsprop rmsprop momentum adadelta adam choice algorithm use point seems depend largely user familiarity algorithm ease hyperparameter tuning chapter optimization training deep models algorithm rmsprop algorithm nesterov momentum require global learning rate decay rate momentum coeﬃcient require initial parameter initial velocity initialize accumulation variable stopping criterion met sample minibatch examples training set corresponding targets compute interim update compute gradient accumulate gradient compute velocity update applied element wise apply update end 
[optimization, training, deep, models, approximate, second-order, methods] section discuss application second order methods training deep networks see earlier treatment subject lecun simplicity exposition objective function examine empirical risk data however methods discuss extend readily general objective functions instance include parameter regularization terms discussed chapter 
[optimization, training, deep, models, approximate, second-order, methods, newton’s, method] sec introduced second order gradient methods contrast ﬁrst order methods second order methods make use second derivatives improve optimization widely used second order method newton method describe newton method detail emphasis application neural network training newton method optimization scheme based using second order tay lor series expansion approximate near point ignoring derivatives chapter optimization training deep models algorithm adam algorithm require step size suggested default require exponential decay rates moment estimates suggested defaults respectively require small constant used numerical stabilization suggested default require initial parameters initialize moment variables initialize time step stopping criterion met sample minibatch examples training set corresponding targets compute gradient update biased ﬁrst moment estimate update biased second moment estimate correct bias ﬁrst moment correct bias second moment compute update operations applied element wise apply update end higher order hessian respect evaluated solve critical point function obtain newton parameter update rule thus locally quadratic function positive deﬁnite rescaling gradient newton method jumps directly minimum objective function convex quadratic higher order terms update iterated yielding training algorithm associated newton method given algorithm surfaces quadratic long hessian remains positive deﬁnite newton method applied iteratively implies two step chapter optimization training deep models algorithm newton method objective require initial parameter require training set examples stopping criterion met compute gradient compute hessian compute hessian inverse compute update apply update end iterative procedure first update compute inverse hessian updating quadratic approximation second update parameters according sec discussed newton method appropriate hessian positive deﬁnite deep learning surface objective function typically non convex many features saddle points problematic newton method eigenvalues hessian positive example near saddle point newton method actually cause updates move wrong direction situation avoided regularizing hessian common regularization strategies include adding constant along diagonal hessian regularized update becomes regularization strategy used approximations newton method levenberg marquardt algorithm levenberg marquardt works fairly well long negative eigenvalues hessian still relatively close zero cases extreme directions curvature value would suﬃciently large oﬀset negative eigenvalues however increases size hessian becomes dominated diagonal direction chosen newton method converges standard gradient divided strong negative curvature present may need large newton method would make smaller steps gradient descent properly chosen learning rate beyond challenges created certain features objective function saddle points application newton method training large neural networks limited signiﬁcant computational burden imposes chapter optimization training deep models number elements hessian squared number parameters parameters even small neural networks number parameters millions newton method would require inversion matrix computational complexity also since parameters change every update inverse hessian computed every training iteration consequence networks small number parameters practically trained via newton method remainder section discuss alternatives attempt gain advantages newton method side stepping computational hurdles 
[optimization, training, deep, models, approximate, second-order, methods, conjugate, gradients] conjugate gradients method eﬃciently avoid calculation inverse hessian iteratively descending conjugate directions inspiration approach follows careful study weakness method steepest descent see sec details line searches applied iteratively direction associated gradient fig illustrates method steepest descent applied quadratic bowl progresses rather ineﬀective back forth zig zag pattern happens line search direction given gradient guaranteed orthogonal previous line search direction let previous search direction minimum line search terminates directional derivative zero direction since gradient point deﬁnes current search direction contribution direction thus orthogonal relationship illustrated fig multiple iterations steepest descent demonstrated ﬁgure choice orthogonal directions descent preserve minimum along previous search directions gives rise zig zag pattern progress descending minimum current gradient direction must minimize objective previous gradient direction thus following gradient end line search sense undoing progress already made direction previous line search method conjugate gradients seeks address problem method conjugate gradients seek ﬁnd search direction conjugate previous line search direction undo progress made direction training iteration next search direction takes form chapter optimization training deep models figure method steepest descent applied quadratic cost surface method steepest descent involves jumping point lowest cost along line deﬁned gradient initial point step resolves problems seen using ﬁxed learning rate fig even optimal step size algorithm still makes back forth progress toward optimum deﬁnition minimum objective along given direction gradient ﬁnal point orthogonal direction coeﬃcient whose magnitude controls much direction add back current search direction two directions deﬁned conjugate straightforward way impose conjugacy would involve calculation eigenvectors choose would satisfy goal developing method computationally viable newton method large problems calculate conjugate directions without resorting calculations fortunately answer yes two popular methods computing fletcher reeves polak ribière chapter optimization training deep models quadratic surface conjugate directions ensure gradient along previous direction increase magnitude therefore stay minimum along previous directions consequence dimensional parameter space conjugate gradients requires line searches achieve minimum conjugate gradient algorithm given algorithm algorithm conjugate gradient method require initial parameters require training set examples initialize initialize initialize stopping criterion met initialize gradient compute gradient compute polak ribière nonlinear conjugate gradient optionally reset zero example multiple constant compute search direction perform line search ﬁnd argmin  truly quadratic cost function analytically solve rather explicitly searching apply update end nonlinear conjugate gradients far discussed method conjugate gradients applied quadratic objective functions course primary interest chapter explore optimization methods training neural networks related deep learning models corresponding objective function far quadratic perhaps surprisingly method conjugate gradients still applicable setting though modiﬁcation without assurance objective quadratic conjugate directions longer assured remain minimum objective previous directions result nonlinear conjugate gradients algorithm includes occasional resets method conjugate gradients restarted line search along unaltered gradient practitioners report reasonable results applications nonlinear conjugate chapter optimization training deep models gradients algorithm training neural networks though often beneﬁcial initialize optimization iterations stochastic gradient descent commencing nonlinear conjugate gradients also nonlinear conjugate gradients algorithm traditionally cast batch method minibatch versions used successfully training neural networks adaptations conjugate gradients speciﬁcally neural networks proposed earlier scaled conjugate gradients algorithm moller 
[optimization, training, deep, models, approximate, second-order, methods, bfgs] broyden fletcher goldfarb shanno bfgs algorithm attempts bring advantages newton method without computational burden respect bfgs similar however bfgs takes direct approach approximation newton update recall newton update given hessian respect evaluated primary computational diﬃculty applying newton update calculation inverse hessian approach adopted quasi newton methods bfgs algorithm prominent approximate inverse matrix iteratively reﬁned low rank updates become better approximation speciﬁcation derivation bfgs approximation given many textbooks optimization including luenberger inverse hessian approximation updated direction descent determined line search performed direction determine size step taken direction ﬁnal update parameters given like method conjugate gradients bfgs algorithm iterates series line searches direction incorporating second order information however unlike conjugate gradients success approach heavily dependent line search ﬁnding point close true minimum along line thus relative conjugate gradients bfgs advantage spend less time reﬁning line search hand bfgs algorithm must store inverse hessian matrix requires memory making bfgs chapter optimization training deep models impractical modern deep learning models typically millions parameters limited memory bfgs bfgs memory costs bfgs algorithm signiﬁcantly decreased avoiding storing complete inverse hessian approximation bfgs algorithm computes approximation using method bfgs algorithm beginning assumption identity matrix rather storing approximation one step next used exact line searches directions deﬁned bfgs mutually conjugate however unlike method conjugate gradients procedure remains well behaved minimum line search reached approximately bfgs strategy storage described generalized include information hessian storing vectors used update time step costs per step 
[optimization, training, deep, models, optimization, strategies, meta-algorithms] many optimization techniques exactly algorithms rather general templates specialized yield algorithms subroutines incorporated many diﬀerent algorithms 
[optimization, training, deep, models, optimization, strategies, meta-algorithms, batch, normalization] batch normalization one exciting recent ioﬀe szegedy innovations optimizing deep neural networks actually optimization algorithm instead method adaptive reparametrization motivated diﬃculty training deep models deep models involve composition several functions layers gradient tells update parameter assumption layers change practice update layers simultaneously make update unexpected results happen many functions composed together changed simultaneously using updates computed assumption functions remain constant simple example suppose deep neural network one unit per layer use activation function hidden layer provides weight used layer output layer output linear function input nonlinear function weights suppose cost function put gradient wish chapter optimization training deep models decrease slightly back propagation algorithm compute gradient consider happens make update ﬁrst order taylor series approximation predicts value decrease  wanted decrease ﬁrst order information available gradient suggests could set learning rate however actual update include second order third order eﬀects eﬀects order new value given    example one second order term arising update term might negligible small might exponentially large weights layers greater makes hard choose appropriate learning rate eﬀects update parameters one layer depends strongly layers second order optimization algorithms address issue computing update takes second order interactions account see deep networks even higher order interactions signiﬁcant even second order optimization algorithms expensive usually require numerous approximations prevent truly accounting signiﬁcant second order interactions building order optimization algorithm thus seems hopeless instead batch normalization provides elegant way reparametrizing almost deep network reparametrization signiﬁcantly reduces problem coordinating updates across many layers batch normalization applied input hidden layer network let minibatch activations layer normalize arranged design matrix activations example appearing row matrix normalize replace vector containing mean unit vector containing standard deviation unit arithmetic based broadcasting vector vector applied every row matrix within row arithmetic element wise normalized subtracting dividing rest network operates exactly way original network operated training time chapter optimization training deep models small positive value imposed avoid encountering undeﬁned gradient crucially back propagate operations computing mean standard deviation applying normalize means gradient never propose operation acts simply increase standard deviation mean normalization operations remove eﬀect action zero component gradient major innovation batch normalization approach previous approaches involved adding penalties cost function encourage units normalized activation statistics involved intervening renormalize unit statistics gradient descent step former approach usually resulted imperfect normalization latter usually resulted signiﬁcant wasted time learning algorithm repeatedly proposed changing mean variance normalization step repeatedly undid change batch normalization reparametrizes model make units always standardized deﬁnition deftly sidestepping problems test time may replaced running averages collected training time allows model evaluated single example without needing use deﬁnitions depend entire minibatch revisiting example see mostly resolve diﬃculties learning model normalizing suppose drawn unit gaussian also come gaussian transformation linear however longer zero mean unit variance applying batch normalization obtain normalized restores zero mean unit variance properties almost update lower layers remain unit gaussian output may learned simple linear function learning model simple parameters lower layers simply eﬀect cases output always renormalized unit gaussian corner cases lower layers eﬀect changing one lower layer weights make output become degenerate changing sign one lower weights ﬂip relationship situations rare without normalization nearly every update would extreme eﬀect statistics batch normalization thus made model signiﬁcantly easier learn example ease learning course came cost making lower layers useless linear example chapter optimization training deep models lower layers longer harmful eﬀect also longer beneﬁcial eﬀect normalized ﬁrst second order statistics linear network inﬂuence deep neural network nonlinear activation functions lower layers perform nonlinear transformations data remain useful batch normalization acts standardize mean variance unit order stabilize learning allows relationships units nonlinear statistics single unit change ﬁnal layer network able learn linear transformation may actually wish remove linear relationships units within layer indeed approach taken provided desjardins inspiration batch normalization unfortunately eliminating linear interactions much expensive standardizing mean standard deviation individual unit far batch normalization remains practical approach normalizing mean standard deviation unit reduce expressive power neural network containing unit order maintain expressive power network common replace batch hidden unit activations rather simply normalized variables learned parameters allow new variable mean standard deviation ﬁrst glance may seem useless set mean introduce parameter allows set back arbitrary value answer new parametrization represent family functions input old parametrization new parametrization diﬀerent learning dynamics old parametrization mean determined complicated interaction parameters layers new parametrization mean determined solely new parametrization much easier learn gradient descent neural network layers take form ﬁxed nonlinear activation function rectiﬁed linear transformation natural wonder whether apply batch normalization input transformed value recommend ioﬀe szegedy latter speciﬁcally replaced normalized version bias term omitted becomes redundant parameter applied batch normalization reparametrization input layer usually output nonlinear activation function rectiﬁed linear function previous layer statistics input thus chapter optimization training deep models non gaussian less amenable standardization linear operations convolutional networks described chapter important apply normalizing every spatial location within feature map statistics feature map remain regardless spatial location 
[optimization, training, deep, models, optimization, strategies, meta-algorithms, coordinate, descent] cases may possible solve optimization problem quickly breaking separate pieces minimize respect single variable minimize respect another variable repeatedly cycling variables guaranteed arrive local minimum practice known coordinate descent optimize one coordinate time generally block coordinate descent refers minimizing respect subset variables simultaneously term coordinate descent often used refer block coordinate descent well strictly individual coordinate descent coordinate descent makes sense diﬀerent variables optimization problem clearly separated groups play relatively isolated roles optimization respect one group variables signiﬁcantly eﬃcient optimization respect variables example consider cost function function describes learning problem called sparse coding goal ﬁnd weight matrix linearly decode matrix activation values reconstruct training set applications sparse coding also involve weight decay constraint norms columns order prevent pathological solution extremely small large function convex however divide inputs training algorithm two sets dictionary parameters code representations minimizing objective function respect either one sets variables convex problem block coordinate descent thus gives optimization strategy allows use eﬃcient convex optimization algorithms alternating optimizing ﬁxed optimizing ﬁxed coordinate descent good strategy value one variable strongly inﬂuences optimal value another variable function chapter optimization training deep models positive constant ﬁrst term encourages two variables similar value second term encourages near zero solution set zero newton method solve problem single step positive deﬁnite quadratic problem however small coordinate descent make slow progress ﬁrst term allow single variable changed value diﬀers signiﬁcantly current value variable 
[optimization, training, deep, models, optimization, strategies, meta-algorithms, polyak, averaging] polyak averaging polyak juditsky consists averaging together several points trajectory parameter space visited optimization algorithm iterations gradient descent visit points output polyak averaging algorithm problem classes gradient descent applied convex problems approach strong convergence guarantees applied neural networks justiﬁcation heuristic performs well practice basic idea optimization algorithm may leap back forth across valley several times without ever visiting point near bottom valley average locations either side close bottom valley though non convex problems path taken optimization trajectory complicated visit many diﬀerent regions including points parameter space distant past may separated current point large barriers cost function seem like useful behavior result applying polyak averaging non convex problems typical use exponentially decaying running average running average approach used numerous applications see szegedy recent example 
[optimization, training, deep, models, optimization, strategies, meta-algorithms, supervised, pretraining] sometimes directly training model solve speciﬁc task ambitious model complex hard optimize task diﬃcult sometimes eﬀective train simpler model solve task make model complex also eﬀective train model solve simpler task move confront ﬁnal task strategies involve chapter optimization training deep models training simple models simple tasks confronting challenge training desired model perform desired task collectively known pretraining greedy algorithms break problem many components solve optimal version component isolation unfortunately combining individually optimal components guaranteed yield optimal complete solution however greedy algorithms computationally much cheaper algorithms solve best joint solution quality greedy solution often acceptable optimal greedy algorithms may also followed ﬁne tuning stage joint optimization algorithm searches optimal solution full problem initializing joint optimization algorithm greedy solution greatly speed improve quality solution ﬁnds pretraining especially greedy pretraining algorithms ubiquitous deep learning section describe speciﬁcally pretraining algorithms break supervised learning problems simpler supervised learning problems approach known greedy supervised pretraining original version greedy supervised pretraining bengio stage consists supervised learning training task involving subset layers ﬁnal neural network example greedy supervised pretraining illustrated fig added hidden layer pretrained part shallow supervised mlp taking input output previously trained hidden layer instead pretraining one layer time simonyan zisserman pretrain deep convolutional network eleven weight layers use ﬁrst four last three layers network initialize even deeper networks nineteen layers weights middle layers new deep network initialized randomly new network jointly trained another option explored use previously outputs trained mlps well raw input inputs added stage would greedy supervised pretraining help hypothesis initially discussed helps provide better guidance bengio intermediate levels deep hierarchy general pretraining may help terms optimization terms generalization approach related supervised pretraining extends idea context transfer learning yosinski pretrain deep convolutional net layers weights set tasks subset imagenet object categories initialize size network ﬁrst layers ﬁrst net layers second network upper layers initialized randomly chapter optimization training deep models figure illustration one form greedy supervised pretraining bengio start training suﬃciently shallow architecture another drawing architecture keep input hidden layer original network discard hidden output layer send output ﬁrst hidden layer input another supervised single hidden layer mlp trained objective ﬁrst network thus adding second hidden layer repeated many layers desired another drawing result viewed feedforward network improve optimization jointly ﬁne tune layers either end stage process chapter optimization training deep models jointly trained perform diﬀerent set tasks another subset imagenet object categories fewer training examples ﬁrst set tasks approaches transfer learning neural networks discussed sec another related line work approach fitnets romero approach begins training network low enough depth great enough width number units per layer easy train network becomes teacher second network designated student network student much deeper thinner eleven nineteen layers would diﬃcult train sgd normal circumstances training student network made easier training student network predict output original task also predict value middle layer teacher network extra task provides set hints hidden layers used simplify optimization problem additional parameters introduced regress middle layer layer teacher network middle layer deeper student network however instead predicting ﬁnal classiﬁcation target objective predict middle hidden layer teacher network lower layers student networks thus two objectives help outputs student network accomplish task well predict intermediate layer teacher network although thin deep network appears diﬃcult train wide shallow network thin deep network may generalize better certainly lower computational cost thin enough far fewer parameters without hints hidden layer student network performs poorly experiments training test set hints middle layers may thus one tools help train neural networks otherwise seem diﬃcult train optimization techniques changes architecture may also solve problem 
[optimization, training, deep, models, optimization, strategies, meta-algorithms, designing, models, aid, optimization] improve optimization best strategy always improve optimization algorithm instead many improvements optimization deep models come designing models easier optimize principle could use activation functions increase decrease jagged non monotonic patterns however would make optimization extremely diﬃcult practice important choose model family easy optimize use powerful optimization algorithm advances neural network learning past years chapter optimization training deep models obtained changing model family rather changing optimization procedure stochastic gradient descent momentum used train neural networks remains use modern state art neural network applications speciﬁcally modern neural networks reﬂect design choice use linear trans formations layers activation functions diﬀerentiable almost everywhere signiﬁcant slope large portions domain par ticular model innovations like lstm rectiﬁed linear units maxout units moved toward using linear functions previous models like deep networks based sigmoidal units models nice properties make optimization easier gradient ﬂows many layers provided jacobian linear transformation reasonable singular values moreover linear functions consistently increase single direction even model output far correct clear simply computing gradient direction output move reduce loss function words modern neural nets designed local gradient information corresponds reasonably well moving toward distant solution model design strategies help make optimization easier example linear paths skip connections layers reduce length shortest path lower layer parameters output thus mitigate vanishing gradient problem srivastava related idea skip connections adding extra copies output attached intermediate hidden layers network googlenet szegedy deeply supervised nets auxiliary heads trained lee perform task primary output top network order ensure lower layers receive large gradient training complete auxiliary heads may discarded alternative pretraining strategies introduced previous section way one train jointly layers single phase change architecture intermediate layers especially lower ones get hints via shorter path hints provide error signal lower layers 
[optimization, training, deep, models, optimization, strategies, meta-algorithms, continuation, methods, curriculum, learning] argued sec many challenges optimization arise global structure cost function cannot resolved merely making better estimates local update directions predominant strategy overcoming problem attempt initialize parameters region connected solution short path parameter space local descent chapter optimization training deep models discover continuation methods family strategies make optimization easier choosing initial points ensure local optimization spends time well behaved regions space idea behind continuation methods construct series objective functions parameters order minimize cost function construct new cost functions cost functions designed increasingly diﬃcult fairly easy minimize diﬃcult true cost function motivating entire process say easier mean well behaved space random initialization likely land region local descent minimize cost function successfully region larger series cost functions designed solution one good initial point next thus begin solving easy problem reﬁne solution solve incrementally harder problems arrive solution true underlying problem traditional continuation methods predating use continuation methods neural network training usually based smoothing objective function see example method review related methods continuation methods also closely related simulated annealing adds noise parameters kirkpatrick continuation methods extremely successful recent years see mobahi fisher overview recent literature especially applications continuation methods traditionally mostly designed goal overcoming challenge local minima speciﬁcally designed reach global minimum despite presence many local minima continuation methods would construct easier cost functions blurring original cost function blurring operation done approximating via sampling intuition approach non convex functions become approximately convex blurred many cases blurring preserves enough information location global minimum ﬁnd global minimum solving progressively less blurred versions problem approach break three diﬀerent ways first might successfully deﬁne series cost functions ﬁrst convex optimum tracks one function next arriving global minimum might require many incremental cost functions cost entire procedure remains high hard optimization problems remain hard even continuation methods chapter optimization training deep models applicable two ways continuation methods fail correspond method applicable first function might become convex matter much blurred consider example function second function may become convex result blurring minimum blurred function may track local rather global minimum original cost function though continuation methods mostly originally designed deal problem local minima local minima longer believed primary problem neural network optimization fortunately continuation methods still help easier objective functions introduced continuation method eliminate ﬂat regions decrease variance gradient estimates improve conditioning hessian matrix anything else either make local updates easier compute improve correspondence local update directions progress toward global solution bengio observed approach called curriculum learning shaping interpreted continuation method curriculum learning based idea planning learning process begin learning simple concepts progress learning complex concepts depend simpler concepts basic strategy previously known accelerate progress animal training skinner peterson krueger dayan machine learning solomonoﬀ elman sanger bengio justiﬁed strategy continuation method earlier made easier increasing inﬂuence simpler examples either assigning contributions cost function larger coeﬃcients sampling frequently experimentally demonstrated better results could obtained following curriculum large scale neural language modeling task curriculum learning successful wide range natural language spitkovsky collobert mikolov honavar computer vision kumar lee grauman supancic ramanan tasks curriculum learning also veriﬁed consistent way humans teach teachers start showing easier khan prototypical examples help learner reﬁne decision surface less obvious cases curriculum based strategies eﬀective teaching humans strategies based uniform sampling examples also increase eﬀectiveness teaching strategies basu christensen another important contribution research curriculum learning arose context training recurrent neural networks capture long term dependencies chapter optimization training deep models zaremba sutskever found much better results obtained stochastic curriculum random mix easy diﬃcult examples always presented learner average proportion diﬃcult examples longer term dependencies gradually increased deterministic curriculum improvement baseline ordinary training full training set observed described basic family neural network models regularize optimize chapters ahead turn specializations neural network family allow neural networks scale large sizes process input data special structure optimization methods discussed chapter often directly applicable specialized architectures little modiﬁcation 
[convolutional, networks] convolutional networks convolutional neural networks also known lecun specialized kind neural network processing data cnns known grid like topology examples include time series data thought grid taking samples regular time intervals image data thought grid pixels convolutional networks tremendously successful practical applications name convolutional neural network indicates network employs mathematical operation called convolution convolution specialized kind linear operation convolutional networks simply neural networks use convolution place general matrix multiplication least one layers chapter ﬁrst describe convolution next explain motivation behind using convolution neural network describe operation called pooling almost convolutional networks employ usually operation used convolutional neural network correspond precisely deﬁnition convolution used ﬁelds engineering pure mathematics describe several variants convolution function widely used practice neural networks also show convolution may applied many kinds data diﬀerent numbers dimensions discuss means making convolution eﬃcient convolutional networks stand example neuroscientiﬁc principles inﬂuencing deep learning discuss neuroscientiﬁc principles conclude comments role convolutional networks played history deep learning one topic chapter address choose architecture convolutional network goal chapter describe kinds tools convolutional networks provide chapter chapter convolutional networks describes general guidelines choosing tools use circumstances research convolutional network architectures proceeds rapidly new best architecture given benchmark announced every weeks months rendering impractical describe best architecture print however best architectures consistently composed building blocks described 
[convolutional, networks, convolution, operation] general form convolution operation two functions real valued argument motivate deﬁnition convolution start examples two functions might use suppose tracking location spaceship laser sensor laser sensor provides single output position spaceship time real valued get diﬀerent reading laser sensor instant time suppose laser sensor somewhat noisy obtain less noisy estimate spaceship position would like average together several measurements course recent measurements relevant want weighted average gives weight recent measurements weighting function age measurement apply weighted average operation every moment obtain new function providing smoothed estimate position spaceship operation called convolution convolution operation typically denoted asterisk example needs valid probability density function output weighted average also needs negative arguments look future presumably beyond capabilities limitations particular example though general convolution deﬁned functions integral deﬁned may used purposes besides taking weighted averages convolutional network terminology ﬁrst argument example function convolution often referred second input chapter convolutional networks argument example function output sometimes kernel referred feature map example idea laser sensor provide measurements every instant time realistic usually work data computer time discretized sensor provide data regular intervals example might realistic assume laser provides measurement per second time index take integer values assume deﬁned integer deﬁne discrete convolution machine learning applications input usually multidimensional array data kernel usually multidimensional array parameters adapted learning algorithm refer multidimensional arrays tensors element input kernel must explicitly stored separately usually assume functions zero everywhere ﬁnite set points store values means practice implement inﬁnite summation summation ﬁnite number array elements finally often use convolutions one axis time example use two dimensional image input probably also want use two dimensional kernel convolution commutative meaning equivalently write usually latter formula straightforward implement machine learning library less variation range valid values commutative property convolution arises ﬂipped kernel relative input sense increases index input increases index kernel decreases reason ﬂip kernel obtain commutative property commutative property chapter convolutional networks useful writing proofs usually important property neural network implementation instead many neural network libraries implement related function called cross correlation convolution without ﬂipping kernel many machine learning libraries implement cross correlation call convolution text follow convention calling operations convolution specify whether mean ﬂip kernel contexts kernel ﬂipping relevant context machine learning learning algorithm learn appropriate values kernel appropriate place algorithm based convolution kernel ﬂipping learn kernel ﬂipped relative kernel learned algorithm without ﬂipping also rare convolution used alone machine learning instead convolution used simultaneously functions combination functions commute regardless whether convolution operation ﬂips kernel see fig example convolution without kernel ﬂipping applied tensor discrete convolution viewed multiplication matrix however matrix several entries constrained equal entries example univariate discrete convolution row matrix constrained equal row shifted one element known toeplitz matrix two dimensions doubly block circulant matrix corresponds convolution addition constraints several elements equal convolution usually corresponds sparse matrix matrix whose entries mostly equal zero kernel usually much smaller input image neural network algorithm works matrix multiplication depend speciﬁc properties matrix structure work convolution without requiring changes neural network typical convolutional neural networks make use specializations order deal large inputs eﬃciently strictly necessary theoretical perspective chapter convolutional networks input kernel output figure example convolution without kernel ﬂipping case restrict output positions kernel lies entirely within image called valid convolution contexts draw boxes arrows indicate upper left element output tensor formed applying kernel corresponding upper left region input tensor chapter convolutional networks 
[convolutional, networks, motivation] convolution leverages three important ideas help improve machine learning system sparse interactions parameter sharing equivariant representa tions moreover convolution provides means working inputs variable size describe ideas turn traditional neural network layers use matrix multiplication matrix parameters separate parameter describing interaction input unit output unit means every output unit interacts every input unit convolutional networks however typically sparse interactions also referred sparse connectivity sparse weights accomplished making kernel smaller input example processing image input image might thousands millions pixels detect small meaningful features edges kernels occupy tens hundreds pixels means need store fewer parameters reduces memory requirements model improves statistical eﬃciency also means computing output requires fewer operations improvements eﬃciency usually quite large inputs outputs matrix multiplication requires parameters algorithms used practice runtime per example limit number connections output may sparsely connected approach requires parameters runtime many practical applications possible obtain good performance machine learning task keeping several orders magnitude smaller graphical demonstrations sparse connectivity see fig fig deep convolutional network units deeper layers may indirectly interact larger portion input shown fig allows network eﬃciently describe complicated interactions many variables constructing interactions simple building blocks describe sparse interactions parameter sharing refers using parameter one function model traditional neural net element weight matrix used exactly computing output layer multiplied one element input never revisited synonym parameter sharing one say network tied weights value weight applied one input tied value weight applied elsewhere convolutional neural net member kernel used every position input except perhaps boundary pixels depending design decisions regarding boundary parameter sharing used convolution operation means rather learning separate set parameters every location learn chapter convolutional networks figure sparse connectivity viewed highlight one input unit also highlight output units aﬀected unit top formed convolution kernel width three outputs aﬀected bottom formed matrix multiplication connectivity longer sparse outputs aﬀected chapter convolutional networks figure sparse connectivity viewed highlight one output unit also highlight input units aﬀect unit units known receptive ﬁeld top formed convolution kernel width three inputs aﬀect bottom formed matrix multiplication connectivity longer sparse inputs aﬀect figure receptive ﬁeld units deeper layers convolutional network larger receptive ﬁeld units shallow layers eﬀect increases network includes architectural features like strided convolution fig pooling sec means even though direct connections convolutional net sparse units deeper layers indirectly connected input image chapter convolutional networks figure parameter sharing black arrows indicate connections use particular parameter two diﬀerent models top black arrows indicate uses central element element kernel convolutional model due parameter sharing single parameter used input locations single black arrow indicates bottom use central element weight matrix fully connected model model parameter sharing parameter used one set aﬀect runtime forward propagation still reduce storage requirements model parameters recall usually several orders magnitude less since usually roughly size practically insigniﬁcant compared convolution thus dramatically eﬃcient dense matrix multiplication terms memory requirements statistical eﬃciency graphical depiction parameter sharing works see fig example ﬁrst two principles action fig shows sparse connectivity parameter sharing dramatically improve eﬃciency linear function detecting edges image case convolution particular form parameter sharing causes layer property called equivariance translation say function equivariant means input changes output changes way speciﬁcally function equivariant function case convolution let function translates input shifts convolution function equivariant example let function giving image brightness integer coordinates let function mapping one image function another image function chapter convolutional networks image function shifts every pixel one unit right apply transformation apply convolution result applied convolution applied transformation output processing time series data means convolution produces sort timeline shows diﬀerent features appear input move event later time input exact representation appear output later time similarly images convolution creates map certain features appear input move object input representation move amount output useful know function small number neighboring pixels useful applied multiple input locations example processing images useful detect edges ﬁrst layer convolutional network edges appear less everywhere image practical share parameters across entire image cases may wish share parameters across entire image example processing images cropped centered individual face probably want extract diﬀerent features diﬀerent locations part network processing top face needs look eyebrows part network processing bottom face needs look chin convolution naturally equivariant transformations changes scale rotation image mechanisms necessary handling kinds transformations finally kinds data cannot processed neural networks deﬁned matrix multiplication ﬁxed shape matrix convolution enables processing kinds data discuss sec 
[convolutional, networks, pooling] typical layer convolutional network consists three stages see fig ﬁrst stage layer performs several convolutions parallel produce set linear activations second stage linear activation run nonlinear activation function rectiﬁed linear activation function stage sometimes called detector stage third stage use pooling function modify output layer pooling function replaces output net certain location summary statistic nearby outputs example max pooling zhou chellappa operation reports maximum output within rectangular chapter convolutional networks figure eﬃciency edge detection image right formed taking pixel original image subtracting value neighboring pixel left shows strength vertically oriented edges input image useful operation object detection images pixels tall input image pixels wide output image pixels wide transformation described convolution kernel containing two elements requires ﬂoating point operations two multiplications one addition per output pixel compute using convolution describe transformation matrix multiplication would take eight billion entries matrix making convolution four billion times eﬃcient representing transformation straightforward matrix multiplication algorithm performs sixteen billion ﬂoating point operations making convolution roughly times eﬃcient computationally course entries matrix would zero stored nonzero entries matrix matrix multiplication convolution would require number ﬂoating point operations compute matrix would still need contain entries convolution extremely eﬃcient way describing transformations apply linear transformation small local region across entire input photo credit paula goodfellow chapter convolutional networks convolutional layer input layer convolution stage transform detector stage nonlinearity rectiﬁed linear pooling stage next layer input layers convolution layer transform detector layer nonlinearity rectiﬁed linear pooling layer next layer complex layer terminology simple layer terminology figure components typical convolutional neural network layer two commonly used sets terminology describing layers left terminology convolutional net viewed small number relatively complex layers layer many stages terminology one one mapping kernel tensors network layers book generally use terminology right terminology convolutional net viewed larger number simple layers every step processing regarded layer right means every layer parameters chapter convolutional networks neighborhood popular pooling functions include average rectangular neighborhood norm rectangular neighborhood weighted average based distance central pixel cases pooling helps make representation become approximately invariant small translations input invariance translation means translate input small amount values pooled outputs change see fig example works invariance local translation useful property care whether feature present exactly example determining whether image contains face need know location eyes pixel perfect accuracy need know eye left side face eye right side face contexts important preserve location feature example want ﬁnd corner deﬁned two edges meeting speciﬁc orientation need preserve location edges well enough test whether meet use pooling viewed adding inﬁnitely strong prior function layer learns must invariant small translations assumption correct greatly improve statistical eﬃciency network pooling spatial regions produces invariance translation pool outputs separately parametrized convolutions features learn transformations become invariant see fig pooling summarizes responses whole neighborhood possible use fewer pooling units detector units reporting summary statistics pooling regions spaced pixels apart rather pixel apart see fig example improves computational eﬃciency network next layer roughly times fewer inputs process number parameters next layer function input size next layer fully connected based matrix multiplication reduction input size also result improved statistical eﬃciency reduced memory requirements storing parameters many tasks pooling essential handling inputs varying size example want classify images variable size input classiﬁcation layer must ﬁxed size usually accomplished varying size oﬀset pooling regions classiﬁcation layer always receives number summary statistics regardless input size example ﬁnal pooling layer network may deﬁned output four sets summary statistics one quadrant image regardless image size theoretical work gives guidance kinds pooling one chapter convolutional networks detector stage pooling stage pooling stage detector stage figure max pooling introduces invariance top view middle output convolutional layer bottom row shows outputs nonlinearity top row shows outputs max pooling stride one pixel pooling regions pooling region width three pixels view network bottom input shifted right one pixel every value bottom row changed half values top row changed max pooling units sensitive maximum value neighborhood exact location chapter convolutional networks large response pooling unit large response pooling unit large response detector unit large response detector unit figure example learned invariances pooling unit pools multiple features learned separate parameters learn invariant transformations input show set three learned ﬁlters max pooling unit learn become invariant rotation three ﬁlters intended detect hand written ﬁlter attempts match slightly diﬀerent orientation appears input corresponding ﬁlter match cause large activation detector unit max pooling unit large activation regardless pooling unit activated show network processes two diﬀerent inputs resulting two diﬀerent detector units activated eﬀect pooling unit roughly either way principle leveraged maxout networks goodfellow convolutional networks max pooling spatial positions naturally invariant translation multi channel approach necessary learning transformations figure pooling downsampling use max pooling pool width three stride pools two reduces representation size factor two reduces computational statistical burden next layer note rightmost pooling region smaller size must included want ignore detector units chapter convolutional networks use various situations also possible dynamically boureau pool features together example running clustering algorithm locations interesting features approach yields boureau diﬀerent set pooling regions image another approach learn single pooling structure applied images jia pooling complicate kinds neural network architectures use top information boltzmann machines autoencoders issues discussed present types networks part iii pooling convolutional boltzmann machines presented sec inverse like operations pooling units needed diﬀerentiable networks covered sec examples complete convolutional network architectures classiﬁcation using convolution pooling shown fig 
[convolutional, networks, convolution, pooling, inﬁnitely, strong, prior] recall concept prior probability distribution sec probability distribution parameters model encodes beliefs models reasonable seen data priors considered weak strong depending concentrated probability density prior weak prior prior distribution high entropy gaussian distribution high variance prior allows data move parameters less freely strong prior low entropy gaussian distribution low variance prior plays active role determining parameters end inﬁnitely strong prior places zero probability parameters says parameter values completely forbidden regardless much support data gives values imagine convolutional net similar fully connected net inﬁnitely strong prior weights inﬁnitely strong prior says weights one hidden unit must identical weights neighbor shifted space prior also says weights must zero except small spatially contiguous receptive ﬁeld assigned hidden unit overall think use convolution introducing inﬁnitely strong prior probability distribution parameters layer prior says function layer learn contains local interactions chapter convolutional networks input image output convolution relu output pooling stride output convolution relu output pooling stride output reshape vector units output matrix multiply units output softmax class probabilities input image output convolution relu output pooling stride output convolution relu output pooling grid output reshape vector units output matrix multiply units output softmax class probabilities input image output convolution relu output pooling stride output convolution relu output convolution output average pooling output softmax class probabilities output pooling stride figure examples architectures classiﬁcation convolutional networks speciﬁc strides depths used ﬁgure advisable real use designed shallow order onto page real convolutional networks also often involve signiﬁcant amounts branching unlike chain structures used simplicity left convolutional network processes ﬁxed image size alternating convolution pooling layers tensor convolutional feature map reshaped ﬂatten spatial dimensions rest network ordinary feedforward network classiﬁer described chapter center convolutional network processes variable sized image still maintains fully connected section network uses pooling operation variably sized pools ﬁxed number pools order provide ﬁxed size vector units fully connected portion network convolutional network right fully connected weight layer instead last convolutional layer outputs one feature map per class model presumably learns map likely class occur spatial location averaging feature map single value provides argument softmax classiﬁer top chapter convolutional networks equivariant translation likewise use pooling inﬁnitely strong prior unit invariant small translations course implementing convolutional net fully connected net inﬁnitely strong prior would extremely computationally wasteful thinking convolutional net fully connected net inﬁnitely strong prior give insights convolutional nets work one key insight convolution pooling cause underﬁtting like prior convolution pooling useful assumptions made prior reasonably accurate task relies preserving precise spatial information using pooling features increase training error convolutional network architectures designed szegedy use pooling channels channels order get highly invariant features features underﬁt translation invariance prior incorrect task involves incorporating information distant locations input prior imposed convolution may inappropriate another key insight view compare convolu tional models convolutional models benchmarks statistical learning performance models use convolution would able learn even permuted pixels image many image datasets separate benchmarks models permutation invariant must discover concept topology via learning models knowledge spatial relationships hard coded designer 
[convolutional, networks, variants, basic, convolution, function] discussing convolution context neural networks usually refer exactly standard discrete convolution operation usually understood mathematical literature functions used practice diﬀer slightly describe diﬀerences detail highlight useful properties functions used neural networks first refer convolution context neural networks usually actually mean operation consists many applications convolution parallel convolution single kernel extract one kind feature albeit many spatial locations usually want layer network extract many kinds features many locations additionally input usually grid real values rather chapter convolutional networks grid vector valued observations example color image red green blue intensity pixel multilayer convolutional network input second layer output ﬁrst layer usually output many diﬀerent convolutions position working images usually think input output convolution tensors one index diﬀerent channels two indices spatial coordinates channel software implementations usually work batch mode actually use tensors fourth axis indexing diﬀerent examples batch omit batch axis description simplicity convolutional networks usually use multi channel convolution linear operations based guaranteed commutative even kernel ﬂipping used multi channel operations commutative operation number output channels input channels assume kernel tensor element giving connection strength unit channel output unit channel input oﬀset rows columns output unit input unit assume input consists observed data element giving value input unit within channel row column assume output consists format produced convolving across without ﬂipping summation values tensor indexing operations inside summation valid linear algebra notation index arrays using ﬁrst entry necessitates formula programming languages python index starting rendering expression even simpler may want skip positions kernel order reduce computational cost expense extracting features ﬁnely think downsampling output full convolution function want sample every pixels direction output deﬁne downsampled convolution function refer downsampled convolution also possible stride deﬁne separate stride direction motion see fig illustration chapter convolutional networks strided convolution downsampling convolution figure convolution stride example use stride two top convolution stride length two implemented single operation bottom convolution stride greater one pixel mathematically equivalent convolution unit stride followed downsampling obviously two step approach involving downsampling computationally wasteful computes many values discarded chapter convolutional networks one essential feature convolutional network implementation ability implicitly zero pad input order make wider without feature width representation shrinks one pixel less kernel width layer zero padding input allows control kernel width size output independently without zero padding forced choose shrinking spatial extent network rapidly using small kernels scenarios signiﬁcantly limit expressive power network see fig example three special cases zero padding setting worth mentioning one extreme case zero padding used whatsoever convolution kernel allowed visit positions entire kernel contained entirely within image matlab terminology called convolution valid case pixels output function number pixels input behavior output pixel somewhat regular however size output shrinks layer input image width kernel width output width rate shrinkage dramatic kernels used large since shrinkage greater limits number convolutional layers included network layers added spatial dimension network eventually drop point additional layers cannot meaningfully considered convolutional another special case zero padding setting enough zero padding added keep size output equal size input matlab calls convolution case network contain many convolutional layers available hardware support since operation convolution modify architectural possibilities available next layer however input pixels near border inﬂuence fewer output pixels input pixels near center make border pixels somewhat underrepresented model motivates extreme case matlab refers full convolution enough zeroes added every pixel visited times direction resulting output image width case output pixels near border function fewer pixels output pixels near center make diﬃcult learn single kernel performs well positions convolutional feature map usually optimal amount zero padding terms test set classiﬁcation accuracy lies somewhere valid convolution cases actually want use convolution rather locally connected layers case adjacency matrix lecun graph mlp every connection weight speciﬁed chapter convolutional networks 
[convolutional, networks, variants, basic, convolution, function] figure eﬀect zero padding network size consider convolutional network kernel width six every layer example use pooling convolution operation shrinks network size top convolutional network use implicit zero padding causes representation shrink ﬁve pixels layer starting input sixteen pixels able three convolutional layers last layer ever move kernel arguably two layers truly convolutional rate shrinking mitigated using smaller kernels smaller kernels less expressive shrinking inevitable kind architecture adding ﬁve implicit zeroes bottom layer prevent representation shrinking depth allows make arbitrarily deep convolutional network chapter convolutional networks tensor indices respectively output channel output row output column input channel row oﬀset within input column oﬀset within input linear part locally connected layer given sometimes also called unshared convolution similar operation discrete convolution small kernel without sharing parameters across locations fig compares local connections convolution full connections locally connected layers useful know feature function small part space reason think feature occur across space example want tell image picture face need look mouth bottom half image also useful make versions convolution locally connected layers connectivity restricted example constrain output channel function subset input channels common way make ﬁrst output channels connect ﬁrst input channels second output channels connect second input channels see fig example modeling interactions channels allows network fewer parameters order reduce memory consumption increase statistical eﬃciency also reduces amount computation needed perform forward back propagation accomplishes goals without reducing number hidden units tiled convolution oﬀers compromise gregor lecun convolutional layer locally connected layer rather learning separate set weights spatial location learn set kernels every rotate move space means immediately neighboring locations diﬀerent ﬁlters like locally connected layer memory requirements storing parameters increase factor size set kernels rather size entire output feature map see fig comparison locally connected layers tiled convolution standard convolution deﬁne tiled convolution algebraically let tensor two dimensions correspond diﬀerent locations output map rather separate index location output map output locations cycle set diﬀerent choices kernel stack direction equal chapter convolutional networks figure comparison local connections convolution full connections top locally connected layer patch size two pixels edge labeled unique letter show edge associated weight parameter center convolutional layer kernel width two pixels model exactly connectivity locally connected layer diﬀerence lies units interact parameters shared locally connected layer parameter sharing convolutional layer uses two weights repeatedly across entire input indicated repetition letters labeling edge bottom fully connected layer resembles locally connected layer sense edge parameter many label explicitly letters diagram however restricted connectivity locally connected layer chapter convolutional networks input tensor output tensor spatial coordinates figure convolutional network ﬁrst two output channels connected ﬁrst two input channels second two output channels connected second two input channels chapter convolutional networks figure comparison locally connected layers tiled convolution standard convolution three sets connections units size kernel used diagram illustrates use kernel two pixels wide diﬀerences methods lies share parameters top locally connected layer sharing indicate connection weight labeling connection unique letter tiled convolution set center diﬀerent kernels illustrate case one kernels edges labeled edges labeled time move one pixel right output move using diﬀerent kernel means like locally connected layer neighboring units output diﬀerent parameters unlike locally connected layer gone available kernels cycle back ﬁrst kernel two output units separated multiple steps share parameters traditional convolution equivalent tiled bottom convolution one kernel applied everywhere indicated diagram using kernel weights labeled everywhere chapter convolutional networks output width locally connected layer modulo operation etc straightforward generalize equation use diﬀerent tiling range dimension locally connected layers tiled convolutional layers interesting interaction max pooling detector units layers driven diﬀerent ﬁlters ﬁlters learn detect diﬀerent transformed versions underlying features max pooled units become invariant learned transformation see fig convolutional layers hard coded invariant speciﬁcally translation operations besides convolution usually necessary implement convolutional network perform learning one must able compute gradient respect kernel given gradient respect outputs simple cases operation performed using convolution operation many cases interest including case stride greater property recall convolution linear operation thus described matrix multiplication ﬁrst reshape input tensor ﬂat vector matrix involved function convolution kernel matrix sparse element kernel copied several elements matrix view helps derive operations needed implement convolutional network multiplication transpose matrix deﬁned convolution one operation operation needed back propagate error derivatives convolutional layer needed train convolutional networks one hidden layer operation also needed wish reconstruct visible units hidden units simard reconstructing visible units operation commonly used models described part book autoencoders rbms sparse coding iii transpose convolution necessary construct convolutional versions models like kernel gradient operation input gradient operation implemented using convolution cases general case requires third operation implemented care must taken coordinate transpose operation forward propagation size output transpose operation return depends zero padding policy stride chapter convolutional networks forward propagation operation well size forward propagation output map cases multiple sizes input forward propagation result size output map transpose operation must explicitly told size original input three operations convolution backprop output weights backprop output inputs suﬃcient compute gradients needed train depth feedforward convolutional network well train convolutional networks reconstruction functions based transpose convolution see full derivation equations goodfellow fully general multi dimensional multi example case give sense equations work present two dimensional single example version suppose want train convolutional network incorporates strided convolution kernel stack applied multi channel image stride deﬁned suppose want minimize loss function forward propagation need use output propagated rest network used compute cost function back propagation receive tensor train network need compute derivatives respect weights kernel use function layer bottom layer network need compute gradient respect order back propagate error farther use function autoencoder networks described chapter feedforward networks trained copy input output simple example pca algorithm copies input approximate reconstruction using function common general autoencoders use multiplication transpose weight matrix pca make models chapter convolutional networks convolutional use function perform transpose convolution operation suppose hidden units format deﬁne reconstruction order train autoencoder receive gradient respect tensor train decoder need obtain gradient respect given train encoder need obtain gradient respect given also possible diﬀerentiate using operations needed back propagation algorithm standard network architectures generally use linear operation order transform inputs outputs convolutional layer generally also add bias term output applying nonlinearity raises question share parameters among biases locally connected layers natural give unit bias tiled convolution natural share biases tiling pattern kernels convolutional layers typical one bias per channel output share across locations within convolution map however input known ﬁxed size also possible learn separate bias location output map separating biases may slightly reduce statistical eﬃciency model also allows model correct diﬀerences image statistics diﬀerent locations example using implicit zero padding detector units edge image receive less total input may need larger biases 
[convolutional, networks, structured, outputs] convolutional networks used output high dimensional structured object rather predicting class label classiﬁcation task real value regression task typically object tensor emitted standard convolutional layer example model might emit tensor probability pixel input network belongs class allows model label every pixel image draw precise masks follow outlines individual objects one issue often comes output plane smaller input plane shown fig kinds architectures typically used classiﬁcation single object image greatest reduction spatial dimensions network comes using pooling layers large stride chapter convolutional networks figure example recurrent convolutional network pixel labeling input image tensor axes corresponding image rows image columns channels red green blue goal output tensor labels probability distribution labels pixel tensor axes corresponding image rows image columns diﬀerent classes rather outputting single shot recurrent network iteratively reﬁnes estimate using previous estimate input creating new estimate parameters used updated estimate estimate reﬁned many times wish tensor convolution kernels used step compute hidden representation given input image kernel tensor used produce estimate labels given hidden values ﬁrst step kernels convolved provide input hidden layer ﬁrst time step term replaced zero parameters used step example recurrent network described chapter order produce output map similar size input one avoid pooling altogether another strategy simply emit lower resolution jain grid labels finally principle one could pinheiro collobert use pooling operator unit stride one strategy pixel wise labeling images produce initial guess image labels reﬁne initial guess using interactions neighboring pixels repeating reﬁnement step several times corresponds using convolutions stage sharing weights last layers deep net makes sequence computations jain performed successive convolutional layers weights shared across layers particular kind recurrent network fig pinheiro collobert shows architecture recurrent convolutional network prediction pixel made various methods used process predictions order obtain segmentation image regions briggman turaga farabet chapter convolutional networks general idea assume large groups contiguous pixels tend associated label graphical models describe probabilistic relationships neighboring pixels alternatively convolutional network trained maximize approximation graphical model training objective ning thompson 
[convolutional, networks, data, types] data used convolutional network usually consists several channels channel observation diﬀerent quantity point space time see table examples data types diﬀerent dimensionalities number channels example convolutional networks applied video see chen far discussed case every example train test data spatial dimensions one advantage convolutional networks also process inputs varying spatial extents kinds input simply cannot represented traditional matrix multiplication based neural networks provides compelling reason use convolutional networks even computational cost overﬁtting signiﬁcant issues example consider collection images image diﬀerent width height unclear model inputs weight matrix ﬁxed size convolution straightforward apply kernel simply applied diﬀerent number times depending size input output convolution operation scales accordingly convolution may viewed matrix multiplication convolution kernel induces diﬀerent size doubly block circulant matrix size input sometimes output network allowed variable size well input example want assign class label pixel input case design work necessary cases network must produce ﬁxed size output example want assign single class label entire image case must make additional design steps like inserting pooling layer whose pooling regions scale size proportional size input order maintain ﬁxed number pooled outputs examples kind strategy shown fig note use convolution processing variable sized inputs makes sense inputs variable size contain varying amounts chapter convolutional networks single channel multi channel audio waveform axis convolve corresponds time discretize time measure amplitude waveform per time step skeleton animation data anima tions computer rendered characters generated alter ing pose skeleton time point time pose character described speciﬁcation angles joints charac ter skeleton channel data feed convolu tional model represents angle one axis one joint audio data prepro cessed fourier transform transform audio wave form tensor dif ferent rows corresponding dif ferent frequencies diﬀerent columns corresponding diﬀer ent points time using convolu tion time makes model equivariant shifts time ing convolution across fre quency axis makes model equivariant frequency melody played dif ferent octave produces representation diﬀerent height network output color image data one channel contains red pixels one green pixels one blue pixels convolution kernel moves horizontal vertical axes image conferring translation equivari ance directions volumetric data common source kind data med ical imaging technology scans color video data one axis corre sponds time one height video frame one width video frame table examples diﬀerent formats data used convolutional networks chapter convolutional networks observation kind thing diﬀerent lengths recordings time diﬀerent widths observations space etc convolution make sense input variable size optionally include diﬀerent kinds observations example processing college applications features consist grades standardized test scores every applicant took standardized test make sense convolve weights features corresponding grades features corresponding test scores 
[convolutional, networks, eﬃcient, convolution, algorithms] modern convolutional network applications often involve networks containing one million units powerful implementations exploiting parallel computation resources discussed sec essential however many cases also possible speed convolution selecting appropriate convolution algorithm convolution equivalent converting input kernel frequency domain using fourier transform performing point wise multiplication two signals converting back time domain using inverse fourier transform problem sizes faster naive implementation discrete convolution dimensional kernel expressed outer product vectors one vector per dimension kernel called separable kernel separable naive convolution ineﬃcient equivalent compose one dimensional convolutions vectors composed approach signiﬁcantly faster performing one dimensional convolution outer product kernel also takes fewer parameters represent vectors kernel elements wide dimension naive multidimensional convolution requires runtime parameter storage space separable convolution requires runtime parameter storage space course every convolution represented way devising faster ways performing convolution approximate convolution without harming accuracy model active area research even tech niques improve eﬃciency forward propagation useful commercial setting typical devote resources deployment network training chapter convolutional networks 
[convolutional, networks, random, unsupervised, features] typically expensive part convolutional network training learning features output layer usually relatively inexpensive due small number features provided input layer passing several layers pooling performing supervised training gradient descent every gradient step requires complete run forward propagation backward propagation entire network one way reduce cost convolutional network training use features trained supervised fashion three basic strategies obtaining convolution kernels without supervised training one simply initialize randomly another design hand example setting kernel detect edges certain orientation scale finally one learn kernels unsupervised criterion example apply coates means clustering small image patches use learned centroid convolution kernel part iii describes many unsupervised learning approaches learning features unsupervised criterion allows determined separately classiﬁer layer top architecture one extract features entire training set essentially constructing new training set last layer learning last layer typically convex optimization problem assuming last layer something like logistic regression svm random ﬁlters often work surprisingly well convolutional networks jarrett saxe pinto cox pinto saxe showed layers consisting convolution following pooling naturally become frequency selective translation invariant assigned random weights argue provides inexpensive way choose architecture convolutional network ﬁrst evaluate performance several convolutional network architectures training last layer take best architectures train entire architecture using expensive approach intermediate approach learn features using methods require full forward back propagation every gradient step multilayer perceptrons use greedy layer wise pretraining train ﬁrst layer isolation extract features ﬁrst layer train second layer isolation given features chapter described perform supervised greedy layer wise pretraining part extends iii greedy layer wise pretraining using unsupervised criterion layer canonical example greedy layer wise pretraining convolutional model convolutional deep belief network convolutional networks oﬀer lee chapter convolutional networks opportunity take pretraining strategy one step possible multilayer perceptrons instead training entire convolutional layer time train model small patch coates means use parameters patch based model deﬁne kernels convolutional layer means possible use unsupervised learning train convolutional network without ever using convolution training process using approach train large models incur high computational cost inference time ranzato jarrett kavukcuoglu coates approach popular roughly labeled datasets small computational power limited today convolutional networks trained purely supervised fashion using full forward back propagation entire network training iteration approaches unsupervised pretraining remains diﬃcult tease apart cause beneﬁts seen approach unsupervised pretraining may oﬀer regularization relative supervised training may simply allow train much larger architectures due reduced computational cost learning rule 
[convolutional, networks, neuroscientiﬁc, basis, convolutional, net-, works] convolutional networks perhaps greatest success story biologically inspired artiﬁcial intelligence though convolutional networks guided many ﬁelds key design principles neural networks drawn neuroscience history convolutional networks begins neuroscientiﬁc experiments long relevant computational models developed neurophysiologists david hubel torsten wiesel collaborated several years determine many basic facts mammalian vision system works hubel wiesel accomplishments eventually recognized nobel prize ﬁndings greatest inﬂuence contemporary deep learning models based recording activity individual neurons cats observed neurons cat brain responded images projected precise locations screen front cat great discovery neurons early visual system responded strongly speciﬁc patterns light precisely oriented bars responded hardly patterns chapter convolutional networks work helped characterize many aspects brain function beyond scope book point view deep learning focus simpliﬁed cartoon view brain function simpliﬁed view focus part brain called also known primary visual cortex ﬁrst area brain begins perform signiﬁcantly advanced processing visual input cartoon view images formed light arriving eye stimulating retina light sensitive tissue back eye neurons retina perform simple preprocessing image substantially alter way represented image passes optic nerve brain region called lateral geniculate nucleus main role far concerned anatomical regions primarily carry signal eye located back head convolutional network layer designed capture three properties arranged spatial map actually two dimensional structure mirroring structure image retina example light arriving lower half retina aﬀects corresponding half convolutional networks capture property features deﬁned terms two dimensional maps contains many simple cells simple cell activity extent characterized linear function image small spatially localized receptive ﬁeld detector units convolutional network designed emulate properties simple cells also contains many complex cells cells respond features similar detected simple cells complex cells invariant small shifts position feature inspires pooling units convolutional networks complex cells also invariant changes lighting cannot captured simply pooling spatial locations invariances inspired cross channel pooling strategies convolutional networks maxout units goodfellow though know generally believed basic principles apply areas visual system cartoon view visual system basic strategy detection followed pooling repeatedly applied move deeper brain pass multiple anatomical layers brain eventually ﬁnd cells respond speciﬁc concept invariant many transformations input cells chapter convolutional networks nicknamed grandmother cells idea person could neuron activates seeing image grandmother regardless whether appears left right side image whether image close face zoomed shot entire body whether brightly lit shadow etc grandmother cells shown actually exist human brain region called medial temporal lobe researchers quiroga tested whether individual neurons would respond photos famous individuals found come called halle berry neuron individual neuron activated concept halle berry neuron ﬁres person sees photo halle berry drawing halle berry even text containing words halle berry course nothing halle berry neurons responded presence bill clinton jennifer aniston etc medial temporal lobe neurons somewhat general modern convolutional networks would automatically generalize identifying person object reading name closest analog convolutional network last layer features brain area called inferotemporal cortex viewing object information ﬂows retina lgn onward happens within ﬁrst glimpsing object person allowed continue looking object time information begin ﬂow backwards brain uses top feedback update activations lower level brain areas however interrupt person gaze observe ﬁring rates result ﬁrst mostly feedforward activation proves similar convolutional network convolutional networks predict ﬁring rates also perform similarly time limited humans object recognition tasks dicarlo said many diﬀerences convolutional networks mammalian vision system diﬀerences well known computational neuroscientists outside scope book diﬀerences yet known many basic questions mammalian vision system works remain unanswered brief list human eye mostly low resolution except tiny patch called fovea fovea observes area size thumbnail held arms length though feel see entire scene high resolution illusion created subconscious part brain stitches together several glimpses small areas convolutional networks actually receive large full resolution photographs input human brain makes chapter convolutional networks several eye movements called saccades glimpse visually salient task relevant parts scene incorporating similar attention mechanisms deep learning models active research direction context deep learning attention mechanisms successful natural language processing described sec several visual models foveation mechanisms developed far become dominant approach larochelle hinton denil human visual system integrated many senses hearing factors like moods thoughts convolutional networks far purely visual human visual system much recognize objects able understand entire scenes including many objects relationships objects processes rich geometric information needed bodies interface world convolutional networks applied problems applications infancy even simple brain areas like heavily impacted feedback higher levels feedback explored extensively neural network models yet shown oﬀer compelling improvement feedforward ﬁring rates capture much information convolutional network features clear similar intermediate computations brain probably uses diﬀerent activation pooling functions individual neuron activation probably well characterized single linear ﬁlter response recent model involves multiple quadratic ﬁlters neuron indeed rust cartoon picture simple cells complex cells might create non existent distinction simple cells complex cells might kind cell parameters enabling continuum behaviors ranging call simple call complex also worth mentioning neuroscience told relatively little train convolutional networks model structures parameter sharing across multiple spatial locations date back early connectionist models vision models use modern marr poggio back propagation algorithm gradient descent example neocognitron fukushima incorporated model architecture design elements modern convolutional network relied layer wise unsupervised clustering algorithm chapter convolutional networks lang hinton introduced use back propagation train time delay neural networks tdnns use contemporary terminology tdnns one dimensional convolutional networks applied time series back propagation applied models inspired neuroscientiﬁc observation considered biologically implausible following success back propagation based training tdnns developed lecun modern convolutional network applying training algorithm convolution applied images far described simple cells roughly linear selective certain features complex cells nonlinear become invariant transformations simple cell features stacks layers alternate selectivity invariance yield grandmother cells speciﬁc phenomena yet described precisely individual cells detect deep nonlinear network diﬃcult understand function individual cells simple cells ﬁrst layer easier analyze responses driven linear function artiﬁcial neural network display image convolution kernel see corresponding channel convolutional layer responds biological neural network access weights instead put electrode neuron display several samples white noise images front animal retina record samples causes neuron activate linear model responses order obtain approximation neuron weights approach known reverse correlation ringach shapley reverse correlation shows cells weights described gabor functions gabor function describes weight point image think image function coordinates likewise think simple cell sampling image set locations deﬁned set coordinates set coordinates applying weights also function location point view response simple cell image given speciﬁcally takes form gabor function exp cos cos sin chapter convolutional networks sin cos parameters control properties gabor function fig shows examples gabor functions diﬀerent settings parameters parameters deﬁne coordinate system translate rotate form speciﬁcally simple cell respond image features centered point respond changes brightness move along line rotated radians horizontal viewed function function responds changes brightness move along axis two important factors one gaussian function cosine function gaussian factor exp seen gating term ensures simple cell respond values near zero words near center cell receptive ﬁeld scaling factor adjusts total magnitude simple cell response control quickly receptive ﬁeld falls cosine factor cos controls simple cell responds changing brightness along axis parameter controls frequency cosine controls phase oﬀset altogether cartoon view simple cells means simple cell responds speciﬁc spatial frequency brightness speciﬁc direction speciﬁc location simple cells excited wave brightness image phase weights occurs image bright weights positive dark weights negative simple cells inhibited wave brightness fully phase weights image dark weights positive bright weights negative cartoon view complex cell computes norm vector containing two simple cells responses important special case occurs parameters except set one quarter cycle phase case form quadrature pair complex cell deﬁned way responds gaussian reweighted image exp contains high amplitude sinusoidal wave frequency direction near regardless phase oﬀset wave words complex cell invariant small translations image direction negating chapter convolutional networks figure gabor functions variety parameter settings white indicates large positive weight black indicates large negative weight background gray corresponds zero weight left gabor functions diﬀerent values parameters control coordinate system gabor function grid assigned value proportional position grid chosen gabor ﬁlter sensitive direction radiating center grid two plots ﬁxed zero gabor functions center diﬀerent gaussian scale parameters gabor functions arranged increasing width decreasing move left right grid increasing height decreasing move top bottom two plots values ﬁxed image width gabor functions diﬀerent sinusoid parameters right move top bottom increases move left right increases two plots ﬁxed ﬁxed image width image replacing black white vice versa striking correspondences neuroscience machine learning come visually comparing features learned machine learning models employed showed olshausen field simple unsupervised learning algorithm sparse coding learns features receptive ﬁelds similar simple cells since found extremely wide variety statistical learning algorithms learn features gabor like functions applied natural images includes deep learning algorithms learn features ﬁrst layer fig shows examples many diﬀerent learning algorithms learn edge detectors diﬃcult conclude speciﬁc learning algorithm right model brain based features learns though certainly bad sign algorithm learn sort edge detector applied natural images features important part statistical structure natural images recovered many diﬀerent approaches statistical modeling see hyvärinen review ﬁeld natural image statistics chapter convolutional networks figure many machine learning algorithms learn features detect edges speciﬁc colors edges applied natural images feature detectors reminiscent gabor functions known present primary visual cortex left weights learned unsupervised learning algorithm spike slab sparse coding applied small image patches convolution kernels learned ﬁrst layer fully supervised right convolutional maxout network neighboring pairs ﬁlters drive maxout unit 
[convolutional, networks, convolutional, networks, history, deep, learning] convolutional networks played important role history deep learning key example successful application insights obtained studying brain machine learning applications also ﬁrst deep models perform well long arbitrary deep models considered viable convolutional networks also ﬁrst neural networks solve important commercial applications remain forefront commercial applications deep learning today example neural network research group atamp developed convolutional network reading checks end system deployed lecun nec reading checks later several ocr handwriting recognition systems based convolutional nets deployed microsoft see chapter details simard applications modern applications convolutional networks see lecun depth history convolutional networks convolutional networks also used win many contests current intensity commercial interest deep learning began krizhevsky imagenet object recognition challenge convolutional networks chapter convolutional networks used win machine learning computer vision contests less impact years earlier convolutional nets ﬁrst working deep networks trained back propagation entirely clear convolutional networks succeeded general back propagation networks considered failed may simply convolutional networks computationally eﬃcient fully connected networks easier run multiple experiments tune implementation hyperparameters larger networks also seem easier train modern hardware large fully connected networks appear perform reasonably many tasks even using datasets available activation functions popular times fully connected networks believed work well may primary barriers success neural networks psychological practitioners expect neural networks work make serious eﬀort use neural networks whatever case fortunate convolutional networks performed well decades ago many ways carried torch rest deep learning paved way acceptance neural networks general convolutional networks provide way specialize neural networks work data clear grid structured topology scale models large size approach successful two dimensional image topology process one dimensional sequential data turn next another powerful specialization neural networks framework recurrent neural networks 
[sequence, modeling, recurrent, recursive, nets] recurrent neural networks rnns family rumelhart neural networks processing sequential data much convolutional network neural network specialized processing grid values image recurrent neural network neural network specialized processing sequence values convolutional networks readily scale images large width height convolutional networks process images variable size recurrent networks scale much longer sequences would practical networks without sequence based specialization recurrent networks also process sequences variable length multi layer networks recurrent networks need take advan tage one early ideas found machine learning statistical models sharing parameters across diﬀerent parts model parameter sharing makes possible extend apply model examples diﬀerent forms diﬀerent lengths generalize across separate parameters value time index could generalize sequence lengths seen training share statistical strength across diﬀerent sequence lengths across diﬀerent positions time sharing particularly important speciﬁc piece information occur multiple positions within sequence example consider two sentences went nepal went nepal ask machine learning model read sentence extract year narrator went nepal would like recognize year relevant piece information whether appears sixth chapter sequence modeling recurrent recursive nets word second word sentence suppose trained feedforward network processes sentences ﬁxed length traditional fully connected feedforward network would separate parameters input feature would need learn rules language separately position sentence comparison recurrent neural network shares weights across several time steps related idea use convolution across temporal sequence convolutional approach basis time delay neural networks lang hinton waibel lang convolution operation allows network share parameters across time shallow output convolution sequence member output function small number neighboring members input idea parameter sharing manifests application convolution kernel time step recurrent networks share parameters diﬀerent way member output function previous members output member output produced using update rule applied previous outputs recurrent formulation results sharing parameters deep computational graph simplicity exposition refer rnns operating sequence contains vectors time step index ranging practice recurrent networks usually operate minibatches sequences diﬀerent sequence length member minibatch omitted minibatch indices simplify notation moreover time step index need literally refer passage time real world position sequence rnns may also applied two dimensions across spatial data images even applied data involving time network may connections backwards time provided entire sequence observed provided network chapter extends idea computational graph include cycles cycles represent inﬂuence present value variable value future time step computational graphs allow deﬁne recurrent neural networks describe many diﬀerent ways construct train use recurrent neural networks information recurrent neural networks available chapter refer reader textbook graves chapter sequence modeling recurrent recursive nets 
[sequence, modeling, recurrent, recursive, nets, unfolding, computational, graphs] computational graph way formalize structure set computations involved mapping inputs parameters outputs loss please refer sec general introduction section explain idea recursive recurrent computation computational unfolding graph repetitive structure typically corresponding chain events unfolding graph results sharing parameters across deep network structure example consider classical form dynamical system called state system recurrent deﬁnition time refers back deﬁnition time ﬁnite number time steps graph unfolded applying deﬁnition times example unfold time steps obtain unfolding equation repeatedly applying deﬁnition way yielded expression involve recurrence expression represented traditional directed acyclic computational graph unfolded computational graph illustrated fig figure classical dynamical system described illustrated unfolded computational graph node represents state time function maps state state parameters value used parametrize used time steps another example let consider dynamical system driven external signal chapter sequence modeling recurrent recursive nets see state contains information whole past sequence recurrent neural networks built many diﬀerent ways much almost function considered feedforward neural network essentially function involving recurrence considered recurrent neural network many recurrent neural networks use similar equation deﬁne values hidden units indicate state hidden units network rewrite using variable represent state illustrated fig typical rnns add extra architectural features output layers read information state make predictions recurrent network trained perform task requires predicting future past network typically learns use kind lossy summary task relevant aspects past sequence inputs summary general necessarily lossy since maps arbitrary length sequence ﬁxed length vector depending training criterion summary might selectively keep aspects past sequence precision aspects example rnn used statistical language modeling typically predict next word given previous words may necessary store information input sequence time rather enough information predict rest sentence demanding situation ask rich enough allow one approximately recover input sequence autoencoder frameworks chapter unfold figure recurrent network outputs recurrent network processes information input incorporating state passed forward time left circuit diagram black square indicates delay time step right network seen unfolded computational graph node associated one particular time instance drawn two diﬀerent ways one way draw rnn diagram containing one node every component might exist chapter sequence modeling recurrent recursive nets physical implementation model biological neural network view network deﬁnes circuit operates real time physical parts whose current state inﬂuence future state left fig throughout chapter use black square circuit diagram indicate interaction takes place delay time step state time state time way draw rnn unfolded computational graph component represented many diﬀerent variables one variable per time step representing state component point time variable time step drawn separate node computational graph right fig call unfolding operation maps circuit left side ﬁgure computational graph repeated pieces right side unfolded graph size depends sequence length represent unfolded recurrence steps function function takes whole past sequence input produces current state unfolded recurrent structure allows factorize repeated application function unfolding process thus introduces two major advantages regardless sequence length learned model always input size speciﬁed terms transition one state another state rather speciﬁed terms variable length history states possible use transition function parameters every time step two factors make possible learn single model operates time steps sequence lengths rather needing learn separate model possible time steps learning single shared model allows generalization sequence lengths appear training set allows model estimated far fewer training examples would required without parameter sharing recurrent graph unrolled graph uses recurrent graph succinct unfolded graph provides explicit description computations perform unfolded graph also helps illustrate idea chapter sequence modeling recurrent recursive nets information ﬂow forward time computing outputs losses backward time computing gradients explicitly showing path along information ﬂows 
[sequence, modeling, recurrent, recursive, nets, recurrent, neural, networks] armed graph unrolling parameter sharing ideas sec design wide variety recurrent neural networks unfold figure computational graph compute training loss recurrent network maps input sequence values corresponding sequence output values loss measures far corresponding training target using softmax outputs assume unnormalized log probabilities loss internally computes softmax compares target rnn input hidden connections parametrized weight matrix hidden hidden recurrent connections parametrized weight matrix hidden output connections parametrized weight matrix deﬁnes forward propagation model left rnn loss drawn recurrent connections right seen time unfolded computational graph node associated one particular time instance examples important design patterns recurrent neural networks include following recurrent networks produce output time step chapter sequence modeling recurrent recursive nets recurrent connections hidden units illustrated fig recurrent networks produce output time step recurrent connections output one time step hidden units next time step illustrated fig recurrent networks recurrent connections hidden units read entire sequence produce single output illustrated fig fig reasonably representative example return throughout chapter recurrent neural network fig universal sense function computable turing machine computed recurrent network ﬁnite size output read rnn number time steps asymptotically linear number time steps used turing machine asymptotically linear length input siegelmann sontag siegelmann siegelmann sontag hyotyniemi functions computable turing machine discrete results regard exact implementation function approximations rnn used turing machine takes binary sequence input outputs must discretized provide binary output possible compute functions setting using single speciﬁc rnn ﬁnite size siegelmann sontag use units input turing machine speciﬁcation function computed network simulates turing machine suﬃcient problems theoretical rnn used proof simulate unbounded stack representing activations weights rational numbers unbounded precision develop forward propagation equations rnn depicted fig ﬁgure specify choice activation function hidden units assume hyperbolic tangent activation function also ﬁgure specify exactly form output loss function take assume output discrete rnn used predict words characters natural way represent discrete variables regard output giving unnormalized log probabilities possible value discrete variable apply softmax operation post processing step obtain vector normalized probabilities output forward propagation begins speciﬁcation initial state time step apply following update equations chapter sequence modeling recurrent recursive nets unfold figure rnn whose recurrence feedback connection output hidden layer time step input hidden layer activations outputs targets loss left circuit diagram right unfolded computational graph rnn less powerful express smaller set functions family represented fig rnn fig choose put information wants past hidden representation transmit future rnn ﬁgure trained put speciﬁc output value information allowed send future direct connections going forward previous connected present indirectly via predictions used produce unless high dimensional rich usually lack important information past makes rnn ﬁgure less powerful may easier train time step trained isolation others allowing greater parallelization training described sec chapter sequence modeling recurrent recursive nets tanh softmax parameters bias vectors along weight matrices respectively input hidden hidden output hidden hidden connections example recurrent network maps input sequence output sequence length total loss given sequence values paired sequence values would sum losses time steps example negative log likelihood given log model model given reading entry model output vector computing gradient loss function respect parameters expensive operation gradient computation involves performing forward propagation pass moving left right illustration unrolled graph fig followed backward propagation pass moving right left graph runtime cannot reduced parallelization forward propagation graph inherently sequential time step may computed previous one states computed forward pass must stored reused backward pass memory cost also back propagation algorithm applied unrolled graph cost called back propagation time bptt discussed sec network recurrence hidden units thus powerful also expensive train alternative 
[sequence, modeling, recurrent, recursive, nets, recurrent, neural, networks, teacher, forcing, networks, output, recurrence] network recurrent connections output one time step hidden units next time step shown fig strictly less powerful lacks hidden hidden recurrent connections example cannot simulate universal turing machine network lacks hidden hidden chapter sequence modeling recurrent recursive nets recurrence requires output units capture information past network use predict future output units explicitly trained match training set targets unlikely capture necessary information past history input unless user knows describe full state system provides part training set targets advantage eliminating hidden hidden recurrence loss function based comparing prediction time training target time time steps decoupled training thus parallelized gradient step computed isolation need compute output previous time step ﬁrst training set provides ideal value output figure time unfolded recurrent neural network single output end sequence network used summarize sequence produce ﬁxed size representation used input processing might target right end depicted gradient output obtained back propagating downstream modules models recurrent connections outputs leading back model may trained teacher forcing teacher forcing procedure emerges maximum likelihood criterion training model receives ground truth output input time see examining sequence two time steps conditional maximum likelihood criterion log chapter sequence modeling recurrent recursive nets train time test time figure illustration teacher forcing teacher forcing training technique applicable rnns connections output hidden states next time step left correct output train time feed drawn train set input right model deployed true output generally known case approximate correct output model output feed output back model chapter sequence modeling recurrent recursive nets log log example see time model trained maximize conditional probability given sequence far previous value training set maximum likelihood thus speciﬁes training rather feeding model output back connections fed target values specifying correct output illustrated fig originally motivated teacher forcing allowing avoid back propagation time models lack hidden hidden connections teacher forcing may still applied models hidden hidden connections long connections output one time step values computed next time step however soon hidden units become function earlier time steps bptt algorithm necessary models may thus trained teacher forcing bptt disadvantage strict teacher forcing arises network going later used open loop mode network outputs samples output distribution fed back input case kind inputs network sees training could quite diﬀerent kind inputs see test time one way mitigate problem train teacher forced inputs free running inputs example predicting correct target number steps future unfolded recurrent output input paths way network learn take account input conditions generates free running mode seen training map state back towards one make network generate proper outputs steps another approach bengio mitigate gap inputs seen train time inputs seen test time randomly chooses use generated values actual data values input approach exploits curriculum learning strategy gradually use generated values input 
[sequence, modeling, recurrent, recursive, nets, recurrent, neural, networks, computing, gradient, recurrent, neural, network] computing gradient recurrent neural network straightforward one simply applies generalized back propagation algorithm sec unrolled computational graph specialized algorithms necessary use back propagation unrolled graph called back propagation time bptt algorithm gradients obtained back propagation may used general purpose gradient based techniques train rnn chapter sequence modeling recurrent recursive nets gain intuition bptt algorithm behaves provide example compute gradients bptt rnn equations nodes computational graph include parameters well sequence nodes indexed node need compute gradient recursively based gradient computed nodes follow graph start recursion nodes immediately preceding ﬁnal loss derivation assume outputs used argument softmax function obtain vector probabilities output also assume loss negative log likelihood true target given input far gradient outputs time step follows work way backwards starting end sequence ﬁnal time step descendent gradient simple iterate backwards time back propagate gradients time noting descendents gradient thus given diag diag indicates diagonal matrix containing elements jacobian hyperbolic tangent associated hidden unit time gradients internal nodes computational graph obtained obtain gradients parameter nodes parameters shared across many time steps must take care denoting calculus operations involving variables equations wish chapter sequence modeling recurrent recursive nets implement use bprop method sec computes contribution single edge computational graph gradient however operator used calculus takes account contribution value due edges computational graph resolve ambiguity introduce dummy variables deﬁned copies used time step may use denote contribution weights time step gradient using notation gradient remaining parameters given diag diag diag need compute gradient respect training parameters ancestors computational graph deﬁning loss 
[sequence, modeling, recurrent, recursive, nets, recurrent, neural, networks, recurrent, networks, directed, graphical, models] example recurrent network developed far losses cross entropies training targets outputs feedforward network principle possible use almost loss recurrent network loss chosen based task feedforward network usually wish interpret output rnn probability distribution chapter sequence modeling recurrent recursive nets usually use cross entropy associated distribution deﬁne loss mean squared error cross entropy loss associated output distribution unit gaussian example feedforward network use predictive log likelihood training objective train rnn estimate conditional distribution next sequence element given past inputs may mean maximize log likelihood log model includes connections output one time step next time step log decomposing joint probability sequence values series one step probabilistic predictions one way capture full joint distribution across whole sequence feed past values inputs condition next step prediction directed graphical model contains edges past current case outputs conditionally independent given sequence values feed actual values prediction actual observed generated values back network directed graphical model contains edges values past current value figure fully connected graphical model sequence every past observation may inﬂuence conditional distribution given previous values parametrizing graphical model directly according graph might ineﬃcient ever growing number inputs parameters element sequence rnns obtain full connectivity eﬃcient parametrization illustrated fig chapter sequence modeling recurrent recursive nets simple example let consider case rnn models sequence scalar random variables additional inputs input time step simply output time step rnn deﬁnes directed graphical model variables parametrize joint distribution observations using chain rule conditional probabilities right hand side bar empty course hence negative log likelihood set values according model log figure introducing state variable graphical model rnn even though deterministic function inputs helps see obtain eﬃcient parametrization based every stage sequence involves structure number inputs node share parameters stages edges graphical model indicate variables depend directly variables many graphical models aim achieve statistical computational eﬃciency omitting edges correspond strong interactions example common make markov assumption graphical model contain edges rather containing edges entire past history however cases believe past inputs inﬂuence next element sequence rnns useful believe distribution may depend value distant past way captured eﬀect chapter sequence modeling recurrent recursive nets one way interpret rnn graphical model view rnn deﬁning graphical model whose structure complete graph able represent direct dependencies pair values graphical model values complete graph structure shown fig complete graph interpretation rnn based ignoring hidden units marginalizing model interesting consider graphical model structure rnns results regarding hidden units random variables including hidden units graphical model reveals rnn provides eﬃcient parametrization joint distribution observations suppose represented arbitrary joint distribution discrete values tabular representation array containing separate entry possible assignment values value entry giving probability assignment occurring take diﬀerent values tabular representation would parameters comparison due parameter sharing number parameters rnn function sequence length number parameters rnn may adjusted control model capacity forced scale sequence length shows rnn parametrizes long term relationships variables eﬃciently using recurrent applications function parameters time step fig illustrates graphical model interpretation incorporating nodes graphical model decouples past future acting intermediate quantity variable distant past may inﬂuence variable via eﬀect structure graph shows model eﬃciently parametrized using conditional probability distributions time step variables observed probability joint assignment variables evaluated eﬃciently even eﬃcient parametrization graphical model operations remain computationally challenging example diﬃcult predict missing values middle sequence price recurrent networks pay reduced number parameters parameters may diﬃcult optimizing parameter sharing used recurrent networks relies assumption parameters used diﬀerent time steps equivalently assumption conditional probability distribution variables conditional distribution variables given parents deterministic perfectly legitimate though somewhat rare design graphical model deterministic hidden units chapter sequence modeling recurrent recursive nets time given variables time meaning relationship stationary previous time step next time step depend principle would possible use extra input time step let learner discover time dependence sharing much diﬀerent time steps would already much better using diﬀerent conditional probability distribution network would extrapolate faced new values complete view rnn graphical model must describe draw samples model main operation need perform simply sample conditional distribution time step however one additional complication rnn must mechanism determining length sequence achieved various ways case output symbol taken vocabulary one add special symbol corresponding end sequence schmidhuber symbol generated sampling process stops training set insert symbol extra member sequence immediately training example another option introduce extra bernoulli output model represents decision either continue generation halt generation time step approach general approach adding extra symbol vocabulary may applied rnn rather rnns output sequence symbols example may applied rnn emits sequence real numbers new output unit usually sigmoid unit trained cross entropy loss approach sigmoid trained maximize log probability correct prediction whether sequence ends continues time step another way determine sequence length add extra output model predicts integer model sample value sample steps worth data approach requires adding extra input recurrent update time step recurrent update aware whether near end generated sequence extra input either consist value consist number remaining time steps without extra input rnn might generate sequences end abruptly sentence ends complete approach based decomposition strategy predicting directly used example goodfellow chapter sequence modeling recurrent recursive nets 
[sequence, modeling, recurrent, recursive, nets, recurrent, neural, networks, modeling, sequences, conditioned, context, rnns] previous section described rnn could correspond directed graphical model sequence random variables inputs course development rnns included sequence inputs general rnns allow extension graphical model view represent joint distribution variables also conditional distribution given discussed context feedforward networks sec model representing variable reinter preted model representing conditional distribution extend model represent distribution using making function case rnn achieved diﬀerent ways review common obvious choices previously discussed rnns take sequence vectors input another option take single vector input ﬁxed size vector simply make extra input rnn generates sequence common ways providing extra input rnn extra input time step initial state ﬁrst common approach illustrated fig interaction input hidden unit vector parametrized newly introduced weight matrix absent model sequence values product added additional input hidden units every time step think choice determining value eﬀectively new bias parameter used hidden units weights remain independent input think model taking parameters non conditional model turning bias parameters within function input rather receiving single vector input rnn may receive sequence vectors input rnn described corresponds chapter sequence modeling recurrent recursive nets figure rnn maps ﬁxed length vector distribution sequences rnn appropriate tasks image captioning single image used input model produces sequence words describing image element observed output sequence serves input current time step training target previous time step chapter sequence modeling recurrent recursive nets figure conditional recurrent neural network mapping variable length sequence values distribution sequences values length compared fig rnn contains connections previous output current state connections allow rnn model arbitrary distribution sequences given sequences length rnn fig able represent distributions values conditionally independent given values chapter sequence modeling recurrent recursive nets conditional distribution makes conditional independence assumption distribution factorizes remove conditional independence assumption add connections output time hidden unit time shown fig model represent arbitrary probability distributions sequence kind model representing distribution sequence given another sequence still one restriction length sequences must describe remove restriction sec figure computation typical bidirectional recurrent neural network meant learn map input sequences target sequences loss step beneﬁt relevant summary past input relevant summary future input chapter sequence modeling recurrent recursive nets 
[sequence, modeling, recurrent, recursive, nets, bidirectional, rnns] recurrent networks considered causal struc ture meaning state time captures information past present input models discussed also allow information past values aﬀect current state values available however many applications want output prediction may depend whole input sequence example speech recognition correct interpretation current sound phoneme may depend next phonemes articulation potentially may even depend next words linguistic dependencies nearby words two interpretations current word acoustically plausible may look far future past disambiguate also true handwriting recognition many sequence sequence learning tasks described next section bidirectional recurrent neural networks bidirectional rnns invented address need schuster paliwal extremely suc cessful graves applications need arises handwriting recognition graves graves schmidhuber speech recogni tion graves schmidhuber graves baldi bioinformatics name suggests bidirectional rnns combine rnn moves forward time beginning start sequence another rnn moves backward time beginning end sequence fig illustrates typical bidirectional rnn standing state sub rnn moves forward time standing state sub rnn moves backward time allows output units compute representation depends past future sensitive input values around time without specify ﬁxed size window around one would feedforward network convolutional network regular rnn ﬁxed size look ahead buﬀer idea naturally extended dimensional input images four rnns one going one four directions left right point grid output could compute representation would capture mostly local information could also depend long range inputs rnn able learn carry information compared convolutional network rnns applied images typically chapter sequence modeling recurrent recursive nets expensive allow long range lateral interactions features feature map visin kalchbrenner indeed forward propagation equations rnns may written form shows use convolution computes bottom input layer prior recurrent propagation across feature map incorporates lateral interactions 
[sequence, modeling, recurrent, recursive, nets, encoder-decoder, sequence-to-sequence, architec-, tures] seen fig rnn map input sequence ﬁxed size vector seen fig rnn map ﬁxed size vector sequence seen fig fig fig fig rnn map input sequence output sequence length discuss rnn trained map input sequence output sequence necessarily length comes many applications speech recognition machine translation question answering input output sequences training set generally length although lengths might related often call input rnn context want produce representation context context might vector sequence vectors summarize input sequence simplest rnn architecture mapping variable length sequence variable length sequence ﬁrst proposed shortly cho sutskever independently developed architecture ﬁrst obtain state art translation using approach former system based scoring proposals generated another machine translation system latter uses standalone recurrent network generate trans lations authors respectively called architecture illustrated fig encoder decoder sequence sequence architecture idea simple encoder reader input rnn processes input sequence encoder emits context usually simple function ﬁnal hidden state decoder writer output rnn conditioned ﬁxed length vector like fig generate output sequence innovation kind architecture presented earlier sections chapter lengths vary previous architectures constrained sequence sequence architecture two rnns chapter sequence modeling recurrent recursive nets encoder decoder figure example encoder decoder sequence sequence rnn architecture learning generate output sequence given input sequence composed encoder rnn reads input sequence decoder rnn generates output sequence computes probability given output sequence ﬁnal hidden state encoder rnn used compute generally ﬁxed size context variable represents semantic summary input sequence given input decoder rnn chapter sequence modeling recurrent recursive nets trained jointly maximize average log pairs sequences training set last state encoder rnn typically used representation input sequence provided input decoder rnn context vector decoder rnn simply vector sequence rnn described sec seen least two ways vector sequence rnn receive input input provided initial state rnn input connected hidden units time step two ways also combined constraint encoder must size hidden layer decoder one clear limitation architecture context output encoder rnn dimension small properly summarize long sequence phenomenon observed context bahdanau machine translation proposed make variable length sequence rather ﬁxed size vector additionally introduced attention mechanism learns associate elements sequence elements output sequence see sec details 
[sequence, modeling, recurrent, recursive, nets, deep, recurrent, networks] computation rnns decomposed three blocks parameters associated transformations input hidden state previous hidden state next hidden state hidden state output rnn architecture fig three blocks associated single weight matrix words network unfolded corresponds shallow transformation shallow transformation mean transformation would represented single layer within deep mlp typically transformation represented learned aﬃne transformation followed ﬁxed nonlinearity would advantageous introduce depth operations experimental evidence graves pascanu strongly suggests experimental evidence agreement idea need enough chapter sequence modeling recurrent recursive nets figure recurrent neural network made deep many ways pascanu hidden recurrent state broken groups organized hierarchically deeper computation mlp introduced input hidden hidden hidden hidden output parts may lengthen shortest path linking diﬀerent time steps path lengthening eﬀect mitigated introducing skip connections chapter sequence modeling recurrent recursive nets depth order perform required mappings see also schmidhuber hihi bengio jaeger earlier work deep rnns graves ﬁrst show signiﬁcant beneﬁt decomposing state rnn multiple layers fig left think lower layers hierarchy depicted fig playing role transforming raw input representation appropriate higher levels hidden state pascanu step propose separate mlp possibly deep three blocks enumerated illustrated fig considerations representational capacity suggest allocate enough capacity three steps adding depth may hurt learning making optimization diﬃcult general easier optimize shallower architectures adding extra depth fig makes shortest path variable time step variable time step become longer example mlp single hidden layer used state state transition doubled length shortest path variables two diﬀerent time steps compared ordinary rnn fig however argued pascanu mitigated introducing skip connections hidden hidden path illustrated fig 
[sequence, modeling, recurrent, recursive, nets, recursive, neural, networks] recursive neural networks represent yet another generalization recurrent net works diﬀerent kind computational graph structured deep tree rather chain like structure rnns typical computational graph recursive network illustrated fig recursive neural networks introduced pollack potential use learning reason described recursive networks successfully bottou applied processing data structures input neural nets frasconi socher natural language processing well computer vision socher one clear advantage recursive nets recurrent nets sequence length depth measured number compositions nonlinear operations drastically reduced log might help deal long term dependencies open question best structure tree one option tree structure depend data suggest abbreviate recursive neural network rnn avoid confusion recurrent neural network chapter sequence modeling recurrent recursive nets figure recursive network computational graph generalizes recurrent network chain tree variable size sequence mapped ﬁxed size representation output ﬁxed set parameters weight matrices ﬁgure illustrates supervised learning case target provided associated whole sequence chapter sequence modeling recurrent recursive nets balanced binary tree application domains external methods suggest appropriate tree structure example processing natural language sentences tree structure recursive network ﬁxed structure parse tree sentence provided natural language parser ideally one would like learner socher discover infer tree structure appropriate given input suggested bottou many variants recursive net idea possible example frasconi frasconi associate data tree structure associate inputs targets individual nodes tree computation performed node traditional artiﬁcial neuron computation aﬃne transformation inputs followed monotone nonlinearity example propose using tensor operations socher bilinear forms previously found useful model relationships concepts weston bordes concepts represented continuous vectors embeddings 
[sequence, modeling, recurrent, recursive, nets, challenge, long-term, dependencies] mathematical challenge learning long term dependencies recurrent net works introduced sec basic problem gradients propagated many stages tend either vanish time explode rarely much damage optimization even assume parameters recurrent network stable store memories gradients exploding diﬃculty long term dependencies arises exponentially smaller weights given long term interactions involving multiplication many jacobians compared short term ones many sources provide deeper treatment hochreiter doya bengio pascanu section describe problem detail remaining sections describe approaches overcoming problem recurrent networks involve composition function multiple times per time step compositions result extremely nonlinear behavior illustrated fig particular function composition employed recurrent neural networks somewhat resembles matrix multiplication think recurrence relation simple recurrent neural network lacking nonlinear activation function chapter sequence modeling recurrent recursive nets figure composing many nonlinear functions like linear tanh layer shown result highly nonlinear typically values associated tiny derivative values large derivative many alternations increasing decreasing plot plot linear projection dimensional hidden state single dimension plotted axis axis coordinate initial state along random direction dimensional space thus view plot linear cross section high dimensional function plots show function time step equivalently number times transition function composed chapter sequence modeling recurrent recursive nets lacking inputs described sec recurrence relation essentially describes power method may simpliﬁed admits eigendecomposition form orthogonal recurrence may simpliﬁed eigenvalues raised power causing eigenvalues magnitude less one decay zero eigenvalues magnitude greater one explode component aligned largest eigenvector eventually discarded problem particular recurrent networks scalar case imagine multiplying weight many times product either vanish explode depending magnitude however make non recurrent network diﬀerent weight time step situation diﬀerent initial state given state time given suppose values generated randomly independently one another zero mean variance variance product obtain desired variance may choose individual weights variance deep feedforward networks carefully chosen scaling thus avoid vanishing exploding gradient problem argued sussillo vanishing exploding gradient problem rnns independently discovered separate researchers hochreiter bengio one may hope problem avoided simply staying region parameter space gradients vanish explode unfortunately order store memories way robust small perturbations rnn must enter region parameter space gradients vanish bengio speciﬁcally whenever model able represent long term dependencies gradient long term interaction exponentially smaller magnitude gradient short term interaction mean impossible learn might take long time learn long term dependencies signal dependencies tend hidden smallest ﬂuctuations arising short term dependencies practice experiments show increase span dependencies bengio chapter sequence modeling recurrent recursive nets need captured gradient based optimization becomes increasingly diﬃcult probability successful training traditional rnn via sgd rapidly reaching sequences length deeper treatment recurrent networks dynamical systems see doya bengio siegelmann sontag review pascanu remaining sections chapter discuss various approaches proposed reduce diﬃculty learning long term dependencies cases allowing rnn learn dependencies across hundreds steps problem learning long term dependencies remains one main challenges deep learning 
[sequence, modeling, recurrent, recursive, nets, echo, state, networks] recurrent weights mapping input weights mapping diﬃcult parameters learn recurrent network one proposed jaeger maass jaeger haas jaeger approach avoiding diﬃculty set recurrent weights recurrent hidden units good job capturing history past inputs learn output weights idea independently proposed echo state networks esns jaeger haas jaeger maass liquid state machines latter similar except uses spiking neurons binary outputs instead continuous valued hidden units used esns esns liquid state machines termed reservoir computing lukoševičius jaeger denote fact hidden units form reservoir temporal features may capture diﬀerent aspects history inputs one way think reservoir computing recurrent networks similar kernel machines map arbitrary length sequence history inputs time ﬁxed length vector recurrent state linear predictor typically linear regression applied solve problem interest training criterion may easily designed convex function output weights example output consists linear regression hidden units output targets training criterion mean squared error convex may solved reliably simple learning algorithms jaeger important question therefore set input recurrent weights rich set histories represented recurrent neural network state answer proposed reservoir computing literature chapter sequence modeling recurrent recursive nets view recurrent net dynamical system set input recurrent weights dynamical system near edge stability original idea make eigenvalues jacobian state state transition function close explained sec important characteristic recurrent network eigenvalue spectrum jacobians particular importance spectral radius deﬁned maximum absolute values eigenvalues understand eﬀect spectral radius consider simple case back propagation jacobian matrix change case happens example network purely linear suppose eigenvector corresponding eigenvalue consider happens propagate gradient vector backwards time begin gradient vector one step back propagation steps consider happens instead back propagate perturbed version begin one step steps see back propagation starting back propagation starting diverge steps back propagation chosen unit eigenvector eigenvalue multiplication jacobian simply scales diﬀerence step two executions back propagation separated distance corresponds largest value perturbation achieves widest possible separation initial perturbation size deviation size grows exponentially large deviation size becomes exponentially small course example assumed jacobian every time step corresponding recurrent network nonlinearity nonlinearity present derivative nonlinearity approach zero many time steps help prevent explosion resulting large spectral radius indeed recent work echo state networks advocates using spectral radius much larger unity yildiz jaeger everything said back propagation via repeated matrix multipli cation applies equally forward propagation network nonlinearity state linear map always shrinks measured norm say map contractive spectral radius less one mapping contractive small change becomes smaller time step necessarily makes network forget information chapter sequence modeling recurrent recursive nets past use ﬁnite level precision bit integers store state vector jacobian matrix tells small change propagates one step forward equivalently gradient propagates one step backward back propagation note neither need symmetric though square real complex valued eigenvalues eigenvectors imaginary components corresponding potentially oscillatory behavior jacobian applied iteratively even though small variation interest back propagation real valued expressed complex valued basis matters happens magnitude complex absolute value possibly complex valued basis coeﬃcients multiply matrix vector eigenvalue magnitude greater one corresponds magniﬁcation exponential growth applied iteratively shrinking exponential decay applied iteratively nonlinear map jacobian free change step dynamics therefore become complicated however remains true small initial variation turn large variation several steps one diﬀerence purely linear case nonlinear case use squashing nonlinearity tanh cause recurrent dynamics become bounded note possible back propagation retain unbounded dynamics even forward propagation bounded dynamics example sequence tanh units middle linear regime connected weight matrices spectral radius greater however rare units simultaneously lie linear activation point tanh strategy echo state networks simply weights spectral radius information carried forward time explode due stabilizing eﬀect saturating nonlinearities like tanh recently shown techniques used set weights esns could used initialize weights fully trainable recurrent net work hidden hidden recurrent weights trained using back propagation time helping learn long term dependencies sutskever sutskever setting initial spectral radius performs well combined sparse initialization scheme described sec chapter sequence modeling recurrent recursive nets 
[sequence, modeling, recurrent, recursive, nets, time, scales] one way deal long term dependencies design model operates multiple time scales parts model operate ﬁne grained time scales handle small details parts operate coarse time scales transfer information distant past present eﬃciently various strategies building ﬁne coarse time scales possible include addition skip connections across time leaky units integrate signals diﬀerent time constants removal connections used model ﬁne grained time scales 
[sequence, modeling, recurrent, recursive, nets, time, scales, adding, skip, connections, time] one way obtain coarse time scales add direct connections variables distant past variables present idea using skip connections dates back follows idea incorporating delays lin feedforward neural networks ordinary recurrent lang hinton network recurrent connection goes unit time unit time possible construct recurrent networks longer delays bengio seen sec gradients may vanish explode exponentially respect number time steps introduced lin recurrent connections time delay mitigate problem gradients diminish exponentially function rather since delayed single step connections gradients may still explode exponentially allows learning algorithm capture longer dependencies although long term dependencies may represented well way 
[sequence, modeling, recurrent, recursive, nets, time, scales, leaky, units, spectrum, diﬀerent, time, scales] another way obtain paths product derivatives close one units linear self connections weight near one connections accumulate running average value applying update parameter example linear self connection near one running average remembers information past long time near zero information past rapidly discarded hidden units linear self connections behave similarly running averages hidden units called leaky units chapter sequence modeling recurrent recursive nets skip connections time steps way ensuring unit always learn inﬂuenced value time steps earlier use linear self connection weight near one diﬀerent way ensuring unit access values past linear self connection approach allows eﬀect adapted smoothly ﬂexibly adjusting real valued rather adjusting integer valued skip length ideas proposed mozer hihi bengio leaky units also found useful context echo state networks jaeger two basic strategies setting time constants used leaky units one strategy manually values remain constant example sampling values distribution initialization time another strategy make time constants free parameters learn leaky units diﬀerent time scales appears help long term dependencies mozer pascanu 
[sequence, modeling, recurrent, recursive, nets, time, scales, removing, connections] another approach handle long term dependencies idea organizing state rnn multiple time scales hihi bengio information ﬂowing easily long distances slower time scales idea diﬀers skip connections time discussed earlier involves actively removing length one connections replacing longer connections units modiﬁed way forced operate long time scale skip connections time add edges units receiving new connections may learn operate long time scale may also choose focus short term connections diﬀerent ways group recurrent units forced operate diﬀerent time scales one option make recurrent units leaky diﬀerent groups units associated diﬀerent ﬁxed time scales proposal successfully used mozer pascanu another option explicit discrete updates taking place diﬀerent times diﬀerent frequency diﬀerent groups units approach worked hihi bengio koutnik well number benchmark datasets chapter sequence modeling recurrent recursive nets 
[sequence, modeling, recurrent, recursive, nets, long, short-term, memory, gated, rnns] writing eﬀective sequence models used practical applications called gated rnns long short term memory include networks based gated recurrent unit like leaky units gated rnns based idea creating paths time derivatives neither vanish explode leaky units connection weights either manually chosen constants parameters gated rnns generalize connection weights may change time step leaky units allow network accumulate information evidence particular feature category long duration however information used might useful neural network forget old state example sequence made sub sequences want leaky unit accumulate evidence inside sub subsequence need mechanism forget old state setting zero instead manually deciding clear state want neural network learn decide gated rnns 
[sequence, modeling, recurrent, recursive, nets, long, short-term, memory, gated, rnns, lstm] clever idea introducing self loops produce paths gradient ﬂow long durations core contribution initial long short term memory lstm model hochreiter schmidhuber crucial addition make weight self loop conditioned context rather ﬁxed making weight self loop gated controlled gers another hidden unit time scale integration changed dynamically case mean even lstm ﬁxed parameters time scale integration change based input sequence time constants output model lstm found extremely successful many applications unconstrained handwriting recognition graves speech recognition graves graves jaitly handwriting generation graves sutskever machine translation image captioning kiros vinyals parsing vinyals lstm block diagram illustrated fig corresponding forward propagation equations given case shallow recurrent chapter sequence modeling recurrent recursive nets input input gate forget gate output gate output state self loop figure block diagram lstm recurrent network cell cells connected recurrently replacing usual hidden units ordinary recurrent networks input feature computed regular artiﬁcial neuron unit value accumulated state sigmoidal input gate allows state unit linear self loop whose weight controlled forget gate output cell shut output gate gating units sigmoid nonlinearity input unit squashing nonlinearity state unit also used extra input gating units black square indicates delay single time step chapter sequence modeling recurrent recursive nets network architecture deeper architectures also successfully used graves pascanu instead unit simply applies element wise nonlinearity aﬃne transformation inputs recurrent units lstm recurrent networks lstm cells internal recurrence self loop addition outer recurrence rnn cell inputs outputs ordinary recurrent network parameters system gating units controls ﬂow information important component state unit linear self loop similar leaky units described previous section however self loop weight associated time constant controlled forget gate unit time step cell sets weight value via sigmoid unit ufeb ufed uff uff current input vector current hidden layer vector containing outputs lstm cells respectively biases input weights recurrent weights forget gates lstm cell internal state thus updated follows conditional self loop weight ufeb ufed uff uff respectively denote biases input weights recurrent weights lstm cell unit external input gate computed similarly forget gate sigmoid unit obtain gating value parameters ufeb ufed uff uff output lstm cell also shut via output gate also uses sigmoid unit gating tanh ufeb ufed uff uff chapter sequence modeling recurrent recursive nets parameters biases input weights recurrent weights respectively among variants one choose use cell state extra input weight three gates unit shown fig would require three additional parameters lstm networks shown learn long term dependencies easily simple recurrent architectures ﬁrst artiﬁcial data sets designed testing ability learn long term dependencies bengio hochreiter schmidhuber hochreiter challenging sequence processing tasks state art performance obtained graves graves sutskever variants alternatives lstm studied used discussed next 
[sequence, modeling, recurrent, recursive, nets, long, short-term, memory, gated, rnns, gated, rnns] pieces lstm architecture actually necessary successful architectures could designed allow network dynamically control time scale forgetting behavior diﬀerent units answers questions given recent work gated rnns whose units also known gated recurrent units grus cho chung jozefowicz chrupala main diﬀerence lstm single gating unit simultaneously controls forgetting factor decision update state unit update equations following ufeb ufed uff uff stands update gate reset gate value deﬁned usual ufeb ufed uff uff ufeb ufed uff uff reset updates gates individually ignore parts state vector update gates act like conditional leaky integrators linearly gate chapter sequence modeling recurrent recursive nets dimension thus choosing copy one extreme sigmoid completely ignore extreme replacing new target state value towards leaky integrator wants converge reset gates control parts state get used compute next target state introducing additional nonlinear eﬀect relationship past state future state many variants around theme designed example reset gate forget gate output could shared across multiple hidden units alternately product global gate covering whole group units entire layer local gate per unit could used combine global control local control however several investigations architectural variations lstm gru found variant would clearly beat across wide range tasks greﬀ jozefowicz greﬀ found crucial ingredient forget gate jozefowicz found adding bias lstm forget gate practice advocated makes lstm strong best gers explored architectural variants 
[sequence, modeling, recurrent, recursive, nets, optimization, long-term, dependencies] sec sec described vanishing exploding gradient problems occur optimizing rnns many time steps interesting idea proposed martens sutskever second derivatives may vanish time ﬁrst derivatives vanish second order optimization algorithms may roughly understood dividing ﬁrst derivative second derivative higher dimension multiplying gradient inverse hessian second derivative shrinks similar rate ﬁrst derivative ratio ﬁrst second derivatives may remain relatively constant unfortunately second order methods many drawbacks including high computational cost need large minibatch tendency attracted saddle points martens sutskever found promising results using second order methods later sutskever found simpler methods nesterov momentum careful initialization could achieve similar results see sutskever detail approaches largely replaced simply using sgd even without momentum applied lstms part continuing theme machine learning often much easier design model easy optimize design powerful optimization algorithm chapter sequence modeling recurrent recursive nets 
[sequence, modeling, recurrent, recursive, nets, optimization, long-term, dependencies, clipping, gradients] discussed sec strongly nonlinear functions computed recurrent net many time steps tend derivatives either large small magnitude illustrated fig fig see objective function function parameters landscape one ﬁnds cliﬀs wide rather ﬂat regions separated tiny regions objective function changes quickly forming kind cliﬀ diﬃculty arises parameter gradient large gradient descent parameter update could throw parameters far region objective function larger undoing much work done reach current solution gradient tells direction corresponds steepest descent within inﬁnitesimal region surrounding current parameters outside inﬁnitesimal region cost function may begin curve back upwards update must chosen small enough avoid traversing much upward curvature typically use learning rates decay slowly enough consecutive steps approximately learning rate step size appropriate relatively linear part landscape often inappropriate causes uphill motion enter curved part landscape next step chapter sequence modeling recurrent recursive nets figure example eﬀect gradient clipping recurrent network two parameters gradient clipping make gradient descent perform reasonably vicinity extremely steep cliﬀs steep cliﬀs commonly occur recurrent networks near recurrent network behaves approximately linearly cliﬀ exponentially steep number time steps weight matrix multiplied time step left gradient descent without gradient clipping overshoots bottom small ravine receives large gradient cliﬀ face large gradient catastrophically propels parameters outside axes plot right gradient descent gradient clipping moderate reaction cliﬀ ascend cliﬀ face step size restricted cannot propelled away steep region near solution figure adapted permission pascanu simple type solution use practitioners many years clipping gradient diﬀerent instances idea mikolov pascanu one option clip parameter gradient minibatch element wise mikolov parameter update another clip norm gradient pascanu parameter update norm threshold used update parameters gradient parameters including diﬀerent groups parameters weights biases renormalized jointly single scaling factor latter method advantage guarantees step still gradient direction experiments suggest forms work similarly although chapter sequence modeling recurrent recursive nets parameter update direction true gradient gradient norm clipping parameter update vector norm bounded bounded gradient avoids performing detrimental step gradient explodes fact even simply taking random step gradient magnitude threshold tends work almost well explosion severe gradient numerically inf nan considered inﬁnite number random step size taken typically move away numerically unstable conﬁguration clipping gradient norm per minibatch change direction gradient individual minibatch however taking average norm clipped gradient many minibatches equivalent clipping norm true gradient gradient formed using examples examples large gradient norm well examples appear minibatch examples contribution ﬁnal direction diminished stands contrast traditional minibatch gradient descent true gradient direction equal average minibatch gradients put another way traditional stochastic gradient descent uses unbiased estimate gradient gradient descent norm clipping introduces heuristic bias know empirically useful element wise clipping direction update aligned true gradient minibatch gradient still descent direction also proposed graves clip back propagated gradient respect hidden units comparison published variants conjecture methods behave similarly 
[sequence, modeling, recurrent, recursive, nets, optimization, long-term, dependencies, regularizing, encourage, information, flow] gradient clipping helps deal exploding gradients help vanishing gradients address vanishing gradients better capture long term dependencies discussed idea creating paths computational graph unfolded recurrent architecture along product gradients associated arcs near one approach achieve lstms self loops gating mechanisms described sec another idea regularize constrain parameters encourage information ﬂow particular would like gradient vector back propagated maintain magnitude even loss function penalizes output end sequence formally want chapter sequence modeling recurrent recursive nets large objective pascanu propose following regularizer ufeb ufed uff uff computing gradient regularizer may appear diﬃcult pascanu propose approximation consider back propagated vectors constants purpose regularizer need back propagate experiments regularizer suggest combined norm clipping heuristic handles gradient explosion regularizer considerably increase span dependencies rnn learn keeps rnn dynamics edge explosive gradients gradient clipping particularly important without gradient clipping gradient explosion prevents learning succeeding key weakness approach eﬀective lstm tasks data abundant language modeling 
[sequence, modeling, recurrent, recursive, nets, explicit, memory] intelligence requires knowledge acquiring knowledge done via learning motivated development large scale deep architectures however diﬀerent kinds knowledge knowledge implicit sub conscious diﬃcult verbalize walk dog looks diﬀerent cat knowledge explicit declarative relatively straightforward put words every day commonsense knowledge like cat kind animal speciﬁc facts need know accomplish current goals like meeting sales team room neural networks excel storing implicit knowledge however struggle memorize facts stochastic gradient descent requires many presentations input stored neural network parameters even input stored especially precisely graves hypothesized neural networks lack equivalent working memory system allows human beings explicitly hold manipulate pieces information relevant achieving goal explicit memory chapter sequence modeling recurrent recursive nets task network controlling memory memory cells writing mechanism reading mechanism figure schematic example network explicit memory capturing key design elements neural turing machine diagram distinguish representation part model task network recurrent net bottom memory part model set cells store facts task network learns control memory deciding read write within memory reading writing mechanisms indicated bold arrows pointing reading writing addresses chapter sequence modeling recurrent recursive nets components would allow systems rapidly intentionally store retrieve speciﬁc facts also sequentially reason need neural networks process information sequence steps changing way input fed network step long recognized important ability reason rather make automatic intuitive responses input hinton resolve diﬃculty weston introduced memory networks include set memory cells accessed via addressing mechanism memory networks originally required supervision signal instructing use memory cells graves introduced neural turing machine able learn read write arbitrary content memory cells without explicit supervision actions undertake allowed end end training without supervision signal via use content based soft attention mechanism see sec bahdanau soft addressing mechanism become standard related architectures emulating algorithmic mechanisms way still allows gradient based opti mization sukhbaatar joulin mikolov kumar vinyals grefenstette memory cell thought extension memory cells lstms grus diﬀerence network outputs internal state chooses cell read write memory accesses digital computer read write speciﬁc address diﬃcult optimize functions produce exact integer addresses alleviate problem ntms actually read write many memory cells simultaneously read take weighted average many cells write modify multiple cells diﬀerent amounts coeﬃcients operations chosen focused small number cells example producing via softmax function using weights non zero derivatives allows functions controlling access memory optimized using gradient descent gradient coeﬃcients indicates whether increased decreased gradient typically large memory addresses receiving large coeﬃcient memory cells typically augmented contain vector rather single scalar stored lstm gru memory cell two reasons increase size memory cell one reason increased cost accessing memory cell pay computational cost producing coeﬃcient many cells expect coeﬃcients cluster around small number cells reading vector value rather scalar value chapter sequence modeling recurrent recursive nets oﬀset cost another reason use vector valued memory cells allow content based addressing weight used read write cell function cell vector valued cells allow retrieve complete vector valued memory able produce pattern matches elements analogous way people recall lyrics song based words think content based read instruction saying retrieve lyrics song chorus live yellow submarine content based addressing useful make objects retrieved large every letter song stored separate memory cell would able ﬁnd way comparison location based addressing allowed refer content memory think location based read instruction saying retrieve lyrics song slot location based addressing often perfectly sensible mechanism even memory cells small content memory cell copied forgotten time steps information contains propagated forward time gradients propagated backward time without either vanishing exploding explicit memory approach illustrated fig see task neural network coupled memory although task neural network could feedforward recurrent overall system recurrent network task network choose read write speciﬁc memory addresses explicit memory seems allow models learn tasks ordinary rnns lstm rnns cannot learn one reason advantage may information gradients propagated forward time backwards time respectively long durations alternative back propagation weighted averages memory cells interpret memory addressing coeﬃcients probabilities stochastically read one cell zaremba sutskever optimizing models make discrete decisions requires specialized optimization algorithms described sec far training stochastic architectures make discrete decisions remains harder training deterministic algorithms make soft decisions whether soft allowing back propagation stochastic hard mech anism choosing address form identical attention mechanism previously introduced context machine translation bah danau discussed sec idea attention mechanisms neural networks introduced even earlier context handwriting generation graves attention mechanism constrained chapter sequence modeling recurrent recursive nets move forward time sequence case machine translation memory networks step focus attention move completely diﬀerent place compared previous step recurrent neural networks provide way extend deep learning sequential data last major tool deep learning toolbox discussion moves choose use tools apply real world tasks 
[practical, methodology] successfully applying deep learning techniques requires good knowledge algorithms exist principles explain work good machine learning practitioner also needs know choose algorithm particular application monitor respond feedback obtained experiments order improve machine learning system day day development machine learning systems practitioners need decide whether gather data increase decrease model capacity add remove regularizing features improve optimization model improve approximate inference model debug software implementation model operations least time consuming try important able determine right course action rather blindly guessing book diﬀerent machine learning models training algo rithms objective functions may give impression important ingredient machine learning expert knowing wide variety machine learning techniques good diﬀerent kinds math prac tice one usually much better correct application commonplace algorithm sloppily applying obscure algorithm correct application algorithm depends mastering fairly simple methodology many recommendations chapter adapted recommend following practical design process determine goals error metric use target value error metric goals error metrics driven problem application intended solve establish working end end pipeline soon possible including chapter practical methodology estimation appropriate performance metrics instrument system well determine bottlenecks performance diag nose components performing worse expected whether due overﬁtting underﬁtting defect data software repeatedly make incremental changes gathering new data adjusting hyperparameters changing algorithms based speciﬁc ﬁndings instrumentation running example use street view address number transcription system purpose application add goodfellow buildings google maps street view cars photograph buildings record gps coordinates associated photograph convolutional network recognizes address number photograph allowing google maps database add address correct location story commercial application developed gives example follow design methodology advocate describe steps process 
[practical, methodology, performance, metrics] determining goals terms error metric use necessary ﬁrst step error metric guide future actions also idea level performance desire keep mind applications impossible achieve absolute zero error bayes error deﬁnes minimum error rate hope achieve even inﬁnite training data recover true probability distribution input features may contain complete information output variable system might intrinsically stochastic also limited ﬁnite amount training data amount training data limited variety reasons goal build best possible real world product service typically collect data must determine value reducing error weigh cost collecting data data collection require time money human suﬀering example data collection process involves performing invasive medical tests goal answer scientiﬁc question algorithm performs better ﬁxed benchmark benchmark chapter practical methodology speciﬁcation usually determines training set allowed collect data one determine reasonable level performance expect typically academic setting estimate error rate attainable based previously published benchmark results real word setting idea error rate necessary application safe cost eﬀective appealing consumers determined realistic desired error rate design decisions guided reaching error rate another important consideration besides target value performance metric choice metric use several diﬀerent performance metrics may used measure eﬀectiveness complete application includes machine learning components performance metrics usually diﬀerent cost function used train model described sec common measure accuracy equivalently error rate system however many applications require advanced metrics sometimes much costly make one kind mistake another example mail spam detection system make two kinds mistakes incorrectly classifying legitimate message spam incorrectly allowing spam message appear inbox much worse block legitimate message allow questionable message pass rather measuring error rate spam classiﬁer may wish measure form total cost cost blocking legitimate messages higher cost allowing spam messages sometimes wish train binary classiﬁer intended detect rare event example might design medical test rare disease suppose one every million people disease easily achieve accuracy detection task simply hard coding classiﬁer always report disease absent clearly accuracy poor way characterize performance system one way solve problem instead measure precision recall precision fraction detections reported model correct recall fraction true events detected detector says one disease would achieve perfect precision zero recall detector says everyone disease would achieve perfect recall precision equal percentage people disease example disease one people million using precision recall common plot curve precision axis recall axis classiﬁer generates score higher event detected occurred example feedforward chapter practical methodology network designed detect disease outputs estimating probability person whose medical results described features disease choose report detection whenever score exceeds threshold varying threshold trade precision recall many cases wish summarize performance classiﬁer single number rather curve convert precision recall score given another option report total area lying beneath curve applications possible machine learning system refuse make decision useful machine learning algorithm estimate conﬁdent decision especially wrong decision harmful human operator able occasionally take street view transcription system provides example situation task transcribe address number photograph order associate location photo taken correct address map value map degrades considerably map inaccurate important add address transcription correct machine learning system thinks less likely human obtain correct transcription best course action allow human transcribe photo instead course machine learning system useful able dramatically reduce amount photos human operators must process natural performance metric use situation coverage coverage fraction examples machine learning system able produce response possible trade coverage accuracy one always obtain accuracy refusing process example reduces coverage street view task goal project reach human level transcription accuracy maintaining coverage human level performance task accuracy many metrics possible example measure click rates collect user satisfaction surveys many specialized application areas application speciﬁc criteria well important determine performance metric improve ahead time concentrate improving metric without clearly deﬁned goals diﬃcult tell whether changes machine learning system make progress chapter practical methodology 
[practical, methodology, default, baseline, models] choosing performance metrics goals next step practical application establish reasonable end end system soon possible section provide recommendations algorithms use ﬁrst baseline approach various situations keep mind deep learning research progresses quickly better default algorithms likely become available soon writing depending complexity problem may even want begin without using deep learning problem chance solved choosing linear weights correctly may want begin simple statistical model like logistic regression know problem falls complete category like object recognition speech recognition machine translation likely well beginning appropriate deep learning model first choose general category model based structure data want perform supervised learning ﬁxed size vectors input use feedforward network fully connected layers input known topological structure example input image use convolutional network cases begin using kind piecewise linear unit relus generalizations like leaky relus prelus maxout input output sequence use gated recurrent net lstm gru reasonable choice optimization algorithm sgd momentum decaying learning rate popular decay schemes perform better worse diﬀerent problems include decaying linearly reaching ﬁxed minimum learning rate decaying exponentially decreasing learning rate factor time validation error plateaus another reasonable alternative adam batch normalization dramatic eﬀect optimization performance especially convolutional networks networks sigmoidal nonlinearities reasonable omit batch normalization ﬁrst baseline introduced quickly optimization appears problematic unless training set contains tens millions examples include mild forms regularization start early stopping used almost universally dropout excellent regularizer easy implement compatible many models training algorithms batch normalization also sometimes reduces generalization error allows dropout omitted due noise estimate statistics used normalize variable chapter practical methodology task similar another task studied extensively probably well ﬁrst copying model algorithm already known perform best previously studied task may even want copy trained model task example common use features convolutional network trained imagenet solve computer vision tasks girshick common question whether begin using unsupervised learning scribed part somewhat domain speciﬁc domains iii natural language processing known beneﬁt tremendously unsuper vised learning techniques learning unsupervised word embeddings domains computer vision current unsupervised learning techniques bring beneﬁt except semi supervised setting number labeled examples small kingma rasmus application context unsupervised learning known important include ﬁrst end end baseline otherwise use unsupervised learning ﬁrst attempt task want solve unsupervised always try adding unsupervised learning later observe initial baseline overﬁts 
[practical, methodology, determining, whether, gather, data] ﬁrst end end system established time measure perfor mance algorithm determine improve many machine learning novices tempted make improvements trying many diﬀerent algorithms however often much better gather data improve learning algorithm one decide whether gather data first determine whether performance training set acceptable performance training set poor learning algorithm using training data already available reason gather data instead try increasing size model adding layers adding hidden units layer also try improving learning algorithm example tuning learning rate hyperparameter large models carefully tuned optimization algorithms work well problem might quality training data data may noisy may include right inputs needed predict desired outputs suggests starting collecting cleaner data collecting richer set features performance training set acceptable measure per chapter practical methodology formance test set performance test set also acceptable nothing left done test set performance much worse training set performance gathering data one eﬀective solutions key considerations cost feasibility gathering data cost feasibility reducing test error means amount data expected necessary improve test set performance signiﬁcantly large internet companies millions billions users feasible gather large datasets expense considerably less alternatives answer almost always gather training data example development large labeled datasets one important factors solving object recognition contexts medical applications may costly infeasible gather data simple alternative gathering data reduce size model improve regularization adjusting hyperparameters weight decay coeﬃcients adding regularization strategies dropout ﬁnd gap train test performance still unacceptable even tuning regularization hyperparameters gathering data advisable deciding whether gather data also necessary decide much gather helpful plot curves showing relationship training set size generalization error like fig extrapolating curves one predict much additional training data would needed achieve certain level performance usually adding small fraction total number examples noticeable impact generalization error therefore recommended experiment training set sizes logarithmic scale example doubling number examples consecutive experiments gathering much data feasible way improve generalization error improve learning algorithm becomes domain research domain advice applied practitioners 
[practical, methodology, selecting, hyperparameters] deep learning algorithms come many hyperparameters control many aspects algorithm behavior hyperparameters aﬀect time memory cost running algorithm hyperparameters aﬀect quality model recovered training process ability infer correct results deployed new inputs two basic approaches choosing hyperparameters choosing manually choosing automatically choosing hyperparameters chapter practical methodology manually requires understanding hyperparameters machine learning models achieve good generalization automatic hyperparameter selection algorithms greatly reduce need understand ideas often much computationally costly 
[practical, methodology, selecting, hyperparameters, manual, hyperparameter, tuning] set hyperparameters manually one must understand relationship hyperparameters training error generalization error computational resources memory runtime means establishing solid foundation fun damental ideas concerning eﬀective capacity learning algorithm chapter goal manual hyperparameter search usually ﬁnd lowest general ization error subject runtime memory budget discuss determine runtime memory impact various hyperparameters highly platform dependent primary goal manual hyperparameter search adjust eﬀective capacity model match complexity task eﬀective capacity constrained three factors representational capacity model ability learning algorithm successfully minimize cost function used train model degree cost function training procedure regularize model model layers hidden units per layer higher representational capacity capable representing complicated functions necessarily actually learn functions though training algorithm cannot discover certain functions good job minimizing training cost regularization terms weight decay forbid functions generalization error typically follows shaped curve plotted function one hyperparameters fig one extreme hyperparameter value corresponds low capacity generalization error high training error high underﬁtting regime extreme hyperparameter value corresponds high capacity generalization error high gap training test error high somewhere middle lies optimal model capacity achieves lowest possible generalization error adding medium generalization gap medium amount training error hyperparameters overﬁtting occurs value hyper parameter large number hidden units layer one example chapter practical methodology increasing number hidden units increases capacity model hyperparameters overﬁtting occurs value hyperparame ter small example smallest allowable weight decay coeﬃcient zero corresponds greatest eﬀective capacity learning algorithm every hyperparameter able explore entire shaped curve many hyperparameters discrete number units layer number linear pieces maxout unit possible visit points along curve hyperparameters binary usually hyperparameters switches specify whether use optional component learning algorithm preprocessing step normalizes input features subtracting mean dividing standard deviation hyperparameters explore two points curve hyperparameters minimum maximum value prevents exploring part curve example minimum weight decay coeﬃcient zero means model underﬁtting weight decay zero enter overﬁtting region modifying weight decay coeﬃcient words hyperparameters subtract capacity learning rate perhaps important hyperparameter time tune one hyperparameter tune learning rate con trols eﬀective capacity model complicated way hyperparameters eﬀective capacity model highest learning rate correct optimization problem learning rate espe cially large especially small learning rate shaped curve training error illustrated fig learning rate large gradient descent inadvertently increase rather decrease training error idealized quadratic case occurs learning rate least twice large optimal value learning rate small training lecun slower may become permanently stuck high training error eﬀect poorly understood would happen convex loss function tuning parameters learning rate requires monitoring training test error diagnose whether model overﬁtting underﬁtting adjusting capacity appropriately error training set higher target error rate choice increase capacity using regularization conﬁdent optimization algorithm performing correctly must add layers network add hidden units unfortunately increases computational costs associated model error test set higher target error rate chapter practical methodology learning rate logarithmic scale figure typical relationship learning rate training error notice sharp rise error learning optimal value ﬁxed training time smaller learning rate may sometimes slow training factor proportional learning rate reduction generalization error follow curve complicated regularization eﬀects arising large small learning rates since poor optimization degree reduce prevent overﬁtting even points equivalent training error diﬀerent generalization error take two kinds actions test error sum training error gap training test error optimal test error found trading quantities neural networks typically perform best training error low thus capacity high test error primarily driven gap train test error goal reduce gap without increasing training error faster gap decreases reduce gap change regularization hyperparameters reduce eﬀective model capacity adding dropout weight decay usually best performance comes large model regularized well example using dropout hyperparameters set reasoning whether increase decrease model capacity examples included table manually tuning hyperparameters lose sight end goal good performance test set adding regularization one way achieve goal long low training error always reduce general ization error collecting training data brute force way practically guarantee success continually increase model capacity training set size task solved approach course increase computational cost training inference feasible given appropriate resources chapter practical methodology hyperparameter increases capacity reason caveats number hid den units increased increasing number hidden units increases representational capacity model increasing number hidden units increases time memory cost essentially every eration model learning rate tuned timally improper learning rate whether high low results model low eﬀective capacity due optimization failure convolution ker nel width increased increasing kernel width increases number rameters model wider kernel results narrower output dimen sion reducing model pacity unless use plicit zero padding duce eﬀect wider kernels require mem ory parameter storage increase runtime narrower output reduces memory cost implicit zero padding increased adding implicit zeros fore convolution keeps representation size large increased time mem ory cost opera tions weight decay eﬃcient decreased decreasing weight cay coeﬃcient frees model parameters come larger dropout rate decreased dropping units less often gives units oppor tunities conspire train ing set table eﬀect various hyperparameters model capacity chapter practical methodology principle approach could fail due optimization diﬃculties many problems optimization seem signiﬁcant barrier provided model chosen appropriately 
[practical, methodology, selecting, hyperparameters, automatic, hyperparameter, optimization, algorithms] ideal learning algorithm takes dataset outputs function without requiring hand tuning hyperparameters popularity several learning algorithms logistic regression svms stems part ability perform well one two tuned hyperparameters neural networks sometimes perform well small number tuned hyperparameters often beneﬁt signiﬁcantly tuning forty hyperparameters manual hyperparameter tuning work well user good starting point one determined others worked type application architecture user months years experience exploring hyperparameter values neural networks applied similar tasks however many applications starting points available cases automated algorithms ﬁnd useful values hyperparameters think way user learning algorithm searches good values hyperparameters realize optimization taking place trying ﬁnd value hyperparameters optimizes objective function validation error sometimes constraints budget training time memory recognition time therefore possible principle develop hyperparameter optimization algorithms wrap learning algorithm choose hyperparameters thus hiding hyperparameters learning algorithm user unfortunately hyperparameter optimization algorithms often hyperparameters range values explored learning algorithm hyperparameters however secondary hyperparameters usually easier choose sense acceptable performance may achieved wide range tasks using secondary hyperparameters tasks 
[practical, methodology, selecting, hyperparameters, grid, search] three fewer hyperparameters common practice perform grid search hyperparameter user selects small ﬁnite set values explore grid search algorithm trains model every joint speciﬁcation hyperparameter values cartesian product set values individual hyperparameter experiment yields best validation set chapter practical methodology grid random figure comparison grid search random search illustration purposes display two hyperparameters typically interested many left perform grid search provide set values hyperparameter search algorithm runs training every joint hyperparameter setting cross product sets perform random search provide probability distribution joint right hyperparameter conﬁgurations usually hyperparameters independent common choices distribution single hyperparameter include uniform log uniform sample log uniform distribution take exp sample uniform distribution search algorithm randomly samples joint hyperparameter conﬁgurations runs training grid search random search evaluate validation set error return best conﬁguration ﬁgure illustrates typical case hyperparameters signiﬁcant inﬂuence result illustration hyperparameter horizontal axis signiﬁcant eﬀect grid search wastes amount computation exponential number non inﬂuential hyperparameters random search tests unique value every inﬂuential hyperparameter nearly every trial chapter practical methodology error chosen found best hyperparameters see left fig illustration grid hyperparameter values lists values search chosen case numerical ordered hyperparameters smallest largest element list chosen conservatively based prior experience similar experiments make sure optimal value likely selected range typically grid search involves picking values approximately logarithmic scale learning rate taken within set number hidden units taken set grid search usually performs best performed repeatedly example suppose ran grid search hyperparameter using values best value found underestimated range best lies shift grid run another search example ﬁnd best value may wish reﬁne estimate zooming running grid search obvious problem grid search computational cost grows exponentially number hyperparameters hyperparameters taking values number training evaluation trials required grows trials may run parallel exploit loose parallelism almost need communication diﬀerent machines carrying search unfortunately due exponential cost grid search even parallelization may provide satisfactory size search 
[practical, methodology, selecting, hyperparameters, random, search] fortunately alternative grid search simple program convenient use converges much faster good values hyperparameters random search bergstra bengio random search proceeds follows first deﬁne marginal distribution hyperparameter bernoulli multinoulli binary discrete hyperparameters uniform distribution log scale positive real valued hyperparameters example log learning rate learning rate log learning rate indicates sample uniform distribution interval similarly log number hidden units may sampled log log chapter practical methodology unlike case grid search one discretize bin values hyperparameters allows one explore larger set values incur additional computational cost fact illustrated fig random search exponentially eﬃcient grid search several hyperparameters strongly aﬀect performance measure studied length found random bergstra bengio search reduces validation set error much faster grid search terms number trials run method grid search one may often want run repeated versions random search reﬁne search based results ﬁrst run main reason random search ﬁnds good solutions faster grid search wasted experimental runs unlike case grid search two values hyperparameter given values hyperparameters would give result case grid search hyperparameters would values two runs whereas random search would usually diﬀerent values hence change two values marginally make much diﬀerence terms validation set error grid search unnecessarily repeat two equivalent experiments random search still give two independent explorations hyperparameters 
[practical, methodology, selecting, hyperparameters, model-based, hyperparameter, optimization] search good hyperparameters cast optimization problem decision variables hyperparameters cost optimized validation set error results training using hyperparameters simpliﬁed settings feasible compute gradient diﬀerentiable error measure validation set respect hyperparameters simply follow gradient bengio bengio maclaurin unfortunately practical settings gradient unavailable either due high computation memory cost due hyperparameters intrinsically non diﬀerentiable interactions validation set error case discrete valued hyperparameters compensate lack gradient build model validation set error propose new hyperparameter guesses performing optimization within model model based algorithms hyperparameter search use bayesian regression model estimate expected value validation set error hyperparameter uncertainty around expectation opti mization thus involves tradeoﬀ exploration proposing hyperparameters chapter practical methodology high uncertainty may lead large improvement may also perform poorly exploitation proposing hyperparameters model conﬁdent perform well hyperparameters seen far usually hyperparameters similar ones seen contemporary approaches hyperparameter optimization include spearmint snoek tpe smac bergstra hutter currently cannot unambiguously recommend bayesian hyperparameter optimization established tool achieving better deep learning results obtaining results less eﬀort bayesian hyperparameter optimization sometimes performs comparably human experts sometimes better fails catastrophically problems may worth trying see works particular problem yet suﬃciently mature reliable said hyperparameter optimization important ﬁeld research often driven primarily needs deep learning holds potential beneﬁt entire ﬁeld machine learning discipline engineering general one drawback common hyperparameter optimization algorithms sophistication random search require training periment run completion able extract information experiment much less eﬃcient sense much infor mation gleaned early experiment manual search human practitioner since one usually tell early set hyperparameters completely pathological introduced early version swersky algorithm maintains set multiple experiments various time points hyperparameter optimization algorithm choose begin new experiment freeze running experiment promising thaw resume experiment earlier frozen appears promising given information 
[practical, methodology, debugging, strategies] machine learning system performs poorly usually diﬃcult tell whether poor performance intrinsic algorithm whether bug implementation algorithm machine learning systems diﬃcult debug variety reasons cases know priori intended behavior algorithm fact entire point using machine learning discover useful behavior able specify train chapter practical methodology neural network classiﬁcation task achieves test error new straightforward way knowing expected behavior sub optimal behavior diﬃculty machine learning models multiple parts adaptive one part broken parts adapt still achieve roughly acceptable performance example suppose training neural net several layers parametrized weights biases suppose manually implemented gradient descent rule parameter separately made error update biases learning rate erroneous update use gradient causes biases constantly become negative throughout learning clearly correct implementation reasonable learning algorithm bug may apparent examining output model though depending distribution input weights may able adapt compensate negative biases debugging strategies neural nets designed get around one two diﬃculties either design case simple correct behavior actually predicted design test exercises one part neural net implementation isolation important debugging tests include visualize model action training model detect objects images view images detections proposed model displayed superimposed image training generative model speech listen speech samples produces may seem obvious easy fall practice looking quantitative performance measurements like accuracy log likelihood directly observing machine learning model performing task help determine whether quantitative performance numbers achieves seem reasonable evaluation bugs devastating bugs mislead believing system performing well visualize worst mistakes models able output sort conﬁdence measure task perform example classiﬁers based softmax output layer assign probability class probability assigned likely class thus gives estimate conﬁdence model classiﬁcation decision typically maximum likelihood training results values overestimates rather accurate probabilities correct prediction chapter practical methodology somewhat useful sense examples actually less likely correctly labeled receive smaller probabilities model viewing training set examples hardest model correctly one often discover problems way data preprocessed labeled example street view transcription system originally problem address number detection system would crop image tightly omit digits transcription network assigned low probability correct answer images sorting images identify conﬁdent mistakes showed systematic problem cropping modifying detection system crop much wider images resulted much better performance overall system even though transcription network needed able process greater variation position scale address numbers reasoning software using train test error often diﬃcult determine whether underlying software correctly implemented clues obtained train test error training error low test error high likely training procedure works correctly model overﬁtting fundamental algorithmic reasons alternative possibility test error measured incorrectly due problem saving model training reloading test set evaluation test data prepared diﬀerently training data train test error high diﬃcult determine whether software defect whether model underﬁtting due fundamental algorithmic reasons scenario requires tests described next fit tiny dataset high error training set determine whether due genuine underﬁtting due software defect usually even small models guaranteed able suﬃciently small dataset example classiﬁcation dataset one example setting biases output layer correctly usually cannot train classiﬁer correctly label single example autoencoder successfully reproduce single example high ﬁdelity generative model consistently emit samples resembling single example software defect preventing successful optimization training set test extended small dataset examples compare back propagated derivatives numerical derivatives using software framework requires implement gradient com putations adding new operation diﬀerentiation library must deﬁne bprop method common source error implementing gradient expression incorrectly one way verify derivatives correct chapter practical methodology compare derivatives computed implementation automatic diﬀerentiation derivatives computed ﬁnite diﬀerences lim approximate derivative using small ﬁnite improve accuracy approximation using centered diﬀerence perturbation size must chosen large enough ensure pertur bation rounded much ﬁnite precision numerical computations usually want test gradient jacobian vector valued function unfortunately ﬁnite diﬀerencing allows take single derivative time either run ﬁnite diﬀerencing times evaluate partial derivatives apply test new function uses random projections input output example apply test implementation derivatives randomly chosen vectors computing correctly requires able back propagate correctly yet eﬃcient ﬁnite diﬀerences single input single output usually good idea repeat test one value reduce chance test overlooks mistakes orthogonal random projection one access numerical computation complex numbers eﬃcient way numerically estimate gradient using complex numbers input function squire trapp method based observation  real imag unlike real valued case cancellation eﬀect due taking diﬀerence value diﬀerent points allows use tiny values like make error insigniﬁcant practical purposes chapter practical methodology monitor histograms activations gradient often useful visualize statistics neural network activations gradients collected large amount training iterations maybe one epoch pre activation value hidden units tell units saturate often example rectiﬁers often units always tanh units average absolute value pre activations tells saturated unit deep network propagated gradients quickly grow quickly vanish optimization may hampered finally useful compare magnitude parameter gradients magnitude parameters suggested would like magnitude parameter updates bottou minibatch represent something like magnitude parameter would make parameters move slowly may groups parameters moving good pace others stalled data sparse like natural language parameters may rarely updated kept mind monitoring evolution finally many deep learning algorithms provide sort guarantee results produced step example part see iii approximate inference algorithms work using algebraic solutions timization problems typically debugged testing guarantees guarantees optimization algorithms oﬀer include objective function never increase one step algorithm gradient respect subset variables zero step algorithm gradient respect variables zero convergence usually due rounding error conditions hold exactly digital computer debugging test include tolerance parameter 
[practical, methodology, example, multi-digit, number, recognition] provide end end description apply design methodology practice present brief account street view transcription system point view designing deep learning components obviously many components complete system street view cars database infrastructure paramount importance point view machine learning task process began data collection cars collected raw data human operators provided labels transcription task preceded signiﬁcant amount dataset chapter practical methodology curation including using machine learning techniques detect house numbers prior transcribing transcription project began choice performance metrics desired values metrics important general principle tailor choice metric business goals project maps useful high accuracy important set high accuracy requirement project speciﬁcally goal obtain human level accuracy level accuracy may always feasible obtain order reach level accuracy street view transcription system sacriﬁces coverage coverage thus became main performance metric optimized project accuracy held convolutional network improved became possible reduce conﬁdence threshold network refuses transcribe input eventually exceeding goal coverage choosing quantitative goals next step recommended methodol ogy rapidly establish sensible baseline system vision tasks means convolutional network rectiﬁed linear units transcription project began model time common convolutional network output sequence predictions order begin simplest possible baseline ﬁrst implementation output layer model consisted diﬀerent softmax units predict sequence characters softmax units trained exactly task classiﬁcation softmax unit trained independently recommended methodology iteratively reﬁne baseline test whether change makes improvement ﬁrst change street view transcription system motivated theoretical understanding coverage metric structure data speciﬁcally network refuses classify input whenever probability output sequence threshold initially deﬁnition hoc based simply multiplying softmax outputs together motivated development specialized output layer cost function actually computed principled log likelihood approach allowed example rejection mechanism function much eﬀectively point coverage still yet obvious theoretical problems approach methodology therefore suggests instrument train test set performance order determine whether problem underﬁtting overﬁtting case train test set error nearly identical indeed main reason project proceeded smoothly availability dataset tens millions labeled examples train chapter practical methodology test set error similar suggested problem either due underﬁtting due problem training data one debugging strategies recommend visualize model worst errors case meant visualizing incorrect training set transcriptions model gave highest conﬁdence proved mostly consist examples input image cropped tightly digits address removed cropping operation example photo address might cropped tightly remaining visible problem could resolved spending weeks improving accuracy address number detection system responsible determining cropping regions instead team took much practical decision simply expand width crop region systematically wider address number detection system predicted single change added ten percentage points transcription system coverage finally last percentage points performance came adjusting hyperparameters mostly consisted making model larger main taining restrictions computational cost train test error remained roughly equal always clear performance deﬁcits due underﬁtting well due remaining problems dataset overall transcription project great success allowed hundreds millions addresses transcribed faster lower cost would possible via human eﬀort hope design principles described chapter lead many similar successes 
[applications] chapter describe use deep learning solve applications com puter vision speech recognition natural language processing application areas commercial interest begin discussing large scale neural network implementations required serious applications next review several speciﬁc application areas deep learning used solve one goal deep learning design algorithms capable solving broad variety tasks far degree specialization needed example vision tasks require processing large number input features pixels per example language tasks require modeling large number possible values words vocabulary per input feature 
[applications, large, scale, deep, learning] deep learning based philosophy connectionism individual biological neuron individual feature machine learning model intelligent large population neurons features acting together exhibit intelligent behavior truly important emphasize fact number neurons must large one key factors responsible improvement neural network accuracy improvement complexity tasks solve today dramatic increase size networks use saw sec network sizes grown exponentially past three decades yet artiﬁcial neural networks large nervous systems insects size neural networks paramount importance deep learning chapter applications requires high performance hardware software infrastructure 
[applications, large, scale, deep, learning, fast, cpu, implementations] traditionally neural networks trained using cpu single machine today approach generally considered insuﬃcient mostly use gpu computing cpus many machines networked together moving expensive setups researchers worked hard demonstrate cpus could manage high computational workload required neural networks description implement eﬃcient numerical cpu code beyond scope book emphasize careful implementation speciﬁc cpu families yield large improvements example best cpus available could run neural network workloads faster using ﬁxed point arithmetic rather ﬂoating point arithmetic creating carefully tuned ﬁxed point implementation vanhoucke obtained speedup strong ﬂoating point system new model cpu diﬀerent performance characteristics sometimes ﬂoating point implementations faster important principle careful specialization numerical computation routines yield large payoﬀ strategies besides choosing whether use ﬁxed ﬂoating point include optimizing data structures avoid cache misses using vector instructions many machine learning researchers neglect implementation details performance implementation restricts size model accuracy model suﬀers 
[applications, large, scale, deep, learning, gpu, implementations] modern neural network implementations based graphics processing units graphics processing units gpus specialized hardware components originally developed graphics applications consumer market video gaming systems spurred development graphics processing hardware performance characteristics needed good video gaming systems turn beneﬁcial neural networks well video game rendering requires performing many operations parallel quickly models characters environments speciﬁed terms lists coordinates vertices graphics cards must perform matrix multiplication division many vertices parallel convert coordinates screen coordinates graphics card must perform many computations pixel parallel determine color pixel cases chapter applications computations fairly simple involve much branching compared computational workload cpu usually encounters example vertex rigid object multiplied matrix need evaluate statement per vertex determine matrix multiply computations also entirely independent thus may parallelized easily computations also involve processing massive buﬀers memory containing bitmaps describing texture color pattern object rendered together results graphics cards designed high degree parallelism high memory bandwidth cost lower clock speed less branching capability relative traditional cpus neural network algorithms require performance characteristics real time graphics algorithms described neural networks usually involve large numerous buﬀers parameters activation values gradient values must completely updated every step training buﬀers large enough fall outside cache traditional desktop computer memory bandwidth system often becomes rate limiting factor gpus oﬀer compelling advantage cpus due high memory bandwidth neural network training algorithms typically involve much branching sophisticated control appropriate gpu hardware since neural networks divided multiple individual neurons processed independently neurons layer neural networks easily beneﬁt parallelism gpu computing gpu hardware originally specialized could used graphics tasks time gpu hardware became ﬂexible allowing custom subroutines used transform coordinates vertices assign colors pixels principle requirement pixel values actually based rendering task gpus could used scientiﬁc computing writing output computation buﬀer pixel values steinkrau implemented two layer fully connected neural network gpu reported speedup cpu based baseline shortly thereafter chellapilla demonstrated technique could used accelerate supervised convolutional networks popularity graphics cards neural network training exploded advent general purpose gpus gpus could execute arbitrary code rendering subroutines nvidia cuda programming language provided way write arbitrary code like language relatively convenient programming model massive parallelism high memory bandwidth chapter applications gpus oﬀer ideal platform neural network programming platform rapidly adopted deep learning researchers soon became available raina ciresan writing eﬃcient code gpus remains diﬃcult task best left spe cialists techniques required obtain good performance gpu diﬀerent used cpu example good cpu based code usually designed read information cache much possible gpu writable memory locations cached actually faster compute value twice rather compute read back memory gpu code also inherently multi threaded diﬀerent threads must coordinated carefully example memory operations faster coalesced coalesced reads writes occur several threads read write value need simultaneously part single memory transaction diﬀerent models gpus able coalesce diﬀerent kinds read write patterns typically memory operations easier coalesce among threads thread accesses byte memory multiple power exact speciﬁcations diﬀer models gpu another common consideration gpus making sure thread group executes instruction simultaneously means branching diﬃcult gpu threads divided small groups called thread warp warps executes instruction cycle diﬀerent threads within warp need execute diﬀerent code paths diﬀerent code paths must traversed sequentially rather parallel due diﬃculty writing high performance gpu code researchers structure workﬂow avoid needing write new gpu code order test new models algorithms typically one building software library high performance operations like convolution matrix multiplication specifying models terms calls library operations example machine learning library pylearn goodfellow speciﬁes machine learning algorithms terms calls theano bergstra bastien cuda convnet provide krizhevsky high performance operations factored approach also ease support multiple kinds hardware example theano program run either cpu gpu without needing change calls theano libraries like tensorflow torch abadi collobert provide similar features chapter applications 
[applications, large, scale, deep, learning, large, scale, distributed, implementations] many cases computational resources available single machine insuﬃcient therefore want distribute workload training inference across many machines distributing inference simple input example want process run separate machine known data parallelism also possible get model parallelism multiple machines work together single datapoint machine running diﬀerent part model feasible inference training data parallelism training somewhat harder increase size minibatch used single sgd step usually get less linear returns terms optimization performance would better allow multiple machines compute multiple gradient descent steps parallel unfortunately standard deﬁnition gradient descent completely sequential algorithm gradient step function parameters produced step solved using asynchronous stochastic gradient descent bengio recht approach several processor cores share memory representing parameters core reads parameters without lock computes gradient increments parameters without lock reduces average amount improvement gradient descent step yields cores overwrite progress increased rate production steps causes learning process faster overall dean pioneered multi machine implementation lock free approach gradient descent parameters managed parameter server rather stored shared memory distributed asynchronous gradient descent remains primary strategy training large deep networks used major deep learning groups industry chilimbi academic deep learning researchers typically cannot aﬀord scale distributed learning systems research focused build distributed networks relatively low cost hardware available university setting coates 
[applications, large, scale, deep, learning, model, compression] many commercial applications much important time memory cost running inference machine learning model low time memory cost training low applications require chapter applications personalization possible train model deploy used billions users many cases end user resource constrained developer example one might train speech recognition network powerful computer cluster deploy mobile phones key strategy reducing cost inference model compression buciluˇa basic idea model compression replace original expensive model smaller model requires less memory runtime store evaluate model compression applicable size original model driven primarily need prevent overﬁtting cases model lowest generalization error ensemble several independently trained models evaluating ensemble members expensive sometimes even single model generalizes better large example regularized dropout large models learn function using many parameters necessary task size necessary due limited number training examples soon function generate training set containing inﬁnitely many examples simply applying randomly sampled points train new smaller model match points order eﬃciently use capacity new small model best sample new points distribution resembling actual test inputs supplied model later done corrupting training examples drawing points generative model trained original training set alternatively one train smaller model original training points train copy features model posterior distribution incorrect classes hinton 
[applications, large, scale, deep, learning, dynamic, structure] one strategy accelerating data processing systems general build systems dynamic structure graph describing computation needed process input data processing systems dynamically determine subset many neural networks run given input individual neural networks also exhibit dynamic structure internally determining subset features hidden units compute given information input form dynamic structure inside neural networks sometimes called conditional computation since many components bengio bengio architecture may relevant small amount possible inputs system chapter applications run faster computing features needed dynamic structure computations basic computer science principle applied generally throughout software engineering discipline simplest versions dynamic structure applied neural networks based determining subset group neural networks machine learning models applied particular input venerable strategy accelerating inference classiﬁer use cascade classiﬁers cascade strategy may applied goal detect presence rare object event know sure object present must use sophisticated classiﬁer high capacity expensive run however object rare usually use much less computation reject inputs containing object situations train sequence classiﬁers ﬁrst classiﬁers sequence low capacity trained high recall words trained make sure wrongly reject input object present ﬁnal classiﬁer trained high precision test time run inference running classiﬁers sequence abandoning example soon one element cascade rejects overall allows verify presence objects high conﬁdence using high capacity model force pay cost full inference every example two diﬀerent ways cascade achieve high capacity one way make later members cascade individually high capacity case system whole obviously high capacity individual members also possible make cascade every individual model low capacity system whole high capacity due combination many small models viola jones used cascade boosted decision trees implement fast robust face detector suitable use handheld digital cameras classiﬁer localizes face using essentially sliding window approach many windows examined rejected contain faces another version cascades uses earlier models implement sort hard attention mechanism early members cascade localize object later members cascade perform processing given location object example google transcribes address numbers street view imagery using two step cascade ﬁrst locates address number one machine learning model transcribes another goodfellow decision trees example dynamic structure node tree determines subtrees evaluated input simple way accomplish union deep learning dynamic structure chapter applications train decision tree node uses neural network make splitting decision though typically guo gelfand done primary goal accelerating inference computations spirit one use neural network called select gater one several expert networks used compute output given current input ﬁrst version idea called mixture experts nowlan jacobs gater outputs set probabilities weights obtained via softmax nonlinearity one per expert ﬁnal output obtained weighted combination output experts case use gater oﬀer reduction computational cost single expert chosen gater example obtain hard mixture experts considerably accelerate training collobert inference time strategy works well number gating decisions small combinatorial want select diﬀerent subsets units parameters possible use soft switch requires enumerating computing outputs gater conﬁgurations deal problem several approaches explored train combinatorial gaters experiment several estimators gradient bengio gating probabilities use bacon bengio reinforcement learning techniques policy gradient learn form conditional dropout blocks hidden units get actual reduction computational cost without impacting negatively quality approximation another kind dynamic structure switch hidden unit receive input diﬀerent units depending context dynamic routing approach interpreted attention mechanism olshausen far use hard switch proven eﬀective large scale applications contemporary approaches instead use weighted average many possible inputs thus achieve possible computational beneﬁts dynamic structure contemporary attention mechanisms described sec one major obstacle using dynamically structured systems decreased degree parallelism results system following diﬀerent code branches diﬀerent inputs means operations network described matrix multiplication batch convolution minibatch examples write specialized sub routines convolve example diﬀerent kernels multiply row design matrix diﬀerent set columns weights unfortunately specialized subroutines diﬃcult implement eﬃciently cpu implementations slow due lack cache coherence gpu implementations slow due lack coalesced chapter applications memory transactions need serialize warps members warp take diﬀerent branches cases issues mitigated partitioning examples groups take branch processing groups examples simultaneously acceptable strategy minimizing time required process ﬁxed amount examples oﬄine setting real time setting examples must processed continuously partitioning workload result load balancing issues example assign one machine process ﬁrst step cascade another machine process last step cascade ﬁrst tend overloaded last tend underloaded similar issues arise machine assigned implement diﬀerent nodes neural decision tree 
[applications, large, scale, deep, learning, specialized, hardware, implementations, deep, networks] since early days neural networks research hardware designers worked specialized hardware implementations could speed training inference neural network algorithms see early recent reviews specialized hardware deep networks lindsey lindblad beiu misra saha diﬀerent forms specialized hardware graf jackel mead ismail kim pham chen developed last decades either asics application speciﬁc inte grated circuit either digital based binary representations numbers analog graf jackel mead ismail based physical imple mentations continuous values voltages currents hybrid implementations combining digital analog components recent years ﬂexible fpga ﬁeld programmable gated array implementations particulars circuit written chip built developed though software implementations general purpose processing units cpus gpus typically use bits precision represent ﬂoating point numbers long known possible use less precision least inference time holt baker holi hwang presley haggard simard graf wawrzynek savich become pressing issue recent years deep learning gained popularity industrial products great impact faster hardware demonstrated gpus another factor motivates current research specialized hardware deep networks rate progress single cpu gpu core slowed recent improvements computing speed come parallelization across cores either cpus chapter applications gpus diﬀerent situation previous neural network era hardware implementations neural networks might take two years inception availability chip could keep rapid progress low prices general purpose cpus building specialized hardware thus way push envelope time new hardware designs developed low power devices phones aiming general public applications deep learning speech computer vision natural language recent work low precision implementations backprop based neural nets vanhoucke courbariaux gupta suggests bits precision suﬃce using training deep neural networks back propagation clear precision required training inference time forms dynamic ﬁxed point representation numbers used reduce many bits required per number traditional ﬁxed point numbers restricted ﬁxed range corresponds given exponent ﬂoating point representation dynamic ﬁxed point representations share range among set numbers weights one layer using ﬁxed point rather ﬂoating point representations using less bits per number reduces hardware surface area power requirements computing time needed performing multiplications multiplications demanding operations needed use train modern deep network backprop 
[applications, computer, vision] computer vision traditionally one active research areas deep learning applications vision task eﬀortless humans many animals challenging computers many ballard popular standard benchmark tasks deep learning algorithms forms object recognition optical character recognition computer vision broad ﬁeld encompassing wide variety ways processing images amazing diversity applications applications computer vision range reproducing human visual abilities recognizing faces creating entirely new categories visual abilities example latter category one recent computer vision application recognize sound waves vibrations induce objects visible video davis deep learning research computer vision focused exotic applications expand realm possible imagery chapter applications rather small core goals aimed replicating human abilities deep learning computer vision used object recognition detection form whether means reporting object present image annotating image bounding boxes around object transcribing sequence symbols image labeling pixel image identity object belongs generative modeling guiding principle deep learning research also large body work image synthesis using deep models image synthesis usually considered nihilo computer vision endeavor models capable image synthesis usually useful image restoration computer vision task involving repairing defects images removing objects images 
[applications, computer, vision, preprocessing] many application areas require sophisticated preprocessing original input comes form diﬃcult many deep learning architectures represent computer vision usually requires relatively little kind prepro cessing images standardized pixels lie reasonable range like mixing images lie images lie usually result failure formatting images scale kind preprocessing strictly necessary many computer vision architectures require images standard size images must cropped scaled size however even rescaling always strictly necessary convolutional models accept variably sized inputs dynamically adjust size pooling regions keep output size constant waibel convolutional models variable sized output automatically scales size input models denoise label pixel image hadsell dataset augmentation may seen way preprocessing training set dataset augmentation excellent way reduce generalization error computer vision models related idea applicable test time show model many diﬀerent versions input example image cropped slightly diﬀerent locations diﬀerent instantiations model vote determine output latter idea interpreted ensemble approach helps reduce generalization error kinds preprocessing applied train test set goal putting example canonical form order reduce amount variation model needs account reducing amount variation data reduce generalization error reduce size chapter applications model needed training set simpler tasks may solved smaller models simpler solutions likely generalize well preprocessing kind usually designed remove kind variability input data easy human designer describe human designer conﬁdent relevance task training large datasets large models kind preprocessing often unnecessary best let model learn kinds variability become invariant example alexnet system classifying imagenet one preprocessing step subtracting mean across training examples pixel krizhevsky contrast normalization one obvious sources variation safely removed many tasks amount contrast image contrast simply refers magnitude diﬀerence bright dark pixels image many ways quantifying contrast image context deep learning contrast usually refers standard deviation pixels image region image suppose image represented tensor red intensity row column giving green intensity giving blue intensity contrast entire image given mean intensity entire image global contrast normalization gcn aims prevent images varying amounts contrast subtracting mean image rescaling standard deviation across pixels equal constant approach complicated fact scaling factor change contrast zero contrast image one whose pixels equal intensity images low non zero contrast often little information content dividing true standard deviation usually accomplishes nothing amplifying sensor noise compression artifacts cases chapter applications motivates introducing small positive regularization parameter bias estimate standard deviation alternately one constrain denominator least given input image gcn produces output image deﬁned max datasets consisting large images cropped interesting objects unlikely contain images nearly constant intensity cases safe practically ignore small denominator problem setting avoid division extremely rare cases setting extremely low value like approach used cifar goodfellow dataset small images cropped randomly likely nearly constant intensity making aggressive regularization useful used coates small randomly selected patches drawn cifar scale parameter usually set done coates chosen make individual pixel standard deviation across examples close done goodfellow standard deviation rescaling norm image assuming mean image already removed preferable deﬁne gcn terms standard deviation rather norm standard deviation includes division number pixels gcn based standard deviation allows used regardless image size however observation norm proportional standard deviation help build useful intuition one understand gcn mapping examples spherical shell see fig illustration useful property neural networks often better responding directions space rather exact locations responding multiple distances direction requires hidden units collinear weight vectors diﬀerent biases coordination diﬃcult learning algorithm discover additionally many shallow graphical models problems representing multiple separated modes along line gcn avoids problems reducing example direction rather direction distance counterintuitively preprocessing operation known sphering operation gcn sphering refer making data lie spherical shell rather rescaling principal components equal variance multivariate normal distribution used pca spherical contours sphering commonly known whitening chapter applications raw input gcn gcn figure gcn maps examples onto sphere left raw input data may norm gcn center maps non zero examples perfectly onto sphere use use gcn based normalizing standard deviation rather norm resulting sphere unit sphere right regularized gcn draws examples toward sphere completely discard variation norm leave global contrast normalization often fail highlight image features would like stand edges corners scene large dark area large bright area city square half image shadow building global contrast normalization ensure large diﬀerence brightness dark area brightness light area however ensure edges within dark region stand motivates local contrast normalization local contrast normalization ensures contrast normalized across small window rather image whole see fig comparison global local contrast normalization various deﬁnitions local contrast normalization possible cases one modiﬁes pixel subtracting mean nearby pixels dividing standard deviation nearby pixels cases literally mean standard deviation pixels rectangular window centered pixel modiﬁed cases weighted mean pinto weighted standard deviation using gaussian weights centered pixel modiﬁed case color images strategies process diﬀerent color channels separately others combine information diﬀerent channels normalize pixel sermanet chapter applications input image gcn lcn figure comparison global local contrast normalization visually eﬀects global contrast normalization subtle places images roughly scale reduces burden learning algorithm handle multiple scales local contrast normalization modiﬁes image much discarding regions constant intensity allows model focus edges regions ﬁne texture houses second row may lose detail due bandwidth normalization kernel high local contrast normalization usually implemented eﬃciently using separable convolution see sec compute feature maps local means local standard deviations using element wise subtraction element wise division diﬀerent feature maps local contrast normalization diﬀerentiable operation also used nonlinearity applied hidden layers network well preprocessing operation applied input global contrast normalization typically need regularize local contrast normalization avoid division zero fact local contrast normalization typically acts smaller windows even important regularize smaller windows likely contain values nearly thus likely zero standard deviation dataset augmentation described sec easy improve generalization classiﬁer increasing size training set adding extra copies training examples modiﬁed transformations change chapter applications class object recognition classiﬁcation task especially amenable form dataset augmentation class invariant many transformations input easily transformed many geometric operations described classiﬁers beneﬁt random translations rotations cases ﬂips input augment dataset specialized computer vision applications advanced transformations commonly used dataset augmentation schemes include random perturbation colors image nonlinear geometric distortions krizhevsky input lecun 
[applications, speech, recognition] task speech recognition map acoustic signal containing spoken natural language utterance corresponding sequence words intended speaker let denote sequence acoustic input vectors traditionally produced splitting audio frames speech recognition systems preprocess input using specialized hand designed features deep learning systems learn features jaitly hinton raw input let denote target output sequence usually sequence words characters automatic speech recognition asr task consists creating function asr computes probable linguistic sequence given acoustic sequence asr arg max true conditional distribution relating inputs targets since state art speech recognition systems primarily combined hidden markov models hmms gaussian mixture models gmms gmms modeled association acoustic features phonemes hmms modeled sequence phonemes bahl gmm hmm model family treats acoustic waveforms generated following process ﬁrst hmm generates sequence phonemes discrete sub phonemic states beginning middle end phoneme gmm transforms discrete symbol brief segment audio waveform although gmm hmm systems dominated asr recently speech recognition actually one ﬁrst areas neural networks applied numerous asr systems late early used chapter applications neural nets bourlard wellekens waibel robinson fallside bengio konig time performance asr based neural nets approximately matched performance gmm hmm systems example robinson fallside achieved phoneme error rate timit corpus garofolo phonemes discriminate better comparable hmm based systems since timit benchmark phoneme recognition playing role similar role mnist plays object recognition however complex engineering involved software systems speech recognition eﬀort invested building systems basis gmm hmms industry see compelling argument switching neural networks consequence late academic industrial research using neural nets speech recognition mostly focused using neural nets learn extra features gmm hmm systems later much larger deeper models much larger datasets recognition accuracy dramatically improved using neural networks replace gmms task associating acoustic features phonemes sub phonemic states starting speech researchers applied form deep learning based unsupervised learning speech recognition approach deep learning based training undirected probabilistic models called restricted boltzmann machines rbms model input data rbms described part solve speech recognition tasks unsupervised pretraining iii used build deep feedforward networks whose layers initialized training rbm networks take spectral acoustic representations ﬁxed size input window around center frame predict conditional probabilities hmm states center frame training deep networks helped signiﬁcantly improve recognition rate timit mohamed bringing phoneme error rate see analysis reasons success mohamed models extensions basic phone recognition pipeline included addition speaker adaptive features reduced mohamed error rate quickly followed work expand architecture phoneme recognition timit focused large vocabulary speech recognition involves recognizing phonemes dahl also recognizing sequences words large vocabulary deep networks speech recognition eventually shifted based pretraining boltzmann machines based techniques rectiﬁed linear units dropout time several major zeiler dahl speech groups industry started exploring deep learning collaboration chapter applications academic researchers describe breakthroughs achieved hinton collaborators deployed products mobile phones later groups explored larger larger labeled datasets incorpo rated methods initializing training setting architecture deep nets realized unsupervised pretraining phase either unnecessary bring signiﬁcant improvement breakthroughs recognition performance word error rate speech recognition unprecedented around improvement following long period ten years error rates improve much traditional gmm hmm technology spite continuously growing size training sets see fig deng created rapid shift speech recognition community towards deep learning matter roughly two years industrial products speech recognition incorporated deep neural networks success spurred new wave research deep learning algorithms architectures asr still ongoing today one innovations use convolutional networks sainath replicate weights across time frequency improving earlier time delay neural networks replicated weights across time new two dimensional convolutional models regard input spectrogram one long vector image one axis corresponding time frequency spectral components another important push still ongoing towards end end deep learning speech recognition systems completely remove hmm ﬁrst major breakthrough direction came graves trained deep lstm rnn see sec using map inference frame phoneme alignment ctc framework lecun graves graves graves deep rnn state variables several layers time step giving unfolded graph two kinds depth ordinary depth due stack layers depth due time unfolding work brought phoneme error rate timit record low see pascanu chung variants deep rnns applied settings another contemporary step toward end end deep learning asr let system learn align acoustic level information phonetic level information chorowski chapter applications 
[applications, natural, language, processing] major advantage gram models neural networks gram models achieve high model capacity storing frequencies many tuples requiring little computation process example looking tuples match current context use hash tables trees access counts computation used grams almost independent capacity comparison doubling neural network number parameters typically also roughly doubles computation time exceptions include models avoid using parameters pass embedding layers index single embedding pass increase vocabulary size without increasing computation time per example models tiled convolutional networks add parameters reducing degree parameter sharing order maintain amount computation however typical neural network layers based matrix multiplication use amount computation proportional number parameters one easy way add capacity thus combine approaches ensemble consisting neural language model gram language model bengio ensemble technique reduce test error ensemble members make independent mistakes ﬁeld ensemble learning provides many ways combining ensemble members predictions including uniform weighting weights chosen validation set mikolov extended ensemble include two models large array models also possible pair neural network maximum entropy model train jointly mikolov approach viewed training neural network extra set inputs connected directly output connected part model extra inputs indicators presence particular grams input context variables high dimensional sparse increase model capacity huge new portion architecture contains parameters amount added computation needed process input minimal extra inputs sparse chapter applications 
[applications, natural, language, processing, neural, language, models] neural language models nlms class language model designed overcome curse dimensionality problem modeling natural language sequences using distributed representation words unlike class bengio based gram models neural language models able recognize two words chapter applications similar without losing ability encode word distinct neural language models share statistical strength one word context similar words contexts distributed representation model learns word enables sharing allowing model treat words features common similarly example word dog word cat map representations share many attributes sentences contain word cat inform predictions made model sentences contain word dog vice versa many attributes many ways generalization happen transferring information training sentence exponentially large number semantically related sentences curse dimensionality requires model generalize number sentences exponential sentence length model counters curse relating training sentence exponential number similar sentences sometimes call word representations word embeddings inter pretation view raw symbols points space dimension equal vocabulary size word representations embed points feature space lower dimension original space every word represented one hot vector every pair words euclidean distance embedding space words frequently appear similar contexts pair words sharing features learned model close often results words similar meanings neighbors fig zooms speciﬁc areas learned word embedding space show semantically similar words map representations close neural networks domains also deﬁne embeddings example hidden layer convolutional network provides image embedding usually nlp practitioners much interested idea embeddings natural language originally lie real valued vector space hidden layer provided qualitatively dramatic change way data represented basic idea using distributed representations improve models natural language processing restricted neural networks may also used graphical models distributed representations form multiple latent variables mnih hinton chapter applications canada europe ontario north english canadian union african africa british france russian china germany french assembly japan iraq south european figure two dimensional visualizations word embeddings obtained neural machine translation model zooming speciﬁc areas bahdanau semantically related words embedding vectors close countries appear left numbers right keep mind embeddings purpose visualization real applications embeddings typically higher dimensionality simultaneously capture many kinds similarity words 
[applications, natural, language, processing, high-dimensional, outputs] many natural language applications often want models produce words rather characters fundamental unit output large vocabularies computationally expensive represent output distribution choice word vocabulary size large many applications contains hundreds thousands words naive approach representing distribution apply aﬃne transformation hidden representation output space apply softmax function suppose vocabulary size weight matrix describing linear component aﬃne transformation large output dimension imposes high memory cost represent matrix high computational cost multiply softmax normalized across outputs necessary perform full matrix multiplication training time well test time cannot calculate dot product weight vector correct output high computational costs output layer thus arise training time compute likelihood gradient test time compute probabilities selected words specialized loss functions gradient computed eﬃciently vincent standard cross entropy loss applied traditional softmax output layer poses chapter applications many diﬃculties suppose top hidden layer used predict output probabilities parametrize transformation learned weights learned biases aﬃne softmax output layer performs following computations contains elements operation thousands hundreds thousands operation dominates computation neural language models use short list ﬁrst neural language models dealt high cost bengio using softmax large number output words limiting vocabulary size words schwenk gauvain schwenk built upon approach splitting vocabulary shortlist frequent words handled neural net tail rare words handled gram model able combine two predictions neural net also predict probability word appearing context belongs tail list may achieved adding extra sigmoid output unit provide estimate extra output used achieve estimate probability distribution words follows provided neural language model provided gram model slight modiﬁcation approach also work using extra output value neural language model softmax layer rather separate sigmoid unit obvious disadvantage short list approach potential gener alization advantage neural language models limited frequent words arguably least useful disadvantage stimulated exploration alternative methods deal high dimensional outputs described chapter applications hierarchical softmax classical approach reducing computational burden goodman high dimensional output layers large vocabulary sets decompose probabilities hierarchically instead necessitating number computations proportional also proportional number hidden units factor reduced low log bengio morin bengio introduced factorized approach context neural language models one think hierarchy building categories words categories categories words categories categories categories words etc nested categories form tree words leaves balanced tree tree depth log probability choosing word given product probabilities choosing branch leading word every node path root tree leaf containing word fig illustrates simple example also describe use mnih hinton multiple paths identify single word order better model words multiple meanings computing probability word involves summation paths lead word predict conditional probabilities required node tree typically use logistic regression model node tree provide context input models correct output encoded training set use supervised learning train logistic regression models typically done using standard cross entropy loss corresponding maximizing log likelihood correct sequence decisions output log likelihood computed eﬃciently low log rather gradients may also computed eﬃciently includes gradient respect output parameters also gradients respect hidden layer activations possible usually practical optimize tree structure minimize expected number computations tools information theory specify choose optimal binary code given relative frequencies words could structure tree number bits associated word approximately equal logarithm frequency word however practice computational savings typically worth eﬀort computation output probabilities one part total computation neural language model example suppose fully connected hidden layers width let weighted average number bits chapter applications figure illustration simple hierarchy word categories words organized three level hierarchy leaves tree represent actual speciﬁc words internal nodes represent groups words node indexed sequence binary decisions left right reach node root super class contains classes respectively contain sets words similarly super class contains classes respectively contain words tree suﬃciently balanced maximum depth number binary decisions order logarithm number words choice one words obtained log operations one nodes path root example computing probability word done multiplying three probabilities associated binary decisions move left right node path root node let binary decision traversing tree towards value probability sampling output decomposes product conditional probabilities using chain rule conditional probabilities node indexed preﬁx bits example node corresponds preﬁx probability decomposed follows chapter applications required identify word weighting given frequency words example number operations needed compute hidden activations grows output computations grow long reduce computation shrinking shrinking indeed often small size vocabulary rarely exceeds million words log possible reduce often much larger around rather carefully optimizing tree branching factor one instead deﬁne tree depth two branching factor tree corresponds simply deﬁning set mutually exclusive word classes simple approach based tree depth two captures computational beneﬁt hierarchical strategy one question remains somewhat open best deﬁne word classes deﬁne word hierarchy general early work used existing hierarchies hierarchy also learned ideally morin bengio jointly neural language model learning hierarchy diﬃcult exact optimization log likelihood appears intractable choice word hierarchy discrete one amenable gradient based optimization however one could use discrete optimization approximately optimize partition words word classes important advantage hierarchical softmax brings computa tional beneﬁts training time test time test time want compute probability speciﬁc words course computing probability words remain expensive even hierarchical softmax another important operation selecting likely word given context unfortunately tree structure provide eﬃcient exact solution problem disadvantage practice hierarchical softmax tends give worse test results sampling based methods describe next may due poor choice word classes importance sampling one way speed training neural language models avoid explicitly computing contribution gradient words appear next position every incorrect word low probability model computationally costly enumerate words instead possible sample subset words using notation introduced chapter applications gradient written follows log log softmax log log vector pre softmax activations scores one element per word ﬁrst term positive phase term pushing second term negative phase term pushing weight since negative phase term expectation estimate monte carlo sample however would require sampling model sampling model requires computing vocabulary precisely trying avoid instead sampling model one sample another distribution called proposal distribution denoted use appropriate weights correct bias introduced sampling wrong distribution bengio sénécal bengio sénécal application general technique called importance sampling described detail sec unfortunately even exact importance sampling eﬃcient requires computing weights computed scores computed solution adopted application called biased importance sampling importance weights normalized sum negative word sampled associated gradient weighted weights used give appropriate importance negative samples used form estimated negative phase contribution gradient unigram bigram distribution works well proposal distribution easy estimate parameters distribution data estimating parameters also possible sample distribution eﬃciently chapter applications importance sampling useful speeding models large softmax outputs generally useful accelerating training large sparse output layers output sparse vector rather choice example bag words bag words sparse vector indicates presence absence word vocabulary document alternately indicate number times word appears machine learning models emit sparse vectors expensive train variety reasons early learning model may actually choose make output truly sparse moreover loss function use training might naturally described terms comparing every element output every element target means always clear computational beneﬁt using sparse outputs model may choose make majority output non zero non zero values need compared corresponding training target even training target zero dauphin demonstrated models accelerated using importance sampling eﬃcient algorithm minimizes loss reconstruction positive words non zero target equal number negative words negative words chosen randomly using heuristic sample words likely mistaken bias introduced heuristic oversampling corrected using importance weights cases computational complexity gradient estimation output layer reduced proportional number negative samples rather proportional size output vector noise contrastive estimation ranking loss approaches based sampling proposed reduce computa tional cost training neural language models large vocabularies early example ranking loss proposed collobert weston views output neural language model word score tries make score correct word ranked high comparison scores ranking loss proposed max gradient zero term score observed word greater score negative word margin one issue criterion provide estimated conditional probabilities chapter applications useful applications including speech recognition text generation including conditional text generation tasks translation recently used training objective neural language model noise contrastive estimation introduced sec approach successfully applied neural language models mnih teh mnih kavukcuoglu 
[applications, natural, language, processing, neural, machine, translation] machine translation task reading sentence one natural language emitting sentence equivalent meaning another language machine translation systems often involve many components high level often one component proposes many candidate translations many translations grammatical due diﬀerences languages example many languages put adjectives nouns translated english directly yield phrases apple red proposal mechanism suggests many variants suggested translation ideally including red apple second component translation system language model evaluates proposed translations score red apple better apple red earliest use neural networks machine translation upgrade language model translation system using neural language model schwenk schwenk previously machine translation systems used gram model component gram based models used machine translation include traditional back gram models jelinek mercer katz chen goodman also maximum entropy language models aﬃne softmax layer berger predicts next word given presence frequent grams context traditional language models simply report probability natural language sentence machine translation involves producing output sentence given input sentence makes sense extend natural language model conditional described sec straightforward extend model deﬁnes marginal distribution variable deﬁne conditional distribution variable given context might single variable list variables beat state art statistical devlin machine translation benchmarks using mlp score phrase target language given phrase source language mlp estimates estimate formed mlp replaces estimate provided conditional gram models drawback mlp based approach requires sequences preprocessed ﬁxed length make translation ﬂexible would like use model accommodate variable length inputs variable length outputs rnn provides ability sec describes several ways constructing rnn represents conditional distribution sequence given input sec describes accomplish conditioning input sequence cases one model ﬁrst reads input sequence emits data structure summarizes input sequence call chapter applications decoder output object english sentence intermediate semantic representation source object french sentence image encoder figure encoder decoder architecture map back forth surface representation sequence words image semantic representation using output encoder data one modality encoder mapping french sentences hidden representations capturing meaning sentences input decoder another modality decoder mapping hidden representations capturing meaning sentences english train systems translate one modality another idea applied successfully machine translation also caption generation images summary context context may list vectors may vector tensor model reads input produce may rnn cho sutskever jean convolutional network kalchbrenner blunsom second model usually rnn reads context generates sentence target language general idea encoder decoder framework machine translation illustrated fig order generate entire sentence conditioned source sentence model must way represent entire source sentence earlier models able represent individual words phrases representation learning point view useful learn representation sentences meaning similar representations regardless whether written source language target language strategy explored ﬁrst using combination convolutions rnns kalchbrenner blunsom later work introduced use rnn scoring proposed translations generating translated sentences cho sutskever jean scaled models larger vocabularies chapter applications using attention mechanism aligning pieces data figure modern attention mechanism introduced bahdanau essentially weighted average context vector formed taking weighted average feature vectors weights applications feature vectors hidden units neural network may also raw input model weights produced model usually values interval intended concentrate around one weighted average approximates reading one speciﬁc time step precisely weights usually direct indexing cannot trained gradient descent using ﬁxed size representation capture semantic details cho sutskever however eﬃcient approach ﬁrst introduced attention mechanism used bahdanau think attention based system three components chapter applications process reads raw data source words source sentence converts distributed representations one feature vector associated word position list feature vectors storing output reader understood containing sequence facts memory retrieved later necessarily order without visit process content memory sequentially perform exploits task time step ability put attention content one memory element diﬀerent weight third component generates translated sentence words sentence written one language aligned correspond ing words translated sentence another language becomes possible relate corresponding word embeddings earlier work showed one could learn kind translation matrix relating word embeddings one language word embeddings another kočiský yielding lower alignment error rates traditional approaches based frequency counts phrase table even earlier work learning cross lingual word vectors klementiev many extensions approach possible example eﬃcient cross lingual alignment allows training larger datasets gouws 
[applications, natural, language, processing, historical, perspective] idea distributed representations symbols introduced rumelhart one ﬁrst explorations back propagation symbols corresponding identity family members neural network capturing relationships family members training examples forming triplets colin mother victoria ﬁrst layer neural network learned representation family member example features colin might represent family tree colin branch tree generation etc one think neural network computing learned rules relating attributes together order obtain desired predictions model make predictions inferring mother colin idea forming embedding symbol extended idea embedding word deerwester embeddings learned using svd later embeddings would learned neural networks chapter applications history natural language processing marked transitions popularity diﬀerent ways representing input model following early work symbols words earliest applications neural networks nlp miikkulainen dyer schmidhuber represented input sequence characters bengio returned focus modeling words introduced neural language models produce interpretable word embeddings neural models scaled deﬁning representations small set symbols millions words including proper nouns misspellings modern applications computational scaling eﬀort led invention techniques described sec initially use words fundamental units language models yielded improved language modeling performance day bengio new techniques continually push character based models sutskever word based models forward recent work even gillick modeling individual bytes unicode characters ideas behind neural language models extended several natural language processing applications parsing henderson collobert part speech tagging semantic role labeling chunking etc sometimes using single multi task learning architecture collobert weston collobert word embeddings shared across tasks two dimensional visualizations embeddings became popular tool alyzing language models following development sne dimensionality reduction algorithm van der maaten hinton high proﬁle appli cation visualization word embeddings joseph turian 
[applications, applications] section cover types applications deep learning diﬀerent standard object recognition speech recognition natural language processing tasks discussed part book expand iii scope even include tasks requiring ability generate rich high dimensional samples unlike next word language models chapter applications 
[applications, applications, recommender, systems] one major families applications machine learning information technology sector ability make recommendations items potential users customers two major types applications distinguished online advertising item recommendations often recommendations still purpose selling product rely predicting association user item either predict probability action user buying product proxy action expected gain may depend value product shown recommendation made regarding product user internet currently ﬁnanced great part various forms online advertising major parts economy rely online shopping companies including amazon ebay use machine learning including deep learning product recommendations sometimes items products actually sale examples include selecting posts display social network news feeds recommending movies watch recommending jokes recommending advice experts matching players video games matching people dating services often association problem handled like supervised learning problem given information item user predict proxy interest user clicks user enters rating user clicks like button user buys product user spends amount money product user spends time visiting page product etc often ends either regression problem predicting conditional expected value probabilistic classiﬁcation problem predicting conditional probability discrete event early work recommender systems relied minimal information inputs predictions user item context way generalize rely similarity patterns values target variable diﬀerent users diﬀerent items suppose user user like items may infer user user similar tastes user likes item strong cue user also like algorithms based principle come name collaborative ﬁltering non parametric approaches nearest neighbor methods based estimated similarity patterns preferences parametric methods possible parametric methods often rely learning distributed representation also called embedding user item bilinear prediction target variable rating simple parametric method highly successful often found component chapter applications state art systems prediction obtained dot product user embedding item embedding possibly corrected constants depend either user item let matrix containing predictions matrix user embeddings rows matrix item embeddings columns let vectors contain respectively kind bias user representing grumpy positive user general item representing general popularity bilinear prediction thus obtained follows typically one wants minimize squared error predicted ratings actual ratings user embeddings item embeddings conveniently visualized ﬁrst reduced low dimension two three used compare users items like word embeddings one way obtain embeddings performing singular value decomposition matrix actual targets ratings corresponds factorizing normalized variant product two factors lower rank matrices one problem svd treats missing entries arbitrary way corresponded target value instead would like avoid paying cost predictions made missing entries fortunately sum squared errors observed ratings also easily minimized gradient based optimization svd bilinear prediction performed well competition netﬂix prize bennett lanning aiming predicting ratings ﬁlms based previous ratings large set anonymous users many machine learning experts participated competition took place raised level research recommender systems using advanced machine learning yielded improvements recommender systems even though win simple bilinear prediction svd component ensemble models presented competitors including winners töscher koren beyond bilinear models distributed representations one ﬁrst uses neural networks collaborative ﬁltering based rbm undirected probabilistic model salakhutdinov rbms important element ensemble methods netﬂix competition töscher koren advanced variants idea factorizing ratings matrix also explored neural networks community salakhutdinov chapter applications mnih however basic limitation collaborative ﬁltering systems new item new user introduced lack rating history means way evaluate similarity items users respectively degree association say new user existing items called problem cold start recommendations general way solving cold start recommendation problem introduce extra information individual users items example extra information could user proﬁle information features item systems use information called content based recommender systems mapping rich set user features item features embedding learned deep learning architecture huang elkahky specialized deep learning architectures convolutional networks also applied learn extract features rich content musical audio tracks music recommendation van den oörd work convolutional net takes acoustic features input computes embedding associated song dot product song embedding embedding user used predict whether user listen song exploration versus exploitation making recommendations users issue arises goes beyond ordinary supervised learning realm reinforcement learning many recommen dation problems accurately described theoretically contextual bandits issue use langford zhang recommendation system collect data get biased incomplete view preferences users see responses users items recommended items addition cases may get information users recommendation made example auctions may price proposed minimum price threshold win auction shown importantly get information outcome would resulted recommending items would like training classiﬁer picking one class training example typically class highest probability according model getting feedback whether correct class clearly example conveys less information supervised case true label directly accessible examples necessary worse careful could end system continues picking wrong decisions even chapter applications data collected correct decision initially low probability learner picks correct decision learn correct decision similar situation reinforcement learning reward selected action observed general reinforcement learning involve sequence many actions many rewards bandits scenario special case reinforcement learning learner takes single action receives single reward bandit problem easier sense learner knows reward associated action general reinforcement learning scenario high reward low reward might caused recent action action distant past term contextual bandits refers case action taken context input variable inform decision example least know user identity want pick item mapping context action also called policy feedback loop learner data distribution depends actions learner central research issue reinforcement learning bandits literature reinforcement learning requires choosing tradeoﬀ exploration exploitation exploitation refers taking actions come current best version learned policy actions know achieve high reward exploration refers taking actions speciﬁcally order obtain training data know given context action gives reward know whether best possible reward may want exploit current policy continue taking action order relatively sure obtaining reward however may also want explore trying action know happen try action hope get reward run risk getting reward either way least gain knowledge exploration implemented many ways ranging occasionally taking random actions intended cover entire space possible actions model based approaches compute choice action based expected reward model amount uncertainty reward many factors determine extent prefer exploration exploitation one prominent factors time scale interested agent short amount time accrue reward prefer exploitation agent long time accrue reward begin exploration future actions planned eﬀectively knowledge time progresses learned policy improves move toward exploitation supervised learning tradeoﬀ exploration exploitation chapter applications supervision signal always speciﬁes output correct input need try diﬀerent outputs determine one better model current output always know label best output another diﬃculty arising context reinforcement learning besides exploration exploitation trade diﬃculty evaluating comparing diﬀerent policies reinforcement learning involves interaction learner environment feedback loop means straightforward evaluate learner performance using ﬁxed set test set input values policy determines inputs seen present dudik techniques evaluating contextual bandits 
[applications, applications, knowledge, representation, reasoning, question, an-, swering] deep learning approaches successful language modeling machine translation natural language processing due use embeddings symbols words rumelhart deerwester bengio embeddings represent semantic knowledge individual words concepts research frontier develop embeddings phrases relations words facts search engines already use machine learning purpose much remains done improve advanced representations knowledge relations question answering indexrelations one interesting research direction determining distributed representations trained capture relations two entities relations allow formalize facts objects objects interact mathematics binary relation set ordered pairs objects pairs set said relation set example deﬁne relation less set entities deﬁning set ordered pairs relation deﬁned use like verb say less say less course entities related one another need numbers could deﬁne relation containing tuples like is_a_type_of dog mammal context think relation sentence syntactically chapter applications simple highly structured language relation plays role verb two arguments relation play role subject object sentences take form triplet tokens subject verb object values entity relation entity also deﬁne concept analogous relation taking attribute one argument entity attribute example could deﬁne has_fur attribute apply entities like dog many applications require representing relations reasoning best within context neural networks machine learning models course require training data infer relations entities training datasets consisting unstructured natural language also structured databases identify relations explicitly common structure databases relational database stores kind information albeit formatted three token sentences database intended convey commonsense knowledge everyday life expert knowledge application area artiﬁcial intelligence system call database knowledge base knowledge bases range general ones like freebase opencyc wordnet wikibase etc specialized knowledge bases like geneontology representations entities relations learned considering triplet knowledge base training example maximizing training objective captures joint distribution bordes addition training data also need deﬁne model family train common approach extend neural language models model entities relations neural language models learn vector provides distributed representation word also learn interactions words word likely come sequence words learning functions vectors extend approach entities relations learning embedding vector relation fact parallel modeling respectively available web sites freebase com cyc com opencyc wordnet princeton edu wikiba geneontology org chapter applications language modeling knowledge encoded relations close researchers trained representations entities using knowledge bases natural language sentences bordes wang combining data multiple relational databases many bordes possibilities exist particular parametrization associated model early work learning relations entities paccanaro hinton posited highly constrained parametric forms linear relational embeddings often using diﬀerent form representation relation entities example used vectors paccanaro hinton bordes entities matrices relations idea relation acts like operator entities alternatively relations considered entity bordes allowing make statements relations ﬂexibility put machinery combines order model joint distribution practical short term application models link prediction predicting missing arcs knowledge graph form generalization new facts based old facts knowledge bases currently exist constructed manual labor tends leave many probably majority true relations absent knowledge base see wang examples lin garcia duran application evaluating performance model link prediction task diﬃcult dataset positive examples facts known true model proposes fact dataset unsure whether model made mistake discovered new previously unknown fact metrics thus somewhat imprecise based testing model ranks held set known true positive facts compared facts less likely true common way construct interesting examples probably negative facts probably false begin true fact create corrupted versions fact example replacing one entity relation diﬀerent entity selected random popular precision metric counts many times model ranks correct fact among top corrupted versions fact another application knowledge bases distributed representations word sense disambiguation navigli velardi bordes task deciding senses word appropriate one context eventually knowledge relations combined reasoning process understanding natural language could allow build general question chapter applications answering system general question answering system must able process input information remember important facts organized way enables retrieve reason later remains diﬃcult open problem solved restricted toy environments currently best approach remembering retrieving speciﬁc declarative facts use explicit memory mechanism described sec memory networks ﬁrst proposed solve toy question answering task weston kumar proposed extension uses gru recurrent nets read input memory produce answer given contents memory deep learning applied many applications besides ones described surely applied even writing would impossible describe anything remotely resembling comprehensive coverage topic survey provides representative sample possible writing concludes part described modern practices involving deep networks comprising successful methods generally speaking methods involve using gradient cost function ﬁnd parameters model approximates desired function enough training data approach extremely powerful turn part step iii territory research methods designed work less training data perform greater variety tasks challenges diﬃcult close solved situations described far 
[deep, learning, research] part book describes ambitious advanced approaches deep learning currently pursued research community previous parts book shown solve supervised learning problems learn map one vector another given enough examples mapping problems might want solve fall category may wish generate new examples determine likely point handle missing values take advantage large set unlabeled examples examples related tasks shortcoming current state art industrial applications learning algorithms require large amounts supervised data achieve good accuracy part book discuss speculative approaches reducing amount labeled data necessary existing models work well applicable across broader range tasks accomplishing goals usually requires form unsupervised semi supervised learning many deep learning algorithms designed tackle unsupervised learning problems none truly solved problem way deep learning largely solved supervised learning problem wide variety tasks part book describe existing approaches unsupervised learning popular thought make progress ﬁeld central cause diﬃculties unsupervised learning high mensionality random variables modeled brings two distinct challenges statistical challenge computational challenge statistical challenge regards generalization number conﬁgurations may want distinguish grow exponentially number dimensions interest quickly becomes much larger number examples one possibly use bounded computational resources computational challenge associated high dimensional distributions arises many algorithms learning using trained model especially based estimating explicit probability function involve intractable computations grow exponentially number dimensions probabilistic models computational challenge arises need perform intractable inference simply need normalize distribution intractable inference inference discussed mostly chapter regards question guessing probable values variables given variables respect model captures joint distribution order even compute conditional probabilities one needs sum values variables well compute normalization constant sums values intractable normalization constants partition function partition function discussed mostly chapter normalizing constants probability functions come inference well learning many probabilistic models involve normalizing constant unfortu nately learning model often requires computing gradient logarithm partition function respect model parameters computation generally intractable computing partition function monte carlo markov chain mcmc methods chapter often used deal partition function computing gradi ent unfortunately mcmc methods suﬀer modes model distribution numerous well separated especially high dimensional spaces sec one way confront intractable computations approximate many approaches proposed discussed third part book another interesting way also discussed would avoid intractable computations altogether design methods require computations thus appealing several generative models proposed recent years motivation wide variety contemporary approaches generative modeling discussed chapter part important researcher someone wants iii derstand breadth perspectives brought ﬁeld deep learning push ﬁeld forward towards true artiﬁcial intelligence 
[linear, factor, models] many research frontiers deep learning involve building probabilistic model input model model principle use probabilistic inference predict variables environment given variables many models also latent variables model model latent variables provide another means representing data distributed representations based latent variables obtain advantages representation learning seen deep feedforward recurrent networks chapter describe simplest probabilistic models latent variables linear factor models models sometimes used building blocks mixture models hinton ghahramani hinton roweis tang larger deep probabilistic models also show many basic approaches necessary build generative models advanced deep models extend linear factor model deﬁned use stochastic linear decoder function generates adding noise linear transformation models interesting allow discover explanatory factors simple joint distribution simplicity using linear decoder made models ﬁrst latent variable models extensively studied linear factor model describes data generation process follows first sample explanatory factors distribution factorial distribution easy chapter linear factor models sample next sample real valued observable variables given factors noise noise typically gaussian diagonal independent across dimensions illustrated fig ois ois figure directed graphical model describing linear factor model family assume observed data vector obtained linear combination independent latent factors plus noise diﬀerent models probabilistic pca factor analysis ica make diﬀerent choices form noise prior 
[linear, factor, models, probabilistic, pca, factor, analysis] probabilistic pca principal components analysis factor analysis linear factor models special cases equations diﬀer choices made noise distribution model prior latent variables observing latent variable factor analysis bartholomew basilevsky prior unit variance gaussian observed variables assumed conditionally independent given speciﬁcally noise assumed drawn diagonal covariance gaussian distribution covariance matrix diag vector per variable variances role latent variables thus capture dependencies diﬀerent observed variables indeed easily shown multivariate normal random variable chapter linear factor models order cast pca probabilistic framework make slight modiﬁcation factor analysis model making conditional variances equal case covariance scalar yields conditional distribution equivalently gaussian noise show tipping bishop iterative algorithm estimating parameters probabilistic pca model takes advantage observation variations data captured latent variables small residual reconstruction error shown tipping bishop probabilistic pca becomes pca case conditional expected value given becomes orthogonal projection onto space spanned columns like pca density model deﬁned probabilistic pca becomes sharp around dimensions spanned columns make model assign low likelihood data data actually cluster near hyperplane 
[linear, factor, models, independent, component, analysis, ica] independent component analysis ica among oldest representation learning algorithms herault ans jutten herault comon hyvärinen hyvärinen hinton teh approach modeling linear factors seeks separate observed signal many underlying signals scaled added together form observed data signals intended fully independent rather merely decorrelated many diﬀerent speciﬁc methodologies referred ica variant similar generative models described variant trains fully parametric generative model pham prior distribution underlying factors must ﬁxed ahead time user model deterministically generates perform see sec discussion diﬀerence uncorrelated variables independent variables chapter linear factor models nonlinear change variables using determine learning model proceeds usual using maximum likelihood motivation approach choosing independent recover underlying factors close possible independent commonly used capture high level abstract causal factors recover low level signals mixed together setting training example one moment time one sensor observation mixed signals one estimate one original signals example might people speaking simultaneously diﬀerent microphones placed diﬀerent locations ica detect changes volume speaker heard microphone separate signals contains one person speaking clearly commonly used neuroscience electroencephalography technology recording electrical signals originating brain many electrode sensors placed subject head used measure many electrical signals coming body experimenter typically interested signals brain signals subject heart eyes strong enough confound measurements taken subject scalp signals arrive electrodes mixed together ica necessary separate electrical signature heart signals originating brain separate signals diﬀerent brain regions mentioned many variants ica possible add noise generation rather using deterministic decoder use maximum likelihood criterion instead aim make elements independent many criteria accomplish goal possible requires taking determinant expensive numerically unstable operation variants ica avoid problematic operation constraining orthonormal variants ica require non gaussian independent prior gaussian components identiﬁable obtain distribution many values diﬀerent linear factor models like probabilistic pca factor analysis often require gaussian order make many operations model closed form solutions maximum likelihood approach user explicitly speciﬁes distribution typical choice use typical choices non gaussian distributions larger peaks near gaussian distribution also see implementations ica learning sparse features chapter linear factor models many variants ica generative models sense use phrase book generative model either represents draw samples many variants ica know transform way representing thus impose distribution example many ica variants aim increase sample kurtosis high kurtosis indicates non gaussian accomplished without explicitly representing ica often used analysis tool separating signals rather generating data estimating density pca generalized nonlinear autoencoders described chapter ica generalized nonlinear generative model use nonlinear function generate observed data see hyvärinen pajunen initial work nonlinear ica successful use ensemble learning roberts everson lappalainen another nonlinear extension ica approach nonlinear independent components estimation nice stacks dinh series invertible transformations encoder stages property determinant jacobian transformation computed eﬃciently makes possible compute likelihood exactly like ica attempts transform data space factorized marginal distribution likely succeed thanks nonlinear encoder encoder associated decoder perfect inverse straightforward generate samples model ﬁrst sampling applying decoder another generalization ica learn groups features statistical dependence allowed within group discouraged groups hyvärinen hoyer hyvärinen groups related units chosen non overlapping called independent subspace analysis also possible assign spatial coordinates hidden unit form overlapping groups spatially neighboring units encourages nearby units learn similar features applied natural images topographic ica approach learns gabor ﬁlters neighboring features similar orientation location frequency many diﬀerent phase oﬀsets similar gabor functions occur within region pooling small regions yields translation invariance 
[linear, factor, models, slow, feature, analysis] slow feature analysis sfa linear factor model uses information chapter linear factor models time signals learn invariant features wiskott sejnowski slow feature analysis motivated general principle called slowness principle idea important characteristics scenes change slowly compared individual measurements make description scene example computer vision individual pixel values change rapidly zebra moves left right across image individual pixel rapidly change black white back zebra stripes pass pixel comparison feature indicating whether zebra image change feature describing zebra position change slowly therefore may wish regularize model learn features change slowly time slowness principle predates slow feature analysis applied wide variety models hinton földiák mobahi bergstra bengio general apply slowness principle diﬀerentiable model trained gradient descent slowness principle may introduced adding term cost function form hyperparameter determining strength slowness regularization term index time sequence examples feature extractor regularized loss function measuring distance common choice mean squared diﬀerence slow feature analysis particularly eﬃcient application slowness principle eﬃcient applied linear feature extractor thus trained closed form like variants ica sfa quite generative model per sense deﬁnes linear map input space feature space deﬁne prior feature space thus impose distribution input space sfa algorithm wiskott sejnowski consists deﬁning linear transformation solving optimization problem min subject constraints chapter linear factor models constraint learned feature zero mean necessary make problem unique solution otherwise could add constant feature values obtain diﬀerent solution equal value slowness objective constraint features unit variance necessary prevent pathological solution features collapse like pca sfa features ordered ﬁrst feature slowest learn multiple features must also add constraint speciﬁes learned features must linearly decorrelated without constraint learned features would simply capture one slowest signal one could imagine using mechanisms minimizing reconstruction error force features diversify decorrelation mechanism admits simple solution due linearity sfa features sfa problem may solved closed form linear algebra package sfa typically used learn nonlinear features applying nonlinear basis expansion running sfa example common replace quadratic basis expansion vector containing elements linear sfa modules may composed learn deep nonlinear slow feature extractors repeatedly learning linear sfa feature extractor applying nonlinear basis expansion output learning another linear sfa feature extractor top expansion trained small spatial patches videos natural scenes sfa quadratic basis expansions learns features share many characteristics complex cells cortex berkes wiskott trained videos random motion within computer rendered environments deep sfa learns features share many characteristics features represented neurons rat brains used navigation franzius sfa thus seems reasonably biologically plausible model major advantage sfa possibly theoretically predict features sfa learn even deep nonlinear setting make theoretical predictions one must know dynamics environment terms conﬁguration space case random motion rendered environment theoretical analysis proceeds knowledge probability distribution position velocity camera given knowledge underlying factors actually change possible analytically solve optimal functions expressing factors practice experiments deep sfa applied simulated data seem recover theoretically predicted functions chapter linear factor models comparison learning algorithms cost function depends highly speciﬁc pixel values making much diﬃcult determine features model learn deep sfa also used learn features object recognition pose estimation franzius far slowness principle become basis state art applications unclear factor limited performance speculate perhaps slowness prior strong rather imposing prior features approximately constant would better impose prior features easy predict one time step next position object useful feature regardless whether object velocity high low slowness principle encourages model ignore position objects high velocity 
[linear, factor, models, sparse, coding] sparse coding linear factor model olshausen field heavily studied unsupervised feature learning feature extraction mechanism strictly speaking term sparse coding refers process inferring value model sparse modeling refers process designing learning model term sparse coding often used refer like linear factor models uses linear decoder plus noise obtain reconstructions speciﬁed speciﬁcally sparse coding models typically assume linear factors gaussian noise isotropic precision distribution chosen one sharp peaks near olshausen field common choices include factorized laplace cauchy factorized student distributions example laplace prior parametrized terms sparsity penalty coeﬃcient given laplace student prior chapter linear factor models training sparse coding maximum likelihood intractable instead training alternates encoding data training decoder better reconstruct data given encoding approach justiﬁed principled approximation maximum likelihood later sec models pca seen use parametric encoder function predicts consists multiplication weight matrix encoder use sparse coding parametric encoder instead encoder optimization algorithm solves optimization problem seek single likely code value arg max combined yields following optimization problem arg max arg max log arg min dropped terms depending divided positive scaling factors simplify equation due imposition norm procedure yield sparse see sec train model rather perform inference alternate minimization respect minimization respect presentation treat hyperparameter typically set role optimization problem shared need hyperparameters principle could also treat parameter model learn presentation discarded terms depend depend learn terms must included collapse approaches sparse coding explicitly build often interested learning dictionary features activation values often zero extracted using inference procedure sample laplace prior fact zero probability event element actually zero generative model especially sparse feature extractor describe approximate goodfellow chapter linear factor models inference diﬀerent model family spike slab sparse coding model samples prior usually contain true zeros sparse coding approach combined use non parametric encoder principle minimize combination reconstruction error log prior better speciﬁc parametric encoder another advantage generalization error encoder parametric encoder must learn map way generalizes unusual resemble training data learned parametric encoder may fail ﬁnd results accurate reconstruction sparse code vast majority formulations sparse coding models inference problem convex optimization procedure always ﬁnd optimal code unless degenerate cases replicated weight vectors occur obviously sparsity reconstruction costs still rise unfamiliar points due generalization error decoder weights rather generalization error encoder lack generalization error sparse coding optimization based encoding process may result better generalization sparse coding used feature extractor classiﬁer parametric function used predict code coates demonstrated sparse coding features generalize better object recognition tasks features related model based parametric encoder linear sigmoid autoencoder inspired work goodfellow showed variant sparse coding generalizes better feature extractors regime extremely labels available twenty fewer labels per class primary disadvantage non parametric encoder requires greater time compute given non parametric approach requires running iterative algorithm parametric autoencoder approach developed chapter uses ﬁxed number layers often one another disadvantage straight forward back propagate non parametric encoder makes diﬃcult pretrain sparse coding model unsupervised criterion ﬁne tune using supervised criterion modiﬁed versions sparse coding permit approximate derivatives exist widely used bagnell bradley sparse coding like linear factor models often produces poor samples shown fig happens even model able reconstruct data well provide useful features classiﬁer reason individual feature may learned well factorial prior hidden code results model including random subsets features generated sample motivates development deeper models impose non chapter linear factor models figure example samples weights spike slab sparse coding model trained mnist dataset left samples model resemble training examples ﬁrst glance one might assume model poorly right weight vectors model learned represent penstrokes sometimes complete digits model thus learned useful features problem factorial prior features results random subsets features combined subsets appropriate form recognizable mnist digit motivates development generative models powerful distributions latent codes figure reproduced permission goodfellow factorial distribution deepest code layer well development sophisticated shallow models 
[linear, factor, models, manifold, interpretation, pca] linear factor models including pca factor analysis interpreted learning manifold view probabilistic pca hinton deﬁning thin pancake shaped region high probability gaussian distribution narrow along axes pancake ﬂat along vertical axis elongated along axes pancake wide along horizontal axes illustrated fig pca interpreted aligning pancake linear manifold higher dimensional space interpretation applies traditional pca also linear autoencoder learns matrices goal making reconstruction lie close possible let encoder chapter linear factor models encoder computes low dimensional representation autoencoder view decoder computing reconstruction figure flat gaussian capturing probability concentration near low dimensional manifold ﬁgure shows upper half pancake manifold plane goes middle variance direction orthogonal manifold small arrow pointing plane considered like noise variances large arrows plane correspond signal coordinate system reduced dimension data choices linear encoder decoder minimize reconstruction error correspond columns form orthonormal basis spans subspace principal eigenvectors covariance matrix case pca columns eigenvectors ordered magnitude corresponding eigenvalues real non negative one also show eigenvalue corresponds variance direction eigenvector chapter linear factor models optimal reconstruction error choosing min hence covariance rank eigenvalues recon struction error furthermore one also show solution obtained maximizing variances elements orthonormal instead minimizing reconstruction error linear factor models simplest generative models simplest models learn representation data much linear classiﬁers linear regression models may extended deep feedforward networks linear factor models may extended autoencoder networks deep probabilistic models perform tasks much powerful ﬂexible model family 
[autoencoders] autoencoder neural network trained attempt copy input output internally hidden layer describes code used represent input network may viewed consisting two parts encoder function decoder produces reconstruction architecture presented fig autoencoder succeeds simply learning set everywhere especially useful instead autoencoders designed unable learn copy perfectly usually restricted ways allow copy approximately copy input resembles training data model forced prioritize aspects input copied often learns useful properties data modern autoencoders generalized idea encoder coder beyond deterministic functions stochastic mappings encoder decoder idea autoencoders part historical landscape neural networks decades lecun bourlard kamp hinton zemel traditionally autoencoders used dimensionality reduction feature learning recently theoretical connections autoencoders latent variable models brought autoencoders forefront generative modeling see chapter autoencoders may thought special case feedforward networks may trained techniques typically minibatch gradient descent following gradients computed back propagation unlike general feedforward networks autoencoders may also trained using recirculation learning hinton mcclelland algorithm based comparing activations network original input chapter autoencoders activations reconstructed input recirculation regarded biologically plausible back propagation rarely used machine learning applications figure general structure autoencoder mapping input output called reconstruction internal representation code autoencoder two components encoder mapping decoder mapping 
[autoencoders, undercomplete, autoencoders] copying input output may sound useless typically interested output decoder instead hope training autoencoder perform input copying task result taking useful properties one way obtain useful features autoencoder constrain smaller dimension autoencoder whose code dimension less input dimension called undercomplete learning undercomplete representation forces autoencoder capture salient features training data learning process described simply minimizing loss function loss function penalizing dissimilar mean squared error decoder linear mean squared error undercomplete autoencoder learns span subspace pca case autoencoder trained perform copying task learned principal subspace training data side eﬀect autoencoders nonlinear encoder functions nonlinear decoder func tions thus learn powerful nonlinear generalization pca unfortu chapter autoencoders nately encoder decoder allowed much capacity autoencoder learn perform copying task without extracting useful information distribution data theoretically one could imagine autoencoder one dimensional code powerful nonlinear encoder could learn represent training example code decoder could learn map integer indices back values speciﬁc training examples speciﬁc scenario occur practice illustrates clearly autoen coder trained perform copying task fail learn anything useful dataset capacity autoencoder allowed become great 
[autoencoders, regularized, autoencoders] undercomplete autoencoders code dimension less input dimension learn salient features data distribution seen autoencoders fail learn anything useful encoder decoder given much capacity similar problem occurs hidden code allowed dimension equal input overcomplete case hidden code dimension greater input cases even linear encoder linear decoder learn copy input output without learning anything useful data distribution ideally one could train architecture autoencoder successfully choosing code dimension capacity encoder decoder based complexity distribution modeled regularized autoencoders provide ability rather limiting model capacity keeping encoder decoder shallow code size small regularized autoencoders use loss function encourages model properties besides ability copy input output properties include sparsity representation smallness derivative representation robustness noise missing inputs regularized autoencoder nonlinear overcomplete still learn something useful data distribution even model capacity great enough learn trivial identity function addition methods described naturally interpreted regularized autoencoders nearly generative model latent variables equipped inference procedure computing latent representations given input may viewed particular form autoencoder two generative modeling approaches emphasize connection autoencoders descendants helmholtz machine variational hinton chapter autoencoders autoencoder sec generative stochastic networks sec models naturally learn high capacity overcomplete encodings input require regularization encodings useful encodings naturally useful models trained approximately maximize probability training data rather copy input output 
[autoencoders, regularized, autoencoders, sparse, autoencoders] sparse autoencoder simply autoencoder whose training criterion involves sparsity penalty code layer addition reconstruction error decoder output typically encoder output sparse autoencoders typically used learn features another task classiﬁcation autoencoder regularized sparse must respond unique statistical features dataset trained rather simply acting identity function way training perform copying task sparsity penalty yield model learned useful features byproduct think penalty simply regularizer term added feedforward network whose primary task copy input output unsupervised learning objective possibly also perform supervised task supervised learning objective depends sparse features unlike regularizers weight decay straightforward bayesian interpretation regularizer described sec training weight decay regularization penalties interpreted map approximation bayesian inference added regularizing penalty corresponding prior probability distribution model parameters view regularized maximum likelihood corresponds maximizing equivalent maximizing log log log term usual data log likelihood term log term log prior parameters incorporates preference particular values view described sec regularized autoencoders defy interpretation regularizer depends data therefore deﬁnition prior formal sense word still think regularization terms implicitly expressing preference functions rather thinking sparsity penalty regularizer copying task think entire sparse autoencoder framework approximating chapter autoencoders maximum likelihood training generative model latent variables suppose model visible variables latent variables explicit joint distribution model model model refer model model prior distribution latent variables representing model beliefs prior seeing diﬀerent way previously used word prior refer distribution encoding beliefs model parameters seen training data log likelihood decomposed log model log model think autoencoder approximating sum point estimate one highly likely value similar sparse coding generative model sec output parametric encoder rather result optimization infers likely point view chosen maximizing log model log model log model log model term sparsity inducing example laplace prior model corresponds absolute value sparsity penalty expressing log prior absolute value penalty obtain log model log const constant term depends typically treat hyperparameter discard constant term since aﬀect parameter learning priors student prior also induce sparsity point view sparsity resulting eﬀect model approximate maximum likelihood learning sparsity penalty regularization term consequence model distribution latent variables view provides diﬀerent motivation training autoencoder way approximately training generative model also provides diﬀerent reason chapter autoencoders features learned autoencoder useful describe latent variables explain input early work sparse autoencoders explored ranzato various forms sparsity proposed connection sparsity penalty log term arises applying maximum likelihood undirected probabilistic model idea minimizing log prevents probabilistic model high probability everywhere imposing sparsity autoencoder prevents autoencoder low reconstruction error everywhere case connection level intuitive understanding general mechanism rather mathematical correspondence interpretation sparsity penalty corresponding log model directed model model model mathematically straightforward one way achieve actual zeros sparse denoising autoencoders introduced idea use rectiﬁed linear units glorot produce code layer prior actually pushes representations zero like absolute value penalty one thus indirectly control average number zeros representation 
[autoencoders, regularized, autoencoders, denoising, autoencoders] rather adding penalty cost function obtain autoencoder learns something useful changing reconstruction error term cost function traditionally autoencoders minimize function loss function penalizing dissimilar norm diﬀerence encourages learn merely identity function capacity denoising autoencoder dae instead minimizes copy corrupted form noise denoising autoencoders must therefore undo corruption rather simply copying input denoising training forces implicitly learn structure data shown denoising alain bengio bengio chapter autoencoders autoencoders thus provide yet another example useful properties emerge byproduct minimizing reconstruction error also example overcomplete high capacity models may used autoencoders long care taken prevent learning identity function denoising autoencoders presented detail sec 
[autoencoders, regularized, autoencoders, regularizing, penalizing, derivatives] another strategy regularizing autoencoder use penalty sparse autoencoders diﬀerent form forces model learn function change much changes slightly penalty applied training examples forces autoencoder learn features capture information training distribution autoencoder regularized way called contractive autoencoder approach theoretical connections denoising autoencoders cae manifold learning probabilistic modeling cae described detail sec 
[autoencoders, representational, power, layer, size, depth] autoencoders often trained single layer encoder single layer decoder however requirement fact using deep encoders decoders oﬀers many advantages recall sec many advantages depth feedforward network autoencoders feedforward networks advantages also apply autoencoders moreover encoder feedforward network decoder components autoencoder individually beneﬁt depth one major advantage non trivial depth universal approximator theorem guarantees feedforward neural network least one hidden layer represent approximation function within broad class chapter autoencoders arbitrary degree accuracy provided enough hidden units means autoencoder single hidden layer able represent identity function along domain data arbitrarily well however mapping input code shallow means able enforce arbitrary constraints code sparse deep autoencoder least one additional hidden layer inside encoder approximate mapping input code arbitrarily well given enough hidden units depth exponentially reduce computational cost representing functions depth also exponentially decrease amount training data needed learn functions see sec review advantages depth feedforward networks experimentally deep autoencoders yield much better compression corre sponding shallow linear autoencoders hinton salakhutdinov common strategy training deep autoencoder greedily pretrain deep architecture training stack shallow autoencoders often encounter shallow autoencoders even ultimate goal train deep autoencoder 
[autoencoders, stochastic, encoders, decoders] autoencoders feedforward networks loss functions output unit types used traditional feedforward networks also used autoencoders described sec general strategy designing output units loss function feedforward network deﬁne output distribution minimize negative log likelihood log setting vector targets class labels case autoencoder target well input however still apply machinery given hidden code may think decoder providing conditional distribution decoder may train autoencoder minimizing log decoder exact form loss function change depending form decoder traditional feedforward networks usually use linear output units parametrize mean gaussian distribution real valued case negative log likelihood yields mean squared error criterion similarly binary values correspond bernoulli distribution whose parameters given sigmoid output unit discrete values correspond softmax distribution chapter autoencoders typically output variables treated conditionally independent given probability distribution inexpensive evaluate techniques mixture density outputs allow tractable modeling outputs correlations encoder decoder figure structure stochastic autoencoder encoder decoder simple functions instead involve noise injection meaning output seen sampled distribution encoder encoder decoder decoder make radical departure feedforward networks seen previously also generalize notion encoding function encoding distribution encoder illustrated fig latent variable model model deﬁnes stochastic encoder encoder model stochastic decoder decoder model general encoder decoder distributions necessarily conditional distributions compatible unique joint distribution model alain showed training encoder decoder denoising autoencoder tend make compatible asymptotically enough capacity examples 
[autoencoders, denoising, autoencoders] denoising autoencoder dae autoencoder receives corrupted data point input trained predict original uncorrupted data point output dae training procedure illustrated fig introduce corruption process represents conditional distribution chapter autoencoders figure computational graph cost function denoising autoencoder trained reconstruct clean data point corrupted version accomplished minimizing loss log decoder corrupted version data example obtained given corruption process typically distribution decoder factorial distribution whose mean parameters emitted feedforward network corrupted samples given data sample autoencoder learns reconstruction distribution reconstruct estimated training pairs follows sample training example training data sample corrupted version use training example estimating autoencoder reconstruction distribution reconstruct decoder output encoder decoder typically deﬁned decoder typically simply perform gradient based approximate minimization minibatch gradient descent negative log likelihood log decoder long encoder deterministic denoising autoencoder feedforward network may trained exactly techniques feedforward network therefore view dae performing stochastic gradient descent following expectation data log decoder data training distribution chapter autoencoders figure denoising autoencoder trained map corrupted data point back original data point illustrate training examples red crosses lying near low dimensional manifold illustrated bold black line illustrate corruption process gray circle equiprobable corruptions gray arrow demonstrates one training example transformed one sample corruption process denoising autoencoder trained minimize average squared errors reconstruction estimates data vector points approximately towards nearest point manifold since estimates center mass clean points could given rise autoencoder thus learns vector ﬁeld indicated green arrows vector ﬁeld estimates score log data multiplicative factor average root mean square reconstruction error chapter autoencoders 
[autoencoders, denoising, autoencoders, estimating, score] score matching alternative maximum likelihood hyvärinen provides consistent estimator probability distributions based encouraging model score data distribution every training point context score particular gradient ﬁeld log score matching discussed sec present discussion regarding autoencoders suﬃcient understand learning gradient ﬁeld log data one way learn structure data important property daes training criterion conditionally gaussian makes autoencoder learn vector ﬁeld estimates score data distribution illustrated fig denoising training speciﬁc kind autoencoder sigmoidal hidden units linear reconstruction units using gaussian noise mean squared error reconstruction cost equivalent training speciﬁc kind vincent undirected probabilistic model called rbm gaussian visible units kind model described detail sec present discussion suﬃces know model provides explicit model rbm trained using denoising score matching kingma lecun learning algorithm equivalent denoising training corresponding autoencoder ﬁxed noise level regularized score matching consistent estimator instead recovers blurred version distribution however noise level chosen approach number examples approaches inﬁnity consistency recovered denoising score matching discussed detail sec connections autoencoders rbms exist score matching applied rbms yields cost function identical reconstruction error combined regularization term similar contractive penalty cae swersky bengio delalleau showed autoen coder gradient provides approximation contrastive divergence training rbms continuous valued denoising criterion gaussian corruption reconstruction distribution yields estimator score applicable general encoder decoder parametrizations alain bengio means generic encoder decoder architecture may made estimate score chapter autoencoders training squared error criterion corruption noise variance see fig illustration works figure vector ﬁeld learned denoising autoencoder around curved manifold near data concentrates space arrow proportional reconstruction minus input vector autoencoder points towards higher probability according implicitly estimated probability distribution vector ﬁeld zeros maxima estimated density function data manifolds minima density function example spiral arm forms one dimensional manifold local maxima connected local minima appear near middle gap two arms norm reconstruction error shown length arrows large means probability signiﬁcantly increased moving direction arrow mostly case places low probability autoencoder maps low probability points higher probability reconstructions probability maximal arrows shrink reconstruction becomes accurate general guarantee reconstruction minus input corresponds gradient function let alone score chapter autoencoders early results specialized particular parametrizations vincent may obtained taking derivative another function kamyshanska memisevic vincent generalized results identifying family shallow autoencoders corresponds score members family far described denoising autoencoder learns represent probability distribution generally one may want use autoencoder generative model draw samples distribution described later sec historical perspective idea using mlps denoising dates back work lecun gallinari behnke also used recurrent networks denoise images denoising autoencoders sense mlps trained denoise however name denoising autoencoder refers model intended merely learn denoise input learn good internal representation side eﬀect learning denoise idea came much later vincent learned representation may used pretrain deeper unsupervised network supervised network like sparse autoencoders sparse coding contractive autoencoders regularized autoencoders motivation daes allow learning high capacity encoder preventing encoder decoder learning useless identity function prior introduction modern dae inayoshi kurita explored goals methods approach minimizes reconstruction error addition supervised objective injecting noise hidden layer supervised mlp objective improve generalization introducing reconstruction error injected noise however method based linear encoder could learn function families powerful modern dae 
[autoencoders, learning, manifolds, autoencoders] like many machine learning algorithms autoencoders exploit idea data concentrates around low dimensional manifold small set manifolds described sec machine learning algorithms exploit idea insofar learn function behaves correctly manifold may unusual behavior given input manifold chapter autoencoders autoencoders take idea aim learn structure manifold understand autoencoders must present important characteristics manifolds important characterization manifold set tangent planes point dimensional manifold tangent plane given basis vectors span local directions variation allowed manifold illustrated fig local directions specify one change inﬁnitesimally staying manifold autoencoder training procedures involve compromise two forces learning representation training example approximately recovered decoder fact drawn training data crucial means autoencoder need successfully reconstruct inputs probable data generating distribution satisfying constraint regularization penalty architec tural constraint limits capacity autoencoder regularization term added reconstruction cost techniques generally prefer solutions less sensitive input clearly neither force alone would useful copying input output useful ignoring input instead two forces together useful force hidden representation capture information structure data generating distribution important principle autoencoder aﬀord represent variations needed reconstruct training examples data generating distribution concentrates near low dimensional manifold yields representations implicitly capture local coordinate system manifold variations tangent manifold around need correspond changes hence encoder learns mapping input space representation space mapping sensitive changes along manifold directions insensitive changes orthogonal manifold one dimensional example illustrated fig showing making reconstruction function insensitive perturbations input around data points recover manifold structure understand autoencoders useful manifold learning instruc tive compare approaches commonly learned characterize manifold representation data points near chapter autoencoders figure illustration concept tangent hyperplane create one dimensional manifold dimensional space take mnist image pixels transform translating vertically amount vertical translation deﬁnes coordinate along one dimensional manifold traces curved path image space plot shows points along manifold visualization projected manifold two dimensional space using pca dimensional manifold dimensional tangent plane every point tangent plane touches manifold exactly point oriented parallel surface point deﬁnes space directions possible move remaining manifold one dimensional manifold single tangent line indicate example tangent line one point image showing tangent direction appears image space gray pixels indicate pixels change move along tangent line white pixels indicate pixels brighten black pixels indicate pixels darken chapter autoencoders identity optimal reconstruction figure autoencoder learns reconstruction function invariant small perturbations near data points captures manifold structure data manifold structure collection dimensional manifolds dashed diagonal line indicates identity function target reconstruction optimal reconstruction function crosses identity function wherever data point horizontal arrows bottom plot indicate reconstruction direction vector base arrow input space always pointing towards nearest manifold single datapoint case denoising autoencoder explicitly tries make derivative reconstruction function small around data points contractive autoencoder encoder although derivative asked small around data points large data points space data points corresponds region manifolds reconstruction function must large derivative order map corrupted points back onto manifold manifold representation particular example also called bedding typically given low dimensional vector less dimensions ambient space manifold low dimensional subset algorithms non parametric manifold learning algorithms discussed directly learn embedding training example others learn general mapping sometimes called encoder representation function maps point ambient space input space embedding manifold learning mostly focused unsupervised learning procedures attempt capture manifolds initial machine learning research learning nonlinear manifolds focused non parametric methods based nearest neighbor graph graph one node per training example edges connecting near neighbors methods schölkopf roweis saul tenenbaum brand belkin chapter autoencoders figure non parametric manifold learning procedures build nearest neighbor graph whose nodes training examples arcs connect nearest neighbors various procedures thus obtain tangent plane associated neighborhood graph well coordinate system associates training example real valued vector position embedding possible generalize representation new examples form interpolation long number examples large enough cover curvature twists manifold approaches work well images qmul multiview face dataset gong niyogi donoho grimes weinberger saul hinton roweis van der maaten hinton associate nodes tangent plane spans directions variations associated diﬀerence vectors example neighbors illustrated fig global coordinate system obtained optimization solving linear system fig illustrates manifold tiled large number locally linear gaussian like patches pancakes gaussians ﬂat tangent directions however fundamental diﬃculty local non parametric approaches manifold learning raised bengio monperrus manifolds smooth many peaks troughs twists one may need large number training examples cover one variations chance generalize unseen variations indeed methods chapter autoencoders figure tangent planes see fig location known tiled form global coordinate system density function local patch thought local euclidean coordinate system locally ﬂat gaussian pancake small variance directions orthogonal pancake large variance directions deﬁning coordinate system pancake mixture gaussians provides estimated density function manifold parzen window algorithm non local neural net based vincent bengio variant bengio generalize shape manifold interpolating neighboring examples unfortunately manifolds involved problems complicated structure diﬃcult capture local interpolation consider example manifold resulting translation shown fig watch one coordinate within input vector image translated observe one coordinate encounters peak trough value every peak trough brightness image words complexity patterns brightness underlying image template drives complexity manifolds generated performing simple image transformations motivates use distributed representations deep learning capturing manifold structure chapter autoencoders 
[autoencoders, contractive, autoencoders] contractive autoencoder introduces explicit regularizer rifai code encouraging derivatives small possible penalty squared frobenius norm sum squared elements jacobian matrix partial derivatives associated encoder function connection denoising autoencoder contractive autoencoder showed limit small gaussian alain bengio input noise denoising reconstruction error equivalent contractive penalty reconstruction function maps words denoising autoencoders make reconstruction function resist small ﬁnite sized perturbations input contractive autoencoders make feature extraction function resist inﬁnitesimal perturbations input using jacobian based contractive penalty pretrain features use classiﬁer best classiﬁcation accuracy usually results applying contractive penalty rather contractive penalty also close connections score matching discussed sec name contractive arises way cae warps space speciﬁ cally cae trained resist perturbations input encouraged map neighborhood input points smaller neighborhood output points think contracting input neighborhood smaller output neighborhood clarify cae contractive locally perturbations training point mapped near globally two diﬀerent points may mapped points farther apart original points plausible expanding far data manifolds see example happens toy example fig penalty applied sigmoidal units one easy way shrink jacobian make sigmoid units saturate encourages cae encode input points extreme values sigmoid may interpreted binary code also ensures cae spread code values throughout hypercube sigmoidal hidden units span think jacobian matrix point approximating nonlinear encoder linear operator allows use word contractive formally theory linear operators linear operator chapter autoencoders said contractive norm remains less equal unit norm words contractive shrinks unit sphere think cae penalizing frobenius norm local linear approximation every training point order encourage local linear operator become contraction described sec regularized autoencoders learn manifolds balancing two opposing forces case cae two forces reconstruction error contractive penalty reconstruction error alone would encourage cae learn identity function contractive penalty alone would encourage cae learn features constant respect compromise two forces yields autoencoder whose derivatives mostly tiny small number hidden units corresponding small number directions input may signiﬁcant derivatives goal cae learn manifold structure data directions large rapidly change likely directions approximate tangent planes manifold experiments rifai show training cae results singular values rifai dropping magnitude therefore becoming contractive however singular values remain reconstruction error penalty encourages cae encode directions local variance directions corresponding largest singular values interpreted tangent directions contractive autoencoder learned ideally tangent directions correspond real variations data example cae applied images learn tangent vectors show image changes objects image gradually change pose shown fig visualizations experimentally obtained singular vectors seem correspond meaningful transformations input image shown fig one practical issue cae regularization criterion although cheap compute case single hidden layer autoencoder becomes much expensive case deeper autoencoders strategy followed rifai separately train series single layer autoencoders trained reconstruct previous autoencoder hidden layer composition autoencoders forms deep autoencoder layer separately trained locally contractive deep autoencoder contractive well result would obtained jointly training entire architecture penalty jacobian deep model captures many desirable qualitative characteristics another practical issue contraction penalty obtain useless results chapter autoencoders input point tangent vectors local pca sharing across regions contractive autoencoder figure illustration tangent vectors manifold estimated local pca contractive autoencoder location manifold deﬁned input image dog drawn cifar dataset tangent vectors estimated leading singular vectors jacobian matrix input code mapping although local pca cae capture local tangents cae able form accurate estimates limited training data exploits parameter sharing across diﬀerent locations share subset active hidden units cae tangent directions typically correspond moving changing parts object head legs impose sort scale decoder example encoder could consist multiplying input small constant decoder could consist dividing code approaches encoder drives contractive penalty approach without learned anything distribution meanwhile decoder maintains perfect reconstruction rifai prevented tying weights standard neural network layers consisting aﬃne transformation followed element wise nonlinearity straightforward set weight matrix transpose weight matrix 
[autoencoders, predictive, sparse, decomposition] predictive sparse decomposition psd model hybrid sparse coding parametric autoencoders kavukcuoglu parametric encoder trained predict output iterative inference psd applied unsupervised feature learning object recognition images video kavukcuoglu jarrett farabet well audio model consists encoder henaﬀ decoder parametric training controlled chapter autoencoders optimization algorithm training proceeds minimizing like sparse coding training algorithm alternates minimization respect minimization respect model parameters minimization respect fast provides good initial value cost function constrains remain near anyway simple gradient descent obtain reasonable values ten steps training procedure used psd diﬀerent ﬁrst training sparse coding model training predict values sparse coding features psd training procedure regularizes decoder use parameters infer good code values predictive sparse coding example learned approximate inference sec topic developed tools presented chapter make clear psd interpreted training directed sparse coding probabilistic model maximizing lower bound log likelihood model practical applications psd iterative optimization used training parametric encoder used compute learned features model deployed evaluating computationally inexpensive compared inferring via gradient descent diﬀerentiable parametric function psd models may stacked used initialize deep network trained another criterion 
[autoencoders, applications, autoencoders] autoencoders successfully applied dimensionality reduction infor mation retrieval tasks dimensionality reduction one ﬁrst applications representation learning deep learning one early motivations studying autoencoders example hinton salakhutdinov trained stack rbms used weights initialize deep autoencoder gradually smaller hidden layers culminating bottleneck units resulting code yielded less reconstruction error pca dimensions learned representation qualitatively easier interpret relate underlying categories categories manifesting well separated clusters lower dimensional representations improve performance many tasks classiﬁcation models smaller spaces consume less memory runtime many forms dimensionality reduction place semantically related examples near chapter autoencoders observed salakhutdinov hinton torralba hints provided mapping lower dimensional space aid generalization one task beneﬁts even usual dimensionality reduction information retrieval task ﬁnding entries database resemble query entry task derives usual beneﬁts dimensionality reduction tasks also derives additional beneﬁt search become extremely eﬃcient certain kinds low dimensional spaces speciﬁcally train dimensionality reduction algorithm produce code low dimensional binary store database entries hash table mapping binary code vectors entries hash table allows perform information retrieval returning database entries binary code query also search slightly less similar entries eﬃciently ﬂipping individual bits encoding query approach information retrieval via dimensionality reduction binarization called semantic hashing salakhutdinov hinton applied textual input salakhutdinov hinton images torralba weiss krizhevsky hinton produce binary codes semantic hashing one typically uses encoding function sigmoids ﬁnal layer sigmoid units must trained saturated nearly nearly input values one trick accomplish simply inject additive noise sigmoid nonlinearity training magnitude noise increase time ﬁght noise preserve much information possible network must increase magnitude inputs sigmoid function saturation occurs idea learning hashing function explored several directions including idea training representations optimize loss directly linked task ﬁnding nearby examples hash table norouzi fleet 
[representation, learning] chapter ﬁrst discuss means learn representations notion representation useful design deep architectures discuss learning algorithms share statistical strength across diﬀerent tasks including using information unsupervised tasks perform supervised tasks shared representations useful handle multiple modalities domains transfer learned knowledge tasks examples given task representation exists finally step back argue reasons success representation learning starting theoretical advantages distributed representations hinton deep representations ending general idea underlying assumptions data generating process particular underlying causes observed data many information processing tasks easy diﬃcult depending information represented general principle applicable daily life computer science general machine learning example straightforward person divide using long division task becomes considerably less straightforward instead posed using roman numeral representation numbers modern people asked divide ccx would begin converting numbers arabic numeral representation permitting long division procedures make use place value system concretely quantify asymptotic runtime various operations using appropriate inappropriate representations example inserting number correct position sorted list numbers operation list represented linked list log list represented red black tree context machine learning makes one representation better chapter representation learning another generally speaking good representation one makes subsequent learning task easier choice representation usually depend choice subsequent learning task think feedforward networks trained supervised learning per forming kind representation learning speciﬁcally last layer network typically linear classiﬁer softmax regression classiﬁer rest network learns provide representation classiﬁer training supervised criterion naturally leads representation every hidden layer near top hidden layer taking properties make classiﬁcation task easier example classes linearly separable input features may become linearly separable last hidden layer principle last layer could another kind model nearest neighbor classiﬁer salakhutdinov hinton features penultimate layer learn diﬀerent properties depending type last layer supervised training feedforward networks involve explicitly imposing condition learned intermediate features kinds representation learning algorithms often explicitly designed shape representation particular way example suppose want learn representation makes density estimation easier distributions independences easier model could design objective function encourages elements representation vector independent like supervised networks unsupervised deep learning algorithms main training objective also learn representation side eﬀect regardless representation obtained used another task alternatively multiple tasks supervised unsupervised learned together shared internal representation representation learning problems face tradeoﬀ preserving much information input possible attaining nice properties independence representation learning particularly interesting provides one way perform unsupervised semi supervised learning often large amounts unlabeled training data relatively little labeled training data training supervised learning techniques labeled subset often results severe overﬁtting semi supervised learning oﬀers chance resolve overﬁtting problem also learning unlabeled data speciﬁcally learn good representations unlabeled data use representations solve supervised learning task humans animals able learn labeled examples chapter representation learning yet know possible many factors could explain improved human performance example brain may use large ensembles classiﬁers bayesian inference techniques one popular hypothesis brain able leverage unsupervised semi supervised learning many ways leverage unlabeled data chapter focus hypothesis unlabeled data used learn good representation 
[representation, learning, greedy, layer-wise, unsupervised, pretraining] unsupervised learning played key historical role revival deep neural networks allowing ﬁrst time train deep supervised network without requiring architectural specializations like convolution recurrence call procedure unsupervised pretraining precisely greedy layer wise unsuper vised pretraining procedure canonical example representation learned one task unsupervised learning trying capture shape input distribution sometimes useful another task supervised learning input domain greedy layer wise unsupervised pretraining relies single layer represen tation learning algorithm rbm single layer autoencoder sparse coding model another model learns latent representations layer pretrained using unsupervised learning taking output previous layer producing output new representation data whose distribution relation variables categories predict hopefully simpler see algorithm formal description greedy layer wise training procedures based unsupervised criteria long used sidestep diﬃculty jointly training layers deep neural net supervised task approach dates back least far neocognitron fukushima deep learning renaissance began discovery greedy learning procedure could used ﬁnd good initialization joint learning procedure layers approach could used successfully train even fully connected architectures hinton hinton salakhutdinov hinton bengio ranzato prior discovery convolutional deep networks networks whose depth resulted recurrence regarded feasible train today know greedy layer wise pretraining required train fully connected deep architectures unsupervised pretraining approach ﬁrst method succeed greedy layer wise pretraining called greedy greedy algorithm chapter representation learning meaning optimizes piece solution independently one piece time rather jointly optimizing pieces called layer wise independent pieces layers network speciﬁcally greedy layer wise pretraining proceeds one layer time training layer keeping previous ones ﬁxed particular lower layers trained ﬁrst adapted upper layers introduced called unsupervised layer trained unsupervised representation learning algorithm however also called pretraining supposed ﬁrst step joint training algorithm applied layers together ﬁne tune context supervised learning task viewed regularizer experiments pretraining decreases test error without decreasing training error form parameter initialization common use word pretraining refer pretraining stage entire two phase protocol combines pretraining phase supervised learning phase supervised learning phase may involve training simple classiﬁer top features learned pretraining phase may involve supervised ﬁne tuning entire network learned pretraining phase matter kind unsupervised learning algorithm model type employed vast majority cases overall training scheme nearly choice unsupervised learning algorithm obviously impact details applications unsupervised pretraining follow basic protocol greedy layer wise unsupervised pretraining also used initialization unsupervised learning algorithms deep autoencoders hinton salakhutdinov probabilistic models many layers latent variables models include deep belief networks deep hinton boltzmann machines salakhutdinov hinton deep generative models described chapter discussed sec also possible greedy layer wise super vised pretraining builds premise training shallow network easier training deep one seems validated several contexts erhan 
[representation, learning, greedy, layer-wise, unsupervised, pretraining, unsupervised, pretraining, work?] many tasks greedy layer wise unsupervised pretraining yield substantial improvements test error classiﬁcation tasks observation responsible renewed interested deep neural networks starting hinton chapter representation learning algorithm greedy layer wise unsupervised pretraining protocol given following unsupervised feature learning algorithm takes training set examples returns encoder feature function raw input data one row per example output ﬁrst stage encoder dataset used second level unsupervised feature learner case ﬁne tuning performed use learner takes initial function input examples supervised ﬁne tuning case associated targets returns tuned function number stages identity function end ﬁne tuning end return bengio ranzato many tasks however unsupervised pretraining either confer beneﬁt even causes noticeable harm studied eﬀect pretraining machine learning models chemical activity prediction found average pretraining slightly harmful many tasks signiﬁcantly helpful unsupervised pretraining sometimes helpful often harmful important understand works order determine whether applicable particular task outset important clarify discussion restricted greedy unsupervised pretraining particular completely diﬀerent paradigms performing semi supervised learning neural networks virtual adversarial training described sec also possible train autoencoder generative model time supervised model examples single stage approach include discriminative rbm larochelle bengio ladder network total rasmus objective explicit sum two terms one using labels one using input unsupervised pretraining combines two diﬀerent ideas first makes use chapter representation learning idea choice initial parameters deep neural network signiﬁcant regularizing eﬀect model lesser extent improve optimization second makes use general idea learning input distribution help learn mapping inputs outputs ideas involve many complicated interactions several parts machine learning algorithm entirely understood ﬁrst idea choice initial parameters deep neural network strong regularizing eﬀect performance least well understood time pretraining became popular understood initializing model location would cause approach one local minimum rather another today local minima longer considered serious problem neural network optimization know standard neural network training procedures usually arrive critical point kind remains possible pretraining initializes model location would otherwise inaccessible example region surrounded areas cost function varies much one example another minibatches give noisy estimate gradient region surrounded areas hessian matrix poorly conditioned gradient descent methods must use small steps however ability characterize exactly aspects pretrained parameters retained supervised training stage limited one reason modern approaches typically use simultaneous unsupervised learning supervised learning rather two sequential stages one may also avoid struggling complicated ideas optimization supervised learning stage preserves information unsupervised learning stage simply freezing parameters feature extractors using supervised learning add classiﬁer top learned features idea learning algorithm use information learned unsupervised phase perform better supervised learning stage better understood basic idea features useful unsupervised task may also useful supervised learning task example train generative model images cars motorcycles need know wheels many wheels image fortunate representation wheels take form easy supervised learner access yet understood mathematical theoretical level always possible predict tasks beneﬁt unsupervised learning way many aspects approach highly dependent speciﬁc models used example wish add linear classiﬁer chapter representation learning top pretrained features features must make underlying classes linearly separable properties often occur naturally always another reason simultaneous supervised unsupervised learning preferable constraints imposed output layer naturally included start point view unsupervised pretraining learning representation expect unsupervised pretraining eﬀective initial representation poor one key example use word embeddings words represented one hot vectors informative every two distinct one hot vectors distance squared distance learned word embeddings naturally encode similarity words distance unsupervised pretraining especially useful processing words less useful processing images perhaps images already lie rich vector space distances provide low quality similarity metric point view unsupervised pretraining regularizer expect unsupervised pretraining helpful number labeled examples small source information added unsupervised pretraining unlabeled data may also expect unsupervised pretraining perform best number unlabeled examples large advantage semi supervised learning via unsupervised pretraining many unlabeled examples labeled examples made particularly clear unsupervised pretraining winning two international transfer learning competitions settings mesnil goodfellow number labeled examples target task small handful dozens examples per class eﬀects also documented carefully controlled experiments paine factors likely involved example unsupervised pretraining likely useful function learned extremely complicated unsupervised learning diﬀers regularizers like weight decay bias learner toward discovering simple function rather toward discovering feature functions useful unsupervised learning task true underlying functions complicated shaped regularities input distribution unsupervised learning appropriate regularizer caveats aside analyze success cases unsupervised pretraining known cause improvement explain known improvement occurs unsupervised pretraining usually used improve classiﬁers usually interesting point view chapter representation learning figure visualization via nonlinear projection learning trajectories diﬀerent neural networks function space parameter space avoid issue many one mappings parameter vectors functions diﬀerent random initializations without unsupervised pretraining point corresponds diﬀerent neural network particular time training process ﬁgure adapted permission coordinate function space inﬁnite erhan dimensional vector associating every input output made erhan linear projection high dimensional space concatenating many speciﬁc points made nonlinear projection isomap tenenbaum color indicates time networks initialized near center plot corresponding region functions produce approximately uniform distributions class inputs time learning moves function outward points make strong predictions training consistently terminates one region using pretraining another non overlapping region using pretraining isomap tries preserve global relative distances hence volumes small region corresponding pretrained models may indicate pretraining based estimator reduced variance chapter representation learning reducing test set error however unsupervised pretraining help tasks classiﬁcation act improve optimization rather merely regularizer example improve train test reconstruction error deep autoencoders hinton salakhutdinov erhan performed many experiments explain several successes unsupervised pretraining improvements training error improvements test error may explained terms unsupervised pretraining taking parameters region would otherwise inaccessible neural network training non deterministic converges diﬀerent function every time run training may halt point gradient becomes small point early stopping ends training prevent overﬁtting point gradient large diﬃcult ﬁnd downhill step due problems stochasticity poor conditioning hessian neural networks receive unsupervised pretraining consistently halt region function space neural networks without pretraining consistently halt another region see fig visualization phenomenon region pretrained networks arrive smaller suggesting pretraining reduces variance estimation process turn reduce risk severe ﬁtting words unsupervised pretraining initializes neural network parameters region escape results following initialization consistent less likely bad without initialization erhan also provide answers pretraining works best mean variance test error reduced pretraining deeper networks keep mind experiments performed invention popularization modern techniques training deep networks rectiﬁed linear units dropout batch normalization less known eﬀect unsupervised pretraining conjunction contemporary approaches important question unsupervised pretraining act regularizer one hypothesis pretraining encourages learning algorithm discover features relate underlying causes generate observed data important idea motivating many algorithms besides unsupervised pretraining described sec compared ways incorporating belief using unsupervised learning unsupervised pretraining disadvantage operates two separate training phases one reason two training phases disadvantageous single hyperparameter predictably reduces increases strength regularization arising unsupervised pretraining instead many hyperparameters whose eﬀect may chapter representation learning measured fact often diﬃcult predict ahead time perform unsupervised supervised learning simultaneously instead using pretraining strategy single hyperparameter usually coeﬃcient attached unsupervised cost determines strongly unsupervised objective regularize supervised model one always predictably obtain less regularization decreasing coeﬃcient case unsupervised pretraining way ﬂexibly adapting strength regularization either supervised model initialized pretrained parameters another disadvantage two separate training phases phase hyperparameters performance second phase usually cannot predicted ﬁrst phase long delay proposing hyperparameters ﬁrst phase able update using feedback second phase principled approach use validation set error supervised phase order select hyperparameters pretraining phase discussed practice hyperparameters larochelle like number pretraining iterations conveniently set pretraining phase using early stopping unsupervised objective ideal computationally much cheaper using supervised objective today unsupervised pretraining largely abandoned except ﬁeld natural language processing natural representation words one hot vectors conveys similarity information large unlabeled sets available case advantage pretraining one pretrain huge unlabeled set example corpus containing billions words learn good representation typically words also sentences use representation ﬁne tune supervised task training set contains substantially fewer examples approach pioneered collobert weston turian collobert remains common use today deep learning techniques based supervised learning regularized dropout batch normalization able achieve human level performance many tasks extremely large labeled datasets techniques outperform unsupervised pretraining medium sized datasets cifar mnist roughly labeled examples per class extremely small datasets alternative splicing dataset bayesian methods outper form methods based unsupervised pretraining srivastava reasons popularity unsupervised pretraining declined nevertheless unsupervised pretraining remains important milestone history deep learning research continues inﬂuence contemporary approaches idea chapter representation learning pretraining generalized supervised pretraining discussed sec common approach transfer learning supervised pretraining transfer learning popular oquab yosinski use convolutional networks pretrained imagenet dataset practitioners publish parameters trained networks purpose like pretrained word vectors published natural language tasks collobert mikolov 
[representation, learning, transfer, learning, domain, adaptation] transfer learning domain adaptation refer situation learned one setting distribution exploited improve generalization another setting say distribution generalizes idea presented previous section transferred representations unsupervised learning task supervised learning task transfer learning learner must perform two diﬀerent tasks assume many factors explain variations relevant variations need captured learning typically understood supervised learning context input target may diﬀerent nature example may learn one set visual categories cats dogs ﬁrst setting learn diﬀerent set visual categories ants wasps second setting signiﬁcantly data ﬁrst setting sampled may help learn representations useful quickly generalize examples drawn many visual categories share low level notions edges visual shapes eﬀects geometric changes changes lighting etc general transfer learning multi task learning sec domain adaptation achieved via representation learning exist features useful diﬀerent settings tasks corresponding underlying factors appear one setting illustrated fig shared lower layers task dependent upper layers however sometimes shared among diﬀerent tasks semantics input semantics output example speech recognition system needs produce valid sentences output layer earlier layers near input may need recognize diﬀerent versions phonemes sub phonemic vocalizations depending person speaking cases like makes sense share upper layers near output neural network task speciﬁc preprocessing chapter representation learning illustrated fig selection switch shared shared figure example architecture multi task transfer learning output variable semantics tasks input variable diﬀerent meaning possibly even diﬀerent dimension task example user called three tasks lower levels selection switch task speciﬁc upper levels shared lower levels learn translate task speciﬁc input generic set features related case task optimal input domain adaptation output mapping remains setting input distribution slightly diﬀerent example consider task sentiment analysis consists determining whether comment expresses positive negative sentiment comments posted web come many categories domain adaptation scenario arise sentiment predictor trained customer reviews media content books videos music later used analyze comments consumer electronics televisions smartphones one imagine underlying function tells whether statement positive neutral negative course vocabulary style may vary one domain another making diﬃcult generalize across domains simple unsupervised pretraining denoising autoencoders found successful sentiment analysis domain adaptation glorot related problem concept drift view form transfer learning due gradual changes data distribution time concept drift transfer learning viewed particular forms multi task learning chapter representation learning phrase multi task learning typically refers supervised learning tasks general notion transfer learning applicable unsupervised learning reinforcement learning well cases objective take advantage data ﬁrst setting extract information may useful learning even directly making predictions second setting core idea representation learning representation may useful settings using representation settings allows representation beneﬁt training data available tasks mentioned unsupervised deep learning transfer learning found success machine learning competitions mesnil goodfellow ﬁrst competitions experimental setup following participant ﬁrst given dataset ﬁrst setting distribution illustrating examples set categories participants must use learn good feature space mapping raw input representation apply learned transformation inputs transfer setting distribution linear classiﬁer trained generalize well labeled examples one striking results found competition architecture makes use deeper deeper representations learned purely unsupervised way data collected ﬁrst setting learning curve new categories second transfer setting becomes much better deep representations fewer labeled examples transfer tasks necessary achieve apparently asymptotic generalization performance two extreme forms transfer learning one shot learning zero shot learning zero data learning sometimes also called one labeled example transfer task given one shot learning labeled examples given zero shot learning task one shot learning fei fei possible representation learns cleanly separate underlying classes ﬁrst stage transfer learning stage one labeled example needed infer label many possible test examples cluster around point representation space works extent factors variation corresponding invariances cleanly separated factors learned representation space somehow learned factors matter discriminating objects certain categories example zero shot learning setting consider problem learner read large collection text solve object recognition problems chapter representation learning may possible recognize speciﬁc object class even without seen image object text describes object well enough example read cat four legs pointy ears learner might able guess image cat without seen cat zero data learning larochelle palatucci zero shot learning socher possible additional information exploited training think zero data learning scenario including three random variables traditional inputs traditional outputs targets additional random variable describing task model trained estimate conditional distribution description task wish model perform example recognizing cats read cats output binary variable indicating yes indicating task variable represents questions answered cat image training set containing unsupervised examples objects live space may able infer meaning unseen instances example recognizing cats without seen image cat important unlabeled text data containing sentences cats four legs cats pointy ears zero shot learning requires represented way allows sort generalization example cannot one hot code indicating object category provide instead distributed representation socher object categories using learned word embedding word associated category similar phenomenon happens machine translation klementiev mikolov gouws words one language relationships words learned unilingual corpora hand translated sentences relate words one language words even though may labeled examples translating word language word language generalize guess translation word learned distributed representation words language distributed representation words language created link possibly two way relating two spaces via training examples consisting matched pairs sentences languages transfer successful three ingredients two representations relations learned jointly zero shot learning particular form transfer learning principle explains one perform multi modal learning capturing representation chapter representation learning test test space relationship embedded points within one domains maps representation spaces space pairs training set encoder function encoder function figure transfer learning two domains enables zero shot learning labeled unlabeled examples allow one learn representation function similarly examples learn application functions appears upward arrow style arrows indicating function applied distance space provides similarity metric pair points space may meaningful distance space likewise distance space provides similarity metric pair points space similarity functions indicated dotted bidirectional arrows labeled examples dashed horizontal lines pairs allow one learn one way two way map solid bidirectional arrow representations representations anchor representations zero data learning enabled follows one associate image test word test even image word ever presented simply word representations test image representations test related via maps representation spaces works although image word never paired respective feature vectors test test related figure inspired suggestion hrant khachatrian chapter representation learning one modality representation relationship general joint distribution pairs consisting one observation one modality another observation modality srivastava salakhutdinov learning three sets parameters representation representation relationship two representations concepts one representation anchored vice versa allowing one meaningfully generalize new pairs procedure illustrated fig 
[representation, learning, semi-supervised, disentangling, causal, factors] important question representation learning makes one repre sentation better another one hypothesis ideal representation one features within representation correspond lying causes observed data separate features directions feature space corresponding diﬀerent causes representation disentangles causes one another hypothesis motivates approaches ﬁrst seek good representation representation may also good representation computing among salient causes idea guided large amount deep learning research since least becker hinton hinton sejnowski detail arguments semi supervised learning outperform pure supervised learning refer reader sec chapelle approaches representation learning often concerned representation easy model example one whose entries sparse independent representation cleanly separates underlying causal factors may necessarily one easy model however part hypothesis motivating semi supervised learning via unsupervised representation learning many tasks two properties coincide able obtain underlying explanations observe generally becomes easy isolate individual attributes others speciﬁcally representation represents many underlying causes observed outputs among salient causes easy predict first let see semi supervised learning fail unsupervised learning help learn consider example case uniformly distributed want learn clearly observing training set values alone gives information chapter representation learning mixture model figure example density mixture three components component identity underlying explanatory factor mixture components natural object classes image data statistically salient modeling unsupervised way labeled example already reveals factor next let see simple example semi supervised learning succeed consider situation arises mixture one mixture component per value illustrated fig mixture components well separated modeling reveals precisely component single labeled example class enough perfectly learn generally could make tied together closely associated one causal factors strongly tied unsupervised representation learning tries disentangle underlying factors variation likely useful semi supervised learning strategy consider assumption one causal factors let represent factors true generative process conceived structured according directed graphical model parent consequence data marginal probability straightforward observation conclude best possible model generalization point view one uncovers true chapter representation learning structure latent variable explains observed variations ideal representation learning discussed thus recover latent factors one closely related one easy learn predict representation also see conditional distribution given tied bayes rule components equation thus marginal intimately tied conditional knowledge structure former helpful learn latter therefore situations respecting assumptions semi supervised learning improve performance important research problem regards fact observations formed extremely large number underlying causes suppose unsupervised learner know brute force solution unsupervised learner learn representation captures reasonably salient generative factors disentangles thus making easy predict regardless associated practice brute force solution feasible possible capture factors variation inﬂuence observation example visual scene representation always encode smallest objects background well documented psychological phenomenon human beings fail perceive changes environment immediately relevant task performing see simons levin important research frontier semi supervised learning determining encode situation currently two main strategies dealing large number underlying causes use supervised learning signal time unsupervised learning signal model choose capture relevant factors variation use much larger representations using purely unsupervised learning emerging strategy unsupervised learning modify deﬁnition underlying causes salient historically autoencoders generative models trained optimize ﬁxed criterion often similar mean squared error ﬁxed criteria determine causes considered salient example mean squared error applied pixels image implicitly speciﬁes underlying cause salient signiﬁcantly changes brightness large number pixels problematic task wish solve involves interacting small objects see fig example chapter representation learning input reconstruction figure autoencoder trained mean squared error robotics task failed reconstruct ping pong ball existence ping pong ball spatial coordinates important underlying causal factors generate image relevant robotics task unfortunately autoencoder limited capacity training mean squared error identify ping pong ball salient enough encode images graciously provided chelsea finn robotics task autoencoder failed learn encode small ping pong ball robot capable successfully interacting larger objects baseballs salient according mean squared error deﬁnitions salience possible example group pixels follow highly recognizable pattern even pattern involve extreme brightness darkness pattern could considered extremely salient one way implement deﬁnition salience use recently developed approach called generative adversarial networks goodfellow approach generative model trained fool feedforward classiﬁer feedforward classiﬁer attempts recognize samples generative model fake samples training set real framework structured pattern feedforward network recognize highly salient generative adversarial network described detail sec purposes present discussion suﬃcient understand learn determine salient showed models lotter trained generate images human heads often neglect generate ears trained mean squared error successfully generate ears trained adversarial framework ears extremely bright dark compared surrounding skin especially salient according mean squared error loss highly recognizable shape consistent chapter representation learning ground truth mse adversarial figure predictive generative networks provide example importance learning features salient example predictive generative network trained predict appearance model human head speciﬁc viewing angle left ground truth correct image network emit image produced predictive generative network trained mean center squared error alone ears cause extreme diﬀerence brightness compared neighboring skin suﬃciently salient model learn represent image produced model trained combination right mean squared error adversarial loss using learned cost function ears salient follow predictable pattern learning underlying causes important relevant enough model important active area research figures graciously provided lotter position means feedforward network easily learn detect making highly salient generative adversarial framework see fig example images generative adversarial networks one step toward determining factors represented expect future research discover better ways determining factors represent develop mechanisms representing diﬀerent factors depending task beneﬁt learning underlying causal factors pointed schölkopf true generative process eﬀect cause modeling robust changes cause eﬀect relationship reversed would true since bayes rule would sensitive changes often consider changes distribution due diﬀerent domains temporal non stationarity changes nature task causal mechanisms remain invariant laws universe constant marginal distribution underlying causes change hence better generalization robustness kinds changes expected via learning generative model attempts recover chapter representation learning causal factors 
[representation, learning, distributed, representation] distributed representations concepts representations composed many ele ments set separately one important tools representation learning distributed representations powerful use features values describe diﬀerent concepts seen throughout book neural networks multiple hidden units probabilistic models multiple latent variables make use strategy distributed representation introduce additional observation many deep learning algorithms motivated assumption hidden units learn represent underlying causal factors explain data discussed sec distributed representations natural approach direction representation space correspond value diﬀerent underlying conﬁguration variable example distributed representation vector binary features take conﬁgurations potentially corresponding diﬀerent region input space illustrated fig compared symbolic representation input associated single symbol category symbols dictionary one imagine feature detectors corresponding detection presence associated category case diﬀerent conﬁgurations representation space possible carving diﬀerent regions input space illustrated fig symbolic representation also called one hot representation since captured binary vector bits mutually exclusive one active symbolic representation speciﬁc example broader class non distributed representations representations may contain many entries without signiﬁcant meaningful separate control entry examples learning algorithms based non distributed representations include clustering methods including means algorithm input point assigned exactly one cluster nearest neighbors algorithms one templates prototype examples associated given input case multiple chapter representation learning figure illustration learning algorithm based distributed representation breaks input space regions example three binary features feature deﬁned thresholding output learned linear transformation feature divides two half planes let set input points set input points illustration line represents decision boundary one corresponding arrow pointing side boundary representation whole takes unique value possible intersection half planes example representation value corresponds region compare non distributed representations fig general case input dimensions distributed representation divides intersecting half spaces rather half planes distributed representation features assigns unique codes diﬀerent regions nearest neighbor algorithm examples assigns unique codes regions distributed representation thus able distinguish exponentially many regions non distributed one keep mind values feasible example linear classiﬁer top distributed representation able assign diﬀerent class identities every neighboring region even deep linear threshold network dimension log number weights combination powerful representation sontag layer weak classiﬁer layer strong regularizer classiﬁer trying learn concept person versus person need assign diﬀerent class input represented woman glasses assigns input represented man without glasses capacity constraint encourages classiﬁer focus encourages learn represent classes linearly separable way chapter representation learning values describing input controlled separately qualify true distributed representation decision trees one leaf nodes path root leaf activated input given gaussian mixtures mixtures experts templates cluster centers experts associated degree activation nearest neighbors algorithm input represented multiple values values cannot readily controlled separately kernel machines gaussian kernel similarly local kernel although degree activation support vector template example continuous valued issue arises gaussian mixtures language translation models based grams set contexts sequences symbols partitioned according tree structure suﬃxes leaf may correspond last two words example separate parameters estimated leaf tree sharing possible non distributed algorithms output constant parts instead interpolates neighboring regions relationship number parameters examples number regions deﬁne remains linear important related concept distinguishes distributed representation symbolic one generalization arises due shared attributes diﬀerent concepts pure symbols cat dog far two symbols however one associates meaningful distributed representation many things said cats generalize dogs vice versa example distributed representation may contain entries has_fur number_of_legs value embedding cat dog neural language models operate distributed representations words generalize much better models operate directly one hot representations words discussed sec distributed representations induce rich similarity space semantically close concepts inputs close distance property absent purely symbolic representations statistical advantage using distributed representation part learning algorithm distributed representations chapter representation learning figure illustration nearest neighbor algorithm breaks input space diﬀerent regions nearest neighbor algorithm provides example learning algorithm based non distributed representation diﬀerent non distributed algorithms may diﬀerent geometry typically break input space regions separate set parameters region advantage non distributed approach given enough parameters training set without solving diﬃcult optimization algorithm straightforward choose diﬀerent output independently region disadvantage non distributed models generalize locally via smoothness prior making diﬃcult learn complicated function peaks troughs available number examples contrast distributed representation fig chapter representation learning statistical advantage apparently complicated structure compactly represented using small number parameters traditional non distributed learning algorithms generalize due smoothness assumption states target function learned property general many ways formalizing assumption end result example know choose estimator approximately satisﬁes constraints changing little possible move nearby input assumption clearly useful suﬀers curse dimensionality order learn target function increases decreases many times many diﬀerent regions may need number examples least large number distinguishable regions one think regions category symbol separate degree freedom symbol region learn arbitrary decoder mapping symbol value however allow generalize new symbols new regions lucky may regularity target function besides smooth example convolutional network max pooling recognize object regardless location image even though spatial translation object may correspond smooth transformations input space let examine special case distributed representation learning algorithm extracts binary features thresholding linear functions input binary feature representation divides pair half spaces illustrated fig exponentially large number intersections corresponding half spaces determines many regions distributed representation learner distinguish many regions generated arrangement hyperplanes applying general result concerning intersection hyperplanes one show zaslavsky pascanu number regions binary feature representation distinguish therefore see growth exponential input size polynomial number hidden units potentially may want learn function whose behavior distinct exponentially many regions dimensional space least diﬀerent values distinguish per dimension might want diﬀer diﬀerent regions requiring training examples chapter representation learning provides geometric argument explain generalization power distributed representation parameters linear threshold features distinctly represent regions input space instead made assumption data used representation one unique symbol region separate parameters symbol recognize corresponding portion specifying regions would require examples generally argument favor distributed representation could extended case instead using linear threshold units use nonlinear possibly continuous feature extractors attributes distributed representation argument case parametric transformation parameters learn regions input space obtaining representation useful task interest could potentially generalize much better way non distributed setting would need examples obtain features associated partitioning input space regions using fewer parameters represent model means fewer parameters thus require far fewer training examples generalize well part argument models based distributed represen tations generalize well capacity remains limited despite able distinctly encode many diﬀerent regions example dimension neural network linear threshold units log number weights sontag limitation arises assign many unique codes representation space cannot use absolutely code space learn arbitrary functions mapping representation space output using linear classiﬁer use distributed representation combined linear classiﬁer thus expresses prior belief classes recognized linearly separable function underlying causal factors captured typically want learn categories set images green objects set images cars categories require nonlinear xor logic example typically want partition data set red cars green trucks one class set green cars red trucks another class ideas discussed far abstract may experimentally validated ﬁnd hidden units deep convolutional zhou network trained imagenet places benchmark datasets learn features often interpretable corresponding label humans would naturally assign practice certainly always case hidden units learn something simple linguistic name interesting see emerge near top levels best computer vision deep networks chapter representation learning figure generative model learned distributed representation disentangles concept gender concept wearing glasses begin repre sentation concept man glasses subtract vector representing concept man without glasses ﬁnally add vector representing concept woman without glasses obtain vector representing concept woman glasses generative model correctly decodes representation vectors images may recognized belonging correct class images reproduced permission radford features common one could imagine learning without see conﬁgurations others radford demonstrated generative model learn representation images faces separate directions representation space capturing diﬀerent underlying factors variation fig demonstrates one direction representation space corresponds whether person male female another corresponds whether person wearing glasses features discovered automatically ﬁxed priori need labels hidden unit classiﬁers gradient descent objective function interest naturally learns semantically interesting features long task requires features learn distinction male female presence absence glasses without characterize conﬁgurations features examples covering combinations values form statistical separability allows one generalize new conﬁgurations person features never seen training chapter representation learning 
[representation, learning, exponential, gains, depth] seen sec multilayer perceptrons universal approximators functions represented exponentially smaller deep networks compared shallow networks decrease model size leads improved statistical eﬃciency section describe similar results apply generally kinds models distributed hidden representations sec saw example generative model learned explanatory factors underlying images faces including person gender whether wearing glasses generative model accomplished task based deep neural network would reasonable expect shallow network linear network learn complicated relationship abstract explanatory factors pixels image tasks factors chosen almost independently order generate data likely high level related highly nonlinear ways input argue demands deep distributed representations higher level features seen functions input factors seen generative causes obtained composition many nonlinearities proven many diﬀerent settings organizing computation composition many nonlinearities hierarchy reused features give exponential boost statistical eﬃciency top exponential boost given using distributed representation many kinds networks saturating nonlinearities boolean gates sum products rbf units single hidden layer shown universal approximators model family universal approximator approximate large class functions including continuous functions non zero tolerance level given enough hidden units however required number hidden units may large theoretical results concerning expressive power deep architectures state families functions represented eﬃciently architecture depth would require exponential number hidden units respect input size insuﬃcient depth depth depth sec saw deterministic feedforward networks universal approximators functions many structured probabilistic models single hidden layer latent variables including restricted boltzmann machines deep belief networks universal approximators probability distributions roux bengio montúfar montúfar krause sec saw suﬃciently deep feedforward network chapter representation learning exponential advantage network shallow results also obtained models probabilistic models one probabilistic model sum product network spn poon domingos models use polynomial circuits compute probability distribution set random variables showed exist delalleau bengio probability distributions minimum depth spn required avoid needing exponentially large model later martens medabalimi showed signiﬁcant diﬀerences every two ﬁnite depths spn constraints used make spns tractable may limit representational power another interesting development set theoretical results expressive power families deep circuits related convolutional nets highlighting exponential advantage deep circuit even shallow circuit allowed approximate function computed deep circuit cohen comparison previous theoretical work made claims regarding case shallow circuit must exactly replicate particular functions 
[representation, learning, providing, clues, discover, underlying, causes] close chapter come back one original questions makes one representation better another one answer ﬁrst introduced sec ideal representation one disentangles underlying causal factors variation generated data especially factors relevant applications strategies representation learning based introducing clues help learning ﬁnd underlying factors variations clues help learner separate observed factors others supervised learning provides strong clue label presented usually speciﬁes value least one factors variation directly generally make use abundant unlabeled data representation learning makes use less direct hints underlying factors hints take form implicit prior beliefs designers learning algorithm impose order guide learner results free lunch theorem show regularization strategies necessary obtain good generalization impossible ﬁnd universally superior regularization strategy one goal deep learning ﬁnd set fairly generic regularization strategies applicable wide variety tasks similar tasks people animals able solve provide list generic regularization strategies list chapter representation learning clearly exhaustive gives concrete examples ways learning algorithms encouraged discover features correspond underlying factors list introduced sec bengio partially expanded smoothness assumption  unit small assumption allows learner generalize training examples nearby points input space many machine learning algorithms leverage idea insuﬃcient overcome curse dimensionality linearity many learning algorithms assume relationships variables linear allows algorithm make predictions even far observed data sometimes lead overly extreme predictions simple machine learning algorithms make smoothness assumption instead make linearity assumption fact diﬀerent assumptions linear functions large weights applied high dimensional spaces may smooth see goodfellow discussion limitations linearity assumption multiple explanatory factors many representation learning algorithms motivated assumption data generated multiple underlying explanatory factors tasks solved easily given state factors sec describes view motivates semi supervised learning via representation learning learning structure requires learning features useful modeling refer underlying explanatory factors sec describes view motivates use distributed representations separate directions representation space corresponding separate factors variation causal factors model constructed way treats factors variation described learned representation causes observed data vice versa discussed sec advantageous semi supervised learning makes learned model robust distribution underlying causes changes use model new task depth hierarchical organization explanatory factors high level abstract concepts deﬁned terms simple concepts forming hierarchy another point view use deep architecture expresses belief task accomplished via multi step program chapter representation learning step referring back output processing accomplished via previous steps shared factors across tasks context many tasks corresponding diﬀerent variables sharing input task associated subset function global input assumption associated diﬀerent subset common pool relevant factors subsets overlap learning via shared intermediate representation allows sharing statistical strength tasks manifolds probability mass concentrates regions con centrates locally connected occupy tiny volume continuous case regions approximated low dimensional manifolds much smaller dimensionality original space data lives many machine learning algorithms behave sensibly manifold machine learning algorithms especially goodfellow autoencoders attempt explicitly learn structure manifold natural clustering many machine learning algorithms assume connected manifold input space may assigned single class data may lie many disconnected manifolds class remains constant within one assumption motivates variety learning algorithms including tangent propagation double backprop manifold tangent classiﬁer adversarial training temporal spatial coherence slow feature analysis related algorithms make assumption important explanatory factors change slowly time least easier predict true underlying explanatory factors predict raw observations pixel values see sec description approach sparsity features presumably relevant describing inputs need use feature detects elephant trunks representing image cat therefore reasonable impose prior feature interpreted present absent absent time simplicity factor dependencies good high level representations factors related simple dependencies simplest possible marginal independence linear chapter representation learning dependencies captured shallow autoencoder also reasonable assumptions seen many laws physics assumed plugging linear predictor factorized prior top learned representation concept representation learning ties together many forms deep learning feedforward recurrent networks autoencoders deep probabilistic models learn exploit representations learning best possible representation remains exciting avenue research 
[structured, probabilistic, models, deep, learning] deep learning draws upon many modeling formalisms researchers use guide design eﬀorts describe algorithms one formalisms idea structured probabilistic models already discussed structured probabilistic models brieﬂy sec brief presentation suﬃcient understand use structured probabilistic models language describe algorithms part part structured probabilistic models iii key ingredient many important research topics deep learning order prepare discuss research ideas chapter describes structured probabilistic models much greater detail chapter intended self contained reader need review earlier introduction continuing chapter structured probabilistic model way describing probability distribution using graph describe random variables probability distribution interact directly use graph graph theory sense set vertices connected one another set edges structure model deﬁned graph models often also referred graphical models graphical models research community large developed many diﬀerent models training algorithms inference algorithms chapter provide basic background central ideas graphical models emphasis concepts proven useful deep learning research community already strong background graphical models may wish skip chapter however even graphical model expert chapter structured probabilistic models deep learning may beneﬁt reading ﬁnal section chapter sec highlight unique ways graphical models used deep learning algorithms deep learning practitioners tend use diﬀerent model structures learning algorithms inference procedures commonly used rest graphical models research community chapter identify diﬀerences preferences explain reasons chapter ﬁrst describe challenges building large scale proba bilistic models next describe use graph describe structure probability distribution approach allows overcome many challenges without complications one major diﬃculties graphical modeling understanding variables need able interact directly graph structures suitable given problem outline two approaches resolving diﬃculty learning dependen cies sec finally close discussion unique emphasis deep learning practitioners place speciﬁc approaches graphical modeling sec 
[structured, probabilistic, models, deep, learning, challenge, unstructured, modeling] goal deep learning scale machine learning kinds challenges needed solve artiﬁcial intelligence means able understand high dimensional data rich structure example would like algorithms able understand natural images audio waveforms representing speech documents containing multiple words punctuation characters classiﬁcation algorithms take input rich high dimensional distribution summarize categorical label object photo word spoken recording topic document process classiﬁcation discards information input produces single output probability distribution values single output classiﬁer also often able ignore many parts input example recognizing object photo usually possible ignore background photo possible ask probabilistic models many tasks tasks often expensive classiﬁcation require producing multiple output values require complete understanding entire structure natural image image might captured camera reasonably ordinary environment opposed synthetically rendered image screenshot web page etc chapter structured probabilistic models deep learning input option ignore sections tasks include following density estimation given input machine learning system returns estimate true density data generating distribution requires single output require complete understanding entire input even one element vector unusual system must assign low probability denoising given damaged incorrectly observed input machine learning system returns estimate original correct example machine learning system might asked remove dust scratches old photograph requires multiple outputs every element estimated clean example understanding entire input since even one damaged area still reveal ﬁnal estimate damaged missing value imputation given observations elements model asked return estimates probability distribution unobserved elements requires multiple outputs model could asked restore elements must understand entire input sampling model generates new samples distribution applications include speech synthesis producing new waveforms sound like natural human speech requires multiple output values good model entire input samples even one element drawn wrong distribution sampling process wrong example sampling task using small natural images see fig modeling rich distribution thousands millions random variables challenging task computationally statistically suppose wanted model binary variables simplest possible case yet already seems overwhelming small pixel color rgb image possible binary images form number times larger estimated number atoms universe general wish model distribution random vector containing discrete variables capable taking values naive approach representing storing lookup table one probability value per possible outcome requires parameters feasible several reasons chapter structured probabilistic models deep learning figure probabilistic modeling natural images top example pixel color images cifar dataset samples krizhevsky hinton bottom drawn structured probabilistic model trained dataset sample appears position grid training example closest euclidean space comparison allows see model truly synthesizing new images rather memorizing training data contrast sets images adjusted display figure reproduced permission courville chapter structured probabilistic models deep learning memory cost storing representation small values representing distribution table require many values store statistical eﬃciency number parameters model increases amount training data needed choose values parameters using statistical estimator table based model astronomical number parameters require astronomically large training set accurately model overﬁt training set badly unless additional assumptions made linking diﬀerent entries table example like back smoothed gram models sec runtime cost inference suppose want perform inference task use model joint distribution compute distribution marginal distribution conditional distribution computing distributions require summing across entire table runtime operations high intractable memory cost storing model runtime cost sampling likewise suppose want draw sample model naive way sample value iterate table adding probability values exceed return outcome whose probability value added last requires reading whole table worst case exponential cost operations problem table based approach explicitly modeling every possible kind interaction every possible subset variables probability distributions encounter real tasks much simpler usually variables inﬂuence indirectly example consider modeling ﬁnishing times team relay race suppose team consists three runners alice bob carol start race alice carries baton begins running around track completing lap around track hands baton bob bob runs lap hands baton carol runs ﬁnal lap model ﬁnishing times continuous random variable alice ﬁnishing time depend anyone else since goes ﬁrst bob ﬁnishing time depends alice bob opportunity start lap alice completed alice ﬁnishes faster bob ﬁnish faster else chapter structured probabilistic models deep learning equal finally carol ﬁnishing time depends teammates alice slow bob probably ﬁnish late consequence carol quite late starting time thus likely late ﬁnishing time well however carol ﬁnishing time depends indirectly alice ﬁnishing time via bob already know bob ﬁnishing time able estimate carol ﬁnishing time better ﬁnding alice ﬁnishing time means model relay race using two interactions alice eﬀect bob bob eﬀect carol omit third indirect interaction alice carol model structured probabilistic models provide formal framework modeling direct interactions random variables allows models signiﬁcantly fewer parameters turn estimated reliably less data smaller models also dramatically reduced computational cost terms storing model performing inference model drawing samples model 
[structured, probabilistic, models, deep, learning, using, graphs, describe, model, structure] structured probabilistic models use graphs graph theory sense nodes vertices connected edges represent interactions random variables node represents random variable edge represents direct interaction direct interactions imply indirect interactions direct interactions need explicitly modeled one way describe interactions probability distribution using graph following sections describe popular useful approaches graphical models largely divided two categories models based directed acyclic graphs models based undirected graphs 
[structured, probabilistic, models, deep, learning, using, graphs, describe, model, structure, directed, models] one kind structured probabilistic model directed graphical model otherwise known belief network bayesian network pearl directed graphical models called directed edges directed judea pearl suggested using term bayesian network one wishes emphasize judgmental nature values computed network highlight usually represent degrees belief rather frequencies events chapter structured probabilistic models deep learning alice bob carol figure directed graphical model depicting relay race example alice ﬁnishing time inﬂuences bob ﬁnishing time bob get start running alice ﬁnishes likewise carol gets start running bob ﬁnishes bob ﬁnishing time directly inﬂuences carol ﬁnishing time point one vertex another direction represented drawing arrow direction arrow indicates variable probability distribution deﬁned terms drawing arrow means deﬁne probability distribution via conditional distribution one variables right side conditioning bar words distribution depends value continuing relay race example sec suppose name alice ﬁnishing time bob ﬁnishing time carol ﬁnishing time saw earlier estimate depends estimate depends directly indirectly draw relationship directed graphical model illustrated fig formally directed graphical model deﬁned variables deﬁned directed acyclic graph whose vertices random variables model set local conditional probability distributions gives parents probability distribution given relay race example means using graph drawn fig ﬁrst time seeing structured probabilistic model action examine cost using order observe structured modeling many advantages relative unstructured modeling suppose represented time discretizing time ranging minute minute second chunks would make discrete variables possible values attempted represent table would need store values values values values minus since probability one conﬁgurations made chapter structured probabilistic models deep learning redundant constraint sum probabilities instead make table conditional probability distributions distribution requires values table deﬁning given requires values table deﬁning given comes total values means using directed graphical model reduced number parameters factor general model discrete variables values cost single table approach scales like observed suppose build directed graphical model variables maximum number variables appearing either side conditioning bar single conditional probability distribution cost tables directed model scales like long design model get dramatic savings words long variable parents graph distribution represented parameters restrictions graph structure requiring tree also guarantee operations like computing marginal conditional distributions subsets variables eﬃcient important realize kinds information cannot encoded graph graph encodes simplifying assumptions variables conditionally independent also possible make kinds simplifying assumptions example suppose assume bob always runs regardless alice performed reality alice performance probably inﬂuences bob performance depending bob personality alice runs especially fast given race might encourage bob push hard match exceptional performance might make overconﬁdent lazy eﬀect alice bob ﬁnishing time must add alice ﬁnishing time total amount time think bob needs run observation allows deﬁne model parameters instead however note still directly dependent assumption represents absolute time bob ﬁnishes total time spends running means graph must still contain arrow assumption bob personal running time independent factors cannot encoded graph instead encode information deﬁnition conditional distribution conditional distribution longer element table indexed slightly complicated formula using parameters directed graphical model syntax place constraint deﬁne chapter structured probabilistic models deep learning conditional distributions deﬁnes variables allowed take arguments 
[structured, probabilistic, models, deep, learning, using, graphs, describe, model, structure, undirected, models] directed graphical models give one language describing structured proba bilistic models another popular language undirected models otherwise known markov random ﬁelds mrfs markov networks kindermann name implies undirected models use graphs whose edges undirected directed models naturally applicable situations clear reason draw arrow one particular direction often situations understand causality causality ﬂows one direction one situation relay race example earlier runners aﬀect ﬁnishing times later runners later runners aﬀect ﬁnishing times earlier runners situations might want model clear direction interactions interactions seem intrinsic direction operate directions may appropriate use undirected model example situation suppose want model distribution three binary variables whether sick whether coworker sick whether roommate sick relay race example make simplifying assumptions kinds interactions take place assuming coworker roommate know unlikely one give disease cold directly event seen rare acceptable model however reasonably likely either could give cold could pass model indirect transmission cold coworker roommate modeling transmission cold coworker transmission cold roommate case easy cause roommate get sick roommate make sick clean uni directional narrative base model motivates using undirected model directed models two nodes undirected model connected edge random variables corresponding nodes interact directly unlike directed models edge undirected model arrow associated conditional probability distribution denote random variable representing health random chapter structured probabilistic models deep learning figure undirected graph representing roommate health health work colleague health aﬀect roommate might infect cold work colleague might assuming roommate colleague know infect indirectly via variable representing roommate health random variable representing colleague health see fig drawing graph representing scenario formally undirected graphical model structured probabilistic model deﬁned undirected graph clique graph factor also called clique potential measures aﬃnity variables clique possible joint states factors constrained non negative together deﬁne unnormalized probability distribution unnormalized probability distribution eﬃcient work long cliques small encodes idea states higher aﬃnity likely however unlike bayesian network little structure deﬁnition cliques nothing guarantee multiplying together yield valid probability distribution see fig example reading factorization information undirected graph example cold spreading roommate colleague contains two cliques one clique contains factor clique deﬁned table might values resembling state indicates good health state indicates poor health infected cold usually healthy clique graph subset nodes connected edge graph chapter structured probabilistic models deep learning corresponding state highest aﬃnity state one sick lowest aﬃnity rare state state sick one infected higher aﬃnity state though still common state healthy complete model would need also deﬁne similar factor clique containing 
[structured, probabilistic, models, deep, learning, using, graphs, describe, model, structure, partition, function] unnormalized probability distribution guaranteed non negative everywhere guaranteed sum integrate obtain valid probability distribution must use corresponding normalized probability distribution value results probability distribution summing integrating think constant functions held constant note functions parameters function parameters common literature write arguments omitted save space normalizing constant known partition function term borrowed statistical physics since integral sum possible joint assignments state often intractable compute order able obtain normalized probability distribution undirected model model structure deﬁnitions functions must conducive computing eﬃciently context deep learning usually intractable due intractability computing exactly must resort approximations approximate algorithms topic chapter one important consideration keep mind designing undirected models possible specify factors way exist happens variables model continuous integral domain diverges example suppose want model single distribution deﬁned normalizing product clique potentials also called gibbs distribution chapter structured probabilistic models deep learning scalar variable single clique potential case since integral diverges probability distribution corresponding choice sometimes choice parameter functions determines whether probability distribution deﬁned example exp parameter determines whether exists positive results gaussian distribution values make impossible normalize one key diﬀerence directed modeling undirected modeling directed models deﬁned directly terms probability distributions start undirected models deﬁned loosely functions converted probability distributions changes intuitions one must develop order work models one key idea keep mind working undirected models domain variables dramatic eﬀect kind probability distribution given set functions corresponds example consider dimensional vector valued random variable undirected model parametrized vector biases suppose one clique element exp kind probability distribution result answer enough information yet speciﬁed domain integral deﬁning diverges probability distribution exists factorizes independent distributions sigmoid domain set elementary basis vectors softmax large value actually reduces often possible leverage eﬀect carefully chosen domain variable order obtain complicated behavior relatively simple set functions explore practical application idea later sec 
[structured, probabilistic, models, deep, learning, using, graphs, describe, model, structure, energy-based, models] many interesting theoretical results undirected models depend sumption convenient way enforce condition use energy based model ebm exp known energy function exp positive guarantees energy function result probability zero state chapter structured probabilistic models deep learning figure graph implies written appropriate choice func tions completely free choose energy function makes learning simpler learned clique potentials directly would need use constrained optimization arbitrarily impose speciﬁc minimal probability value learning energy function use unconstrained optimization probabilities energy based model approach arbitrarily close zero never reach distribution form given example boltzmann distribution reason many energy based models called boltzmann machines fahlman ackley hinton hinton sejnowski accepted guideline call model energy based model call boltzmann machine term boltzmann machine ﬁrst introduced describe model exclusively binary variables today many models mean covariance restricted boltzmann machine incorporate real valued variables well boltzmann machines originally deﬁned encompass models without latent variables term boltzmann machine today often used designate models latent variables boltzmann machines without latent variables often called markov random ﬁelds log linear models cliques undirected graph correspond factors unnormalized probability function exp exp exp means diﬀerent cliques undirected graph correspond diﬀerent terms energy function words energy based model special kind markov network exponentiation makes term energy function correspond factor diﬀerent clique see fig example read form energy function undirected graph structure one view energy based model multiple terms energy function product experts term energy function corresponds another hinton models may still need use constrained optimization make sure exists chapter structured probabilistic models deep learning figure graph implies written appropriate choice per clique energy functions note obtain functions fig setting exponential corresponding negative energy exp factor probability distribution term energy function thought expert determines whether particular soft constraint satisﬁed expert may enforce one constraint concerns low dimensional projection random variables combined multiplication probabilities experts together enforce complicated high dimensional constraint one part deﬁnition energy based model serves functional purpose machine learning point view sign sign could incorporated deﬁnition many functions learning algorithm could simply learn parameters opposite sign sign present primarily preserve compatibility machine learning literature physics literature many advances probabilistic modeling originally developed statistical physicists refers actual physical energy arbitrary sign terminology energy partition function remains associated techniques even though mathematical applicability broader physics context developed machine learning researchers smolensky referred negative energy chosen emit negation harmony standard convention many algorithms operate probabilistic models need compute model log model energy based models latent variables algorithms sometimes phrased terms negative quantity called free energy log exp book usually prefer general log model formulation chapter structured probabilistic models deep learning figure path random variable random variable active observed means separated shaded indicate observed path path inactive conclude separated given 
[structured, probabilistic, models, deep, learning, using, graphs, describe, model, structure, separation, d-separation] edges graphical model tell variables directly interact often need know variables indirectly interact indirect interactions enabled disabled observing variables formally would like know subsets variables conditionally independent given values subsets variables identifying conditional independences graph simple case undirected models case conditional independence implied graph called separation say set variables separated another set variables given third set variables graph structure implies independent given two variables connected path involving unobserved variables variables separated path exists paths contain observed variable separated refer paths involving unobserved variables active paths including observed variable inactive draw graph indicate observed variables shading see fig depiction active inactive paths undirected model look drawn way see fig example reading separation undirected graph similar concepts apply directed models except context directed models concepts referred separation stands dependence separation directed graphs deﬁned separation undirected graphs say set variables separated another set variables given third set variables graph structure implies independent given undirected models examine independences implied graph looking active paths exist graph two variables dependent active path separated chapter structured probabilistic models deep learning figure example reading separation properties undirected graph shaded indicate observed observing blocks path say separated given observation also blocks one path second active path therefore separated given path exists directed nets determining whether path active somewhat complicated see fig guide identifying active paths directed model see fig example reading properties graph important remember separation separation tell conditional independences implied graph requirement graph imply independences present particular always legitimate use complete graph graph possible edges represent distribution fact distributions contain independences possible represent existing graphical notation context speciﬁc independences independences present dependent value variables network example consider model three binary variables suppose independent deterministically equal encoding behavior requires edge connecting graph fails indicate independent general graph never imply independence exists however graph may fail encode independence 
[structured, probabilistic, models, deep, learning, using, graphs, describe, model, structure, converting, undirected, directed, graphs] often refer speciﬁc machine learning model undirected directed example typically refer rbms undirected sparse coding directed choice wording somewhat misleading probabilistic model inherently directed undirected instead models easily described using directed graph easily described using undirected graph chapter structured probabilistic models deep learning figure kinds active paths length two exist random variables path arrows proceeding directly vice versa kind path becomes blocked observed already seen kind path relay race example connected common cause example suppose variable indicating whether hurricane measure wind speed two diﬀerent nearby weather monitoring outposts observe high winds station might expect also see high winds kind path blocked observing already know hurricane expect see high winds regardless observed lower expected wind hurricane would change expectation winds knowing hurricane however observed dependent path active parents called structure collider case structure causes related explaining away eﬀect case path actually active observed example suppose variable indicating colleague work variable represents sick represents vacation observe work presume probably sick vacation especially likely happened time ﬁnd vacation fact suﬃcient absence infer probably also sick explain explaining away eﬀect happens even descendant observed example suppose variable representing whether received report colleague notice received report increases estimate probability work today turn makes likely either sick vacation way block path structure observe none descendants shared child chapter structured probabilistic models deep learning figure graph read several separation properties examples include separated given empty set separated given separated given also see variables longer separated observe variables separated given separated given chapter structured probabilistic models deep learning figure examples complete graphs describe probability distribution show examples four random variables left complete undirected graph undirected case complete graph unique right complete directed graph directed case unique complete graph choose ordering variables draw arc variable every variable comes ordering thus factorial number complete graphs every set random variables example order variables left right top bottom directed models undirected models advantages disad vantages neither approach clearly superior universally preferred instead choose language use task choice partially depend probability distribution wish describe may choose use either directed modeling undirected modeling based approach capture independences probability distribution approach uses fewest edges describe distribution factors aﬀect decision language use even working single probability distribution may sometimes switch diﬀerent modeling languages sometimes diﬀerent language becomes appropriate observe certain subset variables wish perform diﬀerent computational task example directed model description often provides straightforward approach eﬃciently draw samples model described sec undirected model formulation often useful deriving approximate inference procedures see chapter role undirected models highlighted every probability distribution represented either directed model undirected model worst case one always represent distribution using complete graph case directed model complete graph directed acyclic graph impose ordering random variables variable variables precede ordering ancestors graph undirected model complete graph simply graph containing single clique encompassing variables see fig example chapter structured probabilistic models deep learning course utility graphical model graph implies variables interact directly complete graph useful imply independences represent probability distribution graph want choose graph implies many independences possible without implying independences actually exist point view distributions represented eﬃciently using directed models distributions represented eﬃciently using undirected models words directed models encode independences undirected models cannot encode vice versa directed models able use one speciﬁc kind substructure undirected models cannot represent perfectly substructure called immorality structure occurs two random variables parents third random variable edge directly connecting either direction name immorality may seem strange coined graphical models literature joke unmarried parents convert directed model graph undirected model need create new graph every pair variables add undirected edge connecting directed edge either direction connecting parents third variable resulting known moralized graph see fig examples converting directed models undirected models via moralization likewise undirected models include substructures directed model represent perfectly speciﬁcally directed graph cannot capture conditional independences implied undirected graph contains loop length greater three unless loop also contains chord loop sequence variables connected undirected edges last variable sequence connected back ﬁrst variable sequence chord connection two non consecutive variables sequence deﬁning loop loops length four greater chords loops must add chords convert directed model adding chords discards independence information encoded graph formed adding chords known chordal triangulated graph loops described terms smaller triangular loops build directed graph chordal graph need also assign directions edges must create directed cycle result deﬁne valid directed probabilistic model one way assign directions edges impose ordering random chapter structured probabilistic models deep learning figure examples converting directed models top row undirected models bottom row constructing moralized graphs left simple chain converted moralized graph merely replacing directed edges undirected edges resulting undirected model implies exactly set independences conditional independences graph simplest directed model cannot center converted undirected model without losing independences graph consists entirely single immorality parents connected active path observed capture dependence undirected model must include clique encompassing three variables clique fails encode fact general moralization may add many edges graph thus losing many right implied independences example sparse coding graph requires adding moralizing edges every pair hidden units thus introducing quadratic number new direct dependences chapter structured probabilistic models deep learning figure converting undirected model directed model left undirected model cannot converted directed directed model loop length four chords speciﬁcally undirected model encodes two diﬀerent independences directed model capture simultaneously center convert undirected model directed model must triangulate graph ensuring loops greater length three chord either add edge connecting add edge connecting example choose add edge connecting right ﬁnish conversion process must assign direction edge must create directed cycles one way avoid directed cycles impose ordering nodes always point edge node comes earlier ordering node comes later ordering example use variable names impose alphabetical order variables point edge node comes earlier ordering node comes later ordering see fig demonstration 
[structured, probabilistic, models, deep, learning, using, graphs, describe, model, structure, factor, graphs] factor graphs another way drawing undirected models resolve ambiguity graphical representation standard undirected model syntax undirected model scope every function must subset clique graph however necessary exist whose scope contains entirety every clique factor graphs explicitly represent scope function speciﬁcally factor graph graphical representation undirected model consists bipartite undirected graph nodes drawn circles nodes correspond random variables standard undirected model rest nodes drawn squares nodes correspond factors unnormalized probability distribution variables factors may connected undirected edges variable factor connected graph variable one arguments factor unnormalized probability distribution factor may connected another factor graph variable connected chapter structured probabilistic models deep learning variable see fig example factor graphs resolve ambiguity interpretation undirected networks figure example factor graph resolve ambiguity interpretation undirected networks left undirected network clique involving three variables factor graph corresponding undirected center model factor graph one factor three variables right another valid factor graph undirected model factor graph three factors two variables representation inference learning asymptotically cheaper factor graph factor graph depicted center even though require undirected graph represent 
[structured, probabilistic, models, deep, learning, sampling, graphical, models] graphical models also facilitate task drawing samples model one advantage directed graphical models simple eﬃcient procedure called ancestral sampling produce sample joint distribution represented model basic idea sort variables graph topological ordering greater parent variables sampled order words ﬁrst sample sample ﬁnally sample long conditional distribution easy sample whole model easy sample topological sorting operation guarantees read conditional distributions sample order without topological sorting might attempt sample variable parents available graphs one topological ordering possible ancestral sampling may used topological orderings ancestral sampling generally fast assuming sampling condi chapter structured probabilistic models deep learning tional easy convenient one drawback ancestral sampling applies directed graphical models another drawback support every conditional sampling operation wish sample subset variables directed graphical model given variables often require condition ing variables come earlier variables sampled ordered graph case sample local conditional probability distributions speciﬁed model distribution otherwise conditional distributions need sample posterior distributions given observed variables posterior distributions usually explicitly speciﬁed parametrized model inferring posterior distributions costly models case ancestral sampling longer eﬃcient unfortunately ancestral sampling applicable directed models sample undirected models converting directed models often requires solving intractable inference problems determine marginal distribution root nodes new directed graph requires introducing many edges resulting directed model becomes intractable sampling undirected model without ﬁrst converting directed model seems require resolving cyclical dependencies every variable interacts every variable clear beginning point sampling process unfortunately drawing samples undirected graphical model expensive multi pass process conceptually simplest approach suppose gibbs sampling graphical model dimensional vector random variables iteratively visit variable draw sample conditioned variables due separation properties graphical model equivalently condition neighbors unfortunately made one pass graphical model sampled variables still fair sample instead must repeat process resample variables using updated values neighbors asymptotically many repetitions process converges sampling correct distribution diﬃcult determine samples reached suﬃciently accurate approximation desired distribution sampling techniques undirected models advanced topic covered detail chapter chapter structured probabilistic models deep learning 
[structured, probabilistic, models, deep, learning, advantages, structured, modeling] primary advantage using structured probabilistic models allow dramatically reduce cost representing probability distributions well learning inference sampling also accelerated case directed models situation complicated undirected models primary mechanism allows operations use less runtime memory choosing model certain interactions graphical models convey information leaving edges anywhere edge model speciﬁes assumption need model direct interaction less quantiﬁable beneﬁt using structured probabilistic models allow explicitly separate representation knowledge learning knowledge inference given existing knowledge makes models easier develop debug design analyze evaluate learning algorithms inference algorithms applicable broad classes graphs independently design models capture relationships believe important data combine diﬀerent algorithms structures obtain cartesian product diﬀerent possibilities would much diﬃcult design end end algorithms every possible situation 
[structured, probabilistic, models, deep, learning, learning, dependencies] good generative model needs accurately capture distribution observed visible variables often diﬀerent elements highly dependent context deep learning approach commonly used model dependencies introduce several latent hidden variables model capture dependencies pair variables indirectly via direct dependencies direct dependencies good model contain latent variables would need large numbers parents per node bayesian network large cliques markov network representing higher order interactions costly computational sense number parameters must stored memory scales exponentially number members clique also statistical sense exponential number parameters requires wealth data estimate accurately model intended capture dependencies visible variables direct connections usually infeasible connect variables graph chapter structured probabilistic models deep learning must designed connect variables tightly coupled omit edges variables entire ﬁeld machine learning called structure learning devoted problem good reference structure learning see koller friedman structure learning techniques form greedy search structure proposed model structure trained given score score rewards high training set accuracy penalizes model complexity candidate structures small number edges added removed proposed next step search search proceeds new structure expected increase score using latent variables instead adaptive structure avoids need perform discrete searches multiple rounds training ﬁxed structure visible hidden variables use direct interactions visible hidden units impose indirect interactions visible units using simple parameter learning techniques learn model ﬁxed structure imputes right structure marginal latent variables advantages beyond role eﬃciently capturing new variables also provide alternative representation example discussed sec mixture gaussians model learns latent variable corresponds category examples input drawn means latent variable mixture gaussians model used classiﬁcation chapter saw simple probabilistic models like sparse coding learn latent variables used input features classiﬁer coordinates along manifold models used way deeper models models diﬀerent kinds interactions create even richer descriptions input many approaches accomplish feature learning learning latent variables often given model experimental observations show argmax good feature mapping 
[structured, probabilistic, models, deep, learning, inference, approximate, inference] one main ways use probabilistic model ask questions variables related given set medical tests ask disease patient might latent variable model might want extract features describing observed variables sometimes need solve problems order perform tasks often train models using principle maximum likelihood log log log chapter structured probabilistic models deep learning often want compute order implement learning rule examples inference problems must predict value variables given variables predict probability distribution variables given value variables unfortunately interesting deep models inference problems intractable even use structured graphical model simplify graph structure allows represent complicated high dimensional distributions reasonable number parameters graphs used deep learning usually restrictive enough also allow eﬃcient inference straightforward see computing marginal probability general graphical model hard complexity class generalization complexity class problems require determining whether problem solution ﬁnding solution one exists problems require counting number solutions construct worst case graphical model imagine deﬁne graphical model binary variables sat problem impose uniform distribution variables add one binary latent variable per clause indicates whether clause satisﬁed add another latent variable indicating whether clauses satisﬁed done without making large clique building reduction tree latent variables node tree reporting whether two variables satisﬁed leaves tree variables clause root tree reports whether entire problem satisﬁed due uniform distribution literals marginal distribution root reduction tree speciﬁes fraction assignments satisfy problem contrived worst case example hard graphs commonly arise practical real world scenarios motivates use approximate inference context deep learning usually refers variational inference approximate true distribution seeking approximate distribution close true one possible techniques described depth chapter 
[structured, probabilistic, models, deep, learning, probabilistic, models] deep learning practitioners generally use basic computational tools machine learning practitioners work structured probabilistic models chapter structured probabilistic models deep learning however context deep learning usually make diﬀerent design decisions combine tools resulting overall algorithms models diﬀerent ﬂavor traditional graphical models deep learning always involve especially deep graphical models context graphical models deﬁne depth model terms graphical model graph rather computational graph think latent variable depth shortest path observed variable steps usually describe depth model greatest depth kind depth diﬀerent depth induced computational graph many generative models used deep learning latent variables one layer latent variables use deep computational graphs deﬁne conditional distributions within model deep learning essentially always makes use idea distributed represen tations even shallow models used deep learning purposes pretraining shallow models later composed form deep ones nearly always single large layer latent variables deep learning models typically latent variables observed variables complicated nonlinear interactions variables accomplished via indirect connections ﬂow multiple latent variables contrast traditional graphical models usually contain mostly variables least occasionally observed even many variables missing random training examples traditional models mostly use higher order terms structure learning capture complicated nonlinear interactions variables latent variables usually number way latent variables designed also diﬀers deep learning deep learning practitioner typically intend latent variables take speciﬁc semantics ahead time training algorithm free invent concepts needs model particular dataset latent variables usually easy human interpret fact though visualization techniques may allow rough characterization represent latent variables used context traditional graphical models often designed speciﬁc semantics mind topic document intelligence student disease causing patient symptoms etc models often much interpretable human practitioners often theoretical guarantees yet less able scale complex problems reusable many diﬀerent contexts deep models another obvious diﬀerence kind connectivity typically used deep learning approach deep graphical models typically large groups units chapter structured probabilistic models deep learning connected groups units interactions two groups may described single matrix traditional graphical models connections choice connections variable may individually designed design model structure tightly linked choice inference algorithm traditional approaches graphical models typically aim maintain tractability exact inference constraint limiting popular approximate inference algorithm algorithm called loopy belief propagation approaches often work well sparsely connected graphs comparison models used deep learning tend connect visible unit many hidden units provide distributed representation probably several observed variables distributed representations many advantages point view graphical models computational complexity distributed representations disadvantage usually yielding graphs sparse enough traditional techniques exact inference loopy belief propagation relevant consequence one striking diﬀerences larger graphical models community deep graphical models community loopy belief propagation almost never used deep learning deep models instead designed make gibbs sampling variational inference algorithms eﬃcient another consideration deep learning models contain large number latent variables making eﬃcient numerical code essential provides additional motivation besides choice high level inference algorithm grouping units layers matrix describing interaction two layers allows individual steps algorithm implemented eﬃcient matrix product operations sparsely connected generalizations like block diagonal matrix products convolutions finally deep learning approach graphical modeling characterized marked tolerance unknown rather simplifying model quantities might want computed exactly increase power model barely possible train use often use models whose marginal distributions cannot computed satisﬁed simply draw approximate samples models often train models intractable objective function cannot even approximate reasonable amount time still able approximately train model eﬃciently obtain estimate gradient function deep learning approach often ﬁgure minimum amount information absolutely need ﬁgure get reasonable approximation information quickly possible chapter structured probabilistic models deep learning figure rbm drawn markov network 
[structured, probabilistic, models, deep, learning, probabilistic, models, example, restricted, boltzmann, machine] restricted boltzmann machine harmonium rbm smolensky quintessential example graphical models used deep learning rbm deep model instead single layer latent variables may used learn representation input chapter see rbms used build many deeper models show rbm exempliﬁes many practices used wide variety deep graphical models units organized large groups called layers connectivity layers described matrix connectivity relatively dense model designed allow eﬃcient gibbs sampling emphasis model design freeing training algorithm learn latent variables whose semantics speciﬁed designer later sec revisit rbm detail canonical rbm energy based model binary visible hidden units energy function unconstrained real valued learnable parameters see model divided two groups units interaction described matrix model depicted graphically fig ﬁgure makes clear important aspect model direct interactions two visible units two hidden units hence restricted general boltzmann machine may arbitrary connections restrictions rbm structure yield nice properties chapter structured probabilistic models deep learning figure samples trained rbm weights image reproduced permission lisa left samples model trained mnist drawn using gibbs sampling column separate gibbs sampling process row represents output another steps gibbs sampling successive samples highly correlated one another corresponding weight vectors compare right samples weights linear factor model shown fig samples much better rbm prior constrained factorial rbm learn features appear together sampling hand rbm posterior factorial sparse coding posterior sparse coding model may better feature extraction models able non factorial non factorial individual conditionals simple compute well binary rbm obtain together properties allow eﬃcient block gibbs sampling alternates sampling simultaneously sampling simultaneously samples generated gibbs sampling rbm model shown fig since energy function linear function parameters easy take derivatives energy function example two properties eﬃcient gibbs sampling eﬃcient derivatives make training convenient chapter see undirected models may trained computing derivatives applied samples model training model induces representation data often use set features describe chapter structured probabilistic models deep learning overall rbm demonstrates typical deep learning approach graph ical models representation learning accomplished via layers latent variables combined eﬃcient interactions layers parametrized matrices language graphical models provides elegant ﬂexible clear language describing probabilistic models chapters ahead use language among perspectives describe wide variety deep probabilistic models 
[monte, carlo, methods] randomized algorithms fall two rough categories las vegas algorithms monte carlo algorithms las vegas algorithms always return precisely correct answer report failed algorithms consume random amount resources usually memory time contrast monte carlo algorithms return answers random amount error amount error typically reduced expending resources usually running time memory ﬁxed computational budget monte carlo algorithm provide approximate answer many problems machine learning diﬃcult never expect obtain precise answers excludes precise deterministic algorithms las vegas algorithms instead must use deterministic approximate algorithms monte carlo approximations approaches ubiquitous machine learning chapter focus monte carlo methods 
[monte, carlo, methods, sampling, monte, carlo, methods] many important technologies used accomplish machine learning goals based drawing samples probability distribution using samples form monte carlo estimate desired quantity 
[monte, carlo, methods, sampling, monte, carlo, methods, sampling] many reasons may wish draw samples probability distribution sampling provides ﬂexible way approximate many sums chapter monte carlo methods integrals reduced cost sometimes use provide signiﬁcant speedup costly tractable sum case subsample full training cost minibatches cases learning algorithm requires approximate intractable sum integral gradient log partition function undirected model many cases sampling actually goal sense want train model sample training distribution 
[monte, carlo, methods, sampling, monte, carlo, methods, basics, monte, carlo, sampling] sum integral cannot computed exactly example sum exponential number terms exact simpliﬁcation known often possible approximate using monte carlo sampling idea view sum integral expectation distribution approximate expectation corresponding average let sum integral estimate rewritten expectation constraint probability distribution sum probability density integral random variable approximate drawing samples forming empirical average approximation justiﬁed diﬀerent properties ﬁrst trivial observation estimator unbiased since addition law large numbers states samples average converges almost surely expected value lim chapter monte carlo methods provided variance individual terms var bounded see clearly consider variance increases variance var decreases converges long var var var var convenient result also tells estimate uncertainty monte carlo average equivalently amount expected error monte carlo approximation compute empirical average empirical variance divide estimated variance number samples obtain estimator var central limit theorem tells distribution average converges normal distribution mean variance var allows estimate conﬁdence intervals around estimate using cumulative distribution normal density however relies ability easily sample base distribution always possible feasible sample alternative use importance sampling presented sec general approach form sequence estimators converge towards distribution interest approach monte carlo markov chains sec 
[monte, carlo, methods, importance, sampling] important step decomposition integrand summand used monte carlo method deciding part integrand play role probability part integrand play role quantity whose expected value probability distribution estimated unique decomposition always rewritten sample average many cases wish compute expectation given fact problem speciﬁed unbiased estimator variance often preferred sum squared diﬀerences divided instead chapter monte carlo methods start expectation suggests would natural choice decomposition however original speciﬁcation problem may optimal choice terms number samples required obtain given level accuracy fortunately form optimal choice derived easily optimal corresponds called optimal importance sampling identity shown monte carlo estimator transformed importance sampling estimator see readily expected value estimator depend however variance importance sampling estimator greatly sensitive choice variance given var var minimum variance occurs normalization constant chosen sums integrates appropriate better importance sampling distributions put weight integrand larger fact change sign var meaning single sample suﬃcient optimal distribution used course computation essentially solved original problem usually practical use approach drawing single sample optimal distribution choice sampling distribution valid sense yielding correct expected value optimal one sense yielding minimum variance sampling usually infeasible choices feasible still reducing variance somewhat chapter monte carlo methods another approach use biased importance sampling advantage requiring normalized case discrete variables biased importance sampling estimator given bis unnormalized forms samples estimator biased bis except asymptotically denominator converges hence estimator called asymptotically unbiased although good choice greatly improve eﬃciency monte carlo estimation poor choice make eﬃciency much worse going back see samples large variance estimator get large may happen tiny neither small enough cancel distribution usually chosen simple distribution easy sample high dimensional simplicity causes match poorly importance sampling collects useless samples summing tiny numbers zeros hand happen rarely ratio huge latter events rare may show typical sample yielding typical underestimation compensated rarely gross overestimation large small numbers typical high dimensional high dimension dynamic range joint probabilities large spite danger importance sampling variants found useful many machine learning algorithms including deep learning algorithms example see use importance sampling accelerate training neural language models large vocabulary sec neural nets large number outputs see also importance sampling used estimate partition function normalization constant probability chapter monte carlo methods distribution sec estimate log likelihood deep directed models variational autoencoder sec importance sampling may also used improve estimate gradient cost function used train model parameters stochastic gradient descent particularly models classiﬁers total value cost function comes small number misclassiﬁed examples sampling diﬃcult examples frequently reduce variance gradient cases hinton 
[monte, carlo, methods, markov, chain, monte, carlo, methods] many cases wish use monte carlo technique tractable method drawing exact samples distribution model good low variance importance sampling distribution context deep learning often happens model represented undirected model cases introduce mathematical tool called markov chain approximately sample model family algorithms use markov chains perform monte carlo estimates called markov chain monte carlo methods mcmc markov chain monte carlo methods machine learning described greater length koller friedman standard generic guarantees mcmc techniques applicable model assign zero probability state therefore convenient present techniques sampling energy based model ebm exp described sec ebm formulation every state guaranteed non zero probability mcmc methods fact broadly applicable used many probability distributions contain zero probability states however theoretical guarantees concerning behavior mcmc methods must proven case case basis diﬀerent families distributions context deep learning common rely general theoretical guarantees naturally apply energy based models understand drawing samples energy based model diﬃcult consider ebm two variables deﬁning distribution order sample must draw order sample must draw seems intractable chicken egg problem directed models avoid graph directed acyclic perform ancestral sampling one simply samples variables topological order conditioning variable parents guaranteed already sampled sec ancestral sampling deﬁnes eﬃcient single pass method chapter monte carlo methods obtaining sample ebm avoid chicken egg problem sampling using markov chain core idea markov chain state begins arbitrary value time randomly update repeatedly eventually becomes nearly fair sample formally markov chain deﬁned random state transition distribution specifying probability random update state starts state running markov chain means repeatedly updating state value sampled gain theoretical understanding mcmc methods work useful reparametrize problem first restrict attention case random variable countably many states case represent state positive integer diﬀerent integer values map back diﬀerent states original problem consider happens run inﬁnitely many markov chains parallel states diﬀerent markov chains drawn distribution indicates number time steps elapsed beginning distribution used arbitrarily initialize markov chain later inﬂuenced markov chain steps run far goal converge reparametrized problem terms positive integer describe probability distribution using vector consider happens update single markov chain state new state probability single state landing state given using integer parametrization represent eﬀect transition operator using matrix deﬁne using deﬁnition rewrite rather writing terms understand single state updated may use describe entire distribution diﬀerent markov chains run parallel shifts apply update chapter monte carlo methods applying markov chain update repeatedly corresponds multiplying matrix repeatedly words think process exponentiating matrix matrix special structure columns represents probability distribution matrices called stochastic matrices non zero probability transitioning state state power perron frobenius theorem perron frobenius guarantees largest eigenvalue real equal time see eigenvalues exponentiated diag diag process causes eigenvalues equal decay zero additional mild conditions guaranteed one eigenvector eigenvalue process thus converges stationary distribution equilibrium distribution sometimes also called convergence condition holds every additional step eigenvector equation stationary point must eigenvector corresponding eigenvalue condition guarantees reached stationary distribution repeated applications transition sampling procedure change states various markov chains although distribution transition operator change individual state course chosen correctly stationary distribution equal distribution wish sample describe choose shortly sec properties markov chains countable states generalized continuous variables situation authors call markov chain use term markov chain describe conditions harris chain general markov chain transition operator converge mild conditions ﬁxed point described equation discrete case rewriting discrete expectation corresponds sum continuous expectation corresponds integral chapter monte carlo methods regardless whether state continuous discrete markov chain methods consist repeatedly applying stochastic updates eventually state begins yield samples equilibrium distribution running markov chain reaches equilibrium distribution called markov burning chain chain reached equilibrium sequence inﬁnitely many samples may drawn equilibrium distribution identically distributed two successive samples highly correlated ﬁnite sequence samples may thus representative equilibrium distribution one way mitigate problem return every successive samples estimate statistics equilibrium distribution biased correlation mcmc sample next several samples markov chains thus expensive use time required burn equilibrium distribution time required transition one sample another reasonably decorrelated sample reaching equilibrium one desires truly independent samples one run multiple markov chains parallel approach uses extra parallel computation eliminate latency strategy using single markov chain generate samples strategy using one markov chain desired sample two extremes deep learning practitioners usually use number chains similar number examples minibatch draw many samples needed ﬁxed set markov chains commonly used number markov chains another diﬃculty know advance many steps markov chain must run reaching equilibrium distribution length time called also diﬃcult test whether markov mixing time chain reached equilibrium precise enough theory guiding answering question theory tells chain converge much analyze markov chain point view matrix acting vector probabilities know chain mixes eﬀectively lost eigenvalues besides unique eigenvalue means magnitude second largest eigenvalue determine mixing time however practice cannot actually represent markov chain terms matrix number states probabilistic model visit exponentially large number variables infeasible represent eigenvalues due obstacles usually know whether markov chain mixed instead simply run markov chain amount time roughly estimate suﬃcient use heuristic methods determine whether chain mixed heuristic methods include manually inspecting samples measuring correlations successive samples chapter monte carlo methods 
[monte, carlo, methods, gibbs, sampling] far described draw samples distribution repeatedly updating however described ensure useful distribution two basic approaches considered book ﬁrst one derive given learned model described case sampling ebms second one directly parametrize learn stationary distribution implicitly deﬁnes model interest examples second approach discussed sec sec context deep learning commonly use markov chains draw samples energy based model deﬁning distribution model case want markov chain model obtain desired must choose appropriate conceptually simple eﬀective approach building markov chain samples model use sampling gibbs sampling accomplished selecting one variable sampling model conditioned neighbors undirected graph deﬁning structure energy based model also possible sample several variables time long conditionally independent given neighbors shown rbm example sec hidden units rbm may sampled simultaneously conditionally independent given visible units likewise visible units may sampled simultaneously conditionally independent given hidden units gibbs sampling approaches update many variables simultaneously way called block gibbs sampling alternate approaches designing markov chains sample model possible example metropolis hastings algorithm widely used disciplines context deep learning approach undirected modeling rare use approach gibbs sampling improved sampling techniques one possible research frontier 
[monte, carlo, methods, challenge, mixing, separated, modes] primary diﬃculty involved mcmc methods tendency poorly ideally successive samples markov chain designed sample mix would completely independent would visit many diﬀerent regions space proportional probability instead especially high dimensional cases mcmc samples become correlated refer chapter monte carlo methods behavior slow mixing even failure mix mcmc methods slow mixing seen inadvertently performing something resembling noisy gradient descent energy function equivalently noisy hill climbing probability respect state chain random variables sampled chain tends take small steps space state markov chain conﬁguration conﬁguration energy generally lower approximately equal energy preference moves yield lower energy conﬁgurations starting rather improbable conﬁguration higher energy typical ones chain tends gradually reduce energy state occasionally move another mode chain found region low energy example variables pixels image region low energy might connected manifold images object call mode chain tend walk around mode following kind random walk step mode generally return ﬁnds escape route move towards another mode problem successful escape routes rare many interesting distributions markov chain continue sample mode longer clear consider gibbs sampling algorithm sec context consider probability going one mode nearby mode within given number steps determine probability shape energy barrier modes transitions two modes separated high energy barrier region low probability exponentially less likely terms height energy barrier illustrated fig problem arises multiple modes high probability separated regions low probability especially gibbs sampling step must update small subset variables whose values largely determined variables simple example consider energy based model two variables binary sign taking values large positive number model expresses strong belief sign consider updating using gibbs sampling step conditional distribution given large sigmoid saturates probability also assigning close likewise probability assigning close according model signs variables equally likely according model variables sign means gibbs sampling rarely ﬂip signs variables chapter monte carlo methods figure paths followed gibbs sampling three distributions markov chain initialized mode cases left multivariate normal distribution two independent variables gibbs sampling mixes well variables independent multivariate normal distribution highly correlated variables center correlation variables makes diﬃcult markov chain mix variable must updated conditioned correlation reduces rate markov chain move away starting point right mixture gaussians widely separated modes axis aligned gibbs sampling mixes slowly diﬃcult change modes altering one variable time practical scenarios challenge even greater care making transitions two modes generally many modes real model might contain several transitions diﬃcult diﬃculty mixing modes becomes expensive obtain reliable set samples covering modes convergence chain stationary distribution slow sometimes problem resolved ﬁnding groups highly dependent units updating simultaneously block unfortunately dependencies complicated computationally intractable draw sample group problem markov chain originally introduced solve problem sampling large group variables context models latent variables deﬁne joint distribution model often draw samples alternating sampling model sampling model point view mixing rapidly would like model high entropy however point view learning useful representation would like encode chapter monte carlo methods figure illustration slow mixing problem deep probabilistic models panel read left right top bottom left consecutive samples gibbs sampling applied deep boltzmann machine trained mnist dataset consecutive samples similar gibbs sampling performed deep graphical model similarity based semantic rather raw visual features still diﬃcult gibbs chain transition one mode distribution another example changing digit identity consecutive right ancestral samples generative adversarial network ancestral sampling generates sample independently others mixing problem enough information reconstruct well implies high mutual information two goals odds often learn generative models precisely encode able mix well situation arises frequently boltzmann machines sharper distribution boltzmann machine learns harder markov chain sampling model distribution mix well problem illustrated fig could make mcmc methods less useful distribution interest manifold structure separate manifold class distribution concentrated around many modes modes separated vast regions high energy type distribution expect many classiﬁcation problems would make mcmc methods converge slowly poor mixing modes 
[monte, carlo, methods, challenge, mixing, separated, modes, tempering, mix, modes] distribution sharp peaks high probability surrounded regions low probability diﬃcult mix diﬀerent modes distribution chapter monte carlo methods several techniques faster mixing based constructing alternative versions target distribution peaks high surrounding valleys low energy based models provide particularly simple way far described energy based model deﬁning probability distribution exp energy based models may augmented extra parameter controlling sharply peaked distribution exp parameter often described reciprocal temperature reﬂecting origin energy based models statistical physics temperature falls zero rises inﬁnity energy based model becomes deterministic temperature rises inﬁnity falls zero distribution discrete becomes uniform typically model trained evaluated however make use temperatures particularly tempering general strategy mixing modes rapidly drawing samples markov chains based tempered transitions temporarily sample neal higher temperature distributions order mix diﬀerent modes resume sampling unit temperature distribution techniques applied models rbms salakhutdinov another approach use parallel tempering markov chain simulates many iba diﬀerent states parallel diﬀerent temperatures highest temperature states mix slowly lowest temperature states temperature provide accurate samples model transition operator includes stochastically swapping states two diﬀerent temperature levels suﬃciently high probability sample high temperature slot jump lower temperature slot approach also applied rbms desjardins cho although tempering promising approach point allowed researchers make strong advance solving challenge sampling complex ebms one possible reason critical temperatures around temperature transition must slow temperature gradually reduced order tempering eﬀective 
[monte, carlo, methods, challenge, mixing, separated, modes, depth, may, help, mixing] drawing samples latent variable model seen encodes well sampling change chapter monte carlo methods much mixing poor one way resolve problem make deep representation encodes way markov chain space mix easily many representation learning algorithms autoencoders rbms tend yield marginal distribution uniform unimodal original data distribution argued arises trying minimize reconstruction error using available representation space minimizing reconstruction error training examples better achieved diﬀerent training examples easily distinguishable space thus well separated bengio observed deeper stacks regularized autoencoders rbms yield marginal distributions top level space appeared spread uniform less gap regions corresponding diﬀerent modes categories experiments training rbm higher level space allowed gibbs sampling mix faster modes remains however unclear exploit observation help better train sample deep generative models despite diﬃculty mixing monte carlo techniques useful often best tool available indeed primary tool used confront intractable partition function undirected models discussed next 
[confronting, partition, function] sec saw many probabilistic models commonly known undi rected graphical models deﬁned unnormalized probability distribution must normalize dividing partition function order obtain valid probability distribution partition function integral continuous variables sum discrete variables unnormalized probability states operation intractable many interesting models see chapter several deep learning models designed tractable normalizing constant designed used ways involve computing however models directly confront challenge intractable partition functions chapter describe techniques used training evaluating models intractable partition functions chapter confronting partition function 
[confronting, partition, function, log-likelihood, gradient] makes learning undirected models maximum likelihood particularly diﬃcult partition function depends parameters gradient log likelihood respect parameters term corresponding gradient partition function log log log well known decomposition positive phase negative phase learning undirected models interest negative phase diﬃcult models latent variables interactions latent variables typically tractable positive phase quintessential example model straightforward positive phase diﬃcult negative phase rbm hidden units conditionally independent given visible units case positive phase diﬃcult complicated interactions latent variables primarily covered chapter chapter focuses diﬃculties negative phase let look closely gradient log log models guarantee substitute exp log exp log exp log log log log chapter confronting partition function log derivation made use summation discrete similar result applies using integration continuous continuous version derivation use leibniz rule diﬀerentiation integral sign obtain identity identity applicable certain regularity conditions measure theoretic terms conditions unnormalized distribution must lebesgue integrable function every value gradient must exist almost iii must exist integrable function bounds sense max almost fortunately machine learning models interest properties identity log log basis variety monte carlo methods approximately maximizing likelihood models intractable partition functions monte carlo approach learning undirected models provides intuitive framework think positive phase negative phase positive phase increase log drawn data negative phase decrease partition function decreasing log drawn model distribution deep learning literature common parametrize log terms energy function case interpret positive phase pushing energy training examples negative phase pushing energy samples drawn model illustrated fig 
[confronting, partition, function, stochastic, maximum, likelihood, contrastive, divergence] naive way implementing compute burning set markov chains random initialization every time gradient needed learning performed using stochastic gradient descent means chains must burned per gradient step approach leads chapter confronting partition function training procedure presented algorithm high cost burning markov chains inner loop makes procedure computationally infeasible procedure starting point practical algorithms aim approximate algorithm naive mcmc algorithm maximizing log likelihood intractable partition function using gradient ascent set step size small positive number set number gibbs steps high enough allow burn perhaps train rbm small image patch converged sample minibatch examples training set log initialize set samples random values uniform normal distribution possibly distribution marginals matched model marginals gibbs_update end end log end view mcmc approach maximum likelihood trying achieve balance two forces one pushing model distribution data occurs another pushing model distribution model samples occur fig illustrates process two forces correspond maximizing log minimizing log several approximations negative phase possible approximations understood making negative phase computationally cheaper also making push wrong locations negative phase involves drawing samples model distri bution think ﬁnding points model believes strongly negative phase acts reduce probability points generally considered represent model incorrect beliefs world frequently referred literature hallucinations fantasy particles fact negative phase proposed possible explanation chapter confronting partition function positive phase model data negative phase model data figure view algorithm positive phase negative phase left positive phase sample points data distribution push unnormalized probability means points likely data get pushed right negative phase sample points model distribution push unnormalized probability counteracts positive phase tendency add large constant unnormalized probability everywhere data distribution model distribution equal positive phase chance push point negative phase push occurs longer gradient expectation training must terminate dreaming humans animals idea crick mitchison brain maintains probabilistic model world follows gradient log experiencing real events awake follows negative gradient log minimize log sleeping experiencing events sampled current model view explains much language used describe algorithms positive negative phase proven correct neuroscientiﬁc experiments machine learning models usually necessary use positive negative phase simultaneously rather separate time periods wakefulness rem sleep see sec machine learning algorithms draw samples model distribution purposes algorithms could also provide account function dream sleep given understanding role positive negative phase learning attempt design less expensive alternative algorithm main cost naive mcmc algorithm cost burning markov chains random initialization step natural solution initialize markov chains distribution close model distribution chapter confronting partition function burn operation take many steps contrastive divergence indicate gibbs steps algorithm initializes markov chain step samples data distribution hinton approach presented algorithm obtaining samples data distribution free already available data set initially data distribution close model distribution negative phase accurate fortunately positive phase still accurately increase model probability data positive phase time act model distribution closer data distribution negative phase starts become accurate algorithm contrastive divergence algorithm using gradient ascent optimization procedure set step size small positive number set number gibbs steps high enough allow markov chain sampling mix initialized data perhaps train rbm small image patch converged sample minibatch examples training set log end gibbs_update end end log end course still approximation correct negative phase main way qualitatively fails implement correct negative phase fails suppress regions high probability far actual training examples regions high probability model low probability data generating distribution called spurious modes fig illustrates happens essentially modes model distribution far data distribution visited chapter confronting partition function spurious mode model data figure illustration negative phase contrastive divergence algorithm fail suppress spurious modes spurious mode mode present model distribution absent data distribution contrastive divergence initializes markov chains data points runs markov chain steps unlikely visit modes model far data points means sampling model sometimes get samples resemble data also means due wasting probability mass modes model struggle place high probability mass correct modes purpose visualization ﬁgure uses somewhat simpliﬁed concept distance spurious mode far correct mode along number line corresponds markov chain based making local moves single variable deep probabilistic models markov chains based gibbs sampling make non local moves individual variables cannot move variables simultaneously problems usually better consider edit distance modes rather euclidean distance however edit distance high dimensional space diﬃcult depict plot markov chains initialized training points unless large carreira perpiñan hinton showed experimentally estimator biased rbms fully visible boltzmann machines converges diﬀerent points maximum likelihood estimator argue bias small could used inexpensive way initialize model could later ﬁne tuned via expensive mcmc methods bengio delalleau showed interpreted discarding smallest terms correct mcmc update gradient explains bias useful training shallow models like rbms turn stacked initialize deeper models like dbns dbms however provide much help training deeper models directly diﬃcult chapter confronting partition function obtain samples hidden units given samples visible units since hidden units included data initializing training points cannot solve problem even initialize visible units data still need burn markov chain sampling distribution hidden units conditioned visible samples algorithm thought penalizing model markov chain changes input rapidly input comes data means training somewhat resembles autoencoder training even though biased training methods useful pretraining shallow models later stacked earliest models stack encouraged copy information latent variables thereby making available later models thought often exploitable side eﬀect training rather principled design advantage sutskever tieleman showed update direction gradient function allows situations could cycle forever practice serious problem diﬀerent strategy resolves many problems initialize markov chains gradient step states previous gradient step approach ﬁrst discovered name stochastic maximum likelihood sml applied mathematics statistics community younes later independently rediscovered name persistent contrastive divergence pcd pcd indicate use gibbs steps per update deep learning community see algorithm basic tieleman idea approach long steps taken stochastic gradient algorithm small model previous step similar model current step follows samples previous model distribution close fair samples current model distribution markov chain initialized samples require much time mix markov chain continually updated throughout learning process rather restarted gradient step chains free wander far enough ﬁnd model modes sml thus considerably resistant forming models spurious modes moreover possible store state sampled variables whether visible latent sml provides initialization point hidden visible units able provide initialization visible units therefore requires burn deep models sml able train deep models eﬃciently chapter confronting partition function marlin compared sml many criteria presented chapter found sml results best test set log likelihood rbm rbm hidden units used features svm classiﬁer sml results best classiﬁcation accuracy sml vulnerable becoming inaccurate stochastic gradient algorithm move model faster markov chain mix steps happen small large permissible range values unfortunately highly problem dependent known way test formally whether chain successfully mixing steps subjectively learning rate high number gibbs steps human operator able observe much variance negative phase samples across gradient steps rather across diﬀerent markov chains example model trained mnist might sample exclusively one step learning process push strongly mode corresponding model might sample exclusively next step algorithm stochastic maximum likelihood persistent contrastive divergence algorithm using gradient ascent optimization procedure set step size small positive number set number gibbs steps high enough allow markov chain sampling burn starting samples perhaps rbm small image patch complicated model like dbm initialize set samples random values uniform normal distribution possibly distribution marginals matched model marginals converged sample minibatch examples training set log gibbs_update end end log end care must taken evaluating samples model trained sml necessary draw samples starting fresh markov chain chapter confronting partition function initialized random starting point model done training samples present persistent negative chains used training inﬂuenced several recent versions model thus make model appear greater capacity actually berglund raiko performed experiments examine bias variance estimate gradient provided sml proves lower variance estimator based exact sampling sml higher variance cause low variance use training points positive negative phase negative phase initialized diﬀerent training points variance rises estimator based exact sampling methods based using mcmc draw samples model principle used almost variant mcmc means techniques sml improved using enhanced mcmc techniques described chapter parallel tempering desjardins cho one approach accelerating mixing learning relies changing monte carlo sampling technology rather changing parametrization model cost function fast pcd fpcd tieleman hinton involves replacing parameters traditional model expression slow fast twice many parameters added together element wise provide parameters used original model deﬁnition fast copy parameters trained much larger learning rate allowing adapt rapidly response negative phase learning push markov chain new territory forces markov chain mix rapidly though eﬀect occurs learning fast weights free change typically one also applies signiﬁcant weight decay fast weights encouraging converge small values transiently taking large values long enough encourage markov chain change modes one key beneﬁt mcmc based methods described section provide estimate gradient log thus essentially decompose problem log contribution log contribution use method tackle log add negative phase gradient onto method gradient particular means positive phase make use methods provide lower bound methods dealing log presented chapter chapter confronting partition function incompatible bound based positive phase methods 
[confronting, partition, function, pseudolikelihood] monte carlo approximations partition function gradient directly confront partition function approaches sidestep issue training model without computing partition function approaches based observation easy compute ratios probabilities undirected probabilistic model partition function appears numerator denominator ratio cancels pseudolikelihood based observation conditional probabilities take ratio based form thus computed without knowledge partition function suppose partition contains variables want ﬁnd conditional distribution contains variables want condition contains variables part query quantity requires marginalizing eﬃcient operation provided contain many variables extreme case single variable empty making operation require many evaluations values single random variable unfortunately order compute log likelihood need marginalize large sets variables variables total must marginalize set size chain rule probability log log log case made maximally small large simply move reduce computational cost yields pseudolikelihood objective function based predicting value besag feature given features log chapter confronting partition function random variable diﬀerent values requires evaluations compute opposed evaluations needed compute partition function may look like unprincipled hack proven estimation maximizing pseudolikelihood asymptotically consistent mase course case datasets approach large sample limit pseudolikelihood may display diﬀerent behavior maximum likelihood estimator possible trade computational complexity deviation maximum likelihood behavior using generalized pseudolikelihood estimator huang ogata generalized pseudolikelihood estimator uses diﬀerent sets indices variables appear together left side conditioning bar extreme case generalized pseudolikelihood recovers log likelihood extreme case generalized pseudolikelihood recovers pseudolikelihood generalized pseudolikelihood objective function given log performance pseudolikelihood based approaches depends largely model used pseudolikelihood tends perform poorly tasks require good model full joint density estimation sampling however perform better maximum likelihood tasks require conditional distributions used training ﬁlling small amounts missing values generalized pseudolikelihood techniques especially powerful data regular structure allows index sets designed capture important correlations leaving groups variables negligible correlation example natural images pixels widely separated space also weak correlation generalized pseudolikelihood applied set small spatially localized window one weakness pseudolikelihood estimator cannot used approximations provide lower bound variational inference covered chapter appears denominator lower bound denominator provides upper bound expression whole beneﬁt maximizing upper bound makes diﬃcult apply pseudolikelihood approaches deep models deep boltzmann machines since variational methods one dominant approaches approximately marginalizing many layers hidden variables chapter confronting partition function interact however pseudolikelihood still useful deep learning used train single layer models deep models using approximate inference methods based lower bounds pseudolikelihood much greater cost per gradient step sml due explicit computation conditionals however generalized pseudo likelihood similar criteria still perform well one randomly selected conditional computed per example thereby bringing goodfellow computational cost match sml though pseudolikelihood estimator explicitly minimize log still thought something resembling negative phase denominators conditional distribution result learning algorithm suppressing probability states one variable diﬀering training example see marlin freitas theoretical analysis asymptotic eﬃciency pseudolikelihood 
[confronting, partition, function, score, matching, ratio, matching] score matching provides another consistent means training hyvärinen model without estimating derivatives name score matching comes terminology derivatives log density respect argument log called score strategy used score matching minimize expected squared diﬀerence derivatives model log density respect input derivatives data log density respect input log model log data data min objective function avoids diﬃculties associated diﬀerentiating partition function function therefore initially score matching appears new diﬃculty computing score data distribution requires knowledge true distribution generating training data data fortunately minimizing expected value chapter confronting partition function equivalent minimizing expected value log model log model dimensionality score matching requires taking derivatives respect applicable models discrete data however latent variables model may discrete like pseudolikelihood score matching works able evaluate log derivatives directly compatible methods provide lower bound log score matching requires derivatives second derivatives log lower bound conveys information derivatives means score matching cannot applied estimating models complicated interactions hidden units sparse coding models deep boltzmann machines score matching used pretrain ﬁrst hidden layer larger model applied pretraining strategy deeper layers larger model probably hidden layers models usually contain discrete variables score matching explicitly negative phase viewed version contrastive divergence using speciﬁc kind markov chain markov chain case gibbs sampling hyvärinen rather diﬀerent approach makes local moves guided gradient score matching equivalent type markov chain size local moves approaches zero lyu generalized score matching discrete case made error derivation corrected marlin marlin found generalized score matching gsm work high dimensional discrete spaces observed probability many events successful approach extending basic ideas score matching discrete data ratio matching ratio matching applies hyvärinen speciﬁcally binary data ratio matching consists minimizing average examples following objective function ufeb ufed model model uff uff chapter confronting partition function returns bit position ﬂipped ratio matching avoids partition function using trick pseudolikelihood estimator ratio two probabilities partition function cancels marlin found ratio matching outperforms sml pseudolikelihood gsm terms ability models trained ratio matching denoise test set images like pseudolikelihood estimator ratio matching requires evaluations per data point making computational cost per update roughly times higher sml pseudolikelihood estimator ratio matching thought pushing fantasy states one variable diﬀerent training example since ratio matching applies speciﬁcally binary data means acts fantasy states within hamming distance data ratio matching also useful basis dealing high dimensional sparse data word count vectors kind data poses challenge mcmc based methods data extremely expensive represent dense format yet mcmc sampler yield sparse values model learned represent sparsity data distribution dauphin bengio overcame issue designing unbiased stochastic approximation ratio matching approximation evaluates randomly selected subset terms objective require model generate complete fantasy samples see marlin freitas theoretical analysis asymptotic eﬃciency ratio matching 
[confronting, partition, function, denoising, score, matching] cases may wish regularize score matching ﬁtting distribution smoothed data rather true data distribution corruption process usually one forms adding small amount noise denoising score matching especially useful practice usually access true data rather empirical distribution deﬁned samples consistent estimator given enough capacity make model set dirac distributions centered training points smoothing helps reduce problem loss asymptotic consistency property chapter confronting partition function described sec introduced procedure kingma lecun performing regularized score matching smoothing distribution normally distributed noise recall sec several autoencoder training algorithms equivalent score matching denoising score matching autoencoder training algorithms therefore way overcoming partition function problem 
[confronting, partition, function, noise-contrastive, estimation] techniques estimating models intractable partition functions provide estimate partition function sml estimate gradient log partition function rather partition function score matching pseudolikelihood avoid computing quantities related partition function altogether noise contrastive estimation nce gutmann hyvarinen takes diﬀerent strategy approach probability distribution estimated model represented explicitly log model log model explicitly introduced approximation log rather estimating noise contrastive estimation procedure treats another parameter estimates simultaneously using algorithm resulting log model thus may correspond exactly valid probability distribution become closer closer valid estimate improves approach would possible using maximum likelihood criterion estimator maximum likelihood criterion would choose set arbitrarily high rather setting create valid probability distribution nce works reducing unsupervised learning problem estimating learning probabilistic binary classiﬁer one categories corresponds data generated model supervised learning problem constructed way maximum likelihood estimation supervised nce also applicable problems tractable partition function need introduce extra parameter however generated interest means estimating models diﬃcult partition functions chapter confronting partition function learning problem deﬁnes asymptotically consistent estimator original problem speciﬁcally introduce second distribution noise distribution noise noise distribution tractable evaluate sample construct model new binary class variable new joint model specify joint joint model joint noise words switch variable determines whether generate model noise distribution construct similar joint model training data case switch variable determines whether draw noise data distribution formally train train data train noise use standard maximum likelihood learning supervised learning problem ﬁtting joint train arg max train log joint distribution joint essentially logistic regression model applied diﬀerence log probabilities model noise distribution joint model model noise noise model exp log noise model log noise model log model log noise chapter confronting partition function nce thus simple apply long log model easy back propagate speciﬁed noise easy evaluate order evaluate joint sample order generate training data nce successful applied problems random variables work well even random variables take high number values example successfully applied modeling conditional distribution word given context word mnih kavukcuoglu though word may drawn large vocabulary one word nce applied problems many random variables becomes less eﬃcient logistic regression classiﬁer reject noise sample identifying one variable whose value unlikely means learning slows greatly model learned basic marginal statistics imagine learning model images faces using unstructured gaussian noise noise model learns eyes reject almost unstructured noise samples without learned anything facial features mouths constraint noise must easy evaluate easy sample overly restrictive noise simple samples likely obviously distinct data force model improve noticeably like score matching pseudolikelihood nce work lower bound available lower bound could used construct lower bound joint used construct upper bound joint appears half terms nce objective likewise lower bound noise useful provides upper bound joint model distribution copied deﬁne new noise distribution gradient step nce deﬁnes procedure called self contrastive estimation whose expected gradient equivalent expected gradient maximum likelihood goodfellow special case nce noise samples generated model suggests maximum likelihood interpreted procedure forces model constantly learn distinguish reality evolving beliefs noise contrastive estimation achieves reduced computational cost forcing model distinguish reality ﬁxed baseline noise model using supervised task classifying training samples generated samples model energy function used deﬁning classiﬁer provide gradient model introduced earlier various forms welling bengio chapter confronting partition function noise contrastive estimation based idea good generative model able distinguish data noise closely related idea good generative model able generate samples classiﬁer distinguish data idea yields generative adversarial networks sec 
[confronting, partition, function, estimating, partition, function] much chapter dedicated describing methods avoid needing compute intractable partition function associated undirected graphical model section discuss several methods directly estimating partition function estimating partition function important require wish compute normalized likelihood data often important evaluating model monitoring training performance comparing models example imagine two models model deﬁning probabil ity distribution model deﬁning probability distribution common way compare models evaluate compare likelihood models assign test dataset suppose test set consists examples equivalently log log say better model least better model test set sense better test log likelihood unfortunately testing whether condition holds requires knowledge partition function unfortunately seems require evaluating log probability model assigns point turn requires evaluating partition function simplify situation slightly arranging form need know ratio two model partition functions log log log log thus determine whether better model without knowing partition function either model ratio see shortly chapter confronting partition function estimate ratio using importance sampling provided two models similar however wanted compute actual probability test data either would need compute actual value partition functions said knew ratio two partition functions knew actual value one two say could compute value simple way estimate partition function use monte carlo method simple importance sampling present approach terms continuous variables using integrals readily applied discrete variables replacing integrals summation use proposal distribution supports tractable sampling tractable evaluation partition function unnormalized distribution last line make monte carlo estimator integral using samples drawn weight sample ratio unnormalized proposal see also approach allows estimate ratio partition functions value used directly compare two models described distribution close eﬀective way estimating partition function minka unfortunately time chapter confronting partition function complicated usually multimodal deﬁned high dimensional space diﬃcult ﬁnd tractable simple enough evaluate still close enough result high quality approximation close samples low probability therefore make relatively negligible contribution sum samples signiﬁcant weights sum result estimator poor quality due high variance understood quantitatively estimate variance estimate var quantity largest signiﬁcant deviation values importance weights turn two related strategies developed cope challeng ing task estimating partition functions complex distributions high dimensional spaces annealed importance sampling bridge sampling start simple importance sampling strategy introduced attempt overcome problem proposal far introducing intermediate distributions attempt bridge gap 
[confronting, partition, function, estimating, partition, function, annealed, importance, sampling] situations  large little overlap strategy called annealed importance sampling ais attempts bridge gap introducing intermediate distributions jarzynski neal consider sequence distributions ﬁrst last distributions sequence respectively approach allows estimate partition function multimodal distribution deﬁned high dimensional space distribution deﬁned trained rbm begin simpler model known partition function rbm zeroes weights estimate ratio two model partition functions estimate ratio based estimate ratios sequence many similar distributions sequence rbms weights interpolating zero learned weights chapter confronting partition function write ratio provided distributions suﬃciently close reliably estimate factors using simple importance sampling use obtain estimate intermediate distributions come original proposal distribution design choice sequence distributions speciﬁcally constructed suit problem domain one general purpose popular choice intermediate distributions use weighted geometric average target distribution starting proposal distribution partition function known order sample intermediate distributions deﬁne series markov chain transition functions deﬁne conditional probability distribution transitioning given currently transition operator deﬁned leave invariant transitions may constructed markov chain monte carlo method metropolis hastings gibbs including methods involving multiple passes random variables kinds iterations ais sampling strategy generate samples use transition operators sequentially generate samples intermediate distributions arrive samples target distribution sample chapter confronting partition function sample sample sample end sample derive importance weight chaining together importance weights jumps intermediate distributions given avoid numerical issues overﬂow probably best compute log adding subtracting log probabilities rather computing multiplying dividing probabilities sampling procedure thus deﬁned importance weights given estimate ratio partition functions given order verify procedure deﬁnes valid importance sampling scheme show ais procedure corresponds simple neal importance sampling extended state space points sampled product space deﬁne distribution extended space reverse transition operator deﬁned via application bayes rule plugging expression joint distribution extended state space given get chapter confronting partition function means generating samples joint proposal distribution extended sample via sampling scheme given joint distribution given joint distribution extended space given taking proposal distribution extended state space draw samples remains determine importance weights weights proposed ais thus interpret ais simple importance sampling applied extended state validity follows immediately validity importance sampling annealed importance sampling ais ﬁrst discovered jarzynski independently currently common neal way estimating partition function undirected probabilistic models reasons may publication inﬂuential paper salakhutdinov murray describing application estimating partition function restricted boltzmann machines deep belief networks inherent advantage method method described discussion properties ais estimator variance eﬃciency found neal 
[confronting, partition, function, estimating, partition, function, bridge, sampling] bridge sampling another method like ais addresses bennett shortcomings importance sampling rather chaining together series chapter confronting partition function intermediate distributions bridge sampling relies single distribution known bridge interpolate distribution known partition function distribution trying estimate partition function bridge sampling estimates ratio ratio expected impor tance weights bridge distribution chosen carefully large overlap support bridge sampling allow distance two distributions formally  much larger standard importance sampling shown optimal bridging distribution given opt ﬁrst appears unworkable solution would seem require quantity trying estimate however possible start coarse estimate use resulting bridge distribution reﬁne estimate iteratively neal iteratively estimate ratio use iteration update value linked importance sampling ais bridge sampling vantages  large suﬃciently close bridge sampling eﬀective means estimating ratio partition functions ais however two distributions far apart single distribution bridge gap one least use ais potentially many intermediate distributions span distance neal showed linked importance sampling method leveraged power bridge sampling strategy bridge intermediate distributions used ais signiﬁcantly improve overall partition function estimates estimating partition function training ais become accepted standard method estimating partition function many undirected models suﬃciently computationally intensive remains infeasible use training however alternative strategies explored maintain estimate partition function throughout training using combination bridge sampling short chain ais parallel tempering desjardins devised scheme track partition function chapter confronting partition function rbm throughout training process strategy based maintenance independent estimates partition functions rbm every temperature operating parallel tempering scheme authors combined bridge sampling estimates ratios partition functions neighboring chains parallel tempering ais estimates across time come low variance estimate partition functions every iteration learning tools described chapter provide many diﬀerent ways overcoming problem intractable partition functions several diﬃculties involved training using generative models foremost among problem intractable inference confront next 
[approximate, inference] many probabilistic models diﬃcult train diﬃcult perform inference context deep learning usually set visible variables set latent variables challenge inference usually refers diﬃcult problem computing taking expectations respect operations often necessary tasks like maximum likelihood learning many simple graphical models one hidden layer restricted boltzmann machines probabilistic pca deﬁned way makes inference operations like computing taking expectations respect simple unfortunately graphical models multiple layers hidden variables intractable posterior distributions exact inference requires exponential amount time models even models single layer sparse coding problem chapter introduce several techniques confronting intractable inference problems later chapter describe use techniques train probabilistic models would otherwise intractable deep belief networks deep boltzmann machines intractable inference problems deep learning usually arise interactions latent variables structured graphical model see fig examples interactions may due direct interactions undirected models explaining away interactions mutual ancestors visible unit directed models chapter approximate inference figure intractable inference problems deep learning usually result interactions latent variables structured graphical model due edges directly connecting one latent variable another due longer paths activated child structure observed left semi restricted boltzmann machine connections hidden units osindero hinton direct connections latent variables make posterior distribution intractable due large cliques latent variables deep boltzmann machine organized center layers variables without intra layer connections still intractable posterior distribution due connections layers directed model right interactions latent variables visible variables observed every two latent variables parents probabilistic models able provide tractable inference latent variables despite one graph structures depicted possible conditional probability distributions chosen introduce additional independences beyond described graph example probabilistic pca graph structure shown right yet still simple inference due special properties speciﬁc conditional distributions uses linear gaussian conditionals mutually orthogonal basis vectors chapter approximate inference 
[approximate, inference, inference, optimization] many approaches confronting problem diﬃcult inference make use observation exact inference described optimization problem approximate inference algorithms may derived approximating underlying optimization problem construct optimization problem assume probabilistic model consisting observed variables latent variables would like compute log probability observed data log sometimes diﬃcult compute log costly marginalize instead compute lower bound log bound called evidence lower bound elbo another commonly used name lower bound negative variational free energy speciﬁcally evidence lower bound deﬁned log  arbitrary probability distribution diﬀerence log given divergence divergence always non negative see always value desired log probability two equal distribution surprisingly considerably easier compute distributions simple algebra shows rearrange much convenient form log  log log log log log log log log log log yields canonical deﬁnition evidence lower bound log appropriate choice tractable compute choice provides lower bound likelihood better chapter approximate inference approximations lower bound tighter words closer log approximation perfect log thus think inference procedure ﬁnding maximizes exact inference maximizes perfectly searching family functions includes throughout chapter show derive diﬀerent forms approximate inference using approximate optimization ﬁnd make optimization procedure less expensive approximate restricting family distributions optimization allowed search using imperfect optimization procedure may completely maximize merely increase signiﬁcant amount matter choice use lower bound get tighter looser bounds cheaper expensive compute depending choose approach optimization problem obtain poorly matched reduce computational cost using imperfect optimization procedure using perfect optimization procedure restricted family distributions 
[approximate, inference, expectation, maximization] ﬁrst algorithm introduce based maximizing lower bound expectation maximization algorithm popular training algorithm models latent variables describe view algorithm developed neal hinton unlike algorithms describe chapter approach approximate inference rather approach learning approximate posterior algorithm consists alternating two steps convergence expectation step let step denote value parameters beginning step set indices training examples want train batch minibatch variants valid mean deﬁned terms current parameter value vary change remain equal step maximization step completely partially maximize chapter approximate inference respect using optimization algorithm choice viewed coordinate ascent algorithm maximize one step maximize respect maximize respect stochastic gradient ascent latent variable models seen special case algorithm step consists taking single gradient step variants algorithm make much larger steps model families step even performed analytically jumping way optimal solution given current even though step involves exact inference think algorithm using approximate inference sense speciﬁcally step assumes value used values introduce gap true log step moves away value used step fortunately step reduces gap zero enter loop next time algorithm contains diﬀerent insights first basic structure learning process update model parameters improve likelihood completed dataset missing variables values provided estimate posterior distribution particular insight unique algorithm example using gradient descent maximize log likelihood also property log likelihood gradient computations require taking expectations respect posterior distribution hidden units another key insight algorithm continue use one value even moved diﬀerent value particular insight used throughout classical machine learning derive large step updates context deep learning models complex admit tractable solution optimal large step update second insight unique algorithm rarely used 
[approximate, inference, map, inference, sparse, coding] usually use term inference refer computing probability distribution one set variables given another training probabilistic models latent variables usually interested computing alternative form inference compute single likely value missing variables rather infer entire distribution possible values context chapter approximate inference latent variable models means computing arg max known maximum posteriori inference abbreviated map inference map inference usually thought approximate inference compute exact likely value however wish develop learning process based maximizing helpful think map inference procedure provides value sense think map inference approximate inference provide optimal recall sec exact inference consists maximizing log respect unrestricted family probability distributions using exact optimization algorithm derive map inference form approximate inference restricting family distributions may drawn speciﬁcally require take dirac distribution means control entirely via dropping terms vary left optimization problem arg max log equivalent map inference problem arg max thus justify learning procedure similar alternate performing map inference infer update increase log form coordinate ascent alternate using inference optimize respect using parameter updates optimize respect procedure whole justiﬁed fact lower bound log case map inference justiﬁcation rather vacuous bound inﬁnitely loose due dirac distribution diﬀerential entropy negative inﬁnity however adding noise would make bound meaningful chapter approximate inference map inference commonly used deep learning feature extractor learning mechanism primarily used sparse coding models recall sec sparse coding linear factor model imposes sparsity inducing prior hidden units common choice factorial laplace prior visible units generated performing linear transformation adding noise computing even representing diﬃcult every pair variables parents means observed graphical model contains active path connecting hidden units thus participate one massive clique model gaussian interactions could modeled eﬃciently via covariance matrix sparse prior makes interactions non gaussian intractable computation log likelihood gradient thus cannot use exact maximum likelihood learning instead use map inference learn parameters maximizing elbo deﬁned dirac distribution around map estimate concatenate vectors training set matrix concatenate vectors matrix sparse coding learning process consists minimizing applications sparse coding also involve weight decay constraint norms columns order prevent pathological solution extremely small large minimize alternating minimization respect minimization respect sub problems convex fact minimization respect linear regression problem however minimization respect arguments usually convex problem minimization respect requires specialized algorithms feature sign search algorithm lee chapter approximate inference 
[approximate, inference, variational, inference, learning] seen evidence lower bound lower bound log inference viewed maximizing respect learning viewed maximizing respect seen algorithm allows make large learning steps ﬁxed learning algorithms based map inference allow learn using point estimate rather inferring entire distribution develop general approach variational learning core idea behind variational learning maximize restricted family distributions family chosen easy compute log typical way introduce assumptions factorizes common approach variational learning impose restriction factorial distribution called mean ﬁeld approach generally impose graphical model structure choose ﬂexibly determine many interactions want approximation capture fully general graphical model approach called structured variational inference saul jordan beauty variational approach need specify speciﬁc parametric form specify factorize optimization problem determines optimal probability distribution within factorization constraints discrete latent variables means use traditional optimization techniques optimize ﬁnite number variables describing distribution continuous latent variables means use branch mathematics called calculus variations perform optimization space functions actually determine function used represent calculus variations origin names variational learning variational inference though names apply even latent variables discrete calculus variations needed case continuous latent variables calculus variations powerful technique removes much responsibility human designer model must specify factorizes rather needing guess design speciﬁc accurately approximate posterior deﬁned log  think maximizing respect minimizing  chapter approximate inference sense ﬁtting however opposite direction divergence used using ﬁtting approximation use maximum likelihood learning model data minimize data  model illustrated fig means maximum likelihood encourages model high probability everywhere data high probability optimization based inference procedure encourages low probability everywhere true posterior low probability directions divergence desirable undesirable properties choice use depends properties highest priority application case inference optimization problem choose use  computational reasons speciﬁcally computing  involves evaluating expectations respect designing simple simplify required expectations opposite direction divergence would require computing expectations respect true posterior form true posterior determined choice model cannot design reduced cost approach computing  exactly 
[approximate, inference, variational, inference, learning, discrete, latent, variables] variational inference discrete latent variables relatively straightforward deﬁne distribution typically one factor deﬁned lookup table discrete states simplest case binary make mean ﬁeld assumption factorizes individual case parametrize vector whose entries probabilities determining represent simply optimize parameters case discrete latent variables standard optimization problem principle selection could done optimization algorithm gradient descent optimization must occur inner loop learning algorithm must fast achieve speed typically use special optimization algorithms designed solve comparatively small simple problems iterations popular choice iterate ﬁxed point equations words solve repeatedly update diﬀerent elements satisfy convergence chapter approximate inference criterion make concrete show apply variational inference binary sparse coding model present model developed henniges demonstrate traditional generic mean ﬁeld applied model introduce specialized algorithm derivation goes considerable mathematical detail intended reader wishes fully resolve ambiguity high level conceptual description variational inference learning presented far readers plan derive implement variational learning algorithms may safely skip next section without missing new high level concepts readers proceed binary sparse coding example encouraged review list useful properties functions commonly arise probabilistic models sec use properties liberally throughout following derivations without highlighting exactly use one binary sparse coding model input generated model adding gaussian noise sum diﬀerent components present absent component switched corresponding hidden unit learnable set biases learnable weight matrix learnable diagonal precision matrix training model maximum likelihood requires taking derivative respect parameters consider derivative respect one biases log chapter approximate inference figure graph structure binary sparse coding model four hidden units left graph structure note edges directed every two hidden units parents every visible unit graph structure right order account active paths parents posterior distribution needs edge hidden units log requires computing expectations respect unfortunately complicated distribution see fig graph structure posterior distribution corresponds complete graph hidden units variable elimination algorithms help compute required expectations faster brute force resolve diﬃculty using variational inference variational learning instead make mean ﬁeld approximation latent variables binary sparse coding model binary represent factorial simply need model bernoulli distributions natural way represent means bernoulli distributions vector probabilities impose restriction never equal order avoid errors computing example log see variational inference equations never assign chapter approximate inference analytically however software implementation machine rounding error could result values software may wish implement binary sparse coding using unrestricted vector variational parameters obtain via relation thus safely compute log computer using identity log relating sigmoid softplus begin derivation variational learning binary sparse coding model show use mean ﬁeld approximation makes learning tractable evidence lower bound given log log log log log log log log log log log log exp log log log log ufee ufflog ufeb ufed ufee uff uff uffb uff uff uff uffb equations somewhat unappealing aesthetically show expressed small number simple arithmetic operations evidence lower bound therefore tractable use replacement intractable log likelihood principle could simply run gradient ascent would make perfectly acceptable combined inference training algorithm usually however two reasons first would require storing typically prefer algorithms require per example memory diﬃcult scale learning algorithms billions examples must remember dynamically updated vector associated example chapter approximate inference second would like able extract features quickly order recognize content realistic deployed setting would need able compute real time reasons typically use gradient descent compute mean ﬁeld parameters instead rapidly estimate ﬁxed point equations idea behind ﬁxed point equations seeking local maximum respect cannot eﬃciently solve equation respect simultaneously however solve single variable iteratively apply solution equation repeat cycle satisfy converge criterion common convergence criteria include stopping full cycle updates improve tolerance amount cycle change amount iterating mean ﬁeld ﬁxed point equations general technique provide fast variational inference broad variety models make concrete show derive updates binary sparse coding model particular first must write expression derivatives respect substitute left side ufee uff log log log log ufee ufflog ufeb ufed ufee uff uff uffb uff uff uff uffb uff uffb log log log log ufee uff ufeb ufed uff uff uff uffb chapter approximate inference log log apply ﬁxed point update inference rule solve sets ufeb ufed uff uff point see close connection recurrent neural networks inference graphical models speciﬁcally mean ﬁeld ﬁxed point equations deﬁned recurrent neural network task network perform inference described derive network model description also possible train inference network directly several ideas based theme described chapter case binary sparse coding see recurrent network connection speciﬁed consists repeatedly updating hidden units based changing values neighboring hidden units input always sends ﬁxed message hidden units hidden units constantly update message send speciﬁcally two units inhibit weight vectors aligned form competition two hidden units explain input one explains input best allowed remain active competition mean ﬁeld approximation attempt capture explaining away interactions binary sparse coding posterior explaining away eﬀect actually cause multi modal posterior draw samples posterior samples one unit active samples unit active samples active unfortunately explaining away interactions cannot modeled factorial used mean ﬁeld mean ﬁeld approximation forced choose one mode model instance behavior illustrated fig rewrite equivalent form reveals insights ufeb ufec ufed ufeb ufed uff uff uff uff uff reformulation see input step consisting rather thus think unit attempting encode residual chapter approximate inference error given code units thus think sparse coding iterative autoencoder repeatedly encodes decodes input attempting mistakes reconstruction iteration example derived update rule updates single unit time would advantageous able update units simultaneously graphical models deep boltzmann machines structured way solve many entries simultaneously unfortunately binary sparse coding admit block updates instead use heuristic technique called perform block updates damping approach damping solve individually optimal values every element move values small step direction approach longer guaranteed increase step works well practice many models see koller friedman information choosing degree synchrony damping strategies message passing algorithms 
[approximate, inference, variational, inference, learning, calculus, variations] continuing presentation variational learning must brieﬂy introduce important set mathematical tools used variational learning calculus variations many machine learning techniques based minimizing function ﬁnding input vector takes minimal value accomplished multivariate calculus linear algebra solving critical points cases actually want solve function want ﬁnd probability density function random variable calculus variations enables function function known functional much take partial derivatives function respect elements vector valued argument take also known functional derivatives variational derivatives functional respect individual values function speciﬁc value functional derivative functional respect value function point denoted complete formal development functional derivatives beyond scope book purposes suﬃcient state diﬀerentiable functions diﬀerentiable functions continuous derivatives chapter approximate inference gain intuition identity one think vector uncountably many elements indexed real vector somewhat incomplete view identity providing functional derivatives would obtain vector indexed positive integers many results machine learning publications presented using general euler lagrange equation allows depend derivatives well value need fully general form results presented book optimize function respect vector take gradient function respect vector solve point every element gradient equal zero likewise optimize functional solving function functional derivative every point equal zero example process works consider problem ﬁnding probability distribution function maximal diﬀerential entropy recall entropy probability distribution deﬁned log continuous values expectation integral log cannot simply maximize respect function result might probability distribution instead need use lagrange multipliers add constraint integrates also entropy increases without bound variance increases makes question distribution greatest entropy uninteresting instead ask distribution maximal entropy ﬁxed variance finally problem underdetermined distribution shifted arbitrarily without changing entropy impose unique solution add constraint mean distribution lagrangian functional optimization problem chapter approximate inference log minimize lagrangian respect set functional derivatives equal log condition tells functional form algebraically arranging equation obtain exp never assumed directly would take functional form obtained expression analytically minimizing functional ﬁnish minimization problem must choose values ensure constraints satisﬁed free choose values gradient lagrangian respect variables zero long constraints satisﬁed satisfy constraints may set log obtain one reason using normal distribution know true distribution normal distribution maximum entropy impose least possible amount structure making assumption examining critical points lagrangian functional entropy found one critical point corresponding maximizing entropy ﬁxed variance probability distribution function minimizes entropy ﬁnd second critical point corresponding minimum reason speciﬁc function achieves minimal entropy functions place probability density two points place less probability density values lose entropy maintaining desired variance however function placing exactly zero mass two points integrate one valid probability distribution thus single minimal entropy probability distribution function much single minimal positive real number instead say sequence probability distributions converging toward putting mass two points degenerate scenario may chapter approximate inference described mixture dirac distributions dirac distributions described single probability distribution function dirac mixture dirac distribution corresponds single speciﬁc point function space distributions thus invisible method solving speciﬁc point functional derivatives zero limitation method distributions dirac must found methods guessing solution proving correct 
[approximate, inference, variational, inference, learning, continuous, latent, variables] graphical model contains continuous latent variables may still perform variational inference learning maximizing however must use calculus variations maximizing respect cases practitioners need solve calculus variations problems instead general equation mean ﬁeld ﬁxed point updates make mean ﬁeld approximation optimal may obtained normalizing unnormalized distribution exp log long assign probability joint conﬁguration variables carrying expectation inside equation yield correct functional form necessary derive functional forms directly using calculus variations one wishes develop new form variational learning yields mean ﬁeld approximation probabilistic model ﬁxed point equation designed iteratively applied value repeatedly convergence however also tells tells functional form optimal solution take whether arrive ﬁxed point equations means take functional form equation regard values appear parameters optimize optimization algorithm like example consider simple probabilistic model latent variables one visible variable suppose could actually simplify model integrating result gaussian distribution model chapter approximate inference interesting constructed provide simple demonstration calculus variations may applied probabilistic modeling true posterior given normalizing constant exp exp due presence terms multiplying together see true posterior factorize applying ﬁnd exp log exp see eﬀectively two values need obtain writing obtain exp see functional form gaussian thus conclude diagonal variational parameters optimize using technique choose important recall ever assume would gaussian gaussian form derived automatically using calculus variations maximize chapter approximate inference respect using approach diﬀerent model could yield diﬀerent functional form course small case constructed demonstration purposes examples real applications variational learning continuous variables context deep learning see goodfellow 
[approximate, inference, variational, inference, learning, interactions, learning, inference] using approximate inference part learning algorithm aﬀects learning process turn aﬀects accuracy inference algorithm speciﬁcally training algorithm tends adapt model way makes approximating assumptions underlying approximate inference algorithm become true training parameters variational learning increases log speciﬁc increases values high probability decreases values low probability behavior causes approximating assumptions become self fulﬁlling prophecies train model unimodal approximate posterior obtain model true posterior far closer unimodal would obtained training model exact inference computing true amount harm imposed model variational approximation thus diﬃcult exist several methods estimating log often estimate log training model ﬁnd gap small conclude variational approximation accurate speciﬁc value obtained learning process conclude variational approximation accurate general variational approximation little harm learning process measure true amount harm induced variational approximation would need know max log possible log log log hold simultaneously max log induces complicated posterior distribution family capture learning process never approach problem diﬃcult detect know sure happened superior learning algorithm ﬁnd comparison chapter approximate inference 
[approximate, inference, learned, approximate, inference] seen inference thought optimization procedure increases value function explicitly performing optimization via iterative procedures ﬁxed point equations gradient based optimization often expensive time consuming many approaches inference avoid expense learning perform approximate inference speciﬁcally think optimization process function maps input approximate distribution arg max think multi step iterative optimization process function approximate neural network implements approximation 
[approximate, inference, learned, approximate, inference, wake-sleep] one main diﬃculties training model infer supervised training set train model given know appropriate mapping depends choice model family evolves throughout learning process changes wake sleep algorithm hinton frey resolves problem drawing samples model distribution example directed model done cheaply performing ancestral sampling beginning ending inference network trained perform reverse mapping predicting caused present main drawback approach able train inference network values high probability model early learning model distribution resemble data distribution inference network opportunity learn samples resemble data sec saw one possible explanation role dream sleep human beings animals dreams could provide negative phase samples monte carlo training algorithms use approximate negative gradient log partition function undirected models another possible explanation biological dreaming providing samples used train inference network predict given senses explanation satisfying partition function explanation monte carlo algorithms generally perform well run using positive phase gradient several steps negative phase gradient several steps human beings animals usually awake several consecutive hours asleep several consecutive hours readily apparent chapter approximate inference schedule could support monte carlo training undirected model learning algorithms based maximizing run prolonged periods improving prolonged periods improving however role biological dreaming train networks predicting explains animals able remain awake several hours longer awake greater gap log remain lower bound remain asleep several hours generative model modiﬁed sleep without damaging internal models course ideas purely speculative hard evidence suggest dreaming accomplishes either goals dreaming may also serve reinforcement learning rather probabilistic modeling sampling synthetic experiences animal transition model train animal policy sleep may serve purpose yet anticipated machine learning community 
[approximate, inference, learned, approximate, inference, forms, learned, inference] strategy learned approximate inference also applied models salakhutdinov larochelle showed single pass learned inference network could yield faster inference iterating mean ﬁeld ﬁxed point equations dbm training procedure based running inference network applying one step mean ﬁeld improve estimates training inference network output reﬁned estimate instead original estimate already seen sec predictive sparse decomposition model trains shallow encoder network predict sparse code input seen hybrid autoencoder sparse coding possible devise probabilistic semantics model encoder may viewed performing learned approximate map inference due shallow encoder psd able implement kind competition units seen mean ﬁeld inference however problem remedied training deep encoder perform learned approximate inference ista technique gregor lecun learned approximate inference recently become one dominant approaches generative modeling form variational autoencoder elegant approach need kingma rezende construct explicit targets inference network instead inference network simply used deﬁne elegant approach need inference network adapted increase model described depth later sec chapter approximate inference using approximate inference possible train use wide variety models many models described next chapter 
[deep, generative, models] chapter present several speciﬁc kinds generative models built trained using techniques presented chapters models represent probability distributions multiple variables way allow probability distribution function evaluated explicitly others allow evaluation probability distribution function support operations implicitly require knowledge drawing samples distribution models structured probabilistic models described terms graphs factors using language graphical models presented chapter others easily described terms factors represent probability distributions nonetheless 
[deep, generative, models, boltzmann, machines] many variants boltzmann machines possible boltzmann machines may extended diﬀerent training criteria focused boltzmann machines trained approximately maximize generative criterion log also possible train discriminative rbms aim maximize log instead approach often larochelle bengio performs best using linear combination generative discriminative criteria unfortunately rbms seem powerful supervised learners mlps least using existing methodology boltzmann machines used practice second order interactions energy functions meaning energy functions sum many terms individual term includes product two random variables example term also possible train higher order boltzmann machines whose energy function terms sejnowski involve products many variables three way interactions hidden unit two diﬀerent images model spatial transformations one frame video next multiplication memisevic hinton one hot class variable change relationship visible hidden units depending class present one recent example nair hinton use higher order interactions boltzmann machine two groups hidden units one group hidden units interact visible units class label another group hidden units interact input values interpreted encouraging luo hidden units learn model input using features relevant chapter deep generative models class also learn extra hidden units explain nuisance details necessary samples realistic determine class example another use higher order interactions gate features sohn introduced boltzmann machine third order interactions binary mask variables associated visible unit masking variables set zero remove inﬂuence visible unit hidden units allows visible units relevant classiﬁcation problem removed inference pathway estimates class generally boltzmann machine framework rich space models permitting many model structures explored far developing new form boltzmann machine requires care creativity developing new neural network layer often diﬃcult ﬁnd energy function maintains tractability diﬀerent conditional distributions needed use boltzmann machine despite required eﬀort ﬁeld remains open innovation 
[deep, generative, models, restricted, boltzmann, machines] invented name restricted boltzmann harmonium smolensky machines common building blocks deep probabilistic models brieﬂy described rbms previously sec review previous information detail rbms undirected probabilistic graphical models containing layer observable variables single layer latent variables rbms may stacked one top form deeper models see fig examples particular fig shows graph structure rbm bipartite graph connections permitted variables observed layer units latent layer chapter deep generative models figure examples models may built restricted boltzmann machines restricted boltzmann machine undirected graphical model based bipartite graph visible units one part graph hidden units part connections among visible units connections among hidden units typically every visible unit connected every hidden unit possible construct sparsely connected rbms convolutional rbms deep belief network hybrid graphical model involving directed undirected connections like rbm intra layer connections however dbn multiple hidden layers thus connections hidden units separate layers local conditional probability distributions needed deep belief network copied directly local conditional probability distributions constituent rbms alternatively could also represent deep belief network completely undirected graph would need intra layer connections capture dependencies parents deep boltzmann machine undirected graphical model several layers latent variables like rbms dbns dbms lack intra layer connections dbms less closely tied rbms dbns initializing dbm stack rbms necessary modify rbm parameters slightly kinds dbms may trained without ﬁrst training set rbms chapter deep generative models begin binary version restricted boltzmann machine see later extensions types visible hidden units formally let observed layer consist set binary random variables refer collectively vector refer latent hidden layer binary random variables like general boltzmann machine restricted boltzmann machine energy based model joint probability distribution speciﬁed energy function exp energy function rbm given normalizing constant known partition function exp apparent deﬁnition partition function naive method computing exhaustively summing states could computationally intractable unless cleverly designed algorithm could exploit regularities probability distribution compute faster case restricted boltzmann machines formally proved partition function long servedio intractable intractable partition function implies normalized joint probability distribution also intractable evaluate 
[deep, generative, models, restricted, boltzmann, machines, conditional, distributions] though intractable bipartite graph structure rbm special property conditional distributions factorial relatively simple compute sample deriving conditional distributions joint distribution straightfor ward exp exp chapter deep generative models exp uff uff uff uffc uffd uffe exp since conditioning visible units treat constant respect distribution factorial nature conditional follows immediately ability write joint probability vector product unnormalized distributions individual elements simple matter normalizing distributions individual binary exp exp exp express full conditional hidden layer factorial distribution similar derivation show condition interest also factorial distribution  
[deep, generative, models, restricted, boltzmann, machines, training, restricted, boltzmann, machines] rbm admits eﬃcient evaluation diﬀerentiation eﬃcient mcmc sampling form block gibbs sampling readily trained techniques described chapter training models intractable partition functions includes sml pcd ratio matching compared undirected models used deep learning rbm relatively straightforward train compute chapter deep generative models exactly closed form deep models deep boltzmann machine combine diﬃculty intractable partition function diﬃculty intractable inference 
[deep, generative, models, deep, belief, networks] deep belief networks dbns one ﬁrst non convolutional models successfully admit training deep architectures hinton hinton introduction deep belief networks began current deep learning renaissance prior introduction deep belief networks deep models considered diﬃcult optimize kernel machines convex objective functions dominated research landscape deep belief networks demonstrated deep architectures successful outperforming kernelized support vector machines mnist dataset today deep belief hinton networks mostly fallen favor rarely used even compared unsupervised generative learning algorithms still deservedly recognized important role deep learning history deep belief networks generative models several layers latent variables latent variables typically binary visible units may binary real intra layer connections usually every unit layer connected every unit neighboring layer though possible construct sparsely connected dbns connections top two layers undirected connections layers directed arrows pointed toward layer closest data see fig example dbn hidden layers contains weight matrices also contains bias vectors providing biases visible layer probability distribution represented dbn given exp case real valued visible units substitute chapter deep generative models diagonal tractability generalizations exponential family visible units straightforward least theory dbn one hidden layer rbm generate sample dbn ﬁrst run several steps gibbs sampling top two hidden layers stage essentially drawing sample rbm deﬁned top two hidden layers use single pass ancestral sampling rest model draw sample visible units deep belief networks incur many problems associated directed models undirected models inference deep belief network intractable due explaining away eﬀect within directed layer due interaction two hidden layers undirected connections evaluating maximizing standard evidence lower bound log likelihood also intractable evidence lower bound takes expectation cliques whose size equal network width evaluating maximizing log likelihood requires confronting problem intractable inference marginalize latent variables also problem intractable partition function within undirected model top two layers train deep belief network one begins training rbm maximize data log using contrastive divergence stochastic maximum likelihood parameters rbm deﬁne parameters ﬁrst layer dbn next second rbm trained approximately maximize data log probability distribution represented ﬁrst rbm probability distribution represented second rbm words second rbm trained model distribution deﬁned sampling hidden units ﬁrst rbm ﬁrst rbm driven data procedure repeated indeﬁnitely add many layers dbn desired new rbm modeling samples previous one rbm deﬁnes another layer dbn procedure justiﬁed increasing variational lower bound log likelihood data dbn hinton applications eﬀort made jointly train dbn greedy layer wise procedure complete however possible perform generative ﬁne tuning using wake sleep algorithm chapter deep generative models trained dbn may used directly generative model interest dbns arose ability improve classiﬁcation models take weights dbn use deﬁne mlp initializing mlp weights biases learned via generative training dbn may train mlp perform classiﬁcation task additional training mlp example discriminative ﬁne tuning speciﬁc choice mlp somewhat arbitrary compared many inference equations chapter derived ﬁrst principles mlp heuristic choice seems work well practice used consistently literature many approximate inference techniques motivated ability ﬁnd maximally variational lower bound log likelihood tight set constraints one construct variational lower bound log likelihood using hidden unit expectations deﬁned dbn mlp true probability distribution hidden units reason believe mlp provides particularly tight bound particular mlp ignores many important interactions dbn graphical model mlp propagates information upward visible units deepest hidden units propagate information downward sideways dbn graphical model explaining away interactions hidden units within layer well top interactions layers log likelihood dbn intractable may approximated ais salakhutdinov murray permits evaluating quality generative model term deep belief network commonly used incorrectly refer kind deep neural network even networks without latent variable semantics term deep belief network refer speciﬁcally models undirected connections deepest layer directed connections pointing downward pairs consecutive layers term deep belief network may also cause confusion term belief network sometimes used refer purely directed models deep belief networks contain undirected layer deep belief networks also share acronym dbn dynamic bayesian networks dean kanazawa bayesian networks representing markov chains chapter deep generative models figure graphical model deep boltzmann machine one visible layer bottom two hidden layers connections units neighboring layers intra layer layer connections 
[deep, generative, models, deep, boltzmann, machines] deep boltzmann machine dbm salakhutdinov hinton another kind deep generative model unlike deep belief network dbn entirely undirected model unlike rbm dbm several layers latent variables rbms one like rbm within layer variables mutually independent conditioned variables neighboring layers see fig graph structure deep boltzmann machines applied variety tasks including document modeling srivastava like rbms dbns dbms typically contain binary units assume simplicity presentation model straightforward include real valued visible units dbm energy based model meaning joint probability distribution model variables parametrized energy function case deep boltzmann machine one visible layer three hidden layers joint probability given exp simplify presentation omit bias parameters dbm energy function deﬁned follows chapter deep generative models figure deep boltzmann machine arranged reveal bipartite graph structure comparison rbm energy function dbm energy function includes connections hidden units latent variables form weight matrices see connections signiﬁcant consequences model behavior well performing inference model comparison fully connected boltzmann machines every unit con nected every unit dbm oﬀers advantages similar oﬀered rbm speciﬁcally illustrated fig dbm layers organized bipartite graph odd layers one side even layers immediately implies condition variables even layer variables odd layers become conditionally independent course condition variables odd layers variables even layers also become conditionally independent bipartite structure dbm means apply equa tions previously used conditional distributions rbm determine conditional distributions dbm units within layer conditionally independent given values neighboring layers distributions binary variables fully described bernoulli parameters giving probability unit active example two hidden layers activation probabilities given chapter deep generative models bipartite structure makes gibbs sampling deep boltzmann machine eﬃcient naive approach gibbs sampling update one variable time rbms allow visible units updated one block hidden units updated second block one might naively assume dbm layers requires updates iteration updating block consisting one layer units instead possible update units two iterations gibbs sampling divided two blocks updates one including even layers including visible layer including odd layers due bipartite dbm connection pattern given even layers distribution odd layers factorial thus sampled simultaneously independently block likewise given odd layers even layers sampled simultaneously independently block eﬃcient sampling especially important training stochastic maximum likelihood algorithm 
[deep, generative, models, deep, boltzmann, machines, interesting, properties] deep boltzmann machines many interesting properties dbms developed dbns compared dbns posterior distribu tion simpler dbms somewhat counterintuitively simplicity posterior distribution allows richer approximations posterior case dbn perform classiﬁcation using heuristically motivated approximate inference procedure guess reasonable value mean ﬁeld expectation hidden units provided upward pass network mlp uses sigmoid activation functions weights original dbn distribution may used obtain variational lower bound log likelihood heuristic procedure therefore allows obtain bound however bound explicitly optimized way bound may far tight particular heuristic estimate ignores interactions hidden units within layer well top feedback inﬂuence hidden units deeper layers hidden units closer input heuristic mlp based inference procedure dbn able account interactions resulting presumably far chapter deep generative models optimal dbms hidden units within layer conditionally independent given layers lack intra layer interaction makes possible use ﬁxed point equations actually optimize variational lower bound ﬁnd true optimal mean ﬁeld expectations within numerical tolerance use proper mean ﬁeld allows approximate inference procedure dbms capture inﬂuence top feedback interactions makes dbms interesting point view neuroscience human brain known use many top feedback connections property dbms used computational models real neuroscientiﬁc phenomena series reichert one unfortunate property dbms sampling relatively diﬃcult dbns need use mcmc sampling top pair layers layers used end sampling process one eﬃcient ancestral sampling pass generate sample dbm necessary use mcmc across layers every layer model participating every markov chain transition 
[deep, generative, models, deep, boltzmann, machines, dbm, mean, field, inference] conditional distribution one dbm layer given neighboring layers factorial example dbm two hidden layers distributions distribution hidden layers generally factorize interactions layers example two hidden layers factorize due due interaction weights render variables mutually dependent case dbn left seek methods approximate dbm posterior distribution however unlike dbn dbm posterior distribution hidden units complicated easy approximate approximation discussed sec speciﬁcally mean variational ﬁeld approximation mean ﬁeld approximation simple form variational inference restrict approximating distribution fully factorial distri butions context dbms mean ﬁeld equations capture bidirectional interactions layers section derive iterative approximate inference procedure originally introduced salakhutdinov hinton variational approximations inference approach task approxi mating particular target distribution case posterior distribution chapter deep generative models hidden units given visible units reasonably simple family dis tributions case mean ﬁeld approximation approximating family set distributions hidden units conditionally independent develop mean ﬁeld approach example two hidden layers let approximation mean ﬁeld assumption implies mean ﬁeld approximation attempts ﬁnd member family distributions best ﬁts true posterior importantly inference process must run ﬁnd diﬀerent distribution every time use new value one conceive many ways measuring well ﬁts mean ﬁeld approach minimize log general provide parametric form approximating distribution beyond enforcing independence assumptions variational approximation procedure generally able recover functional form approximate distribution however case mean ﬁeld assumption binary hidden units case developing loss generality resulting ﬁxing parametrization model advance parametrize product bernoulli distributions associate probability element parameter speciﬁcally thus following approximation posterior course dbms layers approximate posterior parametrization extended obvious way exploiting bipartite structure graph chapter deep generative models update even layers simultaneously update odd layers simultaneously following schedule gibbs sampling speciﬁed family approximating distributions remains specify procedure choosing member family best ﬁts straightforward way use mean ﬁeld equations speciﬁed equations derived solving derivatives variational lower bound zero describe abstract manner optimize variational lower bound model simply taking expectations respect applying general equations obtain update rules ignoring bias terms ufeb ufed uff uff ﬁxed point system equations local maximum variational lower bound thus ﬁxed point update equations deﬁne iterative algorithm alternate updates using updates using small problems mnist ten iterations suﬃcient ﬁnd approximate positive phase gradient learning ﬁfty usually suﬃce obtain high quality representation single speciﬁc example used high accuracy classiﬁcation extending approximate variational inference deeper dbms straightforward 
[deep, generative, models, deep, boltzmann, machines, dbm, parameter, learning] learning dbm must confront challenge intractable partition function using techniques chapter challenge intractable posterior distribution using techniques chapter described sec variational inference allows construction distribution approximates intractable learning proceeds maximizing variational lower bound intractable log likelihood log chapter deep generative models deep boltzmann machine two hidden layers given log expression still contains log partition function log deep boltzmann machine contains restricted boltzmann machines components hardness results computing partition function sampling apply restricted boltzmann machines also apply deep boltzmann machines means evaluating probability mass function boltzmann machine requires approximate methods annealed importance sampling likewise training model requires approximations gradient log partition function see chapter general description methods dbms typically trained using stochastic maximum likelihood many techniques described chapter applicable techniques pseudolikelihood require ability evaluate unnormalized probabilities rather merely obtain variational lower bound contrastive divergence slow deep boltzmann machines allow eﬃcient sampling hidden units given visible units instead contrastive divergence would require burning markov chain every time new negative phase sample needed non variational version stochastic maximum likelihood algorithm discussed earlier sec variational stochastic maximum likelihood applied dbm given algorithm recall describe simpliﬁed varient dbm lacks bias parameters including trivial 
[deep, generative, models, deep, boltzmann, machines, layer-wise, pretraining] unfortunately training dbm using stochastic maximum likelihood described random initialization usually results failure cases model fails learn represent distribution adequately cases dbm may represent distribution well higher likelihood could obtained rbm dbm small weights ﬁrst layer represents approximately distribution rbm various techniques permit joint training developed described sec however original popular method overcoming joint training problem dbms greedy layer wise pretraining method layer dbm trained isolation rbm ﬁrst layer trained model input data subsequent rbm trained model samples previous rbm posterior distribution chapter deep generative models algorithm variational stochastic maximum likelihood algorithm training dbm two hidden layers set step size small positive number set number gibbs steps high enough allow markov chain burn starting samples initialize three matrices rows set random values bernoulli distributions possibly marginals matched model marginals converged learning loop sample minibatch examples training data arrange rows design matrix initialize matrices possibly model marginals converged mean ﬁeld inference loop end gibbs sampling gibbs block sampled sampled gibbs block sampled end cartoon illustration practice use eﬀective algorithm momentum decaying learning rate end chapter deep generative models rbms trained way combined form dbm dbm may trained pcd typically pcd training make small change model parameters performance measured log likelihood assigns data ability classify inputs see fig illustration training procedure greedy layer wise training procedure coordinate ascent bears passing resemblance coordinate ascent optimize one subset parameters step however case greedy layer wise training procedure actually use diﬀerent objective function step greedy layer wise pretraining dbm diﬀers greedy layer wise pre training dbn parameters individual rbm may copied corresponding dbn directly case dbm rbm parameters must modiﬁed inclusion dbm layer middle stack rbms trained bottom input stack combined form dbm layer bottom top input account eﬀect salakhutdinov hinton advocate dividing weights top bottom rbm half inserting dbm additionally bottom rbm must trained using two copies visible unit weights tied equal two copies means weights eﬀectively doubled upward pass similarly top rbm trained two copies topmost layer obtaining state art results deep boltzmann machine requires modiﬁcation standard sml algorithm use small amount mean ﬁeld negative phase joint pcd training step salakhutdinov hinton speciﬁcally expectation energy gradient computed respect mean ﬁeld distribution units independent parameters mean ﬁeld distribution obtained running mean ﬁeld ﬁxed point equations one step see goodfellow comparison performance centered dbms without use partial mean ﬁeld negative phase 
[deep, generative, models, deep, boltzmann, machines, jointly, training, deep, boltzmann, machines] classic dbms require greedy unsupervised pretraining perform classiﬁcation well require separate mlp based classiﬁer top hidden features extract undesirable properties hard track performance training cannot evaluate properties full dbm training ﬁrst rbm thus hard tell well hyperparameters chapter deep generative models figure deep boltzmann machine training procedure used classify mnist dataset salakhutdinov hinton srivastava train rbm using approximately maximize log train second rbm models target class using approximately maximize log drawn ﬁrst rbm posterior conditioned data increase learning combine two rbms dbm train approximately maximize log using stochastic maximum likelihood delete model deﬁne new set features obtained running mean ﬁeld inference model lacking use features input mlp whose structure additional pass mean ﬁeld additional output layer estimate initialize mlp weights dbm weights train mlp approximately maximize log using stochastic gradient descent dropout figure reprinted goodfellow chapter deep generative models working quite late training process software implementations dbms need many diﬀerent components training individual rbms pcd training full dbm training based back propagation mlp finally mlp top boltzmann machine loses many advantages boltzmann machine probabilistic model able perform inference input values missing two main ways resolve joint training problem deep boltzmann machine ﬁrst centered deep boltzmann machine montavon muller reparametrizes model order make hessian cost function better conditioned beginning learning process yields model trained without greedy layer wise pretraining stage resulting model obtains excellent test set log likelihood produces high quality samples unfortunately remains unable compete appropriately regularized mlps classiﬁer second way jointly train deep boltzmann machine use multi prediction deep boltzmann machine goodfellow model uses alternative training criterion allows use back propagation algorithm order avoid problems mcmc estimates gradient unfortunately new criterion lead good likelihood samples compared mcmc approach lead superior classiﬁcation performance ability reason well missing inputs centering trick boltzmann machine easiest describe return general view boltzmann machine consisting set units weight matrix biases recall energy function given using diﬀerent sparsity patterns weight matrix implement structures boltzmann machines rbms dbms diﬀerent numbers layers accomplished partitioning visible hidden units zeroing elements units interact centered boltzmann machine introduces vector subtracted states typically hyperparameter ﬁxed beginning training usu ally chosen make sure model initialized reparametrization change set probability distributions model represent change dynamics stochastic gradient descent applied likelihood speciﬁcally many cases reparametrization results hessian matrix better conditioned melchior experimentally chapter deep generative models conﬁrmed conditioning hessian matrix improves observed centering trick equivalent another boltzmann machine learning technique enhanced gradient improved conditioning cho hessian matrix allows learning succeed even diﬃcult cases like training deep boltzmann machine multiple layers approach jointly training deep boltzmann machines multi prediction deep boltzmann machine dbm works viewing mean ﬁeld equations deﬁning family recurrent networks approximately solving every possible inference problem rather training goodfellow model maximize likelihood model trained make recurrent network obtain accurate answer corresponding inference problem training process illustrated fig consists randomly sampling training example randomly sampling subset inputs inference network training inference network predict values remaining units general principle back propagating computational graph approximate inference applied models stoyanov brakel models dbm ﬁnal loss lower bound likelihood instead ﬁnal loss typically based approximate conditional distribution approximate inference network imposes missing values means training models somewhat heuristically motivated inspect represented boltzmann machine learned dbm tends somewhat defective sense gibbs sampling yields poor samples back propagation inference graph two main advantages first trains model really used approximate inference means approximate inference example ﬁll missing inputs perform classiﬁcation despite presence missing inputs accurate dbm original dbm original dbm make accurate classiﬁer best classiﬁcation results original dbm based training separate classiﬁer use features extracted dbm rather using inference dbm compute distribution class labels mean ﬁeld inference dbm performs well classiﬁer without special modiﬁcations advantage back propagating approximate inference back propagation computes exact gradient loss better optimization approximate gradients sml training suﬀer bias variance probably explains dbms may trained jointly dbms require greedy layer wise pretraining chapter deep generative models figure illustration multi prediction training process deep boltzmann machine row indicates diﬀerent example within minibatch training step column represents time step within mean ﬁeld inference process example sample subset data variables serve inputs inference process variables shaded black indicate conditioning run mean ﬁeld inference process arrows indicating variables inﬂuence variables process practical applications unroll mean ﬁeld several steps illustration unroll two steps dashed arrows indicate process could unrolled steps data variables used inputs inference process become targets shaded gray view inference process example recurrent network use gradient descent back propagation train recurrent networks produce correct targets given inputs trains mean ﬁeld process dbm produce accurate estimates figure adapted goodfellow chapter deep generative models disadvantage back propagating approximate inference graph provide way optimize log likelihood rather heuristic approximation generalized pseudolikelihood dbm inspired nade raiko extension nade framework described sec dbm connections dropout dropout shares rameters among many diﬀerent computational graphs diﬀerence graph whether includes excludes unit dbm also shares parameters across many computational graphs case dbm diﬀerence graphs whether input unit observed unit observed dbm delete entirely case dropout instead dbm treats latent variable inferred one could imagine applying dropout dbm additionally removing units rather making latent 
[deep, generative, models, boltzmann, machines, real-valued, data] boltzmann machines originally developed use binary data many applications image audio modeling seem require ability represent probability distributions real values cases possible treat real valued data interval representing expectation binary variable example hinton treats grayscale images training set deﬁning probability values pixel deﬁnes probability binary value binary pixels sampled independently common procedure evaluating binary models grayscale image datasets however particularly theoretically satisfying approach binary images sampled independently way noisy appearance section present boltzmann machines deﬁne probability density real valued data 
[deep, generative, models, boltzmann, machines, real-valued, data, gaussian-bernoulli, rbms] restricted boltzmann machines may developed many exponential family conditional distributions welling common rbm binary hidden units real valued visible units conditional distribution visible units gaussian distribution whose mean function hidden units many ways parametrizing gaussian bernoulli rbms first may chapter deep generative models choose whether use covariance matrix precision matrix gaussian distribution present precision formulation modiﬁcation obtain covariance formulation straightforward wish conditional distribution ﬁnd terms need add energy function expanding unnormalized log conditional distribution log encapsulates terms function parameters random variables model discard role normalize distribution partition function whatever energy function choose carry role include terms sign ﬂipped involving energy function add terms involving energy function represent desired conditional freedom regarding conditional distribution note contains term term cannot included entirety includes terms correspond edges hidden units included terms would linear factor model instead restricted boltzmann machine designing boltzmann machine simply omit cross terms omitting change conditional still respected however still choice whether include terms involving single assume diagonal precision matrix ﬁnd hidden unit term used fact include term sign ﬂipped energy function naturally bias turned weights unit large connected visible units high precision choice whether include bias term aﬀect family distributions model represent assuming chapter deep generative models include bias parameters hidden units aﬀect learning dynamics model including term may help hidden unit activations remain reasonable even weights rapidly increase magnitude one way deﬁne energy function gaussian bernoulli rbm thus may also add extra terms parametrize energy terms variance rather precision choose derivation included bias term visible units one could easily added one ﬁnal source variability parametrization gaussian bernoulli rbm choice treat precision matrix may either ﬁxed constant perhaps estimated based marginal precision data learned may also scalar times identity matrix may diagonal matrix typically allow precision matrix non diagonal context operations would require inverting matrix sections ahead see forms boltzmann machines permit modeling covariance structure using various techniques avoid inverting precision matrix 
[deep, generative, models, boltzmann, machines, real-valued, data, undirected, models, conditional, covariance] gaussian rbm canonical energy model real valued data argue gaussian rbm inductive bias ranzato well suited statistical variations present types real valued data especially natural images problem much information content present natural images embedded covariance pixels rather raw pixel values words relationships pixels absolute values useful information images resides since gaussian rbm models conditional mean input given hidden units cannot capture conditional covariance information response criticisms alternative models proposed attempt better account covariance real valued data models include mean covariance rbm mcrbm mean product distribution mpot model spike slab rbm ssrbm term mcrbm pronounced saying name letters pronounced like mcdonald chapter deep generative models mean covariance rbm mcrbm uses hidden units indepen dently encode conditional mean covariance observed units mcrbm hidden layer divided two groups units mean units covariance units group models conditional mean simply gaussian rbm half covariance rbm also called crbm ranzato whose components model conditional covariance structure described speciﬁcally binary mean units binary covariance units mcrbm model deﬁned combination two energy functions standard gaussian bernoulli rbm energy function crbm energy function models conditional covariance information parameter corresponds covariance weight vector associated vector covariance oﬀsets combined energy function deﬁnes joint distribution exp corresponding conditional distribution observations given multivariate gaussian distribution ufeb ufed ufeb ufed uff uff uff uff note covariance matrix non diagonal weight matrix associated gaussian rbm modeling version gaussian bernoulli rbm energy function assumes image data zero mean per pixel pixel oﬀsets easily added model account nonzero pixel means chapter deep generative models conditional means diﬃcult train mcrbm via contrastive divergence persistent contrastive divergence non diagonal conditional covariance structure pcd require sampling joint distribution standard rbm accomplished gibbs sampling conditionals however mcrbm sampling requires computing every iteration learning impractical computational burden larger observations avoid direct sampling ranzato hinton conditional sampling directly marginal using hamiltonian hybrid monte carlo mcrbm free neal energy mean product student distributions mean product student distribution mpot model extends pot model ranzato welling manner similar mcrbm extends crbm achieved including nonzero gaussian means addition gaussian rbm like hidden units like mcrbm pot conditional distribution observation multivariate gaussian non diagonal covariance distribution however unlike mcrbm complementary conditional distribution hidden variables given conditionally independent gamma distributions gamma distribution probability distribution positive real numbers mean necessary detailed understanding gamma distribution understand basic ideas underlying mpot model mpot energy function mpot log covariance weight vector associated unit deﬁned mcrbm mpot model energy function speciﬁes mul tivariate gaussian conditional distribution non diagonal covariance learning mpot model like mcrbm compli cated inability sample non diagonal gaussian conditional mpot also advocate direct sampling ranzato via hamiltonian hybrid monte carlo chapter deep generative models spike slab restricted boltzmann machines spike slab restricted boltzmann machines ssrbms provide another means courville modeling covariance structure real valued data compared mcrbms ssrbms advantage requiring neither matrix inversion hamiltonian monte carlo methods model natural images ssrbm interesting like mcrbm mpot model binary hidden units encode conditional covariance across pixels use auxiliary real valued variables spike slab rbm two sets hidden units binary units spike real valued units slab mean visible units conditioned hidden units given words column deﬁnes component appear input corresponding spike variable determines whether component present corresponding slab variable determines intensity component present spike variable active corresponding slab variable adds variance input along axis deﬁned allows model covariance inputs fortunately contrastive divergence persistent contrastive divergence gibbs sampling still applicable need invert matrix formally ssrbm model deﬁned via energy function oﬀset spike diagonal precision matrix observations parameter scalar precision parameter real valued slab variable parameter non negative diagonal matrix deﬁnes modulated quadratic penalty mean parameter slab variable joint distribution deﬁned via energy function relatively straightforward derive ssrbm conditional distributions example marginalizing slab variables conditional distribution observations given binary spike variables given exp chapter deep generative models last equality holds covariance matrix positive deﬁnite gating spike variables means true marginal distribution sparse diﬀerent sparse coding samples model almost never measure theoretic sense contain zeros code map inference required impose sparsity comparing ssrbm mcrbm mpot models ssrbm parametrizes conditional covariance observation signiﬁcantly diﬀerent way mcrbm mpot model covariance structure observation using activation hidden units enforce constraints conditional covariance direction contrast ssrbm speciﬁes conditional covariance observations using hidden spike activations pinch precision matrix along direction speciﬁed corresponding weight vector ssrbm conditional covariance similar given diﬀerent model product probabilistic principal components analysis poppca williams agakov overcomplete setting sparse activations ssrbm parametrization permit signiﬁcant variance nominal variance given selected directions sparsely activated mcrbm mpot models overcomplete representation would mean capture variation particular direction observation space requires removing potentially constraints positive projection direction would suggest models less well suited overcomplete setting primary disadvantage spike slab restricted boltzmann machine settings parameters correspond covariance matrix positive deﬁnite covariance matrix places unnormalized probability values farther mean causing integral possible outcomes diverge generally issue avoided simple heuristic tricks yet theoretically satisfying solution using constrained optimization explicitly avoid regions probability undeﬁned diﬃcult without overly conservative also preventing model accessing high performing regions parameter space qualitatively convolutional variants ssrbm produce excellent samples natural images examples shown fig ssrbm allows several extensions including higher order interactions average pooling slab variables enables model courville learn excellent features classiﬁer labeled data scarce adding chapter deep generative models term energy function prevents partition function becoming undeﬁned results sparse coding model spike slab sparse coding goodfellow also known 
[deep, generative, models, convolutional, boltzmann, machines] seen chapter extremely high dimensional inputs images place great strain computation memory statistical requirements machine learning models replacing matrix multiplication discrete convolution small kernel standard way solving problems inputs translation invariant spatial temporal structure desjardins bengio showed approach works well applied rbms deep convolutional networks usually require pooling operation spatial size successive layer decreases feedforward convolutional networks often use pooling function maximum elements pooled unclear generalize setting energy based models could introduce binary pooling unit binary detector units enforce max setting energy function whenever constraint violated scale well though requires evaluating diﬀerent energy conﬁgurations compute normalization constant small pooling region requires energy function evaluations per pooling unit lee developed solution problem called probabilistic max pooling confused stochastic pooling technique implicitly constructing ensembles convolutional feedforward networks strategy behind probabilistic max pooling constrain detector units one may active time means total states one state detector units additional state corresponding detector units pooling unit one detector units state units assigned energy zero think describing model single variable states equivalently model variables assigns energy joint assignments variables eﬃcient probabilistic max pooling force detector units mutually exclusive may useful regularizing constraint contexts harmful limit model capacity contexts also support overlapping pooling regions overlapping pooling regions usually required obtain best performance feedforward convolutional networks constraint probably greatly reduces performance convolutional boltzmann chapter deep generative models machines lee demonstrated probabilistic max pooling could used build convolutional deep boltzmann machines model able perform operations ﬁlling missing portions input intellectually appealing model challenging make work practice usually perform well classiﬁer traditional convolutional networks trained supervised learning many convolutional models work equally well inputs many diﬀerent spatial sizes boltzmann machines diﬃcult change input size variety reasons partition function changes size input changes moreover many convolutional networks achieve size invariance scaling size pooling regions proportional size input scaling boltzmann machine pooling regions awkward traditional convolutional neural networks use ﬁxed number pooling units dynamically increase size pooling regions order obtain ﬁxed size representation variable sized input boltzmann machines large pooling regions become expensive naive approach approach making lee detector units pooling region mutually exclusive solves computational problems still allow variable size pooling regions example suppose learn model probabilistic max pooling detector units learn edge detectors enforces constraint one edges may appear region increase size input image direction would expect number edges increase correspondingly instead increase size pooling regions direction mutual exclusivity constraint speciﬁes edges may appear region grow model input image way model generates edges less density course issues arise model must use variable amounts pooling order emit ﬁxed size output vector models use probabilistic max pooling may still accept variable sized input images long output model feature map scale size proportional input image pixels boundary image also pose diﬃculty exac erbated fact connections boltzmann machine symmetric implicitly zero pad input fewer hidden units visible units visible units boundary image modeled publication describes model deep belief network described purely undirected model tractable layer wise mean ﬁeld ﬁxed point updates best ﬁts deﬁnition deep boltzmann machine chapter deep generative models well lie receptive ﬁeld fewer hidden units however implicitly zero pad input hidden units boundary driven fewer input pixels may fail activate needed 
[deep, generative, models, boltzmann, machines, structured, sequential, outputs] structured output scenario wish train model map input output diﬀerent entries related must obey constraints example speech synthesis task waveform entire waveform must sound like coherent utterance natural way represent relationships entries use probability distribution boltzmann machines extended model conditional distributions supply probabilistic model tool conditional modeling boltzmann machine used structured output tasks also sequence modeling latter case rather mapping input output model must estimate probability distribution sequence variables conditional boltzmann machines represent factors form order accomplish task important sequence modeling task video game ﬁlm industry modeling sequences joint angles skeletons used render characters sequences often collected using motion capture systems record movements actors probabilistic model character movement allows generation new previously unseen realistic animations solve sequence modeling task taylor introduced conditional rbm modeling small model rbm whose bias parameters linear function preceding values condition diﬀerent values earlier variables get new rbm weights rbm never change conditioning diﬀerent past values change probability diﬀerent hidden units rbm active activating deactivating diﬀerent subsets hidden units make large changes probability distribution induced variants conditional rbm variants sequence mnih modeling using conditional rbms possible taylor hinton sutskever boulanger lewandowski another sequence modeling task model distribution sequences chapter deep generative models musical notes used compose songs boulanger lewandowski introduced sequence model applied task rnn rnn rbm rbm generative model sequence frames consisting rnn emits rbm parameters time step unlike model described rnn emits parameters rbm including weights train model need able back propagate gradient loss function rnn loss function applied directly rnn outputs instead applied rbm means must approximately diﬀerentiate loss respect rbm parameters using contrastive divergence related algorithm approximate gradient may back propagated rnn using usual back propagation time algorithm 
[deep, generative, models, back-propagation, random, operations] traditional neural networks implement deterministic transformation input variables developing generative models often wish extend neural networks implement stochastic transformations one straightforward way augment neural network extra inputs sampled simple probability distribution uniform gaussian distribution neural network continue perform deterministic computation internally function appear stochastic observer access provided continuous diﬀerentiable compute gradients necessary training using back propagation usual example let consider operation consisting drawing samples gaussian distribution mean variance individual sample produced function rather sampling process whose output changes every time query may seem counterintuitive take derivatives respect parameters distribution however rewrite sampling process transforming underlying random value obtain sample chapter deep generative models desired distribution able back propagate sampling operation regard ing deterministic operation extra input crucially extra input random variable whose distribution function variables whose derivatives want calculate result tells inﬁnitesimal change would change output could repeat sampling operation value able back propagate sampling operation allows incorporate larger graph build elements graph top output sampling distribution example compute derivatives loss function also build elements graph whose outputs inputs parameters sampling operation example could build larger graph augmented graph use back propagation functions derive principle used gaussian sampling example generally appli cable express probability distribution form variable containing parameters applicable inputs given value sampled distribution may turn function variables rewrite source randomness may compute derivatives respect using traditional tools back propagation algorithm applied long continuous diﬀerentiable almost everywhere crucially must function must function technique often called reparametrization trick stochastic back propagation perturbation analysis requirement continuous diﬀerentiable course requires continuous wish back propagate sampling process produces discrete valued samples may still possible estimate gradient using reinforcement learning algorithms variants reinforce algorithm discussed sec williams neural network applications typically choose drawn simple distribution unit uniform unit gaussian distribution chapter deep generative models achieve complex distributions allowing deterministic portion network reshape input idea propagating gradients optimizing stochastic operations dates back mid twentieth century price bonnet ﬁrst used machine learning context reinforcement learning williams recently applied variational approximations opper archambeau stochastic generative neural networks bengio kingma kingma welling rezende goodfellow many networks denoising autoencoders networks regularized dropout also naturally designed take noise input without requiring special reparametrization make noise independent model 
[deep, generative, models, back-propagation, random, operations, back-propagating, discrete, stochastic, operations] model emits discrete variable reparametrization trick applicable suppose model takes inputs parameters encapsulated vector combines random noise produce discrete must step function derivatives step function useful point right step boundary derivatives undeﬁned small problem large problem derivatives zero almost everywhere regions step boundaries derivatives cost function therefore give information update model parameters reinforce algorithm reward increment non negative factor oﬀset reinforcement characteristic eligibility provides framework deﬁning family simple powerful solutions core idea williams even though step function useless derivatives expected cost often smooth function amenable gradient descent although expectation typically tractable high dimensional result composition many discrete stochastic decisions estimated without bias using monte carlo average stochastic estimate gradient used sgd stochastic gradient based optimization techniques simplest version reinforce derived simply diﬀerentiating chapter deep generative models expected cost log log relies assumption reference directly trivial extend approach relax assumption exploits derivative rule logarithm log gives unbiased monte carlo estimator gradient anywhere write section one could equally write parametrized contains present one issue simple reinforce estimator high variance many samples need drawn obtain good estimator gradient equivalently one sample drawn sgd converge slowly require smaller learning rate possible considerably reduce variance estimator using variance reduction methods idea modify estimator wilson ecuyer expected value remains unchanged variance get reduced context reinforce proposed variance reduction methods involve computation baseline used oﬀset note oﬀset depend would change expectation estimated gradient log log chapter deep generative models means log log log log furthermore obtain optimal computing variance log minimizing respect ﬁnd optimal baseline diﬀerent element vector log log gradient estimator respect becomes log estimates estimate usually obtained adding extra outputs neural network training new outputs estimate log log element extra outputs trained mean squared error objective using respectively log log targets sampled given estimate may recovered substituting estimates preferred use single shared output across mnih gregor elements trained target using baseline variance reduction methods introduced reinforcement learning context sutton weaver tao generalizing previous work case binary reward dayan bengio mnih see gregor mnih examples modern uses reinforce algorithm reduced variance context deep learning addition use input dependent baseline found scale mnih gregor could adjusted training dividing standard deviation estimated moving average training kind adaptive learning rate counter eﬀect important variations occur course training magnitude quantity called heuristic mnih gregor variance normalization chapter deep generative models reinforce based estimators understood estimating gradient correlating choices corresponding values good value unlikely current parametrization might take long time obtain chance get required signal conﬁguration reinforced 
[deep, generative, models, directed, generative, nets] discussed chapter directed graphical models make prominent class graphical models directed graphical models popular within greater machine learning community within smaller deep learning community roughly overshadowed undirected models rbm section review standard directed graphical models traditionally associated deep learning community already described deep belief networks partially directed model also already described sparse coding models thought shallow directed generative models often used feature learners context deep learning though tend perform poorly sample generation density estimation describe variety deep fully directed models 
[deep, generative, models, directed, generative, nets, sigmoid, belief, nets] sigmoid belief networks simple form directed graphical model neal speciﬁc kind conditional probability distribution general think sigmoid belief network vector binary states element state inﬂuenced ancestors ufeb ufed jlt uff uff common structure sigmoid belief network one divided many layers ancestral sampling proceeding series many hidden layers ultimately generating visible layer structure similar deep belief network except units beginning sampling process independent rather sampled restricted boltzmann machine structure interesting variety chapter deep generative models reasons one reason structure universal approximator probability distributions visible units sense approximate probability distribution binary variables arbitrarily well given enough depth even width individual layers restricted dimensionality visible layer sutskever hinton generating sample visible units eﬃcient sigmoid belief network operations inference hidden units given visible units intractable mean ﬁeld inference also intractable variational lower bound involves taking expectations cliques encompass entire layers problem remained diﬃcult enough restrict popularity directed discrete networks one approach performing inference sigmoid belief network construct diﬀerent lower bound specialized sigmoid belief networks saul approach applied small networks another approach use learned inference mechanisms described sec helmholtz machine dayan dayan hinton sigmoid belief network combined inference network predicts parameters mean ﬁeld distribution hidden units modern approaches gregor mnih gregor sigmoid belief networks still use inference network approach techniques remain diﬃcult due discrete nature latent variables one cannot simply back propagate output inference network instead must use relatively unreliable machinery back propagating discrete sampling processes described sec recent approaches based importance sampling reweighted wake sleep bornschein bengio bidirectional helmholtz machines bornschein make possible quickly train sigmoid belief networks reach state art performance benchmark tasks special case sigmoid belief networks case latent variables learning case eﬃcient need marginalize latent variables likelihood family models called auto regressive networks generalize fully visible belief network kinds variables besides binary variables structures conditional distributions besides log linear relationships auto regressive networks described later sec 
[deep, generative, models, directed, generative, nets, diﬀerentiable, generator, nets] many generative models based idea using diﬀerentiable generator network model transforms samples latent variables samples chapter deep generative models distributions samples using diﬀerentiable function typically represented neural network model class includes variational autoencoders pair generator net inference net generative adversarial networks pair generator network discriminator network techniques train generator networks isolation generator networks essentially parametrized computational procedures generating samples architecture provides family possible distributions sample parameters select distribution within family example standard procedure drawing samples normal distribution mean covariance feed samples normal distribution zero mean identity covariance simple generator network generator network contains one aﬃne layer given cholesky decomposition pseudorandom number generators also use nonlinear transformations simple distributions example inverse transform sampling devroye draws scalar applies nonlinear transformation scalar case given inverse cumulative distribution function able specify integrate invert resulting function sample without using machine learning generate samples complicated distributions diﬃcult specify directly diﬃcult integrate whose resulting integrals diﬃcult invert use feedforward network represent parametric family nonlinear functions use training data infer parameters selecting desired function think providing nonlinear change variables transforms distribution desired distribution recall invertible diﬀerentiable continuous uecdet implicitly imposes probability distribution uecdet chapter deep generative models course formula may diﬃcult evaluate depending choice often use indirect means learning rather trying maximize log directly cases rather using provide sample directly use deﬁne conditional distribution example could use generator net whose ﬁnal layer consists sigmoid outputs provide mean parameters bernoulli distributions case use deﬁne impose distribution marginalizing approaches deﬁne distribution allow train various criteria using reparametrization trick sec two diﬀerent approaches formulating generator nets emitting parameters conditional distribution versus directly emitting samples complementary strengths weaknesses generator net deﬁnes conditional distribution capable generating discrete data well continuous data generator net provides samples directly capable generating continuous data could introduce discretization forward propagation would lose ability learn model using back propagation advantage direct sampling longer forced use conditional distributions whose form easily written algebraically manipulated human designer approaches based diﬀerentiable generator networks motivated success gradient descent applied diﬀerentiable feedforward networks classiﬁcation context supervised learning deep feedforward networks trained gradient based learning seem practically guaranteed succeed given enough hidden units enough training data recipe success transfer generative modeling generative modeling seems diﬃcult classiﬁcation regression learning process requires optimizing intractable criteria context diﬀerentiable generator nets criteria intractable data specify inputs outputs generator net case supervised learning inputs outputs given optimization procedure needs learn produce speciﬁed mapping case generative modeling learning procedure needs determine arrange space useful way additionally map chapter deep generative models dosovitskiy studied simpliﬁed problem correspondence given speciﬁcally training data computer rendered imagery chairs latent variables parameters given rendering engine describing choice chair model use position chair conﬁguration details aﬀect rendering image using synthetically generated data convolutional network able learn map descriptions content image approximations rendered images suggests contemporary diﬀerentiable generator networks suﬃcient model capacity good generative models contemporary optimization algorithms ability diﬃculty lies determining train generator networks value ﬁxed known ahead time following sections describe several approaches training diﬀerentiable generator nets given training samples 
[deep, generative, models, directed, generative, nets, variational, autoencoders] variational autoencoder vae kingma rezende directed model uses learned approximate inference trained purely gradient based methods generate sample model vae ﬁrst draws sample code distribution model sample run diﬀerentiable generator network finally sampled distribution model model however training approximate inference network encoder used obtain model viewed decoder network key insight behind variational autoencoders may trained maximizing variational lower bound associated data point log model log model model log model recognize ﬁrst term joint log likelihood visible hidden variables approximate posterior latent variables like except use approximate rather exact posterior recognize also second term entropy approximate posterior chosen gaussian distribution noise added predicted mean value maximizing entropy term encourages increasing standard deviation chapter deep generative models noise generally entropy term encourages variational posterior place high probability mass many values could generated rather collapsing single point estimate likely value recognize ﬁrst term reconstruction log likelihood found autoencoders second term tries make approximate posterior distribution model prior model approach traditional approaches variational inference learning infer via optimization algorithm typically iterated ﬁxed point equations sec approaches slow often require ability compute log model closed form main idea behind variational autoencoder train parametric encoder also sometimes called inference network recognition model produces parameters long continuous variable back propagate samples drawn order obtain gradient respect learning consists solely maximizing respect parameters encoder decoder expectations may approximated monte carlo sampling variational autoencoder approach elegant theoretically pleasing simple implement also obtains excellent results among state art approaches generative modeling main drawback samples variational autoencoders trained images tend somewhat blurry causes phenomenon yet known one possibility blurriness intrinsic eﬀect maximum likelihood minimizes data  model illustrated fig means model assign high probability points occur training set may also assign high probability points points may include blurry images part reason model would choose put probability mass blurry images rather part space variational autoencoders used practice usually gaussian distribution model maximizing lower bound likelihood distribution similar training traditional autoencoder mean squared error sense tendency ignore features input occupy pixels cause small change brightness pixels occupy issue speciﬁc vaes shared generative models optimize log likelihood equivalently data  model argued another theis huszar troubling issue contemporary vae models tend use small subset dimensions encoder able transform enough local directions input space space marginal distribution matches factorized prior chapter deep generative models vae framework straightforward extend wide range model architectures key advantage boltzmann machines require extremely careful model design maintain tractability vaes work well diverse family diﬀerentiable operators one particularly sophisticated vae deep recurrent attention writer draw model gregor draw uses recurrent encoder recurrent decoder combined attention mechanism generation process draw model consists sequentially visiting diﬀerent small image patches drawing values pixels points vaes also extended generate sequences deﬁning variational rnns chung using recurrent encoder decoder within vae framework generating sample traditional rnn involves non deterministic operations output space variational rnns also random variability potentially abstract level captured vae latent variables vae framework extended maximize traditional variational lower bound instead importance weighted autoencoder burda objective log model new objective equivalent traditional lower bound however may also interpreted forming estimate true log model using importance sampling proposal distribution importance weighted autoencoder objective also lower bound log model becomes tighter increases variational autoencoders interesting connections dbm approaches involve back propagation approximate inference graph goodfellow stoyanov brakel previous approaches required inference procedure mean ﬁeld ﬁxed point equations provide computational graph variational autoencoder deﬁned arbitrary computational graphs makes applicable wider range probabilistic model families need restrict choice models tractable mean ﬁeld ﬁxed point equations variational autoencoder also advantage increases bound log likelihood model criteria dbm related models heuristic little probabilistic interpretation beyond making results approximate inference accurate one disadvantage variational autoencoder learns inference network one problem inferring given chapter deep generative models older methods able perform approximate inference subset variables given subset variables mean ﬁeld ﬁxed point equations specify share parameters computational graphs diﬀerent problems one nice property variational autoencoder simultaneously training parametric encoder combination generator network forces model learn predictable coordinate system encoder capture makes excellent manifold learning algorithm see fig examples low dimensional manifolds learned variational autoencoder one cases demonstrated ﬁgure algorithm discovered two independent factors variation present images faces angle rotation emotional expression figure examples two dimensional coordinate systems high dimensional mani folds learned variational autoencoder kingma welling two dimensions may plotted directly page visualization gain understanding model works training model latent code even believe intrinsic dimensionality data manifold much higher images shown examples training set images actually generated model simply changing code image corresponds diﬀerent choice code uniform grid left two dimensional map frey faces manifold one dimension discovered horizontal mostly corresponds rotation face vertical corresponds emotional expression right two dimensional map mnist manifold chapter deep generative models 
[deep, generative, models, directed, generative, nets, generative, adversarial, networks] generative adversarial networks gans another goodfellow generative modeling approach based diﬀerentiable generator networks generative adversarial networks based game theoretic scenario generator network must compete adversary generator network directly produces samples adversary discriminator network attempts distinguish samples drawn training data samples drawn generator discriminator emits probability value given indicating probability real training example rather fake sample drawn model simplest way formulate learning generative adversarial networks zero sum game function determines payoﬀ discriminator generator receives payoﬀ learning player attempts maximize payoﬀ convergence arg min max default choice data log model log drives discriminator attempt learn correctly classify samples real fake simultaneously generator attempts fool classiﬁer believing samples real convergence generator samples indistinguishable real data discriminator outputs everywhere discriminator may discarded main motivation design gans learning process requires neither approximate inference approximation partition function gradient case max convex case optimization performed directly space probability density functions procedure guaranteed converge asymptotically consistent unfortunately learning gans diﬃcult practice represented neural networks max convex goodfellow identiﬁed non convergence issue may cause gans underﬁt general simultaneous gradient descent two players costs guaranteed reach equilibrium consider example value function one player controls incurs cost player controls receives cost model player making inﬁnitesimally small chapter deep generative models gradient steps player reducing cost expense player stable circular orbit rather arriving equilibrium point origin note equilibria minimax game local minima instead points simultaneously minima players costs means saddle points local minima respect ﬁrst player parameters local maxima respect second player parameters possible two players take turns increasing decreasing forever rather landing exactly saddle point neither player capable reducing cost known extent non convergence problem aﬀects gans goodfellow identiﬁed alternative formulation payoﬀs game longer zero sum expected gradient maximum likelihood learning whenever discriminator optimal maximum likelihood training converges reformulation gan game also converge given enough samples unfortunatley alternative formulation seem perform well practice possibly due suboptimality discriminator possibly due high variance around expected gradient practice best performing formulation gan game diﬀerent mulation neither zero sum equivalent maximum likelihood introduced goodfellow heuristic motivation best performing formulation generator aims increase log probability discrimina tor makes mistake rather aiming decrease log probability discriminator makes correct prediction reformulation motivated solely observation causes derivative generator cost function respect discriminator logits remain large even situation discriminator conﬁdently rejects generator samples stabilization gan learning remains open problem fortunately gan learning performs well model architecture hyperparameters care fully selected crafted deep convolutional gan dcgan radford performs well image synthesis tasks showed latent representation space captures important factors variation shown fig see fig examples images generated dcgan generator gan learning problem also simpliﬁed breaking generation process many levels detail possible train conditional gans mirza osindero learn sample distribution rather simply sampling marginal distribution denton showed series conditional gans trained ﬁrst generate low resolution version image incrementally add details image chapter deep generative models figure images generated gans trained lsun dataset left images bedrooms generated dcgan model reproduced permission radford images churches generated lapgan model reproduced right permission denton technique called lapgan model due use laplacian pyramid generate images containing varying levels detail lapgan generators able fool discriminator networks also human observers experimental subjects identifying outputs network real data see fig examples images generated lapgan generator one unusual capability gan training procedure proba bility distributions assign zero probability training points rather maximizing log probability speciﬁc points generator net learns trace manifold whose points resemble training points way somewhat para doxically means model may assign log likelihood negative inﬁnity test set still representing manifold human observer judges capture essence generation task clearly advantage disadvantage one may also guarantee generator network assigns non zero probability points simply making last layer generator network add gaussian noise generated values generator networks add gaussian noise manner sample distribution one obtains using generator network parametrize mean conditional gaussian distribution dropout seems important discriminator network particular units stochastically dropped computing gradient generator network follow following gradient deterministic version discriminator weights divided two seem eﬀective chapter deep generative models likewise never using dropout seems yield poor results gan framework designed diﬀerentiable generator networks similar principles used train kinds models example self supervised boosting used train rbm generator fool logistic regression discriminator welling 
[deep, generative, models, directed, generative, nets, generative, moment, matching, networks] generative moment matching networks dziugaite another form generative model based diﬀerentiable generator networks unlike vaes gans need pair generator network network neither inference network used vaes discriminator network used gans networks trained technique called moment matching basic idea behind moment matching train generator way many statistics samples generated model similar possible statistics examples training set context moment expectation diﬀerent powers random variable example ﬁrst moment mean second moment mean squared values multiple dimensions element random vector may raised diﬀerent powers moment may quantity form vector non negative integers upon ﬁrst examination approach seems computationally infeasible example want match moments form need minimize diﬀerence number values quadratic dimension moreover even matching ﬁrst second moments would suﬃcient multivariate gaussian distribution captures linear relationships values ambitions neural networks capture complex nonlinear relationships would require far moments gans avoid problem exhaustively enumerating moments using dynamically updated discriminator automatically focuses attention whichever statistic generator network matching least eﬀectively instead generative moment matching networks trained minimizing cost function called maximum mean discrepancy schölkopf smola gretton mmd cost function measures error ﬁrst moments inﬁnite dimensional space using implicit mapping feature chapter deep generative models space deﬁned kernel function order make computations inﬁnite dimensional vectors tractable mmd cost zero two distributions compared equal visually samples generative moment matching networks somewhat disappointing fortunately improved combining generator network autoencoder first autoencoder trained reconstruct training set next encoder autoencoder used transform entire training set code space generator network trained generate code samples may mapped visually pleasing samples via decoder unlike gans cost function deﬁned respect batch examples training set generator network possible make training update function one training example one sample generator network moments must computed empirical average across many samples batch size small mmd underestimate true amount variation distributions sampled ﬁnite batch size suﬃciently large eliminate problem entirely larger batches reduce amount underestimation batch size large training procedure becomes infeasibly slow many examples must processed order compute single small gradient step gans possible train generator net using mmd even generator net assigns zero probability training points 
[deep, generative, models, directed, generative, nets, convolutional, generative, networks] generating images often useful use generator network includes convolutional structure see example goodfellow dosovitskiy use transpose convolution operator described sec approach often yields realistic images using fewer parameters using fully connected layers without parameter sharing convolutional networks recognition tasks information ﬂow image summarization layer top network often class label image ﬂows upward network information discarded representation image becomes invariant nuisance transformations generator network opposite true rich details must added representation image generated propagates network culminating ﬁnal representation image course image detailed glory object positions poses textures chapter deep generative models lighting primary mechanism discarding information convolutional recognition network pooling layer generator network seems need add information cannot put inverse pooling layer generator network pooling functions invertible simpler operation merely increase spatial size representation approach seems perform acceptably use pooling introduced dosovitskiy layer corresponds inverse max pooling operation certain simplifying conditions first stride max pooling operation constrained equal width pooling region second maximum input within pooling region assumed input upper left corner finally non maximal inputs within pooling region assumed zero strong unrealistic assumptions allow max pooling operator inverted inverse pooling operation allocates tensor zeros copies value spatial coordinate input spatial coordinate output integer value deﬁnes size pooling region even though assumptions motivating deﬁnition pooling operator unrealistic subsequent layers able learn compensate unusual output samples generated model whole visually pleasing 
[deep, generative, models, directed, generative, nets, auto-regressive, networks] auto regressive networks directed probabilistic models latent random variables conditional probability distributions models represented neural networks sometimes extremely simple neural networks logistic regression graph structure models complete graph decompose joint probability observed variables using chain rule probability obtain product conditionals form models called fvbns used fully visible bayes networks successfully many forms ﬁrst logistic regression conditional distribution frey neural networks hidden units bengio bengio larochelle murray forms auto regressive networks nade described larochelle murray sec introduce form parameter sharing brings statistical advantage fewer unique parameters computational advantage less computation one instance recurring deep learning motif reuse features chapter deep generative models figure fully visible belief network predicts variable previous ones top bottom directed graphical model fvbn corresponding computational graph case logistic fvbn prediction made linear predictor 
[deep, generative, models, directed, generative, nets, linear, auto-regressive, networks] simplest form auto regressive network hidden units sharing parameters features parametrized linear model linear regression real valued data logistic regression binary data softmax regression discrete data model introduced frey parameters variables model illustrated fig variables continuous linear auto regressive model merely another way formulate multivariate gaussian distribution capturing linear pairwise interactions observed variables linear auto regressive networks essentially generalization linear classiﬁcation methods generative modeling therefore advantages disadvantages linear classiﬁers like linear classiﬁers may trained convex loss functions sometimes admit closed form solutions gaussian case like linear classiﬁers model oﬀer way increasing capacity capacity must raised using techniques like basis expansions input kernel trick chapter deep generative models figure neural auto regressive network predicts variable previous ones parametrized features groups hidden units denoted functions reused predicting subsequent variables 
[deep, generative, models, directed, generative, nets, neural, auto-regressive, networks] neural auto regressive networks bengio bengio left right graphical model logistic auto regressive networks fig employ diﬀerent parametrization conditional distributions within graphical model structure new parametrization powerful sense capacity increased much needed allowing approximation joint distribution new parametrization also improve generalization introducing parameter sharing feature sharing principle common deep learning general models motivated objective avoiding curse dimensionality arising traditional tabular graphical models sharing structure fig tabular discrete probabilistic models conditional distribution represented table probabilities one entry one parameter possible conﬁguration variables involved using neural network instead two advantages obtained parametrization neural network inputs outputs variables discrete take values encoded one hot allows one estimate conditional probability without requiring exponential number parameters examples yet still able capture high order dependencies random variables instead diﬀerent neural network prediction chapter deep generative models connectivity illustrated fig allows one merge left right neural networks one equivalently means hidden layer features computed predicting reused predicting hidden units thus organized groups particularity units group depend input values parameters used compute hidden units jointly optimized improve prediction variables sequence instance reuse principle recurs throughout deep learning scenarios ranging recurrent convolutional network architectures multi task transfer learning represent conditional distribution outputs neural network predict parameters conditional distribution discussed sec although original neural auto regressive networks initially evaluated context purely discrete multivariate data sigmoid output bernoulli variable softmax output multinoulli variable natural extend models continuous variables joint distributions involving discrete continuous variables 
[deep, generative, models, directed, generative, nets, nade] neural autoregressive density estimator nade successful recent form neural auto regressive network larochelle murray connectivity original neural auto regressive network bengio bengio nade introduces additional parameter sharing scheme illustrated fig parameters hidden units diﬀerent groups shared weights input element group hidden unit shared among groups remaining weights zero larochelle murray chose sharing scheme forward propagation nade model loosely resembles computations performed mean ﬁeld inference ﬁll missing inputs rbm mean ﬁeld inference corresponds running recurrent network shared weights ﬁrst step inference nade diﬀerence nade output weights connecting hidden units output parametrized chapter deep generative models figure illustration neural autoregressive density estimator nade hidden units organized groups inputs participate computing predicting nade diﬀerentiated shared indicated ﬁgure use line pattern every instance replicated weight weights going unit group recall vector denoted weights nade architecture extended mimic one time step raiko mentioned previously auto regressive networks may extend process mixture weights coeﬃcient prior probability component per component conditional mean per component conditional variance model called rnade uses parametrization extend nade real uria values mixture density networks parameters distribution conditional variances reduce diﬃculty uria use pseudo gradient replaces gradient mean back propagation phase chapter deep generative models another interesting extension neural auto regressive architectures gets rid need choose arbitrary order observed variables murray larochelle auto regressive networks idea train network able cope order randomly sampling orders providing information hidden units specifying inputs observed right side conditioning bar predicted thus considered missing left side conditioning bar nice allows one use trained auto regressive network perform inference problem predict sample probability distribution subset variables given subset extremely eﬃciently finally since many orders variables possible variables order variables yields diﬀerent form ensemble models many values ensemble ensemble model usually generalizes better assigns higher probability test set individual model deﬁned single ordering paper authors propose deep versions architecture unfortunately immediately makes computation expensive original neural auto regressive neural network ﬁrst layer bengio bengio output layer still computed multiply add operations regular nade number hidden units size groups fig fig whereas bengio bengio however hidden layers computation every previous group layer participates predicting next group layer assuming groups hidden units layer making group layer depend group murray larochelle layer reduces still times worse regular nade 
[deep, generative, models, drawing, samples, autoencoders] chapter saw many kinds autoencoders learn data distribution close connections score matching denoising autoencoders contractive autoencoders connections demonstrate kinds autoencoders learn data distribution way yet seen draw samples models kinds autoencoders variational autoencoder explicitly chapter deep generative models represent probability distribution admit straightforward ancestral sampling kinds autoencoders require mcmc sampling contractive autoencoders designed recover estimate tangent plane data manifold means repeated encoding decoding injected noise induce random walk along surface manifold rifai mesnil manifold diﬀusion technique kind markov chain also general markov chain sample denoising autoencoder 
[deep, generative, models, drawing, samples, autoencoders, markov, chain, associated, denoising, autoen-, coder] discussion left open question noise inject order obtain markov chain would generate distribution estimated autoencoder showed construct markov bengio chain generalized denoising autoencoders generalized denoising autoencoders speciﬁed denoising distribution sampling estimate clean input given corrupted input step markov chain generates estimated distribution consists following sub steps illustrated fig starting previous state inject corruption noise sampling encode decode obtain parameters sample next state bengio showed autoencoder forms consistent estimator corresponding true conditional distribution stationary distribution markov chain forms consistent estimator albeit implicit one data generating distribution 
[deep, generative, models, drawing, samples, autoencoders, clamping, conditional, sampling] similarly boltzmann machines denoising autoencoders generalizations gsns described used sample conditional distri bution simply clamping observed units resampling chapter deep generative models figure step markov chain associated trained denoising autoen coder generates samples probabilistic model implicitly trained denoising log likelihood criterion step consists injecting noise via corruption process state yielding encoding function yielding decoding result function yielding parameters reconstruction distribution given sampling new state reconstruction distribution typical squared reconstruction error case estimates corruption consists adding gaussian noise sampling consists adding gaussian noise second time reconstruction latter noise level correspond mean squared error reconstructions whereas injected noise hyperparameter controls mixing speed well extent estimator smooths empirical distribution vincent example illustrated conditionals stochastic steps deterministic computations although noise also injected inside autoencoder generative stochastic networks bengio chapter deep generative models free units given sampled latent variables example dbms interpreted form denoising autoencoder able sample missing inputs gsns later generalized ideas present dbms perform operation bengio alain identiﬁed missing condition proposition bengio transition operator deﬁned stochastic mapping going one state chain next satisfy property called detailed balance speciﬁes markov chain equilibrium remain equilibrium whether transition operator run forward reverse experiment clamping half pixels right part image running markov chain half shown fig figure illustration clamping right half image running markov chain resampling left half step samples come gsn trained reconstruct mnist digits time step using walkback procedure 
[deep, generative, models, drawing, samples, autoencoders, walk-back, training, procedure] walk back training procedure proposed way bengio accelerate convergence generative training denoising autoencoders instead performing one step encode decode reconstruction procedure consists alternative multiple stochastic encode decode steps generative chapter deep generative models markov chain initialized training example like contrastive divergence algorithm described sec penalizing last probabilistic reconstructions reconstructions along way training steps equivalent sense achieving stationary distribution training one step practically advantage spurious modes farther data removed eﬃciently 
[deep, generative, models, generative, stochastic, networks] generative stochastic networks gsns generalizations bengio denoising autoencoders include latent variables generative markov chain addition visible variables usually denoted gsn parametrized two conditional probability distributions specify one step markov chain tells generate next visible variable given current latent state reconstruction distribution also found denoising autoencoders rbms dbns dbms tells update latent state variable given previous latent state visible variable denoising autoencoders gsns diﬀer classical probabilistic models directed undirected parametrize generative process rather mathematical speciﬁcation joint distribution visible latent variables instead latter deﬁned implicitly stationary exists distribution generative markov chain conditions existence stationary distribution mild conditions required standard mcmc methods see sec conditions necessary guarantee chain mixes violated choices transition distributions example deterministic one could imagine diﬀerent training criteria gsns one proposed evaluated simply reconstruction log probability bengio visible units like denoising autoencoders achieved clamping observed example maximizing probability generating subsequent time steps maximizing log sampled chain given order estimate gradient log respect pieces model bengio use reparametrization trick introduced sec chapter deep generative models walk back training protocol described sec used bengio improve training convergence gsns 
[deep, generative, models, generative, stochastic, networks, discriminant, gsns] original formulation gsns meant unsupervised bengio learning implicitly modeling observed data possible modify framework optimize example zhou troyanskaya generalize gsns way back propagating reconstruction log probability output variables keep ing input variables ﬁxed applied successfully model sequences protein secondary structure introduced one dimensional convolutional structure transition operator markov chain important member step markov chain one generates new sequence layer sequence input computing layer values say one one next time step hence markov chain really output variable associated higher level hidden layers input sequence serves condition chain back propagation allowing learn input sequence condition output distribution implicitly represented markov chain therefore case using gsn context structured outputs simple parametric form instead components statistically dependent given complicated ways zöhrer pernkopf introduced hybrid model combines super vised objective work unsupervised objective original gsn work simply adding diﬀerent weight supervised unsupervised costs reconstruction log probabilities respectively hybrid criterion previously introduced rbms larochelle bengio show improved classiﬁcation performance using scheme 
[deep, generative, models, generation, schemes] methods described far use either mcmc sampling ancestral sampling mixture two generate samples popular approaches generative modeling means approaches chapter deep generative models sohl dickstein developed training scheme diﬀusion inversion learning generative model based non equilibrium thermodynamics approach based idea probability distributions wish sample structure structure gradually destroyed diﬀusion process incrementally changes probability distribution entropy form generative model run process reverse training model gradually restores structure unstructured distribution iteratively applying process brings distribution closer target one gradually approach target distribution approach resembles mcmc methods sense involves many iterations produce sample however model deﬁned probability distribution produced ﬁnal step chain sense approximation induced iterative procedure approach introduced sohl dickstein also close generative interpretation denoising autoencoder sec like denoising autoencoder training objective trains transition operator attempts probabilistically undo eﬀect adding noise trying undo one step diﬀusion process compare walkback training procedure sec denoising autoencoders gsns main diﬀerence instead reconstructing towards observed training point objective function tries reconstruct towards previous point diﬀusion trajectory started easier addresses following dilemma present ordinary reconstruction log likelihood objective denoising autoencoders small levels noise learner sees conﬁgurations near data points large levels noise asked almost impossible job denoising distribution going highly complex multi modal diﬀusion inversion objective learner learn precisely shape density around data points well remove spurious modes could show far data points another approach sample generation approximate bayesian computa tion abc framework approach samples rejected rubin modiﬁed order make moments selected functions samples match desired distribution idea uses moments samples like moment matching diﬀerent moment matching modiﬁes samples rather training model automatically emit samples correct moments bachman precup showed use ideas abc context deep learning using abc shape mcmc trajectories gsns expect many possible approaches generative modeling await discovery chapter deep generative models 
[deep, generative, models, evaluating, generative, models] researchers studying generative models often need compare one generative model another usually order demonstrate newly invented generative model better capturing distribution pre existing models diﬃcult subtle task many cases actually evaluate log probability data model approximation cases important think communicate clearly exactly measured example suppose evaluate stochastic estimate log likelihood model deterministic lower bound log likelihood model model gets higher score model better care determining model better internal representation distribution actually cannot tell unless way determining loose bound model however care well use model practice example perform anomaly detection fair say model preferable based criterion speciﬁc practical task interest based ranking test examples ranking criteria precision recall another subtlety evaluating generative models evaluation metrics often hard research problems diﬃcult establish models compared fairly example suppose use ais estimate log order compute log log new model invented computationally economical implementation ais may fail ﬁnd several modes model distribution underestimate result overestimating log thus diﬃcult tell whether high likelihood estimate due good model bad ais implementation ﬁelds machine learning usually allow variation pre processing data example comparing accuracy object recognition algorithms usually acceptable preprocess input images slightly diﬀerently algorithm based kind input requirements generative modeling diﬀerent changes preprocessing even small subtle ones completely unacceptable change input data changes distribution captured fundamentally alters task example multiplying input artiﬁcially increase likelihood factor issues preprocessing commonly arise benchmarking generative models mnist dataset one popular generative modeling benchmarks mnist consists grayscale images models treat mnist images points chapter deep generative models real vector space others treat binary yet others treat grayscale values probabilities binary samples essential compare real valued models real valued models binary valued models binary valued models otherwise likelihoods measured space binary valued models log likelihood zero real valued models arbitrarily high since measurement density among binary models important compare models using exactly kind binarization example might binarize gray pixel thresholding drawing random sample whose probability given gray pixel intensity use random binarization might binarize whole dataset might draw diﬀerent random example step training draw multiple samples evaluation three schemes yields wildly diﬀerent likelihood numbers comparing diﬀerent models important models use binarization scheme training evaluation fact researchers apply single random binarization step share ﬁle containing results random binarization diﬀerence results based diﬀerent outcomes binarization step able generate realistic samples data distribution one goals generative model practitioners often evaluate generative models visually inspecting samples best case done researchers experimental subjects know source samples denton unfortunately possible poor probabilistic model produce good samples common practice verify model copies training examples illustrated fig idea show generated samples nearest neighbor training set according euclidean distance space test intended detect case model overﬁts training set reproduces training instances even possible simultaneously underﬁt overﬁt yet still produce samples individually look good imagine generative model trained images dogs cats simply learns reproduce training images dogs model clearly overﬁt produces images training set also underﬁt assigns probability training images cats yet human observer would judge individual image dog high quality simple example would easy human observer inspect many samples determine cats absent realistic settings generative model trained data tens thousands modes may ignore small number modes human observer would easily able inspect remember chapter deep generative models enough images detect missing variation since visual quality samples reliable guide often also evaluate log likelihood model assigns test data computationally feasible unfortunately cases likelihood seems measure attribute model really care example real valued models mnist obtain arbitrarily high likelihood assigning arbitrarily low variance background pixels never change models algorithms detect constant features reap unlimited rewards even though useful thing potential achieve cost approaching negative inﬁnity present kind maximum likelihood problem real values especially problematic generative models mnist many output values trivial predict strongly suggests need developing ways evaluating generative models theis review many issues involved evaluating generative models including many ideas described highlight fact many diﬀerent uses generative models choice metric must match intended use model example generative models better assigning high probability realistic points generative models better rarely assigning high probability unrealistic points diﬀerences result whether generative model designed minimize data  model model  data illustrated fig unfortunately even restrict use metric task suited metrics currently use continue serious weaknesses one important research topics generative modeling therefore improve generative models fact designing new techniques measure progress 
[deep, generative, models, conclusion] training generative models hidden units powerful way make models understand world represented given training data learning model model representation model generative model provide answers many inference problems relationships input variables provide many diﬀerent ways representing taking expectations diﬀerent layers hierarchy generative models hold promise provide systems framework many diﬀerent intuitive concepts need understand ability reason concepts face uncertainty hope readers ﬁnd new ways make chapter deep generative models approaches powerful continue journey understanding principles underlie learning intelligence 
