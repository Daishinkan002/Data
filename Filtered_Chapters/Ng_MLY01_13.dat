[machine, learning, strategy] machine learning foundation countless important applications including web search email anti spam speech recognition product recommendations assume team working machine learning application want make rapid progress book help 
[machine, learning, strategy, example, building, cat, picture, startup] say building startup provide endless stream cat pictures cat lovers use neural network build computer vision system detecting cats pictures tragically learning algorithm accuracy yet good enough tremendous pressure improve cat detector team lot ideas get data collect pictures cats collect diverse training set example pictures cats unusual positions cats unusual coloration pictures shot variety camera settings train algorithm longer running gradient descent iterations try bigger neural network layers hidden units parameters page machine learning yearning draft andrew try smaller neural network try adding regularization regularization change neural network architecture activation function number hidden units etc choose well among possible directions build leading cat picture platform lead company success choose poorly might waste months proceed book tell machine learning problems leave clues tell useful try useful try learning read clues save months years development time page machine learning yearning draft andrew 
[use, book, help, team] finishing book deep understanding set technical direction machine learning project teammates might understand recommending particular direction perhaps want team define single number evaluation metric convinced persuade made chapters short print get teammates read pages need know changes prioritization huge effect team productivity helping team changes hope become superhero team page machine learning yearning draft andrew 
[prerequisites, notation] taken machine learning course machine learning mooc coursera experience applying supervised learning able understand text assume familiar ubsupervised learning learning function maps using labeled training examples supervised learning algorithms include linear regression logistic regression neural networks many forms machine learning majority machine learning practical value today comes supervised learning frequently refer neural networks also known deep learning need basic understanding follow text familiar concepts mentioned watch first three weeks videos machine learning course coursera page machine learning yearning draft andrew 
[scale, drives, machine, learning, progress] many ideas deep learning neural networks around decades ideas taking two biggest drivers recent progress data availability people spending time digital devices laptops mobile devices digital activities generate huge amounts data feed learning algorithms computational scale ubwe started years ago able train neural networks big enough take advantage huge datasets detail even accumulate data usually performance older learning algorithms logistic regression plateaus means learning curve flattens algorithm stops improving even give data older algorithms know data train small neutral network supervised learning task might get slightly better performance page machine learning yearning draft andrew small mean neural network small number hidden units layers parameters finally train larger larger neural networks obtain even better performance thus obtain best performance train large neural network green curve huge amount data many details neural network architecture also important much innovation one reliable ways improve algorithm performance today still train bigger network get data diagram shows nns better regime small datasets effect less consistent effect nns well regime huge datasets small data regime depending features hand engineered traditional algorithms may may better example training examples might matter much whether use logistic regression neural network hand engineering features bigger effect choice algorithm million examples would favor neural network page machine learning yearning draft andrew process accomplish surprisingly complex book discuss details length start general strategies useful traditional learning algorithms neural networks build modern strategies building deep learning systems page machine learning yearning draft andrew setting development test sets page machine learning yearning draft andrew 
[development, test, sets] let return earlier cat pictures example run mobile app users uploading pictures many different things app want automatically find cat pictures team gets large training set downloading pictures cats positive examples non cats negative examples different websites split dataset training test sets using data build cat detector works well training test sets deploy classifier mobile app find performance really poor happened figure pictures users uploading different look website images make training set users uploading pictures taken mobile phones tend lower resolution blurrier poorly lit since training test sets made website images algorithm generalize well actual distribution care mobile phone pictures modern era big data common rule machine learning use random split form training test sets practice work bad idea applications training distribution website images page machine learning yearning draft andrew example different distribution ultimately care mobile phone images usually define training set run learning algorithm dev development set use tune parameters select features make decisions regarding learning algorithm sometimes also called hold cross validation set test set use evaluate performance algorithm make decisions regarding learning algorithm parameters use define dev set development set test set team try lot ideas different learning algorithm parameters see works best dev test sets allow team quickly see well algorithm words ubthe purpose dev test sets direct team toward important changes make machine learning system following choose dev test sets reflect data expect get future want well words test set simply available data especially expect future data mobile phone images different nature training set website images yet launched mobile app might users yet thus might able get data accurately reflects well future might still try approximate example ask friends take mobile phone pictures cats send app launched update dev test sets using actual user data really way getting data approximates expect get future perhaps start using website images aware risk leading system generalize well requires judgment decide much invest developing great dev test sets assume training distribution test distribution try pick test page machine learning yearning draft andrew examples reflect ultimately want perform well rather whatever data happen training page machine learning yearning draft andrew 
[dev, test, sets, come, distribution] cat app image data segmented four regions based largest markets china iii india come dev set test set say put india dev set china test set words randomly assign two segments dev set two test set right define dev test sets team focused improving dev set performance thus dev set reflect task want improve well four geographies two second problem different dev test set distributions chance team build something works well dev set find poorly test set seen result much frustration wasted effort avoid letting happen example suppose team develops system works well dev set test set dev test sets come distribution would clear diagnosis went wrong overfit dev set obvious cure get dev set data dev test sets come different distributions options less clear several things could gone wrong overfit dev set test set harder dev set algorithm might well could expected significant improvement possible page machine learning yearning draft andrew test set necessarily harder different dev set works well dev set work well test set case lot work improve dev set performance might wasted effort working machine learning applications hard enough mismatched dev test sets introduces additional uncertainty whether improving dev set distribution also improves test set performance mismatched dev test sets makes harder figure working thus makes harder prioritize work working party benchmark problem creator might specified dev test sets come different distributions luck rather skill greater impact performance benchmarks compared dev test sets come distribution important research problem develop learning algorithms trained one distribution generalize well another goal make progress specific machine learning application rather make research progress recommend trying choose dev test sets drawn distribution make team efficient page machine learning yearning draft andrew 
[large, dev/test, sets, need, be] dev set large enough detect differences algorithms trying example classifier accuracy classifier accuracy dev set examples would able detect difference compared machine learning problems seen example dev set small dev sets sizes examples common examples good chance detecting improvement mature important applications example advertising web search product recommendations also seen teams highly motivated eke even improvement since direct impact company profits case dev set could much larger order detect even smaller improvements size test set large enough give high confidence overall performance system one popular heuristic use data test set works well modest number examples say examples era big data machine learning problems sometimes billion examples fraction data allocated dev test sets shrinking even absolute number examples dev test sets growing need excessively large dev test sets beyond needed evaluate performance algorithms theory one could also test change algorithm makes statistically significant difference dev set practice teams bother unless publishing academic research papers usually find statistical significance tests useful measuring interim progress page machine learning yearning draft andrew 
[establish, single-number, evaluation, metric, team, optimize] classification accuracy example ubsingle number evaluation metric run classifier dev set test set get back single number fraction examples classified correctly according metric classifier obtains accuracy classifier obtains accuracy judge classifier superior contrast precision recall single number evaluation metric gives two numbers assessing classifier multiple number evaluation metrics makes harder compare algorithms suppose algorithms perform follows classifier precision recall neither classifier obviously superior immediately guide toward picking one classifier precision recall score development team try lot ideas algorithm architecture model parameters choice features etc ubsingle number evaluation metric accuracy allows sort models according performance metric quickly decide working best really care precision recall recommend using one standard ways combine single number example one could take average precision recall end single number alternatively compute precision cat classifier fraction images dev test set labeled cats really cats recall percentage cat images dev test set correctly labeled cat often tradeoff high precision high recall page machine learning yearning draft andrew score modified way computing average works better simply taking mean classifier precision recall score single number evaluation metric speeds ability make decision selecting among large number classifiers gives clear preference ranking among therefore clear direction progress final example suppose separately tracking accuracy cat classifier four key markets china iii india gives four metrics taking average weighted average four numbers end single number metric taking average weighted average one common ways combine multiple metrics one want learn score see harmonic mean precision recall calculated precision recall page machine learning yearning draft andrew 
[optimizing, satisficing, metrics] another way combine multiple evaluation metrics suppose care accuracy running time learning algorithm need choose three classifiers classifier accuracy running time seems unnatural derive single metric putting accuracy running time single formula accuracy runningtime instead first define acceptable running time lets say anything runs acceptable maximize accuracy subject classifier meeting running time criteria running time satisficing metric classifier good enough metric sense take accuracy optimizing metric trading different criteria binary file size model important mobile apps since users want download large apps running time accuracy might consider setting criteria satisficing metrics simply require meet certain value define final one optimizing metric example set threshold acceptable binary file size running time try optimize accuracy given constraints final example suppose building hardware device uses microphone listen user saying particular wakeword causes system wake examples include amazon echo listening alexa apple siri listening hey siri android listening okay google baidu apps listening hello baidu care false positive rate frequency system wakes even one said wakeword well false negative rate often fails wake someone says wakeword one reasonable goal performance system page machine learning yearning draft andrew minimize false negative rate optimizing metric subject one false positive every hours operation satisficing metric team aligned evaluation metric optimize able make faster progress page machine learning yearning draft andrew 
[dev, set, metric, speeds, iterations] difficult know advance approach work best new problem even experienced machine learning researchers usually try many dozens ideas discover something satisfactory building machine learning system often start ubidea build system implement idea ubcode carry ubexperiment tells well idea worked usually first ideas work based learnings back generate ideas keep iterating iterative process faster round loop faster make progress dev test sets metric important time try idea measuring idea performance dev set lets quickly decide heading right direction contrast suppose specific dev set metric time team develops new cat classifier incorporate app play app hours get sense whether new classifier improvement would incredibly slow also team improves classifier accuracy might able detect improvement playing app yet lot progress system made gradually accumulating dozens improvements dev set metric allows quickly detect ideas successfully giving small large improvements therefore lets quickly decide ideas keep refining ones discard page machine learning yearning draft andrew 
[change, dev/test, sets, metrics] starting new project try quickly choose dev test sets since gives team well defined target aim typically ask teams come initial dev test set initial metric less one week rarely longer better come something imperfect get going quickly rather overthink one week timeline apply mature applications example anti spam mature deep learning application seen teams working already mature systems spend months acquire even better dev test sets later realize initial dev test set metric missed mark means change quickly example dev set metric ranks classifier classifier team thinks classifier actually superior product might sign need change dev test sets evaluation metric three main possible causes dev set metric incorrectly rating classifier higher actual distribution need well different dev test sets suppose initial dev test set mainly pictures adult cats ship cat app find users uploading lot kitten images expected dev test set distribution representative actual distribution need well case update dev test sets representative page machine learning yearning draft andrew overfit dev set process repeatedly evaluating ideas dev set causes algorithm gradually overfit dev set done developing evaluate system test set find dev set performance much better test set performance sign overfit dev set case get fresh dev set need track team progress also evaluate system regularly say per week per month test set use test set make decisions regarding algorithm including whether roll back previous week system start overfit test set longer count give completely unbiased estimate system performance would need publishing research papers perhaps using metric make important business decisions metric measuring something project needs optimize suppose cat application metric classification accuracy metric currently ranks classifier superior classifier suppose try algorithms find classifier allowing occasional pornographic images slip even though classifier accurate bad impression left occasional pornographic image means performance unacceptable metric failing identify fact algorithm fact better algorithm product longer trust metric pick best algorithm time change evaluation metrics example change metric heavily penalize letting pornographic images would strongly recommend picking new metric using new metric explicitly define new goal team rather proceeding long without trusted metric reverting manually choosing among classifiers quite common change dev test sets evaluation metrics project initial dev test set metric helps iterate quickly ever find dev test sets metric longer pointing team right direction big deal change make sure team knows new direction page machine learning yearning draft andrew 
[takeaways, setting, development, test, sets] choose dev test sets distribution reflects data expect get future want well may training data distribution choose dev test sets distribution possible choose single number evaluation metric team optimize multiple goals care consider combining single formula averaging multiple error metrics defining satisficing optimizing metrics machine learning highly iterative process may try many dozens ideas finding one satisfied dev test sets single number evaluation metric helps quickly evaluate algorithms therefore iterate faster starting brand new application try establish dev test sets metric quickly say less week might okay take longer mature applications old heuristic train test split apply problems lot data dev test sets much less data dev set large enough detect meaningful changes accuracy algorithm necessarily much larger test set big enough give confident estimate final performance system dev set metric longer pointing team right direction quickly change overfit dev set get dev set data actual distribution care different dev test set distribution get new dev test set data iii metric longer measuring important change metric page machine learning yearning draft andrew basic error analysis page machine learning yearning draft andrew 
[build, first, system, quickly, iterate] want build new email anti spam system team several ideas collect huge training set spam email example set honeypot deliberately send fake email addresses known spammers automatically harvest spam messages send addresses develop features understanding text content email develop features understanding email envelope header features show set internet servers message went even though worked extensively anti spam would still hard time picking one directions even harder expert application area start trying design build perfect system instead build train basic system quickly perhaps days even basic system far best system build valuable examine basic system functions quickly find clues show promising directions invest time next chapters show read clues advice meant readers wanting build applications rather whose goal publish academic papers later return topic research page machine learning yearning draft andrew 
[error, analysis, look, dev, set, examples, evaluate, ideas] play cat app notice several examples mistakes dogs cats dogs look like cats team member proposes incorporating party software make system better dog images changes take month team member enthusiastic ask ahead investing month task recommend first estimate much actually improve system accuracy rationally decide worth month development time better using time tasks detail gather sample dev set examples system misclassified examples system made error look examples manually count fraction dog images process looking misclassified examples called uberror analysis example find misclassified images dogs matter much improve algorithm performance dog images get rid errors words ceiling meaning maximum possible amount much proposed project could help thus overall system currently accurate error improvement likely result best accuracy error less error original error page machine learning yearning draft andrew contrast find mistakes dogs confident proposed project big impact could boost accuracy relative reduction error simple counting procedure error analysis gives quick way estimate possible value incorporating party software dog images provides quantitative basis decide whether make investment error analysis often help figure promising different directions seen many engineers reluctant carry error analysis often feels exciting jump implement idea rather question idea worth time investment common mistake might result team spending month realize afterward resulted little benefit manually examining examples take long even take one minute per image done two hours two hours could save month wasted effort error analysis refers process examining dev set examples algorithm misclassified understand underlying causes errors help prioritize projects example inspire new directions discuss next next chapters also present best practices carrying error analyses page machine learning yearning draft andrew 
[evaluating, multiple, ideas, parallel, error, analysis] team several ideas improving cat detector fix problem algorithm recognizing dogs cats fix problem algorithm recognizing great cats lions panthers etc house cats pets improve system performance blurry images efficiently evaluate ideas parallel usually create spreadsheet fill looking misclassified dev set images also jot comments might help remember specific examples illustrate process let look spreadsheet might produce small dev set four examples image dog great cat blurry comments unusual pitbull color lion picture taken zoo rainy day panther behind tree total image great cat blurry columns checked furthermore possible one example associated multiple categories percentages bottom may add although may first formulate categories dog great cat blurry categorize examples hand practice start looking examples probably inspired propose new error categories example say dozen images realize lot mistakes occur instagram filtered pictures back add new instagram column spreadsheet manually looking examples algorithm misclassified asking whether human could labeled page machine learning yearning draft andrew picture correctly often inspire come new categories errors solutions helpful error categories ones idea improving example instagram category helpful add idea undo instagram filters recover original image restrict error categories know improve goal process build intuition promising areas focus error analysis iterative process worry start categories mind looking couple images might come ideas error categories manually categorizing images might think new categories examine images light new categories suppose finish carrying error analysis misclassified dev set examples get following image dog great cat blurry comments usual pitbull color lion picture taken zoo rainy day panther behind tree total know working project address dog mistakes eliminate errors working great cat blurry image errors could help eliminate errors therefore might pick one two latter categories focus team enough people pursue multiple directions parallel also ask engineers work great cats others work blurry images error analysis produce rigid mathematical formula tells highest priority task also take account much progress expect make different categories amount work needed tackle one page machine learning yearning draft andrew 
[cleaning, mislabeled, dev, test, set, examples] error analysis might notice examples dev set mislabeled say mislabeled mean pictures already mislabeled human labeler even algorithm encountered class label example incorrect value example perhaps pictures cats mislabeled containing cat vice versa suspect fraction mislabeled images significant add category keep track fraction examples mislabeled image dog great cat blurry mislabeled comments labeler missed cat background drawing cat real cat total correct labels dev set remember goal dev set help quickly evaluate algorithms tell algorithm better fraction dev set mislabeled impedes ability make judgments worth spending time fix mislabeled dev set labels example suppose classifier performance overall accuracy dev set overall error errors due mislabeled examples dev set errors errors due causes dev set errors inaccuracy due mislabeling might significant enough relative errors could improving harm manually fixing mislabeled images dev set crucial might fine knowing whether system overall error suppose keep improving cat classifier reach following performance page machine learning yearning draft andrew overall accuracy dev set overall error errors due mislabeled examples dev set errors errors due causes dev set errors errors due mislabeled dev set images adding significant error estimates accuracy worthwhile improve quality labels dev set tackling mislabeled examples help figure classifier error closer significant relative difference uncommon start tolerating mislabeled dev test set examples later change mind system improves fraction mislabeled examples grows relative total set errors last chapter explained improve error categories dog great cat blurry algorithmic improvements learned chapter work mislabeled category well improving data labels whatever process apply fixing dev set labels remember apply test set labels dev test sets continue drawn distribution fixing dev test sets together would prevent problem discussed chapter team optimizes dev set performance realize later judged different criterion based different test set decide improve label quality consider double checking labels examples system misclassified well labels examples correctly classified possible original label learning algorithm wrong example fix labels examples system misclassified might introduce bias evaluation dev set examples classifier accuracy easier examine examples misclassified examine examples classified correctly easier practice check misclassified examples bias creep dev sets bias acceptable interested developing product application would problem plan use result academic research paper need completely unbiased measure test set accuracy page machine learning yearning draft andrew 
[large, dev, set, split, two, subsets, one, look] suppose large dev set examples error rate thus algorithm misclassifying dev images takes long time manually examine images might decide use error analysis case would explicitly split dev set two subsets one look one rapidly overfit portion manually looking use portion manually looking tune parameters let ubs continue example algorithm misclassifying dev set examples suppose want manually examine errors error analysis errors randomly select dev set place call ubeyeball dev set remind looking eyes project speech recognition would listening audio clips perhaps would call set ear dev set instead eyeball dev set therefore examples would expect algorithm misclassify second subset dev set called ubblackbox dev set remaining examples use blackbox dev set evaluate classifiers automatically measuring error rates also use select among algorithms tune hyperparameters however avoid looking eyes use term blackbox use subset data obtain blackbox evaluations classifiers page machine learning yearning draft andrew explicitly separate dev set eyeball blackbox dev sets since gain intuition examples eyeball dev set start overfit eyeball dev set faster see performance eyeball dev set improving much rapidly performance blackbox dev set overfit eyeball dev set case might need discard find new eyeball dev set moving examples blackbox dev set eyeball dev set acquiring new labeled data explicitly splitting dev set eyeball blackbox dev sets allows tell manual error analysis process causing overfit eyeball portion data page machine learning yearning draft andrew 
[big, eyeball, blackbox, dev, sets, be] eyeball dev set large enough give sense algorithm major error categories working task humans well recognizing cats images rough guidelines eyeball dev set classifier makes mistakes would considered small errors hard accurately estimate impact different error categories little data cannot afford put eyeball dev set ubs better nothing help project prioritization classifier makes mistakes eyeball dev examples would start get rough sense major error sources mistakes would get good sense major error sources mistakes would get good sense major sources errors seen people manually analyze even errors sometimes many harm long enough data say classifier error rate make sure misclassified examples eyeball dev set eyeball dev set would examples since lower classifier error rate larger eyeball dev set needs order get large enough set errors analyze working task even humans cannot well exercise examining eyeball dev set helpful harder figure algorithm classify example correctly case might omit eyeball dev set discuss guidelines problems later chapter page machine learning yearning draft andrew blackbox dev set previously said dev sets around examples common refine statement blackbox dev set examples often give enough data tune hyperparameters select among models though little harm even data blackbox dev set would small still useful small dev set might enough data split eyeball blackbox dev sets large enough serve purposes instead entire dev set might used eyeball dev set would manually examine dev set data eyeball blackbox dev sets consider eyeball dev set important assuming working problem humans solve well examining examples helps gain insight eyeball dev set perform error analyses model selection hyperparameter tuning set downside eyeball dev set risk overfitting dev set greater plentiful access data size eyeball dev set would determined mainly many examples time manually analyze example rarely seen anyone manually analyze errors page machine learning yearning draft andrew 
[takeaways, basic, error, analysis] start new project especially area expert hard correctly guess promising directions start trying design build perfect system instead build train basic system quickly possible perhaps days use error analysis help identify promising directions iteratively improve algorithm carry error analysis manually examining dev set examples algorithm misclassifies counting major categories errors use information prioritize types errors work fixing consider splitting dev set eyeball dev set manually examine blackbox dev set manually examine performance eyeball dev set much better blackbox dev set overfit eyeball dev set consider acquiring data eyeball dev set big enough algorithm misclassifies enough examples analyze blackbox dev set examples sufficient many applications dev set big enough split way use entire dev set eyeball dev set manual error analysis model selection hyperparameter tuning 
[bias, variance, two, big, sources, error] suppose training dev test sets come distribution always try get training data since improve performance right even though data hurt unfortunately always help much might hope could waste time work getting data decide add data bother two major sources error machine learning bias variance understanding help decide whether adding data well tactics improve performance good use time suppose hope build cat recognizer error right training set error rate dev set error rate case adding training data probably help much focus changes indeed adding examples training set makes harder algorithm well training set explain later chapter error rate training set accuracy target error accuracy first problem solve improve algorithm ubs performance training set dev test set performance usually worse training set performance getting accuracy examples algorithm seen way getting accuracy examples algorithm even seen suppose algorithm error accuracy dev set break error two components first algorithm error rate training set example think informally algorithm ubbias second much worse algorithm dev test set training set example worse dev set training set think informally algorithm ubvariance field statistics formal definitions bias variance worry roughly bias error rate algorithm training set large training set variance much worse test set compared training set page machine learning yearning draft andrew changes learning algorithm address first component error ubbias improve performance training set changes address second component ubvariance help generalize better training set dev test sets select promising changes incredibly useful understand two components error pressing address developing good intuition bias variance help choose effective changes algorithm setting error metric mean squared error write formulas specifying two quantities prove total error bias variance purposes deciding make progress problem informal definition bias variance given suffice also methods simultaneously reduce bias variance making major changes system architecture tend harder identify implement page machine learning yearning draft andrew 
[examples, bias, variance] consider cat classification task ideal classifier human might achieve nearly perfect performance task suppose algorithm performs follows training error dev error problem applying definitions previous chapter estimate bias variance thus ubhigh variance classifier low training error failing generalize dev set also called uboverfitting consider training error dev error estimate bias variance classifier fitting training set poorly error error dev set barely higher training error classifier therefore ubhigh bias low variance say algorithm underfitting consider training error dev error estimate bias variance classifier ubhigh bias high variance poorly training set therefore high bias performance dev set even worse also high variance overfitting underfitting terminology hard apply since classifier simultaneously overfitting underfitting page machine learning yearning draft andrew finally consider training error dev error classifier well low bias low variance congratulations achieving great performance page machine learning yearning draft andrew 
[comparing, optimal, error, rate] cat recognition example ideal error rate one achievable optimal classifier nearly human looking picture would able recognize contains cat almost time thus hope machine would well problems harder example suppose building speech recognition system find audio clips much background noise unintelligible even human cannot recognize said case even optimal speech recognition system might error around suppose speech recognition problem algorithm achieves training error dev error training set performance already close optimal error rate thus much room improvement terms bias terms training set performance however algorithm generalizing well dev set thus ample room improvement errors due variance example similar third example previous chapter also training error dev error optimal error rate training error leaves much room improvement suggests bias reducing changes might fruitful optimal error rate training set performance tells little room improvement classifier bias problems optimal error rate far zero ubs detailed breakdown algorithm ubs error continuing speech recognition example total dev set error broken follows similar analysis applied test set error optimal error rate unavoidable bias suppose decide even best possible speech system world would still suffer error think unavoidable part learning algorithm ubs bias page machine learning yearning draft andrew avoidable bias calculated difference training error optimal error rate variance difference dev error training error relate earlier definitions bias avoidable bias related follows bias optimal error rate unavoidable bias avoidable bias avoidable bias reflects much worse algorithm performs training set optimal classifier concept variance remains theory always reduce variance nearly zero training massive training set thus variance avoidable sufficiently large dataset thing unavoidable variance consider one example optimal error rate training error dev error whereas previous chapter called high bias classifier would say error avoidable bias error variance thus algorithm already well little room improvement worse optimal error rate see examples knowing optimal error rate helpful guiding next steps statistics optimal error rate also called ubbayes error rate bayes rate know optimal error rate tasks humans reasonably good recognizing pictures transcribing audio clips ask human provide labels measure accuracy human labels relative training set would give estimate optimal error rate working problem even number negative better training set optimal error rate means overfitting training set algorithm memorized training set focus variance reduction methods rather bias reduction methods definitions chosen convey insight improve learning algorithm definitions different statisticians define bias variance technically define bias called error attribute bias avoidable bias error attribute learning algorithm bias optimal error rate page machine learning yearning draft andrew humans hard time solving predicting movie recommend show user hard estimate optimal error rate section comparing human level performance chapters discuss detail process comparing learning algorithm performance human level performance last chapters learned estimate avoidable unavoidable bias variance looking training dev set error rates next chapter discuss use insights analysis prioritize techniques reduce bias techniques reduce variance different techniques apply depending whether project current problem high avoidable bias high variance read page machine learning yearning draft andrew 
[addressing, bias, variance] simplest formula addressing bias variance issues high avoidable bias increase size model example increase size neural network adding layers neurons high variance add data training set able increase neural network size increase training data without limit possible well many learning problems practice increasing size model eventually cause run computational problems training large models slow might also exhaust ability acquire training data even internet finite number cat pictures different model architectures example different neural network architectures different amounts bias variance problem lot recent deep learning research developed many innovative model architectures using neural networks academic literature great source inspiration also many great open source implementations github results trying new architectures less predictable simple formula increasing model size adding data increasing model size generally reduces bias might also increase variance risk overfitting however overfitting problem usually arises using regularization include well designed regularization method usually safely increase size model without increasing overfitting suppose applying deep learning regularization dropout regularization parameter performs best dev set increase model size usually performance stay improve unlikely worsen significantly reason avoid using bigger model increased computational cost page machine learning yearning draft andrew 
[bias, vs, variance, tradeoff] might heard bias variance tradeoff changes could make learning algorithms reduce bias errors cost increasing variance vice versa creates trade bias variance example increasing size model adding neurons layers neural network adding input features generally reduces bias could increase variance alternatively adding regularization generally increases bias reduces variance modern era often access plentiful data use large neural networks deep learning therefore less tradeoff options reducing bias without hurting variance vice versa example usually increase neural network size tune regularization method reduce bias without noticeably increasing variance adding training data also usually reduce variance without affecting bias select model architecture well suited task might also reduce bias variance simultaneously selecting architecture difficult next chapters discuss additional specific techniques addressing bias variance page machine learning yearning draft andrew 
[techniques, reducing, avoidable, bias] learning algorithm suffers high avoidable bias might try following techniques increase model size number neurons layers technique reduces bias since allow fit training set better find increases variance use regularization usually eliminate increase variance modify input features based insights error analysis say error analysis inspires create additional features help algorithm eliminate particular category errors discuss next chapter new features could help bias variance theory adding features could increase variance find case use regularization usually eliminate increase variance reduce eliminate regularization regularization regularization dropout reduce avoidable bias increase variance modify model architecture neural network architecture suitable problem technique affect bias variance one method helpful add training data technique helps variance problems usually significant effect bias page machine learning yearning draft andrew 
[error, analysis, training, set] algorithm must perform well training set expect perform well dev test sets addition techniques described earlier address high bias sometimes also carry error analysis training data following protocol similar error analysis eyeball dev set useful algorithm high bias fitting training set well example suppose building speech recognition system app collected training set audio clips volunteers system well training set might consider listening set examples algorithm poorly understand major categories training set errors similar dev set error analysis count errors different categories audio clip loud background noise user spoke quickly far microphone comments car noise restaurant noise user shouting across living room coffeeshop total example might realize algorithm particularly hard time training examples lot background noise thus might focus techniques allow better fit training examples background noise might also double check whether possible person transcribe audio clips given input audio learning algorithm much background noise simply impossible anyone make said might unreasonable expect algorithm correctly recognize utterances discuss benefits comparing algorithm human level performance later section page machine learning yearning draft andrew 
[techniques, reducing, variance] learning algorithm suffers high variance might try following techniques add training data simplest reliable way address variance long access significantly data enough computational power process data add regularization regularization regularization dropout technique reduces variance increases bias add early stopping stop gradient descent early based dev set error technique reduces variance increases bias early stopping behaves lot like regularization methods authors call regularization technique feature selection decrease number type input features technique might help variance problems might also increase bias reducing number features slightly say going features unlikely huge effect bias reducing significantly say going features reduction likely significant effect long excluding many useful features modern deep learning data plentiful shift away feature selection likely give features algorithm let algorithm sort ones use based data training set small feature selection useful decrease model size number neurons layers use caution technique could decrease variance possibly increasing bias however recommend technique addressing variance adding regularization usually gives better classification performance advantage reducing model size reducing computational cost thus speeding quickly train models speeding model training useful means consider decreasing model size goal reduce variance concerned computational cost consider adding regularization instead two additional tactics repeated previous chapter addressing bias modify input features based insights error analysis say error analysis inspires create additional features help algorithm eliminate particular category errors new features could help bias variance page machine learning yearning draft andrew theory adding features could increase variance find case use regularization usually eliminate increase variance modify model architecture neural network architecture suitable problem technique affect bias variance page machine learning yearning draft andrew learning curves page machine learning yearning draft andrew 
[diagnosing, bias, variance, learning, curves] seen ways estimate much error attributed avoidable bias variance estimating optimal error rate computing algorithm training set dev set errors let discuss technique even informative plotting learning curve learning curve plots dev set error number training examples plot would run algorithm using different training set sizes example examples might train separate copies algorithm examples could plot dev set error varies training set size example training set size increases dev set error decrease often desired error rate hope learning algorithm eventually achieve example hope human level performance human error rate could desired error rate learning algorithm serves product delivering cat pictures might intuition level performance needed give users great experience page machine learning yearning draft andrew worked important application long time might intuition much progress reasonably make next quarter year add desired level performance learning curve visually extrapolate red dev error curve guess much closer could get desired level performance adding data example looks plausible doubling training set size might allow reach desired performance dev error curve plateaued flattened immediately tell adding data get goal looking learning curve might therefore help avoid spending months collecting twice much training data realize help page machine learning yearning draft andrew one downside process look dev error curve hard extrapolate predict exactly red curve data one additional plot help estimate impact adding data training error page machine learning yearning draft andrew 
[plotting, training, error] dev set test set error decrease training set size grows training set error usually increases training set size grows let illustrate effect example suppose training set examples one cat image one non cat image easy learning algorithms memorize examples training set get training set error even either training examples mislabeled still easy algorithm memorize labels suppose training set examples perhaps even examples mislabeled ambiguous images blurry even humans cannot tell cat perhaps learning algorithm still memorize training set harder obtain accuracy increasing training set examples find training set accuracy drop slightly finally suppose training set examples case becomes even harder algorithm perfectly fit examples especially ambiguous mislabeled thus learning algorithm even worse training set let add plot training error earlier figures see blue training error curve increases size training set furthermore algorithm usually better training set dev set thus red dev error curve usually lies strictly blue training error curve let discuss next interpret plots page machine learning yearning draft andrew 
[interpreting, learning, curves, high, bias] suppose dev error curve looks like 
[interpreting, learning, curves, cases] consider learning curve plot indicate high bias high variance blue training error curve relatively low red dev error curve much higher blue training error thus bias small variance large adding training data probably help close gap dev error training error consider time training error large much higher desired level performance dev error also much larger training error thus significant bias significant variance find way reduce bias variance algorithm page machine learning yearning draft andrew 
[plotting, learning, curves] suppose small training set examples train algorithm using randomly chosen subset examples examples increasing number examples intervals ten use data points plot learning curve might find curve looks slightly noisy meaning values higher lower expected smaller training set sizes training randomly chosen examples might unlucky particularly bad training set one many ambiguous mislabeled examples might get lucky get particularly good training set small training set means dev training errors may randomly fluctuate machine learning application heavily skewed toward one class cat classification task fraction negative examples much larger positive examples huge number classes recognizing different animal species chance selecting especially unrepresentative bad training set also larger example examples negative examples positive examples chance training set examples contains negative examples thus making difficult algorithm learn something meaningful noise training curve makes hard see true trends two solutions instead training one model examples instead select several say different randomly chosen training sets examples sampling replacement original set train different model compute training dev set error resulting models compute plot average training error average dev set error training set skewed towards one class many classes choose balanced subset instead training examples random set example make sure examples positive examples sampling replacement means would randomly pick different examples form first training set form second training set would pick examples without taking account chosen first training set thus possible one specific example appear first second training sets contrast sampling without replacement second training set would chosen examples chosen first time around practice sampling without replacement make huge difference former common practice page machine learning yearning draft andrew negative generally make sure fraction examples class close possible overall fraction original training set would bother either techniques unless already tried plotting learning curves concluded curves noisy see underlying trends training set large say examples class distribution skewed probably need techniques finally plotting learning curve may computationally expensive example might train ten models way examples training models small datasets much faster training models large datasets thus instead evenly spacing training set sizes linear scale might train models examples still give clear sense trends learning curves course technique relevant computational cost training additional models significant 
[compare, human-level, performance] many machine learning systems aim automate things humans well examples include image recognition speech recognition email spam classification learning algorithms also improved much surpassing human level performance tasks several reasons building system easier trying task people well ease obtaining data human labelers example since people recognize cat images well straightforward people provide high accuracy labels learning algorithm error analysis draw human intuition suppose speech recognition algorithm worse human level recognition say incorrectly transcribes audio clip recipe calls pear apples mistaking pair pear draw human intuition try understand information person uses get correct transcription use knowledge modify learning algorithm use human level performance estimate optimal error rate also set desired error rate suppose algorithm achieves error task person achieves error know optimal error rate lower avoidable bias least thus try bias reducing techniques even though item might sound important find reasonable achievable target error rate helps accelerate team progress knowing algorithm high avoidable bias incredibly valuable opens menu options try tasks even humans good example picking book recommend picking show user website predicting stock market computers already surpass performance people tasks applications run following problems harder obtain labels example hard human labelers annotate database users optimal book recommendation operate website app sells books obtain data showing books users seeing buy operate site need find creative ways get data page machine learning yearning draft andrew human intuition harder count example pretty much one predict stock market stock prediction algorithm better random guessing hard figure improve hard know optimal error rate reasonable desired error rate ubsuppose already book recommendation system quite well know much improve without human baseline page machine learning yearning draft andrew 
[define, human-level, performance] suppose working medical imaging application automatically makes diagnoses ray images typical person previous medical background besides basic training achieves error task junior doctor achieves error experienced doctor achieves error small team doctors discuss debate image achieves error one error rates defines human level performance case would use human level performance proxy optimal error rate also set desired performance level three reasons previous chapter comparing human level performance apply ease obtaining labeled data human labelers get team doctors provide labels error rate error analysis draw human intuition ubby discussing images team doctors draw intuitions use human level performance estimate optimal error rate also set achievable desired error rate reasonable use error estimate optimal error rate optimal error rate could even lower cannot higher since possible team doctors achieve error contrast reasonable use estimate optimal error rate since know estimates necessarily high comes obtaining labeled data might want discuss every image entire team doctors since time expensive perhaps single junior doctor label vast majority cases bring harder cases experienced doctors team doctors system currently error matter much whether use junior doctor error experienced doctor error label data provide intuitions system already error defining human level reference gives better tools keep improving system page machine learning yearning draft andrew 
[surpassing, human-level, performance] working speech recognition dataset audio clips suppose dataset many noisy audio clips even humans error suppose system already achieves error use three techniques described chapter continue making rapid progress identify subset data humans significantly surpass system still use techniques drive rapid progress example suppose system much better people recognizing speech noisy audio humans still better transcribing rapidly spoken speech subset data rapidly spoken speech still obtain transcripts humans higher quality algorithm output draw human intuition understand correctly heard rapidly spoken utterance system use human level performance rapidly spoken speech desired performance target generally long dev set examples humans right algorithm wrong many techniques described earlier apply true even averaged entire dev test set performance already surpassing human level performance many important machine learning applications machines surpass human level performance example machines better predicting movie ratings long takes delivery car drive somewhere whether approve loan applications subset techniques apply humans hard time identifying examples algorithm clearly getting wrong consequently progress usually slower problems machines already surpass human level performance progress faster machines still trying catch humans page machine learning yearning draft andrew 
[train, test, different, distributions] users cat pictures app uploaded images manually labeled containing cats also larger set images downloaded internet define train dev test sets since user images closely reflect actual probability distribution data want well might use dev test sets training data hungry deep learning algorithm might give additional internet images training thus training dev test sets come different probability distributions affect work instead partitioning data train dev test sets could take images randomly shuffle train dev test sets case data comes distribution recommend method dev test data would come internet images reflect actual distribution want well remember recommendation choosing dev test sets choose dev test sets reflect data expect get future want well academic literature machine learning assumes training set dev set test set come distribution early days machine learning data scarce usually one dataset drawn probability distribution would randomly split data train dev test sets assumption data coming source usually satisfied academic research training testing different distributions examples include domain adaptation transfer learning multitask learning still huge gap theory practice train dataset test different type data luck could huge effect well algorithm performs luck includes researcher hand designed features particular task well factors understand yet makes academic study training testing different distributions difficult carry systematic way page machine learning yearning draft andrew era big data access huge training sets cat internet images even training set comes different distribution dev test set still want use learning since provide lot information cat detector example instead putting user uploaded images dev test sets might instead put dev test sets put remaining user uploaded examples training set way training set examples contains data comes dev test distribution along internet images discuss later chapter method helpful let consider second example suppose building speech recognition system transcribe street addresses voice controlled mobile map navigation app examples users speaking street addresses also examples audio clips users speaking topics might take examples street addresses dev test sets use remaining plus additional examples training continue assume dev data test data come distribution important understand different training dev test distributions offer special challenges page machine learning yearning draft andrew 
[decide, whether, use, data] suppose cat detector training set includes user uploaded images data comes distribution separate dev test set represents distribution care well also additional images downloaded internet provide images learning algorithm training set discard internet images fear biasing learning algorithm using earlier generations learning algorithms hand designed computer vision features followed simple linear classifier real risk merging types data would cause perform worse thus engineers warn including internet images modern era powerful flexible learning algorithms large neural networks risk greatly diminished afford build neural network large enough number hidden units layers safely add images training set adding images likely increase performance observation relies fact mapping works well types data words exists system inputs either internet image mobile app image reliably predicts label even without knowing source image adding additional images following effects gives neural network examples cats look like helpful since internet images user uploaded mobile app images share similarities neural network apply knowledge acquired internet images mobile app images forces neural network expend capacity learn properties specific internet images higher resolution different distributions images framed etc properties differ greatly mobile app images use representational capacity neural network thus less capacity recognizing data drawn distribution mobile app images really care theoretically could hurt algorithms performance page machine learning yearning draft andrew describe second effect different terms turn fictional character sherlock holmes says brain like attic finite amount space says every addition knowledge forget something knew highest importance therefore useless facts elbowing useful ones fortunately computational capacity needed build big enough neural network big enough attic serious concern enough capacity learn internet mobile app images without two types data competing capacity algorithm brain big enough worry running attic space big enough neural network another highly flexible learning algorithm pay attention training data matching dev test set distribution think data benefit leave data computational reasons example suppose dev test sets contain mainly casual pictures people places landmarks animals suppose also large collection scanned historical documents documents contain anything resembling cat also look completely unlike dev test distribution point including data negative examples benefit first effect negligible almost nothing neural network learn data apply dev test set distribution including would waste computation resources representation capacity neural network study scarlet ubby arthur conan doyle page machine learning yearning draft andrew 
[decide, whether, include, inconsistent, data] suppose want learn predict housing prices new york city given size house input feature want predict price target label housing prices new york city high suppose second dataset housing prices detroit michigan housing prices much lower include data training set given size price house different depending whether new york city detroit care predicting new york city housing prices putting two datasets together hurt performance case would better leave inconsistent detroit data new york city detroit example different mobile app internet cat images example cat image example different given input picture one reliably predict label indicating whether cat even without knowing image internet image mobile app image function reliably maps input target output even without knowing origin thus task recognition internet images consistent task recognition mobile app images means little downside computational cost including data possible significant upside contrast new york city detroit michigan data consistent given size house price different depending house 
[weighting, data] suppose images internet images mobile app users ratio size datasets theory long build huge neural network train long enough images harm trying make algorithm well internet images mobile images practice many internet images mobile app images might mean need spend much computational resources model compared trained images huge computational resources could give internet images much lower weight compromise example suppose optimization objective squared error good choice classification task simplify explanation thus learning algorithm tries optimize first sum mobile images second sum internet images instead optimize additional parameter ub set ub algorithm would give equal weight mobile images internet images also set parameter ub values perhaps tuning dev set weighting additional internet images less build massive neural network make sure algorithm well types tasks type weighting needed suspect additional data internet images different distribution dev test set additional data much larger data came distribution dev test set mobile images page machine learning yearning draft andrew 
[generalizing, training, set, dev, set] suppose applying setting training dev test distributions different say training set contains internet images mobile images dev test sets contain mobile images however algorithm working well much higher dev test set error would like possibilities might wrong well training set problem high avoidable bias training set distribution well training set generalize well previously unseen data drawn distribution training set high variance generalizes well new data drawn distribution training set data drawn dev test set distribution call problem ubdata mismatch since training set data poor match dev test set data example suppose humans achieve near perfect performance cat recognition task algorithm achieves error training set error data drawn distribution training set algorithm seen error dev set case clearly data mismatch problem address might try make training data similar dev test data discuss techniques later order diagnose extent algorithm suffers problems useful another dataset specifically rather giving algorithm available training data split two subsets actual training set algorithm train separate set call training dev set train four subsets data page machine learning yearning draft andrew training set data algorithm learn internet images mobile images drawn distribution really care dev test set distribution training dev set data drawn distribution training set internet images mobile images usually smaller training set needs large enough evaluate track progress learning algorithm dev set drawn distribution test set reflects distribution data ultimately care well mobile images test set drawn distribution dev set mobile images armed four separate datasets evaluate training error evaluating training set algorithm ability generalize new data drawn training set distribution evaluating training dev set algorithm performance task care evaluating dev test sets guidelines chapters picking size dev set also apply training dev set 
[identifying, bias, variance, data, mismatch, errors] suppose humans achieve almost perfect performance error cat detection task thus optimal error rate suppose error training set error training dev set error dev set tell know high variance variance reduction techniques described earlier allow make progress suppose algorithm achieves error training set error training dev set error dev set tells high avoidable bias training set algorithm poorly training set bias reduction techniques help two examples algorithm suffered high avoidable bias high variance possible algorithm suffer subset high avoidable bias high variance data mismatch example error training set error training dev set error dev set algorithm suffers high avoidable bias data mismatch however suffer high variance training set distribution might easier understand different types errors relate drawing entries table page machine learning yearning draft andrew continuing example ube cat image detector see two different distributions data axis axis ubve three types error human level error error examples algorithm trained error examples algorithm trained fill boxes different types errors identified previous chapter wish also fill remaining two boxes table fill upper right box human level performance mobile images asking humans label mobile cat images data measure error fill next box taking mobile cat images distribution putting small fraction training set neural network learns measure learned model error subset data filling two additional entries may sometimes give additional insight algorithm two different distributions distribution data understanding types error algorithm suffers better positioned decide whether focus reducing bias reducing variance reducing data mismatch page machine learning yearning draft andrew 
[addressing, data, mismatch] suppose developed speech recognition system well training set training dev set however poorly dev set data mismatch problem recommend try understand properties data differ training dev set distributions try find training data better matches dev set examples algorithm trouble example suppose carry error analysis speech recognition dev set manually examples try understand algorithm making mistakes find system poorly audio clips dev set taken within car whereas training examples recorded quiet background engine road noise dramatically worsen performance speech system case might try acquire training data comprising audio clips taken car purpose error analysis understand significant differences training dev set leads data mismatch training training dev sets include audio recorded within car also double check system performance subset data well car data training set car data training dev set validates hypothesis getting car data would help discussed possibility including training set data drawn distribution dev test set previous chapter allows compare performance car data training set dev test set unfortunately guarantees process example way get training data better match dev set data might clear path towards improving performance also research domain adaptation train algorithm one distribution generalize different distribution methods typically applicable special types problems much less widely used ideas described chapter page machine learning yearning draft andrew 
[artificial, data, synthesis] speech system needs data sounds taken within car rather collecting lot data driving around might easier way get data artificially synthesizing suppose obtain large quantity car road noise audio clips download data several websites suppose also large training set people speaking quiet room take audio clip person speaking add audio clip car road noise obtain audio clip sounds person speaking noisy car using process synthesize huge amounts data sound collected inside car generally several circumstances artificial data synthesis allows create huge dataset reasonably matches dev set let use cat image detector second example notice dev set images much motion blur tend come cellphone users moving phone slightly taking picture take non blurry images training set internet images add simulated motion blur thus making similar dev set keep mind artificial data synthesis challenges sometimes easier create synthetic data appears realistic person create data appears realistic computer example suppose hours speech training data hour car noise repeatedly use hour car noise different portions original hours training data end synthetic dataset car noise repeated person listening audio probably would able tell car noise sounds possible learning algorithm would overfit hour car noise thus could generalize poorly new audio clip car noise happens sound different alternatively suppose unique hours car noise taken different cars case possible algorithm overfit cars perform poorly tested audio different car unfortunately problems hard spot page machine learning yearning draft andrew take one example suppose building computer vision system recognize cars suppose partner video gaming company computer graphics models several cars train algorithm use models generate synthetic images cars even synthesized images look realistic approach independently proposed many people probably work well video game might car designs entire video game expensive build car model car playing game probably notice seeing cars perhaps painted differently data looks realistic compared set cars roads therefore likely see dev test sets set synthesized cars captures minuscule fraction world distribution cars thus training examples come cars system overfit specific car designs fail generalize well dev test sets include car designs synthesizing data put thought whether really synthesizing representative set examples try avoid giving synthesized data properties makes possible learning algorithm distinguish synthesized non synthesized examples synthesized data comes one car designs synthesized audio comes hour car noise advice hard follow working data synthesis teams sometimes taken weeks produced data details close enough actual distribution synthesized data significant effect able get details right suddenly access far larger training set page machine learning yearning draft andrew debugging inference algorithms page machine learning yearning draft andrew 
[optimization, verification, test] suppose building speech recognition system system works inputting audio clip computing score possible output sentence example might try estimate score probability correct output transcription sentence given input audio given way compute score still find english sentence maximizes compute arg max english language words possible sentences length far many exhaustively enumerate need apply approximate search algorithm try find value optimizes maximizes score one example search algorithm beam search keeps top candidates search process purposes chapter need understand details beam search algorithms like guaranteed find value ubthat maximizes score suppose audio clip records someone saying love machine learning instead outputting correct transcription system outputs incorrect love robots two possibilities went wrong search algorithm problem approximate search algorithm beam search failed find value maximizes score objective scoring function problem estimates score inaccurate particular choice score failed recognize love machine learning correct transcription depending cause failure prioritize efforts differently problem work improving search algorithm problem work learning algorithm estimates score facing situation researchers randomly decide work search algorithm others randomly work better way learn values score unless know underlying cause error efforts could wasted decide systematically work page machine learning yearning draft andrew let output transcription love robots let correct transcription love machine learning order understand whether problem perform uboptimization verification test first compute score score check whether score score two possibilities case score score case learning algorithm correctly given higher score nevertheless approximate search algorithm chose rather tells approximate search algorithm failing choose value maximizes score case optimization verification test tells search algorithm problem focus example could try increasing beam width beam search case score score case know way computing score fault failing give strictly higher score correct output incorrect optimization verification test tells objective scoring function problem thus focus improving learn approximate score different sentences discussion focused single example apply optimization verification test practice examine errors dev set error would test whether score score dev example inequality holds get marked error caused optimization algorithm example hold score score gets counted mistake due way computing score example suppose find errors due scoring function score due optimization algorithm know matter much improve optimization procedure would realistically eliminate errors thus instead focus improving estimate score page machine learning yearning draft andrew 
[general, form, optimization, verification, test] apply optimization verification test given input know compute score indicates good response input furthermore using approximate algorithm try find arg max score suspect search algorithm sometimes failing find maximum previous speech recognition example audio clip output transcript suppose correct output algorithm instead outputs key test measure whether score score inequality holds blame optimization algorithm mistake refer previous chapter make sure understand logic behind otherwise blame computation score let look one example suppose building chinese english machine translation system system works inputting chinese sentence computing score possible translation example might use score probability translation given input sentence algorithm translates sentences trying compute however set possible english sentences ubis large rely heuristic search algorithm suppose algorithm outputs incorrect translation rather correct translation optimization verification test would ask compute whether score score inequality holds score correctly recognized superior output thus would attribute error approximate search algorithm otherwise attribute error computation score common design pattern first learn approximate scoring function score use approximate maximization algorithm able spot pattern able use optimization verification test understand source errors page machine learning yearning draft andrew 
[reinforcement, learning, example] suppose using machine learning teach helicopter fly complex maneuvers time lapse photo computer controller helicopter executing landing engine turned called autorotation maneuver allows helicopters land even engine unexpectedly fails human pilots practice maneuver part training goal use learning algorithm fly helicopter trajectory ubthat ends safe landing apply reinforcement learning develop reward function gives score measuring good possible trajectory example ubresults helicopter crashing perhaps reward huge negative reward trajectory resulting safe landing might result positive ubwith exact value depending smooth landing reward function typically chosen hand quantify desirable different trajectories trade bumpy landing whether helicopter landed exactly desired spot rough ride passengers easy design good reward functions page machine learning yearning draft andrew given reward function ubthe job reinforcement learning algorithm control helicopter achieves max ubhowever reinforcement learning algorithms make many approximations may succeed achieving maximization suppose picked reward run learning algorithm however performance appears far worse human pilot landings bumpier seem less safe human pilot achieves tell fault reinforcement learning algorithm trying carry trajectory achieves max fault reward function trying measure well specify ideal tradeoff ride bumpiness accuracy landing spot apply optimization verification test let human trajectory achieved human pilot let trajectory achieved algorithm according description human superior trajectory thus key test following hold true human case inequality holds reward function correctly rating human superior reinforcement learning algorithm finding inferior suggests working improving reinforcement learning algorithm worthwhile case inequality hold human means assigns worse score human even though superior trajectory work improving better capture tradeoffs correspond good landing many machine learning applications pattern optimizing approximate scoring function score using approximate search algorithm sometimes specified input reduces score example scoring function reward function score optimization algorithm reinforcement learning algorithm trying execute good trajectory one difference earlier examples rather comparing optimal output instead comparing human level performance human assumed human pretty good even optimal general long example human superior output performance current learning algorithm even optimal output optimization verification test indicate whether promising improve optimization algorithm scoring function page machine learning yearning draft andrew end end deep learning page machine learning yearning draft andrew 
[rise, end-to-end, learning] suppose want build system examine online product reviews automatically tell writer liked disliked product example hope recognize following review highly positive great mop following highly negative mop low quality regret buying problem recognizing positive negative opinions called sentiment classification build system might build pipeline two components parser system annotates text information identifying important words example might use parser label adjectives nouns would therefore get following annotated text great adjective mop noun sentiment classifier learning algorithm takes input annotated text predicts overall sentiment parser annotation could help learning algorithm greatly giving adjectives higher weight algorithm able quickly hone important words great ignore less important words visualize pipeline two components follows recent trend toward replacing pipeline systems single learning algorithm ubend end learning algorithm task would simply take input raw original text great mop try directly recognize sentiment parser gives much richer annotation text simplified description suffice explaining end end deep learning page machine learning yearning draft andrew neural networks commonly used end end learning systems term end end refers fact asking learning algorithm directly input desired output learning algorithm directly connects input end system output end problems data abundant end end systems remarkably successful always good choice next chapters give examples end end systems well give advice use page machine learning yearning draft andrew 
[end-to-end, learning, examples] suppose want build speech recognition system might build system three components components work follows compute features extract hand designed features mfcc ubmel frequency cepstrum coefficients features ubwhich try capture content utterance disregarding less relevant properties speaker pitch phoneme recognizer linguists believe basic units sound called phonemes example initial sound keep phoneme sound cake system tries recognize phonemes audio clip final recognizer take sequence recognized phonemes try string together output transcript contrast end end system might input audio clip try directly output transcript far described machine learning pipelines completely linear output sequentially passed one staged next pipelines complex example simple architecture autonomous car page machine learning yearning draft andrew three components one detects cars using camera images one detects pedestrians final component plans path car avoids cars pedestrians every component pipeline learned example literature robot motion planning numerous algorithms final path planning step car many algorithms involve learning contrast end end approach might try take sensor inputs directly output steering direction even though end end learning seen many successes always best approach example end end speech recognition works well skeptical end end learning autonomous driving next chapters explain page machine learning yearning draft andrew 
[pros, cons, end-to-end, learning] consider speech pipeline earlier example many parts pipeline hand engineered mfccs set hand designed audio features although provide reasonable summary audio input also simplify input signal throwing information away phonemes invention linguists imperfect representation speech sounds extent phonemes poor approximation reality forcing algorithm use phoneme representation limit speech system performance hand engineered components limit potential performance speech system however allowing hand engineered components also advantages mfcc features robust properties speech affect content speaker pitch thus help simplify problem learning algorithm extent phonemes reasonable representation speech also help learning algorithm understand basic sound components therefore improve performance hand engineered components generally allows speech system learn less data hand engineered knowledge captured mfccs phonemes supplements knowledge algorithm acquires data much data knowledge useful consider end end system page machine learning yearning draft andrew system lacks hand engineered knowledge thus training set small might worse hand engineered pipeline however training set large hampered limitations mfcc phoneme based representation learning algorithm large enough neural network trained enough training data potential well perhaps even approach optimal error rate end end learning systems tend well lot labeled data ends input end output end example require large dataset audio transcript pairs type data available approach end end learning great caution working machine learning problem training set small algorithm knowledge come human insight hand engineering components choose use end end system decide steps pipeline plug together next chapters give suggestions designing pipelines page machine learning yearning draft andrew 
[choosing, pipeline, components, data, availability] building non end end pipeline system good candidates components pipeline design pipeline greatly impact overall system performance one important factor whether easily collect data train components example consider autonomous driving architecture use machine learning detect cars pedestrians hard obtain data numerous computer vision datasets large numbers labeled cars pedestrians also use crowdsourcing amazon mechanical turk obtain even larger datasets thus relatively easy obtain training data build car detector pedestrian detector contrast consider pure end end approach train system would need large dataset image steering direction pairs time consuming expensive people drive cars around record steering direction collect data need fleet specially instrumented cars huge amount driving cover wide range possible scenarios makes end end system difficult train much easier obtain large dataset labeled car pedestrian images generally lot data available training intermediate modules pipeline car detector pedestrian detector might consider using page machine learning yearning draft andrew pipeline multiple stages structure could superior could use available data train intermediate modules end end data becomes available believe non end end approach significantly promising autonomous driving architecture better matches availability data page machine learning yearning draft andrew 
[choosing, pipeline, components, task, simplicity] data availability also consider second factor picking components pipeline simple tasks solved individual components try choose pipeline components individually easy build learn mean component easy learn consider machine learning tasks listed order increasing difficulty classifying whether image overexposed like example classifying whether image taken indoor outdoor classifying whether image contains cat classifying whether image contains cat black white fur classifying whether image contains siamese cat particular breed cat binary image classification task input image output either tasks earlier list seem much easier neural network learn able learn easier tasks fewer training examples machine learning yet good formal definition makes task easy hard rise deep learning multi layered neural networks sometimes say task easy carried fewer computation steps corresponding shallow neural network hard requires computation steps requiring deeper neural network informal definitions information theory concept kolmogorov complexity says complexity learned function length shortest computer program produce function however theoretical concept found practical applications see also https wikipedia org wiki kolmogorov_complexity page machine learning yearning draft andrew able take complex task break simpler sub tasks coding steps sub tasks explicitly giving algorithm prior knowledge help learn task efficiently suppose building siamese cat detector pure end end architecture contrast alternatively use pipeline two steps first step cat detector detects cats image page machine learning yearning draft andrew second step passes cropped images detected cats one time cat species classifier finally outputs cats detected siamese cat compared training purely end end classifier using labels two components pipeline cat detector cat breed classifier seem much easier learn require significantly less data familiar practical object detection algorithms recognize learn image labels instead trained bounding boxes provided part training data discussion beyond scope chapter see deep learning specialization coursera would like learn algorithms page machine learning yearning draft andrew one final example let revisit autonomous driving pipeline using pipeline telling algorithm key steps driving detect cars detect pedestrians plan path car relatively simpler function thus learned less data purely end end approach summary deciding components pipeline try build pipeline component relatively simple function therefore learned modest amount data 
[directly, learning, rich, outputs] image classification algorithm input image output integer indicating object category algorithm instead output entire sentence describing image example yellow bus driving road green trees green grass background traditional applications supervised learning learned function output usually integer real number example problem spam classification email spam spam image recognition image integer label housing price prediction features house price dollars product recommendation product amp user features chance purchase one exciting developments end end deep learning letting directly learn much complex number image captioning example neural network input image directly output caption page machine learning yearning draft andrew examples problem example citation image captioning image text mao machine translation english text french text suskever question answering text question pair answer text bordes speech recognition audio transcription hannun tts text features audio van der oord accelerating trend deep learning right input output labeled pairs sometimes learn end end even output sentence image audio outputs richer single number page machine learning yearning draft andrew error analysis parts page machine learning yearning draft andrew 
[error, analysis, parts] suppose system built using complex machine learning pipeline would like improve system performance part pipeline work improving attributing errors specific parts pipeline decide prioritize work let use siamese cat classifier example first part cat detector detects cats crops image second part cat breed classifier decides siamese cat possible spend years working improving either two pipeline components decide component focus carrying uberror analysis parts try attribute mistake algorithm makes one sometimes two parts pipeline example algorithm misclassifies image containing siamese cat even though correct label let manually examine two steps algorithm suppose siamese cat detector detected cat follows page machine learning yearning draft andrew means cat breed classifier given following image cat breed classifier correctly classifies image containing siamese cat thus cat breed classifier blameless given pile rocks outputted reasonable label indeed human classifying cropped image would also predicted thus clearly attribute error cat detector hand cat detector outputted following bounding box would conclude cat detector done job cat breed classifier fault say misclassified dev set images find errors attributable cat detector errors attributable cat breed classifier safely conclude focus attention improving cat detector page machine learning yearning draft andrew also conveniently found examples cat detector outputted incorrect bounding boxes use examples carry deeper level error analysis cat detector see improve description attribute error one part pipeline informal far look output parts see decide one made mistake informal method could need next chapter also see formal way attributing error page machine learning yearning draft andrew 
[attributing, error, one, part] let continue use example suppose cat detector outputted bounding box cat breed classifier thus given cropped image whereupon incorrectly outputs cat picture cat detector job poorly however highly skilled human could arguably still recognize siamese cat poorly cropped image attribute error cat detector cat breed classifier ambiguous number ambiguous cases like small make whatever decision want get similar result formal test lets definitively attribute error exactly one part replace cat detector output hand labeled bounding box page machine learning yearning draft andrew run corresponding cropped image cat breed classifier cat breed classifier still misclassifies attribute error cat breed classifier otherwise attribute error cat detector words run experiment give cat breed classifier perfect input two cases case even given perfect bounding box cat breed classifier still incorrectly outputs case clearly cat breed classifier fault case given perfect bounding box breed classifier correctly outputs shows cat detector given perfect bounding box overall system output would correct thus attribute error cat detector carrying analysis misclassified dev set images unambiguously attribute error one component allows estimate fraction errors due component pipeline therefore decide focus attention page machine learning yearning draft andrew 
[general, case, error, attribution] general steps error attribution suppose pipeline three steps feeds directly feeds directly mistake system makes dev set try manually modifying output perfect output perfect bounding box cat run rest pipeline output algorithm gives correct output shows given better output overall algorithm output would correct thus attribute error component otherwise step try manually modifying output perfect output algorithm gives correct output attribute error component otherwise step attribute error component let look complex example self driving car uses pipeline use error analysis parts decide component focus map three components follows detect cars detect pedestrians plan path car page machine learning yearning draft andrew following procedure described suppose test car closed track find case car chooses jarring steering direction skilled driver would self driving world case usually called ubscenario would try manually modifying detecting cars output perfect output manually tell cars run rest pipeline allow plan path use perfect output algorithm plans much better path car shows given better output overall algorithm output would better thus attribute error component otherwise step try manually modifying detect pedestrian output perfect output algorithm gives correct output attribute error component otherwise step attribute error component components pipeline ordered according directed acyclic graph dag meaning able compute fixed left right order later components depend earlier components outputs long mapping components order follows dag ordering error analysis fine might get slightly different results swap detect pedestrians previously detect cars detect cars previously detect pedestrians plan path car results analysis would still valid give good guidance focus attention 
[error, analysis, parts, comparison, human-level, performance] carrying error analysis learning algorithm like using data science analyze system mistakes order derive insights next basic error analysis parts tells component performance worth greatest effort improve say dataset customers buying things website data scientist may many different ways analyzing data may draw many different conclusions whether website raise prices lifetime value customers acquired different marketing campaigns one right way analyze dataset many possible useful insights one could draw similarly one right way carry error analysis chapters learned many common design patterns drawing useful insights system feel free experiment ways analyzing errors well let return self driving application car detection algorithm outputs location perhaps velocity nearby cars pedestrian detection algorithm outputs location nearby pedestrians two outputs finally used plan path car debug pipeline rather rigorously following procedure saw previous chapter could informally ask far detect cars component human level performance detecting cars far detect pedestrians component human level performance page machine learning yearning draft andrew far overall system performance human level performance human level performance assumes human plan path car given outputs previous two pipeline components rather access camera images words plan path component performance compare human human given input find one components far human level performance good case focus improving performance component many error analysis processes work best trying automate something humans thus benchmark human level performance preceding examples implicit assumption building system final output intermediate components things even humans cannot well procedures apply another advantage working problems humans solve powerful error analysis tools thus prioritize team work efficiently 
[spotting, flawed, pipeline] individual component pipeline performing human level performance near human level performance overall pipeline falls far short human level usually means pipeline flawed needs redesigned error analysis also help understand need redesign pipeline previous chapter posed question whether three components performance human level suppose answer three questions yes detect cars component roughly human level performance detecting cars camera images detect pedestrians component roughly human level performance detecting cars camera images compared human plan path car given outputs previous two pipeline components rather access camera images plan path component performance similar level however overall self driving car performing significantly human level performance humans given access camera images plan significantly better paths car conclusion draw possible conclusion pipeline flawed case plan path component well given inputs inputs contain enough information ask information outputs two earlier pipeline components needed plan paths well car drive words information skilled human driver need page machine learning yearning draft andrew example suppose realize human driver also needs know location lane markings suggests redesign pipeline follows ultimately think pipeline whole achieve human level performance even every individual component human level performance remember comparing human given input component pipeline flawed redesigned self driving example theory one could solve problem also feeding raw camera image planning component however would violate design principle task simplicity described chapter path planning module needs input raw image complex task solve adding detect lane markings component better choice helps get important previously missing information lane markings path planning module avoid making particular module overly complex build train page machine learning yearning draft andrew conclusion page machine learning yearning draft andrew 
[building, superhero, team, get, teammates, read] congratulations finishing book chapter talked book help become superhero team thing better superhero part superhero team hope give copies book friends teammates help create superheroes page machine learning yearning draft andrew 
